<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-18T00:00:00Z">2024-10-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are AI Detectors Good Enough? A Survey on Quality of Datasets With
  Machine-Generated Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        German Gritsai, Anastasia Voznyuk, Andrey Grabovoy, Yury Chekhovich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of autoregressive Large Language Models (LLMs) has
significantly improved the quality of generated texts, necessitating reliable
machine-generated text detectors. A huge number of detectors and collections
with AI fragments have emerged, and several detection methods even showed
recognition quality up to 99.9% according to the target metrics in such
collections. However, the quality of such detectors tends to drop dramatically
in the wild, posing a question: Are detectors actually highly trustworthy or do
their high benchmark scores come from the poor quality of evaluation datasets?
In this paper, we emphasise the need for robust and qualitative methods for
evaluating generated data to be secure against bias and low generalising
ability of future model. We present a systematic review of datasets from
competitions dedicated to AI-generated content detection and propose methods
for evaluating the quality of datasets containing AI-generated fragments. In
addition, we discuss the possibility of using high-quality generated data to
achieve two goals: improving the training of detection models and improving the
training datasets themselves. Our contribution aims to facilitate a better
understanding of the dynamics between human and machine text, which will
ultimately support the integrity of information in an increasingly automated
world.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SudoLM: Learning Access Control of Parametric Knowledge with
  Authorization Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qin Liu, Fei Wang, Chaowei Xiao, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing preference alignment is a one-size-fits-all alignment mechanism,
where the part of the large language model (LLM) parametric knowledge with
non-preferred features is uniformly blocked to all the users. However, this
part of knowledge can be useful to advanced users whose expertise qualifies
them to handle these information. The one-size-fits-all alignment mechanism
undermines LLM's utility for these qualified users. To address this problem, we
propose SudoLM, a framework that lets LLMs learn access control over specific
parametric knowledge for users with different credentials via authorization
alignment. SudoLM allows authorized users to unlock their access to all the
parametric knowledge with an assigned SUDO key while blocking access to
non-qualified users. Experiments on two application scenarios demonstrate that
SudoLM effectively controls the user's access to the parametric knowledge and
maintains its general utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Large Language Models' Situated Faithfulness to External
  Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukun Huang, Sanxing Chen, Hongyi Cai, Bhuwan Dhingra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are often augmented with external information as
contexts, but this external information can sometimes be inaccurate or even
intentionally misleading. We argue that robust LLMs should demonstrate situated
faithfulness, dynamically calibrating their trust in external information based
on their confidence in the internal knowledge and the external context. To
benchmark this capability, we evaluate LLMs across several QA datasets,
including a newly created dataset called RedditQA featuring in-the-wild
incorrect contexts sourced from Reddit posts. We show that when provided with
both correct and incorrect contexts, both open-source and proprietary models
tend to overly rely on external information, regardless of its factual
accuracy. To enhance situated faithfulness, we propose two approaches:
Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning
(RCR). SCR enables models to self-access the confidence of external information
relative to their own internal knowledge to produce the most accurate answer.
RCR, in contrast, extracts explicit confidence signals from the LLM and
determines the final answer using predefined rules. Our results show that for
LLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR
outperforms RCR, achieving improvements of up to 24.2% over a direct input
augmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR
outperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct
Preference Optimization (CR-DPO) method improves performance on both seen and
unseen datasets, yielding an average improvement of 8.9% on Llama-3-8B. In
addition to quantitative results, we offer insights into the relative strengths
of SCR and RCR. Our findings highlight promising avenues for improving situated
faithfulness in LLMs. The data and code are released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NaturalBench: Evaluating <span class="highlight-title">Vision-Language</span> Models on Natural Adversarial
  Samples <span class="chip">NeurIPS 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, Deva Ramanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have made significant progress in recent
visual-question-answering (VQA) benchmarks that evaluate complex
visio-linguistic reasoning. However, are these models truly effective? In this
work, we show that VLMs still struggle with natural images and questions that
humans can easily answer, which we term natural adversarial samples. We also
find it surprisingly easy to generate these VQA samples from natural image-text
corpora using off-the-shelf models like CLIP and ChatGPT. We propose a
semi-automated approach to collect a new benchmark, NaturalBench, for reliably
evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a
$\textbf{vision-centric}$ design by pairing each question with two images that
yield different answers, preventing blind solutions from answering without
using the images. This makes NaturalBench more challenging than previous
benchmarks that can be solved with commonsense priors. We evaluate 53
state-of-the-art VLMs on NaturalBench, showing that models like
LLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o
lag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is
hard from two angles: (1) Compositionality: Solving NaturalBench requires
diverse visio-linguistic skills, including understanding attribute bindings,
object relationships, and advanced reasoning like logic and counting. To this
end, unlike prior work that uses a single tag per sample, we tag each
NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2)
Biases: NaturalBench exposes severe biases in VLMs, as models often choose the
same answer regardless of the image. Lastly, we apply our benchmark curation
method to diverse data sources, including long captions (over 100 words) and
non-English languages like Chinese and Hindi, highlighting its potential for
dynamic evaluations of VLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 24; We open-source our dataset at:
  https://huggingface.co/datasets/BaiqiL/NaturalBench; Project page at:
  https://linzhiqiu.github.io/papers/naturalbench/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MiCEval: Unveiling <span class="highlight-title">Multimodal</span> Chain of Thought's Quality via Image
  Description and Reasoning Steps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiongtao Zhou, Jie He, Lanyu Chen, jingyu li, Haojing Chen, Victor Gutierrez Basulto, Jeff Z. Pan, Hanjie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Chain of Thought (MCoT) is a popular prompting strategy for
improving the performance of multimodal large language models (MLLMs) across a
range of complex reasoning tasks. Despite its popularity, there is a notable
absence of automated methods for evaluating the quality of reasoning steps in
MCoT. To address this gap, we propose Multimodal Chain-of-Thought Evaluation
(MiCEval), a framework designed to assess the correctness of reasoning chains
by evaluating the quality of both the description and each reasoning step. The
evaluation of the description component focuses on the accuracy of the image
descriptions, while the reasoning step evaluates the quality of each step as it
is conditionally generated based on the preceding steps. MiCEval is built upon
a fine-grained dataset with annotations that rate each step according to
correctness, relevance, and informativeness. Extensive experiments on four
state-of-the-art MLLMs show that step-wise evaluations using MiCEval align more
closely with human judgments compared to existing methods based on cosine
similarity or fine-tuning approaches. MiCEval datasets and code can be found in
https://github.com/alenai97/MiCEval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie
  Character-Aware Discourse Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maitreya Prafulla Chitale, Uday Bindal, Rajakrishnan Rajkumar, Rahul Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Summarizing movie screenplays presents a unique set of challenges compared to
standard document summarization. Screenplays are not only lengthy, but also
feature a complex interplay of characters, dialogues, and scenes, with numerous
direct and subtle relationships and contextual nuances that are difficult for
machine learning models to accurately capture and comprehend. Recent attempts
at screenplay summarization focus on fine-tuning transformer-based pre-trained
models, but these models often fall short in capturing long-term dependencies
and latent relationships, and frequently encounter the "lost in the middle"
issue. To address these challenges, we introduce DiscoGraMS, a novel resource
that represents movie scripts as a movie character-aware discourse graph (CaD
Graph). This approach is well-suited for various downstream tasks, such as
summarization, question-answering, and salience detection. The model aims to
preserve all salient information, offering a more comprehensive and faithful
representation of the screenplay's content. We further explore a baseline
method that combines the CaD Graph with the corresponding movie script through
a late fusion of graph and text modalities, and we present very initial
promising results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Fake News from Adversarial Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanxing Chen, Yukun Huang, Bhuwan Dhingra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that existing evaluations for fake news detection based on
conventional sources, such as claims on fact-checking websites, result in an
increasing accuracy over time for LLM-based detectors -- even after their
knowledge cutoffs. This suggests that recent popular political claims, which
form the majority of fake news on such sources, are easily classified using
surface-level shallow patterns. Instead, we argue that a proper fake news
detection dataset should test a model's ability to reason factually about the
current world by retrieving and reading related evidence. To this end, we
develop a novel pipeline that leverages natural language feedback from a
RAG-based detector to iteratively modify real-time news into deceptive fake
news that challenges LLMs. Our iterative rewrite decreases the binary
classification AUC by an absolute 17.5 percent for a strong RAG GPT-4o
detector. Our experiments reveal the important role of RAG in both detecting
and generating fake news, as retrieval-free LLM detectors are vulnerable to
unseen events and adversarial attacks, while feedback from RAG detection helps
discover more deceitful patterns in fake news.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distance between Relevant Information Pieces Causes Bias in Long-Context
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runchu Tian, Yanghao Li, Yuepeng Fu, Siyang Deng, Qinyu Luo, Cheng Qian, Shuo Wang, Xin Cong, Zhong Zhang, Yesai Wu, Yankai Lin, Huadong Wang, Xiaojiang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Positional bias in large language models (LLMs) hinders their ability to
effectively process long inputs. A prominent example is the "lost in the
middle" phenomenon, where LLMs struggle to utilize relevant information
situated in the middle of the input. While prior research primarily focuses on
single pieces of relevant information, real-world applications often involve
multiple relevant information pieces. To bridge this gap, we present
LongPiBench, a benchmark designed to assess positional bias involving multiple
pieces of relevant information. Thorough experiments are conducted with five
commercial and six open-source models. These experiments reveal that while most
current models are robust against the "lost in the middle" issue, there exist
significant biases related to the spacing of relevant information pieces. These
findings highlight the importance of evaluating and reducing positional biases
to advance LLM's capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenEOL: Harnessing the Generative Power of LLMs for Training-Free
  Sentence Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghuveer Thirukovalluru, Bhuwan Dhingra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training-free embedding methods directly leverage pretrained large language
models (LLMs) to embed text, bypassing the costly and complex procedure of
contrastive learning. Previous training-free embedding methods have mainly
focused on optimizing embedding prompts and have overlooked the benefits of
utilizing the generative abilities of LLMs. We propose a novel method, GenEOL,
which uses LLMs to generate diverse transformations of a sentence that preserve
its meaning, and aggregates the resulting embeddings of these transformations
to enhance the overall sentence embedding. GenEOL significantly outperforms the
existing training-free embedding methods by an average of 2.85 points across
several LLMs on the sentence semantic text similarity (STS) benchmark. Our
analysis shows that GenEOL stabilizes representation quality across LLM layers
and is robust to perturbations of embedding prompts. GenEOL also achieves
notable gains on multiple clustering, reranking and pair-classification tasks
from the MTEB benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diverging Preferences: When do Annotators Disagree and do Models Know? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael JQ Zhang, Zhilin Wang, Jena D. Hwang, Yi Dong, Olivier Delalleau, Yejin Choi, Eunsol Choi, Xiang Ren, Valentina Pyatkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine diverging preferences in human-labeled preference datasets. We
develop a taxonomy of disagreement sources spanning 10 categories across four
high-level classes -- task underspecification, response style, refusals, and
annotation errors. We find that the majority of disagreements are in opposition
with standard reward modeling approaches, which are designed with the
assumption that annotator disagreement is noise. We then explore how these
findings impact two areas of LLM development: reward modeling and evaluation.
In our experiments, we demonstrate how standard reward modeling methods, like
the Bradley-Terry model, fail to differentiate whether a given preference
judgment is the result of unanimous agreement among annotators or the majority
opinion among diverging user preferences. We also find that these tendencies
are also echoed by popular LLM-as-Judge evaluation methods, which consistently
identify a winning response in cases of diverging preferences. These findings
highlight remaining challenges in LLM evaluations, which are greatly influenced
by divisive features like response style, and in developing pluralistically
aligned LLMs. To address these issues, we develop methods for identifying
diverging preferences to mitigate their influence on evaluation and training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CELI: Controller-Embedded Language Model Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan-Samuel Wagner, Dave DeCaprio, Abishek Chiffon Muthu Raja, Jonathan M. Holman, Lauren K. Brady, Sky C. Cheung, Hosein Barzekar, Eric Yang, Mark Anthony Martinez II, David Soong, Sriram Sridhar, Han Si, Brandon W. Higgs, Hisham Hamadeh, Scott Ogden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Controller-Embedded Language Model Interactions (CELI), a
framework that integrates control logic directly within language model (LM)
prompts, facilitating complex, multi-stage task execution. CELI addresses
limitations of existing prompt engineering and workflow optimization techniques
by embedding control logic directly within the operational context of language
models, enabling dynamic adaptation to evolving task requirements. Our
framework transfers control from the traditional programming execution
environment to the LMs, allowing them to autonomously manage computational
workflows while maintaining seamless interaction with external systems and
functions. CELI supports arbitrary function calls with variable arguments,
bridging the gap between LMs' adaptive reasoning capabilities and conventional
software paradigms' structured control mechanisms. To evaluate CELI's
versatility and effectiveness, we conducted case studies in two distinct
domains: code generation (HumanEval benchmark) and multi-stage content
generation (Wikipedia-style articles). The results demonstrate notable
performance improvements across a range of domains. CELI achieved a 4.9
percentage point improvement over the best reported score of the baseline GPT-4
model on the HumanEval code generation benchmark. In multi-stage content
generation, 94.4% of CELI-produced Wikipedia-style articles met or exceeded
first draft quality when optimally configured, with 44.4% achieving high
quality. These outcomes underscore CELI's potential for optimizing AI-driven
workflows across diverse computational domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ You Shall Know a Tool by the Traces it Leaves: The Predictability of
  Sentiment Analysis Tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Baumartz, Mevlüt Bagci, Alexander Henlein, Maxim Konca, Andy Lücking, Alexander Mehler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  If sentiment analysis tools were valid classifiers, one would expect them to
provide comparable results for sentiment classification on different kinds of
corpora and for different languages. In line with results of previous studies
we show that sentiment analysis tools disagree on the same dataset. Going
beyond previous studies we show that the sentiment tool used for sentiment
annotation can even be predicted from its outcome, revealing an algorithmic
bias of sentiment analysis. Based on Twitter, Wikipedia and different news
corpora from the English, German and French languages, our classifiers separate
sentiment tools with an averaged F1-score of 0.89 (for the English corpora). We
therefore warn against taking sentiment annotations as face value and argue for
the need of more and systematic NLP evaluation studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiSCo Meets LLMs: A Unified Approach for Sparse Retrieval and Contextual
  Distillation in Conversational Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Lupart, Mohammad Aliannejadi, Evangelos Kanoulas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Search (CS) is the task of retrieving relevant documents from
a corpus within a conversational context, combining retrieval with
conversational context modeling. With the explosion of Large Language Models
(LLMs), the CS field has seen major improvements with LLMs rewriting user
queries, accounting for conversational context. However, engaging LLMs at
inference time harms efficiency. Current methods address this by distilling
embeddings from human-rewritten queries to learn the context modeling task.
Yet, these approaches predominantly focus on context modeling, and only treat
the contrastive component of the retrieval task within a
distillation-independent loss term. To address these limitations, we propose a
new distillation method, as a relaxation of the previous objective, unifying
retrieval and context modeling. We relax the existing training objectives by
distilling similarity scores between conversations and documents, rather than
relying solely on representation learning. Our proposed distillation objective
allows for more freedom in the representation space and leverages the
contrastive nature of document relevance. Through experiments on Learned Sparse
Retrieval (LSR) across 5 CS datasets, our approach demonstrates substantial
improvements in both in-domain and out-of-domain retrieval performance,
outperforming state-of-the-art with gains of up to 6 points in recall for
out-of-domain datasets. Additionally, through the relaxation of the objective,
we propose a multi-teacher distillation, using multiple LLMs as teachers,
yielding additional gains, and outperforming the teachers themselves in
in-domain experiments. Finally, analysis of the sparsity of the models reveals
that our distillation allows for better control over the sparsity of the
trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teaching Models to Balance Resisting and Accepting Persuasion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elias Stengel-Eskin, Peter Hase, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are susceptible to persuasion, which can pose
risks when models are faced with an adversarial interlocutor. We take a first
step towards defending models against persuasion while also arguing that
defense against adversarial (i.e. negative) persuasion is only half of the
equation: models should also be able to accept beneficial (i.e. positive)
persuasion to improve their answers. We show that optimizing models for only
one side results in poor performance on the other. In order to balance positive
and negative persuasion, we introduce Persuasion-Balanced Training (or PBT),
which leverages multi-agent recursive dialogue trees to create data and trains
models via preference optimization to accept persuasion when appropriate. PBT
consistently improves resistance to misinformation and resilience to being
challenged while also resulting in the best overall performance on holistic
data containing both positive and negative persuasion. Crucially, we show that
PBT models are better teammates in multi-agent debates. We find that without
PBT, pairs of stronger and weaker models have unstable performance, with the
order in which the models present their answers determining whether the team
obtains the stronger or weaker model's performance. PBT leads to better and
more stable results and less order dependence, with the stronger model
consistently pulling the weaker one up.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/esteng/persuasion_balanced_training</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and
  Tool Knowledge Bases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elias Lumer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks
like secure database interactions and multi-agent code development. However,
scaling tool capacity beyond agent reasoning or model limits remains a
challenge. In this paper, we address these challenges by introducing Toolshed
Knowledge Bases, a tool knowledge base (vector database) designed to store
enhanced tool representations and optimize tool selection for large-scale
tool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a
novel ensemble of tool-applied advanced retrieval-augmented generation (RAG)
techniques across the pre-retrieval, intra-retrieval, and post-retrieval
phases, without requiring model fine-tuning. During pre-retrieval, tool
documents are enhanced with key information and stored in the Toolshed
Knowledge Base. Intra-retrieval focuses on query planning and transformation to
increase retrieval accuracy. Post-retrieval refines the retrieved tool
documents and enables self-reflection. Furthermore, by varying both the total
number of tools (tool-M) an Agent has access to and the tool selection
threshold (top-k), we address trade-offs between retrieval accuracy, agent
performance, and token cost. Our approach achieves 46%, 56%, and 47% absolute
improvements on the ToolE single-tool, ToolE multi-tool and Seal-Tools
benchmark datasets, respectively (Recall@5).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dialetto, ma Quanto Dialetto? Transcribing and Evaluating Dialects on a
  Continuum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Soh-Eun Shim, Barbara Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is increasing interest in looking at dialects in NLP. However, most
work to date still treats dialects as discrete categories. For instance,
evaluative work in variation-oriented NLP for English often works with Indian
English or African-American Venacular English as homogeneous categories (Faisal
et al., 2024; Ziems et al., 2023), yet even within one variety there is
substantial variation. We examine within-dialect variation and show that
performance critically varies within categories. We measure speech-to-text
performance on Italian dialects, and empirically observe a geographical
performance disparity. This disparity correlates substantially (-0.5) with
linguistic similarity to the highest performing dialect variety. We
cross-examine our results against dialectometry methods, and interpret the
performance disparity to be due to a bias towards dialects that are more
similar to the standard variety in the speech-to-text model examined. We
additionally leverage geostatistical methods to predict zero-shot performance
at unseen sites, and find the incorporation of geographical information to
substantially improve prediction performance, indicating there to be
geographical structure in the performance distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLMs estimate uncertainty well in instruction-following? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyeon Heo, Miao Xiong, Christina Heinze-Deml, Jaya Narain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) could be valuable personal AI agents across
various domains, provided they can precisely follow user instructions. However,
recent studies have shown significant limitations in LLMs'
instruction-following capabilities, raising concerns about their reliability in
high-stakes applications. Accurately estimating LLMs' uncertainty in adhering
to instructions is critical to mitigating deployment risks. We present, to our
knowledge, the first systematic evaluation of the uncertainty estimation
abilities of LLMs in the context of instruction-following. Our study identifies
key challenges with existing instruction-following benchmarks, where multiple
factors are entangled with uncertainty stems from instruction-following,
complicating the isolation and comparison across methods and models. To address
these issues, we introduce a controlled evaluation setup with two benchmark
versions of data, enabling a comprehensive comparison of uncertainty estimation
methods under various conditions. Our findings show that existing uncertainty
methods struggle, particularly when models make subtle errors in instruction
following. While internal model states provide some improvement, they remain
inadequate in more complex scenarios. The insights from our controlled
evaluation setups provide a crucial understanding of LLMs' limitations and
potential for uncertainty estimation in instruction-following tasks, paving the
way for more trustworthy AI agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Attention with Mirror Descent: Generalized Max-Margin Token
  Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Alvarado Kristanto Julistiono, Davoud Ataee Tarzanagh, Navid Azizan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention mechanisms have revolutionized several domains of artificial
intelligence, such as natural language processing and computer vision, by
enabling models to selectively focus on relevant parts of the input data. While
recent work has characterized the optimization dynamics of gradient descent
(GD) in attention-based models and the structural properties of its preferred
solutions, less is known about more general optimization algorithms such as
mirror descent (MD). In this paper, we investigate the convergence properties
and implicit biases of a family of MD algorithms tailored for softmax attention
mechanisms, with the potential function chosen as the $p$-th power of the
$\ell_p$-norm. Specifically, we show that these algorithms converge in
direction to a generalized hard-margin SVM with an $\ell_p$-norm objective when
applied to a classification problem using a softmax attention model. Notably,
our theoretical results reveal that the convergence rate is comparable to that
of traditional GD in simpler models, despite the highly nonlinear and nonconvex
nature of the present problem. Additionally, we delve into the joint
optimization dynamics of the key-query matrix and the decoder, establishing
conditions under which this complex joint optimization converges to their
respective hard-margin SVM solutions. Lastly, our numerical experiments on real
data demonstrate that MD algorithms improve generalization over standard GD and
excel in optimal token selection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Are Overparameterized Text Encoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thennal D K, Tim Fischer, Chris Biemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate strong performance as text embedding
models when finetuned with supervised contrastive training. However, their
large size balloons inference time and memory requirements. In this paper, we
show that by pruning the last $p\%$ layers of an LLM before supervised training
for only 1000 steps, we can achieve a proportional reduction in memory and
inference time. We evaluate four different state-of-the-art LLMs on text
embedding tasks and find that our method can prune up to 30\% of layers with
negligible impact on performance and up to 80\% with only a modest drop. With
only three lines of code, our method is easily implemented in any pipeline for
transforming LLMs to text encoders. We also propose $\text{L}^3 \text{Prune}$,
a novel layer-pruning strategy based on the model's initial loss that provides
two optimal pruning configurations: a large variant with negligible performance
loss and a small variant for resource-constrained settings. On average, the
large variant prunes 21\% of the parameters with a $-0.3$ performance drop, and
the small variant only suffers from a $-5.1$ decrease while pruning 74\% of the
model. We consider these results strong evidence that LLMs are
overparameterized for text embedding tasks, and can be easily pruned.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages of content + 1 for limitations and ethical considerations, 14
  pages in total including references and appendix, 5+1 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachel S. Y. Teo, Tan M. Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled
scalability in deep learning. SMoE has the potential to exponentially increase
parameter count while maintaining the efficiency of the model by only
activating a small subset of these parameters for a given sample. However, it
has been observed that SMoE suffers from unstable training and has difficulty
adapting to new distributions, leading to the model's lack of robustness to
data contamination. To overcome these limitations, we first establish a
connection between the dynamics of the expert representations in SMoEs and
gradient descent on a multi-objective optimization problem. Leveraging our
framework, we then integrate momentum into SMoE and propose a new family of
SMoEs named MomentumSMoE. We theoretically prove and numerically demonstrate
that MomentumSMoE is more stable and robust than SMoE. In particular, we verify
the advantages of MomentumSMoE over SMoE on a variety of practical tasks
including ImageNet-1K object recognition and WikiText-103 language modeling. We
demonstrate the applicability of MomentumSMoE to many types of SMoE models,
including those in the Sparse MoE model for vision (V-MoE) and the Generalist
Language Model (GLaM). We also show that other advanced momentum-based
optimization methods, such as Adam, can be easily incorporated into the
MomentumSMoE framework for designing new SMoE models with even better
performance, almost negligible additional computation cost, and simple
implementations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages in the main text. Published at NeurIPS 2024. The code is
  available at https://github.com/rachtsy/MomentumSMoE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAG-ConfusionQA: A Benchmark for Evaluating LLMs on Confusing Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Peng, Jinming Nian, Alexandre Evfimievski, Yi Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational AI agents use Retrieval Augmented Generation (RAG) to provide
verifiable document-grounded responses to user inquiries. However, many natural
questions do not have good answers: about 25\% contain false
assumptions~\cite{Yu2023:CREPE}, and over 50\% are
ambiguous~\cite{Min2020:AmbigQA}. RAG agents need high-quality data to improve
their responses to confusing questions. This paper presents a novel synthetic
data generation method to efficiently create a diverse set of context-grounded
confusing questions from a given document corpus. We conduct an empirical
comparative evaluation of several large language models as RAG agents to
measure the accuracy of confusion detection and appropriate response
generation. We contribute a benchmark dataset to the public domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tell me what I need to know: Exploring LLM-based (Personalized)
  Abstractive Multi-Source Meeting Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederic Kirstein, Terry Ruas, Robert Kratel, Bela Gipp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meeting summarization is crucial in digital communication, but existing
solutions struggle with salience identification to generate personalized,
workable summaries, and context understanding to fully comprehend the meetings'
content. Previous attempts to address these issues by considering related
supplementary resources (e.g., presentation slides) alongside transcripts are
hindered by models' limited context sizes and handling the additional
complexities of the multi-source tasks, such as identifying relevant
information in additional files and seamlessly aligning it with the meeting
content. This work explores multi-source meeting summarization considering
supplementary materials through a three-stage large language model approach:
identifying transcript passages needing additional context, inferring relevant
details from supplementary materials and inserting them into the transcript,
and generating a summary from this enriched transcript. Our multi-source
approach enhances model understanding, increasing summary relevance by ~9% and
producing more content-rich outputs. We introduce a personalization protocol
that extracts participant characteristics and tailors summaries accordingly,
improving informativeness by ~10%. This work further provides insights on
performance-cost trade-offs across four leading model families, including
edge-device capable options. Our approach can be extended to similar complex
generative tasks benefitting from additional resources and personalization,
such as dialogue systems and action planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLMs "know" internally when they follow instructions? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyeon Heo, Christina Heinze-Deml, Oussama Elachqar, Shirley Ren, Udhay Nallasamy, Andy Miller, Kwan Ho Ryan Chan, Jaya Narain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction-following is crucial for building AI agents with large language
models (LLMs), as these models must adhere strictly to user-provided
constraints and guidelines. However, LLMs often fail to follow even simple and
clear instructions. To improve instruction-following behavior and prevent
undesirable outputs, a deeper understanding of how LLMs' internal states relate
to these outcomes is required. Our analysis of LLM internal states reveal a
dimension in the input embedding space linked to successful
instruction-following. We demonstrate that modifying representations along this
dimension improves instruction-following success rates compared to random
changes, without compromising response quality. Further investigation reveals
that this dimension is more closely related to the phrasing of prompts rather
than the inherent difficulty of the task or instructions. This discovery also
suggests explanations for why LLMs sometimes fail to follow clear instructions
and why prompt engineering is often effective, even when the content remains
largely unchanged. This work provides insight into the internal workings of
LLMs' instruction-following, paving the way for reliable LLM agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SignAttention: On the Interpretability of Transformer Models for Sign
  Language Translation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Alejandro Dal Bianco, Oscar Agustín Stanchi, Facundo Manuel Quiroga, Franco Ronchetti, Enzo Ferrante
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the first comprehensive interpretability analysis of a
Transformer-based Sign Language Translation (SLT) model, focusing on the
translation from video-based Greek Sign Language to glosses and text.
Leveraging the Greek Sign Language Dataset, we examine the attention mechanisms
within the model to understand how it processes and aligns visual input with
sequential glosses. Our analysis reveals that the model pays attention to
clusters of frames rather than individual ones, with a diagonal alignment
pattern emerging between poses and glosses, which becomes less distinct as the
number of glosses increases. We also explore the relative contributions of
cross-attention and self-attention at each decoding step, finding that the
model initially relies on video frames but shifts its focus to previously
predicted tokens as the translation progresses. This work contributes to a
deeper understanding of SLT models, paving the way for the development of more
transparent and reliable translation systems essential for real-world
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IAI Workshop @ NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining Entropy and Matrix Nuclear Norm for Enhanced Evaluation of
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Vo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) continue to advance, the need for precise and
efficient evaluation metrics becomes more pressing. Traditional approaches,
while informative, often face limitations in computational demands and
interpretability. In this paper, we introduce a novel hybrid evaluation method
that integrates two established techniques: entropy derived from covariance
matrices and the Matrix Nuclear Norm (MNN). Our method begins by normalizing
hidden states from LLMs, then computes the covariance matrix and MNN from these
representations. We further calculate the entropy of the covariance matrix to
capture uncertainty and redundancy in the model's outputs. By combining these
metrics into a composite score, we offer a comprehensive evaluation framework
that balances accuracy with computational efficiency. Additionally, our
approach allows for flexibility in adjusting the weightings between entropy and
MNN, tailoring the evaluation for different objectives. Through a series of
experiments on various LLMs, we demonstrate the robustness and efficacy of our
method, offering deeper insights into model performance. This work contributes
to the ongoing development of LLM evaluation and opens avenues for future
innovations in model assessment techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The method is currently under experimentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        You Wu, Haoyi Wu, Kewei Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, sharing key-value (KV) cache across layers has been found effective
in efficient inference of large language models (LLMs). To systematically
investigate different techniques of cross-layer KV sharing, we propose a
unified framework that covers several recent methods and their novel variants.
We conduct comprehensive experiments on all the configurations of the
framework, evaluating their generation throughput and performance in language
modeling and downstream tasks. We find that when reducing the size of the KV
cache by 2x, most configurations can achieve competitive performance to and
higher throughput than standard transformers, but when further reducing the
size of the KV cache, pairing queries of all layers with KVs of upper layers
can better maintain performance, although it also introduces additional
training cost and prefilling latency. We hope that this work will help users
choose the appropriate approach according to their requirements and facilitate
research on the acceleration of LLM inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Zhao, Xiaobao Wu, Cong-Duy Nguyen, Meihuizi Jia, Yichao Feng, Luu Anh Tuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) can bridge the gap between large
language models (LLMs) and downstream tasks. However, PEFT has been proven
vulnerable to malicious attacks. Research indicates that poisoned LLMs, even
after PEFT, retain the capability to activate internalized backdoors when input
samples contain predefined triggers. In this paper, we introduce a novel
weak-to-strong unlearning algorithm to defend against backdoor attacks based on
feature alignment knowledge distillation, named W2SDefense. Specifically, we
first train a small-scale language model through full-parameter fine-tuning to
serve as the clean teacher model. Then, this teacher model guides the
large-scale poisoned student model in unlearning the backdoor, leveraging PEFT.
Theoretical analysis suggests that W2SDefense has the potential to enhance the
student model's ability to unlearn backdoor features, preventing the activation
of the backdoor. We conduct experiments on text classification tasks involving
three state-of-the-art language models and three different backdoor attack
algorithms. Our empirical results demonstrate the outstanding performance of
W2SDefense in defending against backdoor attacks without compromising model
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of
  Language Models for Fact Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Denitsa Saynova, Lovisa Hagström, Moa Johansson, Richard Johansson, Marco Kuhlmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous interpretations of language models (LMs) miss important distinctions
in how these models process factual information. For example, given the query
"Astrid Lindgren was born in" with the corresponding completion "Sweden", no
difference is made between whether the prediction was based on having the exact
knowledge of the birthplace of the Swedish author or assuming that a person
with a Swedish-sounding name was born in Sweden. In this paper, we investigate
four different prediction scenarios for which the LM can be expected to show
distinct behaviors. These scenarios correspond to different levels of model
reliability and types of information being processed - some being less
desirable for factual predictions. To facilitate precise interpretations of LMs
for fact completion, we propose a model-specific recipe called PrISM for
constructing datasets with examples of each scenario based on a set of
diagnostic criteria. We apply a popular interpretability method, causal tracing
(CT), to the four prediction scenarios and find that while CT produces
different results for each scenario, aggregations over a set of mixed examples
may only represent the results from the scenario with the strongest measured
signal. In summary, we contribute tools for a more granular study of fact
completion in language models and analyses that provide a more nuanced
understanding of how LMs process fact-related queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Magdalena Wysocka, Danilo S. Carvalho, Oskar Wysocki, Marco Valentino, Andre Freitas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Syllogistic reasoning is crucial for Natural Language Inference (NLI). This
capability is particularly significant in specialized domains such as
biomedicine, where it can support automatic evidence interpretation and
scientific discovery. This paper presents SylloBio-NLI, a novel framework that
leverages external ontologies to systematically instantiate diverse syllogistic
arguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language
Models (LLMs) on identifying valid conclusions and extracting supporting
evidence across 28 syllogistic schemes instantiated with human genome pathways.
Extensive experiments reveal that biomedical syllogistic reasoning is
particularly challenging for zero-shot LLMs, which achieve an average accuracy
between 70% on generalized modus ponens and 23% on disjunctive syllogism. At
the same time, we found that few-shot prompting can boost the performance of
different LLMs, including Gemma (+14%) and LLama-3 (+43%). However, a deeper
analysis shows that both techniques exhibit high sensitivity to superficial
lexical variations, highlighting a dependency between reliability, models'
architecture, and pre-training regime. Overall, our results indicate that,
while in-context examples have the potential to elicit syllogistic reasoning in
LLMs, existing models are still far from achieving the robustness and
consistency required for safe biomedical NLI applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative AI, Pragmatics, and Authenticity in Second Language Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Godwin-Jones`
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are obvious benefits to integrating generative AI (artificial
intelligence) into language learning and teaching. Those include using AI as a
language tutor, creating learning materials, or assessing learner output.
However, due to how AI systems under-stand human language, based on a
mathematical model using statistical probability, they lack the lived
experience to be able to use language with the same social aware-ness as
humans. Additionally, there are built-in linguistic and cultural biases based
on their training data which is mostly in English and predominantly from
Western sources. Those facts limit AI suitability for some language learning
interactions. Stud-ies have clearly shown that systems such as ChatGPT often do
not produce language that is pragmatically appropriate. The lack of linguistic
and cultural authenticity has important implications for how AI is integrated
into second language acquisition as well as in instruction targeting
development of intercultural communication compe-tence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing Context Utilization of LLMs in Document-Level Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wafaa Mohammed, Vlad Niculae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM) are increasingly strong contenders in machine
translation. We study document-level translation, where some words cannot be
translated without context from outside the sentence. We investigate the
ability of prominent LLMs to utilize context by analyzing models' robustness to
perturbed and randomized document context. We find that LLMs' improved
document-translation performance is not always reflected in pronoun translation
performance. We highlight the need for context-aware finetuning of LLMs with a
focus on relevant parts of the context to improve their reliability for
document-level translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Do Multilingual Models Remember? Investigating Multilingual Factual
  Recall Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Constanza Fierro, Negar Foroutan, Desmond Elliott, Anders Søgaard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) store and retrieve vast amounts of factual
knowledge acquired during pre-training. Prior research has localized and
identified mechanisms behind knowledge recall; however, it has primarily
focused on English monolingual models. The question of how these processes
generalize to other languages and multilingual LLMs remains unexplored. In this
paper, we address this gap by conducting a comprehensive analysis of two highly
multilingual LLMs. We assess the extent to which previously identified
components and mechanisms of factual recall in English apply to a multilingual
context. Then, we examine when language plays a role in the recall process,
uncovering evidence of language-independent and language-dependent mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Fine-Tuning</span> <span class="highlight-title">Pre-train</span>ed Language Models for Robust Causal Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialin Yu, Yuxiang Zhou, Yulan He, Nevin L. Zhang, Ricardo Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fine-tuning of pre-trained language models (PLMs) has been shown to be
effective across various domains. By using domain-specific supervised data, the
general-purpose representation derived from PLMs can be transformed into a
domain-specific representation. However, these methods often fail to generalize
to out-of-domain (OOD) data due to their reliance on non-causal
representations, often described as spurious features. Existing methods either
make use of adjustments with strong assumptions about lack of hidden common
causes, or mitigate the effect of spurious features using multi-domain data. In
this work, we investigate how fine-tuned pre-trained language models aid
generalizability from single-domain scenarios under mild assumptions, targeting
more general and practical real-world scenarios. We show that a robust
representation can be derived through a so-called causal front-door adjustment,
based on a decomposition assumption, using fine-tuned representations as a
source of data augmentation. Comprehensive experiments in both synthetic and
real-world settings demonstrate the superior generalizability of the proposed
method compared to existing approaches. Our work thus sheds light on the domain
generalization problem by introducing links between fine-tuning and causal
mechanisms into representation learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficiently Computing Susceptibility to Context in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Liu, Kevin Du, Mrinmaya Sachan, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One strength of modern language models is their ability to incorporate
information from a user-input context when answering queries. However, they are
not equally sensitive to the subtle changes to that context. To quantify this,
Du et al. (2024) gives an information-theoretic metric to measure such
sensitivity. Their metric, susceptibility, is defined as the degree to which
contexts can influence a model's response to a query at a distributional level.
However, exactly computing susceptibility is difficult and, thus, Du et al.
(2024) falls back on a Monte Carlo approximation. Due to the large number of
samples required, the Monte Carlo approximation is inefficient in practice. As
a faster alternative, we propose Fisher susceptibility, an efficient method to
estimate the susceptibility based on Fisher information. Empirically, we
validate that Fisher susceptibility is comparable to Monte Carlo estimated
susceptibility across a diverse set of query domains despite its being
$70\times$ faster. Exploiting the improved efficiency, we apply Fisher
susceptibility to analyze factors affecting the susceptibility of language
models. We observe that larger models are as susceptible as smaller ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Critical Questions Generation: Motivation and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Blanca Calvo Figueras, Rodrigo Agerri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of Large Language Models (LLMs) has brought impressive
performances on mitigation strategies against misinformation, such as
counterargument generation. However, LLMs are still seriously hindered by
outdated knowledge and by their tendency to generate hallucinated content. In
order to circumvent these issues, we propose a new task, namely, Critical
Questions Generation, consisting of processing an argumentative text to
generate the critical questions (CQs) raised by it. In argumentation theory CQs
are tools designed to lay bare the blind spots of an argument by pointing at
the information it could be missing. Thus, instead of trying to deploy LLMs to
produce knowledgeable and relevant counterarguments, we use them to question
arguments, without requiring any external knowledge. Research on CQs Generation
using LLMs requires a reference dataset for large scale experimentation. Thus,
in this work we investigate two complementary methods to create such a
resource: (i) instantiating CQs templates as defined by Walton's argumentation
theory and (ii), using LLMs as CQs generators. By doing so, we contribute with
a procedure to establish what is a valid CQ and conclude that, while LLMs are
reasonable CQ generators, they still have a wide margin for improvement in this
task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures, 7 tables, to be published in the 28th Conference
  on Computational Natural Language Learning (CoNLL 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoGU: Long-form Generation with Uncertainty Expressions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihan Yang, Caiqi Zhang, Zhisong Zhang, Xinting Huang, Sen Yang, Nigel Collier, Dong Yu, Deqing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) demonstrate impressive capabilities, they
still struggle with generating factually incorrect content (i.e.,
hallucinations). A promising approach to mitigate this issue is enabling models
to express uncertainty when unsure. Previous research on uncertainty modeling
has primarily focused on short-form QA, but realworld applications often
require much longer responses. In this work, we introduce the task of Long-form
Generation with Uncertainty(LoGU). We identify two key challenges: Uncertainty
Suppression, where models hesitate to express uncertainty, and Uncertainty
Misalignment, where models convey uncertainty inaccurately. To tackle these
challenges, we propose a refinement-based data collection framework and a
two-stage training pipeline. Our framework adopts a divide-and-conquer
strategy, refining uncertainty based on atomic claims. The collected data are
then used in training through supervised fine-tuning (SFT) and direct
preference optimization (DPO) to enhance uncertainty expression. Extensive
experiments on three long-form instruction following datasets show that our
method significantly improves accuracy, reduces hallucinations, and maintains
the comprehensiveness of responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SwaQuAD-24: QA Benchmark Dataset in Swahili 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alfred Malengo Kondoro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes the creation of a Swahili Question Answering (QA)
benchmark dataset, aimed at addressing the underrepresentation of Swahili in
natural language processing (NLP). Drawing from established benchmarks like
SQuAD, GLUE, KenSwQuAD, and KLUE, the dataset will focus on providing
high-quality, annotated question-answer pairs that capture the linguistic
diversity and complexity of Swahili. The dataset is designed to support a
variety of applications, including machine translation, information retrieval,
and social services like healthcare chatbots. Ethical considerations, such as
data privacy, bias mitigation, and inclusivity, are central to the dataset
development. Additionally, the paper outlines future expansion plans to include
domain-specific content, multimodal integration, and broader crowdsourcing
efforts. The Swahili QA dataset aims to foster technological innovation in East
Africa and provide an essential resource for NLP research and applications in
low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EcomEdit: An Automated E-commerce Knowledge Editing Framework for
  Enhanced Product and Purchase Intention Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching Ming Samuel Lau, Weiqi Wang, Haochen Shi, Baixuan Xu, Jiaxin Bai, Yangqiu Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Editing (KE) aims to correct and update factual information in
Large Language Models (LLMs) to ensure accuracy and relevance without
computationally expensive fine-tuning. Though it has been proven effective in
several domains, limited work has focused on its application within the
e-commerce sector. However, there are naturally occurring scenarios that make
KE necessary in this domain, such as the timely updating of product features
and trending purchase intentions by customers, which necessitate further
exploration. In this paper, we pioneer the application of KE in the e-commerce
domain by presenting ECOMEDIT, an automated e-commerce knowledge editing
framework tailored for e-commerce-related knowledge and tasks. Our framework
leverages more powerful LLMs as judges to enable automatic knowledge conflict
detection and incorporates conceptualization to enhance the semantic coverage
of the knowledge to be edited. Through extensive experiments, we demonstrate
the effectiveness of ECOMEDIT in improving LLMs' understanding of product
descriptions and purchase intentions. We also show that LLMs, after our
editing, can achieve stronger performance on downstream e-commerce tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REEF: Representation Encoding Fingerprints for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Zhang, Dongrui Liu, Chen Qian, Linfeng Zhang, Yong Liu, Yu Qiao, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protecting the intellectual property of open-source Large Language Models
(LLMs) is very important, because training LLMs costs extensive computational
resources and data. Therefore, model owners and third parties need to identify
whether a suspect model is a subsequent development of the victim model. To
this end, we propose a training-free REEF to identify the relationship between
the suspect and victim models from the perspective of LLMs' feature
representations. Specifically, REEF computes and compares the centered kernel
alignment similarity between the representations of a suspect model and a
victim model on the same samples. This training-free REEF does not impair the
model's general capabilities and is robust to sequential fine-tuning, pruning,
model merging, and permutations. In this way, REEF provides a simple and
effective way for third parties and models' owners to protect LLMs'
intellectual property together. The code is available at
https://github.com/tmylla/REEF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoDification: Mixture of Depths Made Easy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zhang, Meizhi Zhong, Qimeng Wang, Xuantao Lu, Zheyu Ye, Chengqiang Lu, Yan Gao, Yao Hu, Kehai Chen, Min Zhang, Dawei Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-context efficiency has recently become a trending topic in serving large
language models (LLMs). And mixture of depths (MoD) is proposed as a perfect
fit to bring down both latency and memory. In this paper, however, we discover
that MoD can barely transform existing LLMs without costly training over an
extensive number of tokens. To enable the transformations from any LLMs to MoD
ones, we showcase top-k operator in MoD should be promoted to threshold-p
operator, and refinement to architecture and data should also be crafted along.
All these designs form our method termed MoDification. Through a comprehensive
set of experiments covering model scales from 3B to 70B, we exhibit
MoDification strikes an excellent balance between efficiency and effectiveness.
MoDification can achieve up to ~1.2x speedup in latency and ~1.8x reduction in
memory compared to original LLMs especially in long-context applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures, 5 tables, work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Good Parenting is all you need -- Multi-agentic LLM Hallucination
  Mitigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Edward,  Kwartler, Matthew Berman, Alan Aqrawi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores the ability of Large Language Model (LLM) agents to
detect and correct hallucinations in AI-generated content. A primary agent was
tasked with creating a blog about a fictional Danish artist named Flipfloppidy,
which was then reviewed by another agent for factual inaccuracies. Most LLMs
hallucinated the existence of this artist. Across 4,900 test runs involving
various combinations of primary and reviewing agents, advanced AI models such
as Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in
identifying hallucinations and successfully revised outputs in 85% to 100% of
cases following feedback. These findings underscore the potential of advanced
AI models to significantly enhance the accuracy and reliability of generated
content, providing a promising approach to improving AI workflow orchestration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Binary: Towards Fine-Grained LLM-Generated Text Detection via
  Role Recognition and Involvement Measurement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Cheng, Li Zhou, Feng Jiang, Benyou Wang, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of large language models (LLMs), like ChatGPT, has
resulted in the widespread presence of LLM-generated content on social media
platforms, raising concerns about misinformation, data biases, and privacy
violations, which can undermine trust in online discourse. While detecting
LLM-generated content is crucial for mitigating these risks, current methods
often focus on binary classification, failing to address the complexities of
real-world scenarios like human-AI collaboration. To move beyond binary
classification and address these challenges, we propose a new paradigm for
detecting LLM-generated content. This approach introduces two novel tasks: LLM
Role Recognition (LLM-RR), a multi-class classification task that identifies
specific roles of LLM in content generation, and LLM Influence Measurement
(LLM-IM), a regression task that quantifies the extent of LLM involvement in
content creation. To support these tasks, we propose LLMDetect, a benchmark
designed to evaluate detectors' performance on these new tasks. LLMDetect
includes the Hybrid News Detection Corpus (HNDC) for training detectors, as
well as DetectEval, a comprehensive evaluation suite that considers five
distinct cross-context variations and multi-intensity variations within the
same LLM role. This allows for a thorough assessment of detectors'
generalization and robustness across diverse contexts. Our empirical validation
of 10 baseline detection methods demonstrates that fine-tuned PLM-based models
consistently outperform others on both tasks, while advanced LLMs face
challenges in accurately detecting their own generated content. Our
experimental results and analysis offer insights for developing more effective
detection models for LLM-generated content. This research enhances the
understanding of LLM-generated content and establishes a foundation for more
nuanced detection methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Social Media, Large Language Models, LLM-generated Text Detection,
  AI-assisted News Detection</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nova: An Iterative Planning and Search Approach to Enhance Novelty and
  Diversity of LLM Generated Ideas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific innovation is pivotal for humanity, and harnessing large language
models (LLMs) to generate research ideas could transform discovery. However,
existing LLMs often produce simplistic and repetitive suggestions due to their
limited ability in acquiring external knowledge for innovation. To address this
problem, we introduce an enhanced planning and search methodology designed to
boost the creative potential of LLM-based systems. Our approach involves an
iterative process to purposely plan the retrieval of external knowledge,
progressively enriching the idea generation with broader and deeper insights.
Validation through automated and human assessments indicates that our framework
substantially elevates the quality of generated ideas, particularly in novelty
and diversity. The number of unique novel ideas produced by our framework is
3.4 times higher than without it. Moreover, our method outperforms the current
state-of-the-art, generating at least 2.5 times more top-rated ideas based on
170 seed papers in a Swiss Tournament evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesizing Post-Training Data for LLMs through Multi-Agent Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Tang, Xianghe Pang, Zexi Liu, Bohan Tang, Rui Ye, Xiaowen Dong, Yanfeng Wang, Siheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training is essential for enabling large language models (LLMs) to
follow human instructions. Inspired by the recent success of using LLMs to
simulate human society, we leverage multi-agent simulation to automatically
generate diverse text-based scenarios, capturing a wide range of real-world
human needs. We propose MATRIX, a multi-agent simulator that creates realistic
and scalable scenarios. Leveraging these outputs, we introduce a novel
scenario-driven instruction generator MATRIX-Gen for controllable and highly
realistic data synthesis. Extensive experiments demonstrate that our framework
effectively generates both general and domain-specific data. Notably, on
AlpacaEval 2 and Arena-Hard benchmarks, Llama-3-8B-Base, post-trained on
datasets synthesized by MATRIX-Gen with just 20K instruction-response pairs,
outperforms Meta's Llama-3-8B-Instruct model, which was trained on over 10M
pairs; see our project at https://github.com/ShuoTang123/MATRIX-Gen.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Blind Guessing: Calibration of Selection Bias in
  Multiple-Choice Question Answering by Video Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Loginova, Oleksandr Bezrukov, Alexey Kravets
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating Video Language Models (VLMs) is a challenging task. Due to its
transparency, Multiple-Choice Question Answering (MCQA) is widely used to
measure the performance of these models through accuracy. However, existing
MCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to
selection bias, when models disproportionately favor certain answer options
based on positional patterns observed during training. In this work, we conduct
a comprehensive empirical analysis of several VLM architectures across major
datasets designed to assess complex video-focused reasoning. We identify where
the bias is most pronounced and demonstrate to what extent model responses
reflect genuine understanding of video content and related questions, as
opposed to reliance on arbitrary patterns or superficial cues, such as answer
position. By decomposing the MCQA task and adapting fairness bias metrics to
VLMs, we introduce a post-processing calibration technique BOLD to balance this
bias. Our results show that reducing selection bias improves not only debiasing
metrics but also overall model performance, including Accuracy and F1 Mean
score. Our method, by suppressing "blind guessing", offers a more cost- and
time-effective approach to mitigating selection bias compared to existing
techniques. This study represents the first focused investigation of selection
bias in video-to-text LLM-powered models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Method to Metigate Demographic and Expert Bias in ICD Coding
  with Causal Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Zhang, Junli Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ICD(International Classification of Diseases) coding involves assigning ICD
codes to patients visit based on their medical notes. Considering ICD coding as
a multi-label text classification task, researchers have developed
sophisticated methods. Despite progress, these models often suffer from label
imbalance and may develop spurious correlations with demographic factors.
Additionally, while human coders assign ICD codes, the inclusion of irrelevant
information from unrelated experts introduces biases. To combat these issues,
we propose a novel method to mitigate Demographic and Expert biases in ICD
coding through Causal Inference (DECI). We provide a novel causality-based
interpretation in ICD Coding that models make predictions by three distinct
pathways. And based counterfactual reasoning, DECI mitigate demographic and
expert biases. Experimental results show that DECI outperforms state-of-the-art
models, offering a significant advancement in accurate and unbiased ICD coding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust Knowledge Representations in Multilingual LLMs for
  Equivalence and Inheritance based Consistent Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaurav Arora, Srujana Merugu, Shreya Jain, Vaibhav Saxena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning and linguistic skills form the cornerstone of human intelligence,
facilitating problem-solving and decision-making. Recent advances in Large
Language Models (LLMs) have led to impressive linguistic capabilities and
emergent reasoning behaviors, fueling widespread adoption across application
domains. However, LLMs still struggle with complex reasoning tasks,
highlighting their systemic limitations. In this work, we focus on evaluating
whether LLMs have the requisite representations to reason using two
foundational relationships: "equivalence" and "inheritance". We introduce novel
tasks and benchmarks spanning six languages and observe that current SOTA LLMs
often produce conflicting answers to the same questions across languages in
17.3-57.5% of cases and violate inheritance constraints in up to 37.2% cases.
To enhance consistency across languages, we propose novel "Compositional
Representations" where tokens are represented as composition of equivalent
tokens across languages, with resulting conflict reduction (up to -4.7%)
indicating benefits of shared LLM representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Large Language Models Generated Texts: A Multi-Level
  Fine-Grained Detection Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Tao, Zhiyu Li, Runyu Chen, Dinghao Xi, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have transformed human writing by enhancing
grammar correction, content expansion, and stylistic refinement. However, their
widespread use raises concerns about authorship, originality, and ethics, even
potentially threatening scholarly integrity. Existing detection methods, which
mainly rely on single-feature analysis and binary classification, often fail to
effectively identify LLM-generated text in academic contexts. To address these
challenges, we propose a novel Multi-level Fine-grained Detection (MFD)
framework that detects LLM-generated text by integrating low-level structural,
high-level semantic, and deep-level linguistic features, while conducting
sentence-level evaluations of lexicon, grammar, and syntax for comprehensive
analysis. To improve detection of subtle differences in LLM-generated text and
enhance robustness against paraphrasing, we apply two mainstream evasion
techniques to rewrite the text. These variations, along with original texts,
are used to train a text encoder via contrastive learning, extracting
high-level semantic features of sentence to boost detection generalization.
Furthermore, we leverage advanced LLM to analyze the entire text and extract
deep-level linguistic features, enhancing the model's ability to capture
complex patterns and nuances while effectively incorporating contextual
information. Extensive experiments on public datasets show that the MFD model
outperforms existing methods, achieving an MAE of 0.1346 and an accuracy of
88.56%. Our research provides institutions and publishers with an effective
mechanism to detect LLM-generated text, mitigating risks of compromised
authorship. Educators and editors can use the model's predictions to refine
verification and plagiarism prevention protocols, ensuring adherence to
standards.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Joint <span class="highlight-title">Multimodal</span> Entity-Relation Extraction via
  Knowledge-Enhanced Cross-modal <span class="highlight-title">Prompt</span> Model <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Yuan, Yi Cai, Junsheng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Joint Multimodal Entity-Relation Extraction (JMERE) is a challenging task
that aims to extract entities and their relations from text-image pairs in
social media posts. Existing methods for JMERE require large amounts of labeled
data. However, gathering and annotating fine-grained multimodal data for JMERE
poses significant challenges. Initially, we construct diverse and comprehensive
multimodal few-shot datasets fitted to the original data distribution. To
address the insufficient information in the few-shot setting, we introduce the
\textbf{K}nowledge-\textbf{E}nhanced \textbf{C}ross-modal \textbf{P}rompt
\textbf{M}odel (KECPM) for JMERE. This method can effectively address the
problem of insufficient information in the few-shot setting by guiding a large
language model to generate supplementary background knowledge. Our proposed
method comprises two stages: (1) a knowledge ingestion stage that dynamically
formulates prompts based on semantic similarity guide ChatGPT generating
relevant knowledge and employs self-reflection to refine the knowledge; (2) a
knowledge-enhanced language model stage that merges the auxiliary knowledge
with the original input and utilizes a transformer-based model to align with
JMERE's required output format. We extensively evaluate our approach on a
few-shot dataset derived from the JMERE dataset, demonstrating its superiority
over strong baselines in terms of both micro and macro F$_1$ scores.
Additionally, we present qualitative analyses and case studies to elucidate the
effectiveness of our model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ACM MM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Paths-over-Graph: Knowledge Graph Enpowered Large Language Model
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Wenjie Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved impressive results in various
tasks but struggle with hallucination problems and lack of relevant knowledge,
especially in deep complex reasoning and knowledge-intensive tasks. Knowledge
Graphs (KGs), which capture vast amounts of facts in a structured format, offer
a reliable source of knowledge for reasoning. However, existing KG-based LLM
reasoning methods face challenges like handling multi-hop reasoning,
multi-entity questions, and effectively utilizing graph structures. To address
these issues, we propose Paths-over-Graph (PoG), a novel method that enhances
LLM reasoning by integrating knowledge reasoning paths from KGs, improving the
interpretability and faithfulness of LLM outputs. PoG tackles multi-hop and
multi-entity questions through a three-phase dynamic multi-hop path
exploration, which combines the inherent knowledge of LLMs with factual
knowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant
information from the graph exploration first and introduces efficient
three-step pruning techniques that incorporate graph structures, LLM prompting,
and a pre-trained language model (e.g., SBERT) to effectively narrow down the
explored candidate paths. This ensures all reasoning paths contain highly
relevant information captured from KGs, making the reasoning faithful and
interpretable in problem-solving. PoG innovatively utilizes graph structure to
prune the irrelevant noise and represents the first method to implement
multi-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive
experiments on five benchmark KGQA datasets demonstrate PoG outperforms the
state-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an
average accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo
surpasses ToG with GPT-4 by up to 23.9%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Montessori-Instruct: Generate Influential Training Data Tailored for
  Student Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochuan Li, Zichun Yu, Chenyan Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data has been widely used to train large language models, but their
generative nature inevitably introduces noisy, non-informative, and misleading
learning signals. In this paper, we propose Montessori-Instruct, a novel data
synthesis framework that tailors the data synthesis ability of the teacher
language model toward the student language model's learning process.
Specifically, we utilize local data influence of synthetic training data points
on students to characterize students' learning preferences. Then, we train the
teacher model with Direct Preference Optimization (DPO) to generate synthetic
data tailored toward student learning preferences. Experiments with
Llama3-8B-Instruct (teacher) and Llama3-8B (student) on Alpaca Eval and
MT-Bench demonstrate that Montessori-Instruct significantly outperforms
standard synthesis methods by 18.35\% and 46.24\% relatively. Our method also
beats data synthesized by a stronger teacher model, GPT-4o. Further analysis
confirms the benefits of teacher's learning to generate more influential
training data in the student's improved learning, the advantages of local data
influence in accurately measuring student preferences, and the robustness of
Montessori-Instruct across different student models. Our code and data are
open-sourced at https://github.com/cxcscmu/Montessori-Instruct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Codes and data are open-sourced at
  https://github.com/cxcscmu/Montessori-Instruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MediTOD: An English Dialogue Dataset for Medical History Taking with
  Comprehensive Annotations <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishal Vivek Saley, Goonjan Saha, Rocktim Jyoti Das, Dinesh Raghu,  Mausam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical task-oriented dialogue systems can assist doctors by collecting
patient medical history, aiding in diagnosis, or guiding treatment selection,
thereby reducing doctor burnout and expanding access to medical services.
However, doctor-patient dialogue datasets are not readily available, primarily
due to privacy regulations. Moreover, existing datasets lack comprehensive
annotations involving medical slots and their different attributes, such as
symptoms and their onset, progression, and severity. These comprehensive
annotations are crucial for accurate diagnosis. Finally, most existing datasets
are non-English, limiting their utility for the larger research community.
  In response, we introduce MediTOD, a new dataset of doctor-patient dialogues
in English for the medical history-taking task. Collaborating with doctors, we
devise a questionnaire-based labeling scheme tailored to the medical domain.
Then, medical professionals create the dataset with high-quality comprehensive
annotations, capturing medical slots and their attributes. We establish
benchmarks in supervised and few-shot settings on MediTOD for natural language
understanding, policy learning, and natural language generation subtasks,
evaluating models from both TOD and biomedical domains. We make MediTOD
publicly available for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024 Camera Ready Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay
  Scoring with Rationale Generated by LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SeongYeub Chu, JongWoo Kim, Bryan Wong, MunYong Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing automated essay scoring (AES) has solely relied on essay text
without using explanatory rationales for the scores, thereby forgoing an
opportunity to capture the specific aspects evaluated by rubric indicators in a
fine-grained manner. This paper introduces Rationale-based Multiple Trait
Scoring (RMTS), a novel approach for multi-trait essay scoring that integrates
prompt-engineering-based large language models (LLMs) with a fine-tuning-based
essay scoring model using a smaller large language model (S-LLM). RMTS uses an
LLM-based trait-wise rationale generation system where a separate LLM agent
generates trait-specific rationales based on rubric guidelines, which the
scoring model uses to accurately predict multi-trait scores. Extensive
experiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize,
show that RMTS significantly outperforms state-of-the-art models and vanilla
S-LLMs in trait-specific scoring. By assisting quantitative assessment with
fine-grained qualitative rationales, RMTS enhances the trait-wise reliability,
providing partial explanations about essays.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E3D-GPT: Enhanced 3D Visual Foundation for Medical <span class="highlight-title">Vision-Language</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Lai, Zihang Jiang, Qingsong Yao, Rongsheng Wang, Zhiyang He, Xiaodong Tao, Wei Wei, Weifu Lv, S. Kevin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of 3D medical vision-language models holds significant
potential for disease diagnosis and patient treatment. However, compared to 2D
medical images, 3D medical images, such as CT scans, face challenges related to
limited training data and high dimension, which severely restrict the progress
of 3D medical vision-language models. To address these issues, we collect a
large amount of unlabeled 3D CT data and utilize self-supervised learning to
construct a 3D visual foundation model for extracting 3D visual features. Then,
we apply 3D spatial convolutions to aggregate and project high-level image
features, reducing computational complexity while preserving spatial
information. We also construct two instruction-tuning datasets based on BIMCV-R
and CT-RATE to fine-tune the 3D vision-language model. Our model demonstrates
superior performance compared to existing methods in report generation, visual
question answering, and disease diagnosis. Code and data will be made publicly
available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supervised Chain of Thought 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Zhang, Dujian Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized natural language processing
and hold immense potential for advancing Artificial Intelligence. However, the
core architecture of most mainstream LLMs -- the Transformer -- has inherent
limitations in computational depth, rendering them theoretically incapable of
solving many reasoning tasks that demand increasingly deep computations. Chain
of Thought (CoT) prompting has emerged as a technique to address these
architectural limitations, as evidenced by several theoretical studies. It
offers a promising approach to solving complex reasoning tasks that were
previously beyond the capabilities of these models. Despite its successes, CoT
and its variants (such as Tree of Thought, Graph of Thought, etc.) rely on a
"one-prompt-for-all" approach, using a single prompt structure (e.g., "think
step by step") for a wide range of tasks -- from counting and sorting to
solving mathematical and algorithmic problems. This approach poses significant
challenges for models to generate the correct reasoning steps, as the model
must navigate through a vast prompt template space to find the appropriate
template for each task. In this work, we build upon previous theoretical
analyses of CoT to demonstrate how the one-prompt-for-all approach can
negatively affect the computability of LLMs. We partition the solution search
space into two: the prompt space and the answer space. Our findings show that
task-specific supervision is essential for navigating the prompt space
accurately and achieving optimal performance. Through experiments with
state-of-the-art LLMs, we reveal a gap in reasoning performance when
supervision is applied versus when it is not.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speciesism in Natural Language Processing Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masashi Takeshita, Rafal Rzepka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) research on AI Safety and social bias in AI
has focused on safety for humans and social bias against human minorities.
However, some AI ethicists have argued that the moral significance of nonhuman
animals has been ignored in AI research. Therefore, the purpose of this study
is to investigate whether there is speciesism, i.e., discrimination against
nonhuman animals, in NLP research. First, we explain why nonhuman animals are
relevant in NLP research. Next, we survey the findings of existing research on
speciesism in NLP researchers, data, and models and further investigate this
problem in this study. The findings of this study suggest that speciesism
exists within researchers, data, and models, respectively. Specifically, our
survey and experiments show that (a) among NLP researchers, even those who
study social bias in AI, do not recognize speciesism or speciesist bias; (b)
among NLP data, speciesist bias is inherent in the data annotated in the
datasets used to evaluate NLP models; (c) OpenAI GPTs, recent NLP models,
exhibit speciesist bias by default. Finally, we discuss how we can reduce
speciesism in NLP research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article is a preprint and has not been peer-reviewed. The
  postprint has been accepted for publication in AI and Ethics. Please cite the
  final version of the article once it is published</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaAlign: Align Large Language Models with Diverse Preferences during
  Inference Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mozhi Zhang, Pengyu Wang, Chenkun Tan, Mianqiu Huang, Dong Zhang, Yaqian Zhou, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) acquire extensive knowledge and remarkable
abilities from extensive text corpora, making them powerful tools for various
applications. To make LLMs more usable, aligning them with human preferences is
essential. Existing alignment techniques, such as Reinforcement Learning from
Human Feedback (RLHF) and Direct Preference Optimization (DPO), typically embed
predefined preferences directly within the model's parameters. These methods,
however, often result in a static alignment that can not account for the
diversity of human preferences in practical applications. In response to this
challenge, we propose an effective method, \textbf{MetaAlign}, which aims to
help LLMs dynamically align with various explicit or implicit preferences
specified at inference time. Experimental results show that LLMs optimized on
our meticulously constructed MetaAlign Dataset can effectively align with any
preferences specified at the inference stage, validating the feasibility of
MetaAlign. We hope that our work can provide some insights into the alignment
of language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujun Zhou, Jingdong Yang, Kehan Guo, Pin-Yu Chen, Tian Gao, Werner Geyer, Nuno Moniz, Nitesh V Chawla, Xiangliang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Laboratory accidents pose significant risks to human life and property,
underscoring the importance of robust safety protocols. Despite advancements in
safety training, laboratory personnel may still unknowingly engage in unsafe
practices. With the increasing reliance on large language models (LLMs) for
guidance in various fields, including laboratory settings, there is a growing
concern about their reliability in critical safety-related decision-making.
Unlike trained human researchers, LLMs lack formal lab safety education,
raising questions about their ability to provide safe and accurate guidance.
Existing research on LLM trustworthiness primarily focuses on issues such as
ethical compliance, truthfulness, and fairness but fails to fully cover
safety-critical real-world applications, like lab safety. To address this gap,
we propose the Laboratory Safety Benchmark (LabSafety Bench), a comprehensive
evaluation framework based on a new taxonomy aligned with Occupational Safety
and Health Administration (OSHA) protocols. This benchmark includes 765
multiple-choice questions verified by human experts, assessing LLMs and vision
language models (VLMs) performance in lab safety contexts. Our evaluations
demonstrate that while GPT-4o outperforms human participants, it is still prone
to critical errors, highlighting the risks of relying on LLMs in
safety-critical environments. Our findings emphasize the need for specialized
benchmarks to accurately assess the trustworthiness of LLMs in real-world
safety applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XForecast: Evaluating Natural Language Explanations for Time Series
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Aksu, Chenghao Liu, Amrita Saha, Sarah Tan, Caiming Xiong, Doyen Sahoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series forecasting aids decision-making, especially for stakeholders who
rely on accurate predictions, making it very important to understand and
explain these models to ensure informed decisions. Traditional explainable AI
(XAI) methods, which underline feature or temporal importance, often require
expert knowledge. In contrast, natural language explanations (NLEs) are more
accessible to laypeople. However, evaluating forecast NLEs is difficult due to
the complex causal relationships in time series data. To address this, we
introduce two new performance metrics based on simulatability, assessing how
well a human surrogate can predict model forecasts using the explanations.
Experiments show these metrics differentiate good from poor explanations and
align with human judgments. Utilizing these metrics, we further evaluate the
ability of state-of-the-art large language models (LLMs) to generate
explanations for time series data, finding that numerical reasoning, rather
than model size, is the main factor influencing explanation quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiChartQA: Benchmarking <span class="highlight-title">Vision-Language</span> Models on Multi-Chart
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifeng Zhu, Mengzhao Jia, Zhihan Zhang, Lang Li, Meng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have demonstrated impressive
abilities across various tasks, including visual question answering and chart
comprehension, yet existing benchmarks for chart-related tasks fall short in
capturing the complexity of real-world multi-chart scenarios. Current
benchmarks primarily focus on single-chart tasks, neglecting the multi-hop
reasoning required to extract and integrate information from multiple charts,
which is essential in practical applications. To fill this gap, we introduce
MultiChartQA, a benchmark that evaluates MLLMs' capabilities in four key areas:
direct question answering, parallel question answering, comparative reasoning,
and sequential reasoning. Our evaluation of a wide range of MLLMs reveals
significant performance gaps compared to humans. These results highlight the
challenges in multi-chart comprehension and the potential of MultiChartQA to
drive advancements in this field. Our code and data are available at
https://github.com/Zivenzhu/Multi-chart-QA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM The Genius Paradox: A Linguistic and Math Expert's Struggle with
  Simple Word-based Counting Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Xu, Xuezhe Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interestingly, LLMs yet struggle with some basic tasks that humans find
trivial to handle, e.g., counting the number of character r's in the word
"strawberry". There are several popular conjectures (e.g., tokenization,
architecture and training data) regarding the reason for deficiency of LLMs in
simple word-based counting problems, sharing the similar belief that such
failure stems from model pretraining hence probably inevitable during
deployment. In this paper, we carefully design multiple evaluation settings to
investigate validity of prevalent conjectures. Meanwhile, we measure
transferability of advanced mathematical and coding reasoning capabilities from
specialized LLMs to simple counting tasks. Although specialized LLMs suffer
from counting problems as well, we find conjectures about inherent deficiency
of LLMs invalid and further seek opportunities to elicit knowledge and
capabilities from LLMs that are beneficial to counting tasks. Compared with
strategies such as finetuning and in-context learning that are commonly adopted
to enhance performance on new or challenging tasks, we show that engaging
reasoning is the most robust and efficient way to help LLMs better perceive
tasks with more accurate responses.
  We hope our conjecture validation design could provide insights into the
study of future critical failure modes of LLMs. Based on challenges in
transferring advanced capabilities to much simpler tasks, we call for more
attention to model capability acquisition and evaluation. We also highlight the
importance of cultivating consciousness of "reasoning before responding" during
model pretraining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Genre-Aware Article Scoring and Feedback Using Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chihang Wang, Yuxin Dong, Zhenhong Zhang, Ruotong Wang, Shuo Wang, Jiajing Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on the development of an advanced intelligent article
scoring system that not only assesses the overall quality of written work but
also offers detailed feature-based scoring tailored to various article genres.
By integrating the pre-trained BERT model with the large language model
Chat-GPT, the system gains a deep understanding of both the content and
structure of the text, enabling it to provide a thorough evaluation along with
targeted suggestions for improvement. Experimental results demonstrate that
this system outperforms traditional scoring methods across multiple public
datasets, particularly in feature-based assessments, offering a more accurate
reflection of the quality of different article types. Moreover, the system
generates personalized feedback to assist users in enhancing their writing
skills, underscoring the potential and practical value of automated scoring
technologies in educational contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Autoregression: Discrete Diffusion for Complex Reasoning and
  Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, Lingpeng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive language models, despite their impressive capabilities,
struggle with complex reasoning and long-term planning tasks. We introduce
discrete diffusion models as a novel solution to these challenges. Through the
lens of subgoal imbalance, we demonstrate how diffusion models effectively
learn difficult subgoals that elude autoregressive approaches. We propose
Multi-granularity Diffusion Modeling (MDM), which prioritizes subgoals based on
difficulty during learning. On complex tasks like Countdown, Sudoku, and
Boolean Satisfiability Problems, MDM significantly outperforms autoregressive
models without using search techniques. For instance, MDM achieves 91.5\% and
100\% accuracy on Countdown and Sudoku, respectively, compared to 45.8\% and
20.7\% for autoregressive models. Our work highlights the potential of
diffusion-based approaches in advancing AI capabilities for sophisticated
language understanding and problem-solving tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Faithful Natural Language Explanations: A Study Using Activation
  Patching in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Jie Yeo, Ranjan Satapthy, Erik Cambria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are capable of generating persuasive Natural
Language Explanations (NLEs) to justify their answers. However, the
faithfulness of these explanations should not be readily trusted at face value.
Recent studies have proposed various methods to measure the faithfulness of
NLEs, typically by inserting perturbations at the explanation or feature level.
We argue that these approaches are neither comprehensive nor correctly designed
according to the established definition of faithfulness. Moreover, we highlight
the risks of grounding faithfulness findings on out-of-distribution samples. In
this work, we leverage a causal mediation technique called activation patching,
to measure the faithfulness of an explanation towards supporting the explained
answer. Our proposed metric, Causal Faithfulness quantifies the consistency of
causal attributions between explanations and the corresponding model outputs as
the indicator of faithfulness. We experimented across models varying from 2B to
27B parameters and found that models that underwent alignment tuning tend to
produce more faithful and plausible explanations. We find that Causal
Faithfulness is a promising improvement over existing faithfulness tests by
taking into account the model's internal computations and avoiding out of
distribution concerns that could otherwise undermine the validity of
faithfulness assessments. We release the code in
\url{https://github.com/wj210/Causal-Faithfulness}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SRAP-Agent: Simulating and Optimizing Scarce Resource Allocation Policy
  with LLM-based Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Ji, Yang Li, Hongtao Liu, Zhicheng Du, Zhewei Wei, Weiran Shen, Qi Qi, Yankai Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Public scarce resource allocation plays a crucial role in economics as it
directly influences the efficiency and equity in society. Traditional studies
including theoretical model-based, empirical study-based and simulation-based
methods encounter limitations due to the idealized assumption of complete
information and individual rationality, as well as constraints posed by limited
available data. In this work, we propose an innovative framework, SRAP-Agent
(Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based
Agent), which integrates Large Language Models (LLMs) into economic
simulations, aiming to bridge the gap between theoretical models and real-world
dynamics. Using public housing allocation scenarios as a case study, we conduct
extensive policy simulation experiments to verify the feasibility and
effectiveness of the SRAP-Agent and employ the Policy Optimization Algorithm
with certain optimization objectives. The source code can be found in
https://github.com/jijiarui-cather/SRAPAgent_Framework
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilizing Large Language Models for Event Deconstruction to Enhance
  <span class="highlight-title">Multimodal</span> Aspect-Based Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyong Huang, Heli Sun, Qunshu Gao, Wenjie Huang, Ruichen Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of the internet, the richness of User-Generated
Contentcontinues to increase, making Multimodal Aspect-Based Sentiment Analysis
(MABSA) a research hotspot. Existing studies have achieved certain results in
MABSA, but they have not effectively addressed the analytical challenges in
scenarios where multiple entities and sentiments coexist. This paper
innovatively introduces Large Language Models (LLMs) for event decomposition
and proposes a reinforcement learning framework for Multimodal Aspect-based
Sentiment Analysis (MABSA-RL) framework. This framework decomposes the original
text into a set of events using LLMs, reducing the complexity of analysis,
introducing reinforcement learning to optimize model parameters. Experimental
results show that MABSA-RL outperforms existing advanced methods on two
benchmark datasets. This paper provides a new research perspective and method
for multimodal aspect-level sentiment analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in
  <span class="highlight-title">Vision-Language</span> Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhang Cui, An Zhang, Yiyang Zhou, Zhaorun Chen, Gelei Deng, Huaxiu Yao, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancements in large language models (LLMs) and pre-trained
vision models have accelerated the development of vision-language large models
(VLLMs), enhancing the interaction between visual and linguistic modalities.
Despite their notable success across various domains, VLLMs face challenges in
modality alignment, which can lead to issues like hallucinations and unsafe
content generation. Current alignment techniques often rely on coarse feedback
and external datasets, limiting scalability and performance. In this paper, we
propose FiSAO (Fine-Grained Self-Alignment Optimization), a novel
self-alignment method that utilizes the model's own visual encoder as a
fine-grained verifier to improve vision-language alignment without the need for
additional data. By leveraging token-level feedback from the vision encoder,
FiSAO significantly improves vision-language alignment, even surpassing
traditional preference tuning methods that require additional data. Through
both theoretical analysis and experimental validation, we demonstrate that
FiSAO effectively addresses the misalignment problem in VLLMs, marking the
first instance of token-level rewards being applied to such models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAPE: A Chinese Dataset for Appraisal-based Emotional Generation using
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        June M. Liu, He Cao, Renliang Sun, Rui Wang, Yu Li, Jiaxing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating emotionally appropriate responses in conversations with large
language models presents a significant challenge due to the complexities of
human emotions and cognitive processes, which remain largely underexplored in
their critical role in social interactions. In this study, we introduce a
two-stage automatic data generation framework to create CAPE, a Chinese dataset
named Cognitive Appraisal theory-based Emotional corpus. This corpus
facilitates the generation of dialogues with contextually appropriate emotional
responses by accounting for diverse personal and situational factors. We
propose two tasks utilizing this dataset: emotion prediction and next utterance
prediction. Both automated and human evaluations demonstrate that agents
trained on our dataset can deliver responses that are more aligned with human
emotional expressions. Our study shows the potential for advancing emotional
expression in conversational agents, paving the way for more nuanced and
meaningful human-computer interactions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Lightweight Multi Aspect Controlled Text Generation Solution For Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Zhang, Jiayi Lin, Haibo Tong, Bingxuan Hou, Dongyu Zhang, Jialin Li, Junli Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) show remarkable abilities with instruction
tuning. However, they fail to achieve ideal tasks when lacking high-quality
instruction tuning data on target tasks. Multi-Aspect Controllable Text
Generation (MCTG) is a representative task for this dilemma, where aspect
datasets are usually biased and correlated. Existing work exploits additional
model structures and strategies for solutions, limiting adaptability to LLMs.
To activate MCTG ability of LLMs, we propose a lightweight MCTG pipeline based
on data augmentation. We analyze bias and correlations in traditional datasets,
and address these concerns with augmented control attributes and sentences.
Augmented datasets are feasible for instruction tuning. In our experiments,
LLMs perform better in MCTG after data augmentation, with a 20% accuracy rise
and less aspect correlations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coherence-Driven <span class="highlight-title">Multimodal</span> Safety Dialogue with Active Learning for
  Embodied Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabit Hassan, Hye-Young Chung, Xiang Zhi Tan, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When assisting people in daily tasks, robots need to accurately interpret
visual cues and respond effectively in diverse safety-critical situations, such
as sharp objects on the floor. In this context, we present M-CoDAL, a
multimodal-dialogue system specifically designed for embodied agents to better
understand and communicate in safety-critical situations. The system leverages
discourse coherence relations to enhance its contextual understanding and
communication abilities. To train this system, we introduce a novel
clustering-based active learning mechanism that utilizes an external Large
Language Model (LLM) to identify informative instances. Our approach is
evaluated using a newly created multimodal dataset comprising 1K safety
violations extracted from 2K Reddit images. These violations are annotated
using a Large Multimodal Model (LMM) and verified by human annotators. Results
with this dataset demonstrate that our approach improves resolution of safety
situations, user sentiment, as well as safety of the conversation. Next, we
deploy our dialogue system on a Hello Robot Stretch robot and conduct a
within-subject user study with real-world participants. In the study,
participants role-play two safety scenarios with different levels of severity
with the robot and receive interventions from our model and a baseline system
powered by OpenAI's ChatGPT. The study results corroborate and extend the
findings from automated evaluation, showing that our proposed system is more
persuasive and competent in a real-world embodied agent setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViConsFormer: Constituting Meaningful Phrases of Scene Texts using
  Transformer-based Method in Vietnamese Text-based Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nghia Hieu Nguyen, Tho Thanh Quan, Ngan Luu-Thuy Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-based VQA is a challenging task that requires machines to use scene
texts in given images to yield the most appropriate answer for the given
question. The main challenge of text-based VQA is exploiting the meaning and
information from scene texts. Recent studies tackled this challenge by
considering the spatial information of scene texts in images via embedding 2D
coordinates of their bounding boxes. In this study, we follow the definition of
meaning from linguistics to introduce a novel method that effectively exploits
the information from scene texts written in Vietnamese. Experimental results
show that our proposed method obtains state-of-the-art results on two
large-scale Vietnamese Text-based VQA datasets. The implementation can be found
at this link.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoran Zhang, Yongxiang Li, Zijian Kan, Keyuan Cheng, Lijie Hu, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The locate-then-edit paradigm has shown significant promise for knowledge
editing (KE) in Large Language Models (LLMs). While previous methods perform
well on single-hop fact recall tasks, they consistently struggle with multi-hop
factual recall tasks involving newly edited knowledge. In this paper,
leveraging tools in mechanistic interpretability, we first identify that in
multi-hop tasks, LLMs tend to retrieve implicit subject knowledge from deeper
MLP layers, unlike single-hop tasks, which rely on earlier layers. This
distinction explains the poor performance of current methods in multi-hop
queries, as they primarily focus on editing shallow layers, leaving deeper
layers unchanged. To address this, we propose IFMET, a novel locate-then-edit
KE approach designed to edit both shallow and deep MLP layers. IFMET employs
multi-hop editing prompts and supplementary sets to locate and modify knowledge
across different reasoning stages. Experimental results demonstrate that IFMET
significantly improves performance on multi-hop factual recall tasks,
effectively overcoming the limitations of previous locate-then-edit methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ System 2 thinking in OpenAI's o1-preview model: Near-perfect performance
  on a mathematics exam 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joost de Winter, Dimitra Dodou, Yke Bauke Eisma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The processes underlying human cognition are often divided into System 1,
which involves fast, intuitive thinking, and System 2, which involves slow,
deliberate reasoning. Previously, large language models were criticized for
lacking the deeper, more analytical capabilities of System 2. In September
2024, OpenAI introduced the o1 model series, designed to handle System 2-like
reasoning. While OpenAI's benchmarks are promising, independent validation is
still needed. In this study, we tested the o1-preview model twice on the Dutch
'Mathematics B' final exam. It scored a near-perfect 76 and 74 out of 76
points. For context, only 24 out of 16,414 students in the Netherlands achieved
a perfect score. By comparison, the GPT-4o model scored 66 and 62 out of 76,
well above the Dutch average of 40.63 points. Neither model had access to the
exam figures. Since there was a risk of model contamination (i.e., the
knowledge cutoff of o1-preview and GPT-4o was after the exam was published
online), we repeated the procedure with a new Mathematics B exam that was
published after the cutoff date. The results again indicated that o1-preview
performed strongly (97.8th percentile), which suggests that contamination was
not a factor. We also show that there is some variability in the output of
o1-preview, which means that sometimes there is 'luck' (the answer is correct)
or 'bad luck' (the output has diverged into something that is incorrect). We
demonstrate that a self-consistency approach, where repeated prompts are given
and the most common answer is selected, is a useful strategy for identifying
the correct answer. It is concluded that while OpenAI's new model series holds
great potential, certain risks must be considered.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Liger Kernel: Efficient Triton Kernels for LLM Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10989v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10989v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni, Haowen Ning, Yanning Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training Large Language Models (LLMs) efficiently at scale presents a
formidable challenge, driven by their ever-increasing computational demands and
the need for enhanced performance. In this work, we introduce Liger-Kernel, an
open-sourced set of Triton kernels developed specifically for LLM training.
With kernel optimization techniques like kernel operation fusing and input
chunking, our kernels achieve on average a 20% increase in training throughput
and a 60% reduction in GPU memory usage for popular LLMs compared to
HuggingFace implementations. In addition, Liger-Kernel is designed with
modularity, accessibility, and adaptability in mind, catering to both casual
and expert users. Comprehensive benchmarks and integration tests are built in
to ensure compatibility, performance, correctness, and convergence across
diverse computing environments and model architectures.
  The source code is available under a permissive license at:
github.com/linkedin/Liger-Kernel.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Document Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02525v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02525v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John X. Morris, Alexander M. Rush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense document embeddings are central to neural retrieval. The dominant
paradigm is to train and construct embeddings by running encoders directly on
individual documents. In this work, we argue that these embeddings, while
effective, are implicitly out-of-context for targeted use cases of retrieval,
and that a contextualized document embedding should take into account both the
document and neighboring documents in context - analogous to contextualized
word embeddings. We propose two complementary methods for contextualized
document embeddings: first, an alternative contrastive learning objective that
explicitly incorporates the document neighbors into the intra-batch contextual
loss; second, a new contextual architecture that explicitly encodes neighbor
document information into the encoded representation. Results show that both
methods achieve better performance than biencoders in several settings, with
differences especially pronounced out-of-domain. We achieve state-of-the-art
results on the MTEB benchmark with no hard negative mining, score distillation,
dataset-specific instructions, intra-GPU example-sharing, or extremely large
batch sizes. Our method can be applied to improve performance on any
contrastive learning dataset and any biencoder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Linear Attention in Polynomial Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morris Yau, Ekin Akyürek, Jiayuan Mao, Joshua B. Tenenbaum, Stefanie Jegelka, Jacob Andreas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous research has explored the computational expressivity of Transformer
models in simulating Boolean circuits or Turing machines. However, the
learnability of these simulators from observational data has remained an open
question. Our study addresses this gap by providing the first polynomial-time
learnability results (specifically strong, agnostic PAC learning) for
single-layer Transformers with linear attention. We show that linear attention
may be viewed as a linear predictor in a suitably defined RKHS. As a
consequence, the problem of learning any linear transformer may be converted
into the problem of learning an ordinary linear predictor in an expanded
feature space, and any such predictor may be converted back into a multiheaded
linear transformer. Moving to generalization, we show how to efficiently
identify training datasets for which every empirical risk minimizer is
equivalent (up to trivial symmetries) to the linear Transformer that generated
the data, thereby guaranteeing the learned model will correctly generalize
across all inputs. Finally, we provide examples of computations expressible via
linear attention and therefore polynomial-time learnable, including associative
memories, finite automata, and a class of Universal Turing Machine (UTMs) with
polynomially bounded computation histories. We empirically validate our
theoretical findings on three tasks: learning random linear attention networks,
key--value associations, and learning to execute finite automata. Our findings
bridge a critical gap between theoretical expressivity and learnability of
Transformers, and show that flexible and general models of computation are
efficiently learnable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One size doesn't fit all: Predicting the Number of Examples for
  In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06402v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06402v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manish Chandra, Debasis Ganguly, Iadh Ounis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) refers to the process of adding a small number of
localized examples (ones that are semantically similar to the input) from a
training set of labelled data to an LLM's prompt with an objective to
effectively control the generative process seeking to improve the downstream
task performance. Existing ICL approaches use an identical number of examples
(a pre-configured hyper-parameter) for each data instance. Our work alleviates
the limitations of this 'one fits all' approach by dynamically predicting the
number of examples for each data instance to be used in few-shot inference with
LLMs. In particular, we employ a multi-label classifier, the parameters of
which are fitted using a training set, where the label for each instance in the
training set indicates if using a specific value of k (number of most similar
examples from 0 up to a maximum value) leads to correct k-shot downstream
predictions. Our experiments on a number of text classification benchmarks show
that AICL substantially outperforms standard ICL by up to 17%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Movie101v2: Improved Movie Narration Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Yue, Yepeng Zhang, Ziheng Wang, Qin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic movie narration aims to generate video-aligned plot descriptions to
assist visually impaired audiences. Unlike standard video captioning, it
involves not only describing key visual details but also inferring plots that
unfold across multiple movie shots, presenting distinct and complex challenges.
To advance this field, we introduce Movie101v2, a large-scale, bilingual
dataset with enhanced data quality specifically designed for movie narration.
Revisiting the task, we propose breaking down the ultimate goal of automatic
movie narration into three progressive stages, offering a clear roadmap with
corresponding evaluation metrics. Based on our new benchmark, we baseline a
range of large vision-language models, including GPT-4V, and conduct an
in-depth analysis of the challenges in narration generation. Our findings
highlight that achieving applicable movie narration generation is a fascinating
goal that requires significant research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MCQG-SRefine: Multiple Choice Question Generation and Evaluation with
  Iterative Self-Critique, Correction, and Comparison Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonghai Yao, Aditya Parashar, Huixue Zhou, Won Seok Jang, Feiyun Ouyang, Zhichao Yang, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic question generation (QG) is essential for AI and NLP, particularly
in intelligent tutoring, dialogue systems, and fact verification. Generating
multiple-choice questions (MCQG) for professional exams, like the United States
Medical Licensing Examination (USMLE), is particularly challenging, requiring
domain expertise and complex multi-hop reasoning for high-quality questions.
However, current large language models (LLMs) like GPT-4 struggle with
professional MCQG due to outdated knowledge, hallucination issues, and prompt
sensitivity, resulting in unsatisfactory quality and difficulty. To address
these challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique
and Correction) framework for converting medical cases into high-quality
USMLE-style questions. By integrating expert-driven prompt engineering with
iterative self-critique and self-correction feedback, MCQG-SRefine
significantly enhances human expert satisfaction regarding both the quality and
difficulty of the questions. Furthermore, we introduce an LLM-as-Judge-based
automatic metric to replace the complex and costly expert evaluation process,
ensuring reliable and expert-aligned assessments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Equal contribution for the first two authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advocating Character Error Rate for Multilingual ASR Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07400v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07400v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thennal D K, Jesin James, Deepa P Gopinath, Muhammed Ashraf K
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic speech recognition (ASR) systems have traditionally been evaluated
using English datasets, with the word error rate (WER) serving as the
predominant metric. WER's simplicity and ease of interpretation have
contributed to its widespread adoption, particularly for English. However, as
ASR systems expand to multilingual contexts, WER fails in various ways,
particularly with morphologically complex languages or those without clear word
boundaries. Our work documents the limitations of WER as an evaluation metric
and advocates for the character error rate (CER) as the primary metric in
multilingual ASR evaluation. We show that CER avoids many of the challenges WER
faces and exhibits greater consistency across writing systems. We support our
proposition by conducting human evaluations of ASR transcriptions in three
languages: Malayalam, English, and Arabic, which exhibit distinct morphological
characteristics. We show that CER correlates more closely with human judgments
than WER, even for English. To facilitate further research, we release our
human evaluation dataset for future benchmarking of ASR metrics. Our findings
suggest that CER should be prioritized, or at least supplemented, in
multilingual ASR evaluations to account for the varying linguistic
characteristics of different languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ English offensive text detection using CNN based Bi-GRU model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15652v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15652v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tonmoy Roy, Md Robiul Islam, Asif Ahammad Miazee, Anika Antara, Al Amin, Sunjim Hossain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the years, the number of users of social media has increased
drastically. People frequently share their thoughts through social platforms,
and this leads to an increase in hate content. In this virtual community,
individuals share their views, express their feelings, and post photos, videos,
blogs, and more. Social networking sites like Facebook and Twitter provide
platforms to share vast amounts of content with a single click. However, these
platforms do not impose restrictions on the uploaded content, which may include
abusive language and explicit images unsuitable for social media. To resolve
this issue, a new idea must be implemented to divide the inappropriate content.
Numerous studies have been done to automate the process. In this paper, we
propose a new Bi-GRU-CNN model to classify whether the text is offensive or
not. The combination of the Bi-GRU and CNN models outperforms the existing
model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages and 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Reward Models with Synthetic Critiques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20850v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20850v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihuiwen Ye, Fraser Greenlee-Scott, Max Bartolo, Phil Blunsom, Jon Ander Campos, Matthias Gallé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models (RMs) play a critical role in aligning language models through
the process of reinforcement learning from human feedback. RMs are trained to
predict a score reflecting human preference, which requires significant time
and cost for human annotation. Additionally, RMs tend to quickly overfit on
superficial features in the training set, hindering their generalization
performance on unseen distributions. We propose a novel approach using
synthetic natural language critiques generated by large language models to
provide additional feedback, evaluating aspects such as instruction following,
correctness, and style. This offers richer signals and more robust features for
RMs to assess and score on. We demonstrate that high-quality critiques improve
the performance and data efficiency of RMs initialized from different
pretrained models, reducing the reliance on costly human annotations.
Furthermore, incorporating critiques improves both the interpretability and
robustness of RM training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ With Ears to See and Eyes to Hear: Sound Symbolism Experiments with
  <span class="highlight-title">Multimodal</span> Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler Loakman, Yucheng Li, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Large Language Models (LLMs) and Vision Language Models (VLMs) have
demonstrated aptitude as potential substitutes for human participants in
experiments testing psycholinguistic phenomena. However, an understudied
question is to what extent models that only have access to vision and text
modalities are able to implicitly understand sound-based phenomena via abstract
reasoning from orthography and imagery alone. To investigate this, we analyse
the ability of VLMs and LLMs to demonstrate sound symbolism (i.e., to recognise
a non-arbitrary link between sounds and concepts) as well as their ability to
"hear" via the interplay of the language and vision modules of open and
closed-source multimodal models. We perform multiple experiments, including
replicating the classic Kiki-Bouba and Mil-Mal shape and magnitude symbolism
tasks, and comparing human judgements of linguistic iconicity with that of
LLMs. Our results show that VLMs demonstrate varying levels of agreement with
human labels, and more task information may be required for VLMs versus their
human counterparts for in silico experimentation. We additionally see through
higher maximum agreement levels that Magnitude Symbolism is an easier pattern
for VLMs to identify than Shape Symbolism, and that an understanding of
linguistic iconicity is highly dependent on model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Camera Ready)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Crossroads of Continents: Automated Artifact Extraction for Cultural
  Adaptation with Large <span class="highlight-title">Multimodal</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02067v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02067v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjishnu Mukherjee, Ziwei Zhu, Antonios Anastasopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a comprehensive three-phase study to examine (1) the cultural
understanding of Large Multimodal Models (LMMs) by introducing DalleStreet, a
large-scale dataset generated by DALL-E 3 and validated by humans, containing
9,935 images of 67 countries and 10 concept classes; (2) the underlying
implicit and potentially stereotypical cultural associations with a cultural
artifact extraction task; and (3) an approach to adapt cultural representation
in an image based on extracted associations using a modular pipeline,
CultureAdapt. We find disparities in cultural understanding at geographic
sub-region levels with both open-source (LLaVA) and closed-source (GPT-4V)
models on DalleStreet and other existing benchmarks, which we try to understand
using over 18,000 artifacts that we identify in association to different
countries. Our findings reveal a nuanced picture of the cultural competence of
LMMs, highlighting the need to develop culture-aware systems. Dataset and code
are available at https://github.com/iamshnoo/crossroads
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What's under the hood: Investigating Automatic Metrics on Meeting
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11124v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11124v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederic Kirstein, Jan Philip Wahle, Terry Ruas, Bela Gipp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meeting summarization has become a critical task considering the increase in
online interactions. While new techniques are introduced regularly, their
evaluation uses metrics not designed to capture meeting-specific errors,
undermining effective evaluation. This paper investigates what the frequently
used automatic metrics capture and which errors they mask by correlating
automatic metric scores with human evaluations across a broad error taxonomy.
We commence with a comprehensive literature review on English meeting
summarization to define key challenges like speaker dynamics and contextual
turn-taking and error types such as missing information and linguistic
inaccuracy, concepts previously loosely defined in the field. We examine the
relationship between characteristic challenges and errors by using annotated
transcripts and summaries from Transformer-based sequence-to-sequence and
autoregressive models from the general summary QMSum dataset. Through
experimental validation, we find that different model architectures respond
variably to challenges in meeting transcripts, resulting in different
pronounced links between challenges and errors. Current default-used metrics
struggle to capture observable errors, showing weak to mid-correlations, while
a third of the correlations show trends of error masking. Only a subset reacts
accurately to specific errors, while most correlations show either
unresponsiveness or failure to reflect the error's impact on summary quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Debiasing Text Embeddings Through Context Injection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12874v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12874v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Uriot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current advances in Natural Language Processing (NLP) have made it
increasingly feasible to build applications leveraging textual data. Generally,
the core of these applications rely on having a good semantic representation of
text into vectors, via embedding models. However, it has been shown that these
embeddings capture and perpetuate biases already present in text. While a few
techniques have been proposed to debias embeddings, they do not take advantage
of the recent advances in context understanding of modern embedding models. In
this paper, we fill this gap by conducting a review of 19 embedding models by
quantifying their biases and how well they respond to context injection as a
mean of debiasing. We show that higher performing models are more prone to
capturing biases, but are also better at incorporating context. Surprisingly,
we find that while models can easily embed affirmative semantics, they fail at
embedding neutral semantics. Finally, in a retrieval task, we show that biases
in embeddings can lead to non-desirable outcomes. We use our new-found insights
to design a simple algorithm for top $k$ retrieval, where $k$ is dynamically
selected. We show that our algorithm is able to retrieve all relevant gendered
and neutral chunks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Train & Constrain: Phonologically Informed Tongue-Twister Generation
  from Topics and Paraphrases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13901v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13901v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler Loakman, Chen Tang, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous work in phonologically and phonetically grounded language generation
has mainly focused on domains such as puns and poetry. In this article, we
present new work on the generation of English tongue twisters - a form of
language that is required to be conditioned on a phoneme level to maximize
sound overlap, while maintaining semantic consistency with an input topic or
phrase and still being grammatically correct. We present TwisterLister, a
pipeline for generating phonologically informed tongue twisters from large
language models (LLMs) that we use to generate TwistList 2.0, the largest
annotated dataset of tongue twisters to date, consisting of 17K+ examples from
a combination of human and LLM authors. Our generation pipeline involves the
use of a phonologically constrained vocabulary alongside LLM prompting to
generate novel, non-derivative tongue twister examples. We additionally present
the results of automatic and human evaluation of smaller models trained on our
generated dataset to demonstrate the extent to which phonologically motivated
language types can be generated without explicit injection of phonological
knowledge. Additionally, we introduce a phoneme-aware constrained decoding
module (PACD) that can be integrated into an autoregressive language model and
demonstrate that this method generates good quality tongue twisters both with
and without fine-tuning the underlying language model. We also design and
implement a range of automatic metrics for the task of tongue twister
generation that is phonologically motivated and captures the unique essence of
tongue twisters, primarily based on phonemic edit distance (PED)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted Final Version to Computational Linguistics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Error Span Annotation: A Balanced Approach for Human Evaluation of
  Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11580v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11580v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Kocmi, Vilém Zouhar, Eleftherios Avramidis, Roman Grundkiewicz, Marzena Karpinska, Maja Popović, Mrinmaya Sachan, Mariya Shmatova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality Machine Translation (MT) evaluation relies heavily on human
judgments. Comprehensive error classification methods, such as Multidimensional
Quality Metrics (MQM), are expensive as they are time-consuming and can only be
done by experts, whose availability may be limited especially for low-resource
languages. On the other hand, just assigning overall scores, like Direct
Assessment (DA), is simpler and faster and can be done by translators of any
level, but is less reliable. In this paper, we introduce Error Span Annotation
(ESA), a human evaluation protocol which combines the continuous rating of DA
with the high-level error severity span marking of MQM. We validate ESA by
comparing it to MQM and DA for 12 MT systems and one human reference
translation (English to German) from WMT23. The results show that ESA offers
faster and cheaper annotations than MQM at the same quality level, without the
requirement of expensive MQM experts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BlackDAN: A Black-Box Multi-Objective Approach for Effective and
  Contextual Jailbreaking of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09804v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09804v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Wang, Victor Shea-Jay Huang, Renmiao Chen, Hao Wang, Chengwei Pan, Lei Sha, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) exhibit remarkable capabilities across
various tasks, they encounter potential security risks such as jailbreak
attacks, which exploit vulnerabilities to bypass security measures and generate
harmful outputs. Existing jailbreak strategies mainly focus on maximizing
attack success rate (ASR), frequently neglecting other critical factors,
including the relevance of the jailbreak response to the query and the level of
stealthiness. This narrow focus on single objectives can result in ineffective
attacks that either lack contextual relevance or are easily recognizable. In
this work, we introduce BlackDAN, an innovative black-box attack framework with
multi-objective optimization, aiming to generate high-quality prompts that
effectively facilitate jailbreaking while maintaining contextual relevance and
minimizing detectability. BlackDAN leverages Multiobjective Evolutionary
Algorithms (MOEAs), specifically the NSGA-II algorithm, to optimize jailbreaks
across multiple objectives including ASR, stealthiness, and semantic relevance.
By integrating mechanisms like mutation, crossover, and Pareto-dominance,
BlackDAN provides a transparent and interpretable process for generating
jailbreaks. Furthermore, the framework allows customization based on user
preferences, enabling the selection of prompts that balance harmfulness,
relevance, and other factors. Experimental results demonstrate that BlackDAN
outperforms traditional single-objective methods, yielding higher success rates
and improved robustness across various LLMs and multimodal LLMs, while ensuring
jailbreak responses are both relevant and less detectable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Retrieval in Sponsored Search by Leveraging Query Context
  Signals <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14346v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14346v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akash Kumar Mohankumar, Gururaj K, Gagan Madan, Amit Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately retrieving relevant bid keywords for user queries is critical in
Sponsored Search but remains challenging, particularly for short, ambiguous
queries. Existing dense and generative retrieval models often fail to capture
nuanced user intent in these cases. To address this, we propose an approach to
enhance query understanding by augmenting queries with rich contextual signals
derived from web search results and large language models, stored in an online
cache. Specifically, we use web search titles and snippets to ground queries in
real-world information and utilize GPT-4 to generate query rewrites and
explanations that clarify user intent. These signals are efficiently integrated
through a Fusion-in-Decoder based Unity architecture, enabling both dense and
generative retrieval with serving costs on par with traditional context-free
models. To address scenarios where context is unavailable in the cache, we
introduce context glancing, a curriculum learning strategy that improves model
robustness and performance even without contextual signals during inference.
Extensive offline experiments demonstrate that our context-aware approach
substantially outperforms context-free models. Furthermore, online A/B testing
on a prominent search engine across 160+ countries shows significant
improvements in user engagement and revenue.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Internals-based Answer Attribution for Trustworthy
  Retrieval-Augmented Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13663v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13663v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jirui Qi, Gabriele Sarti, Raquel Fernández, Arianna Bisazza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the verifiability of model answers is a fundamental challenge for
retrieval-augmented generation (RAG) in the question answering (QA) domain.
Recently, self-citation prompting was proposed to make large language models
(LLMs) generate citations to supporting documents along with their answers.
However, self-citing LLMs often struggle to match the required format, refer to
non-existent sources, and fail to faithfully reflect LLMs' context usage
throughout the generation. In this work, we present MIRAGE --Model
Internals-based RAG Explanations -- a plug-and-play approach using model
internals for faithful answer attribution in RAG applications. MIRAGE detects
context-sensitive answer tokens and pairs them with retrieved documents
contributing to their prediction via saliency methods. We evaluate our proposed
approach on a multilingual extractive QA dataset, finding high agreement with
human answer attribution. On open-ended QA, MIRAGE achieves citation quality
and efficiency comparable to self-citation while also allowing for a
finer-grained control of attribution parameters. Our qualitative evaluation
highlights the faithfulness of MIRAGE's attributions and underscores the
promising application of model internals for RAG answer attribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 Main Conference. Code and data released at
  https://github.com/Betswish/MIRAGE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI:
  The First Romanian Natural Language Inference Corpus <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11877v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11877v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduard Poesina, Cornelia Caragea, Radu Tudor Ionescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language inference (NLI), the task of recognizing the entailment
relationship in sentence pairs, is an actively studied topic serving as a proxy
for natural language understanding. Despite the relevance of the task in
building conversational agents and improving text classification, machine
translation and other NLP tasks, to the best of our knowledge, there is no
publicly available NLI corpus for the Romanian language. To this end, we
introduce the first Romanian NLI corpus (RoNLI) comprising 58K training
sentence pairs, which are obtained via distant supervision, and 6K validation
and test sentence pairs, which are manually annotated with the correct labels.
We conduct experiments with multiple machine learning methods based on distant
learning, ranging from shallow models based on word embeddings to
transformer-based neural networks, to establish a set of competitive baselines.
Furthermore, we improve on the best model by employing a new curriculum
learning strategy based on data cartography. Our dataset and code to reproduce
the baselines are available at https://github.com/Eduard6421/RONLI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2024 (Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Lifelong Dialogue Agents via Relation-aware Memory Construction
  and Timeline-augmented Response Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Tzu-iunn Ong, Namyoung Kim, Minju Gwak, Hyungjoo Chae, Taeyoon Kwon, Yohan Jo, Seung-won Hwang, Dongha Lee, Jinyoung Yeo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve lifelong human-agent interaction, dialogue agents need to
constantly memorize perceived information and properly retrieve it for response
generation (RG). While prior work focuses on getting rid of outdated memories
to improve retrieval quality, we argue that such memories provide rich,
important contextual cues for RG (e.g., changes in user behaviors) in long-term
conversations. We present Theanine, a framework for LLM-based lifelong dialogue
agents. Theanine discards memory removal and manages large-scale memories by
linking them based on their temporal and cause-effect relation. Enabled by this
linking structure, Theanine augments RG with memory timelines - series of
memories representing the evolution or causality of relevant past events. Along
with Theanine, we introduce TeaFarm, a counterfactual-driven evaluation scheme,
addressing the limitation of G-Eval and human efforts in measuring
memory-augmented dialogue agents. A supplementary video for Theanine and data
for TeaFarm are at https://huggingface.co/spaces/ResearcherScholar/Theanine.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models, scientific knowledge and factuality: A framework
  to streamline human expert evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17819v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17819v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Magdalena Wysocka, Oskar Wysocki, Maxime Delmas, Vincent Mutel, Andre Freitas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper introduces a framework for the evaluation of the encoding of
factual scientific knowledge, designed to streamline the manual evaluation
process typically conducted by domain experts. Inferring over and extracting
information from Large Language Models (LLMs) trained on a large corpus of
scientific literature can potentially define a step change in biomedical
discovery, reducing the barriers for accessing and integrating existing medical
evidence. This work explores the potential of LLMs for dialoguing with
biomedical background knowledge, using the context of antibiotic discovery. The
framework involves of three evaluation steps, each assessing different aspects
sequentially: fluency, prompt alignment, semantic coherence, factual knowledge,
and specificity of the generated responses. By splitting these tasks between
non-experts and experts, the framework reduces the effort required from the
latter. The work provides a systematic assessment on the ability of eleven
state-of-the-art models LLMs, including ChatGPT, GPT-4 and Llama 2, in two
prompting-based tasks: chemical compound definition generation and chemical
compound-fungus relation determination. Although recent models have improved in
fluency, factual accuracy is still low and models are biased towards
over-represented entities. The ability of LLMs to serve as biomedical knowledge
bases is questioned, and the need for additional systematic evaluation
frameworks is highlighted. While LLMs are currently not fit for purpose to be
used as biomedical factual knowledge bases in a zero-shot setting, there is a
promising emerging property in the direction of factuality as the models become
domain specialised, scale-up in size and level of human feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Journal of Biomedical Informatics, Volume 158,
  October 2024, 104724</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-LLM QA with Embodied Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10918v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10918v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhrij Patel, Vishnu Sashank Dorbala, Amrit Singh Bedi, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have grown in popularity due to their natural
language interface and pre trained knowledge, leading to rapidly increasing
success in question-answering (QA) tasks. More recently, multi-agent systems
with LLM-based agents (Multi-LLM) have been utilized increasingly more for QA.
In these scenarios, the models may each answer the question and reach a
consensus or each model is specialized to answer different domain questions.
However, most prior work dealing with Multi-LLM QA has focused on scenarios
where the models are asked in a zero-shot manner or are given information
sources to extract the answer. For question answering of an unknown
environment, embodied exploration of the environment is first needed to answer
the question. This skill is necessary for personalizing embodied AI to
environments such as households. There is a lack of insight into whether a
Multi-LLM system can handle question-answering based on observations from
embodied exploration. In this work, we address this gap by investigating the
use of Multi-Embodied LLM Explorers (MELE) for QA in an unknown environment.
Multiple LLM-based agents independently explore and then answer queries about a
household environment. We analyze different aggregation methods to generate a
single, final answer for each query: debating, majority voting, and training a
central answer module (CAM). Using CAM, we observe a $46\%$ higher accuracy
compared against the other non-learning-based aggregation methods. We provide
code and the query dataset for further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 Figures, 5 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular
  Property Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyan Liu, Sirui Ding, Sheng Zhou, Wenqi Fan, Qiaoyu Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Molecular property prediction (MPP) is a fundamental and crucial task in drug
discovery. However, prior methods are limited by the requirement for a large
number of labeled molecules and their restricted ability to generalize for
unseen and new tasks, both of which are essential for real-world applications.
To address these challenges, we present MolecularGPT for few-shot MPP. From a
perspective on instruction tuning, we fine-tune large language models (LLMs)
based on curated molecular instructions spanning over 1000 property prediction
tasks. This enables building a versatile and specialized LLM that can be
adapted to novel MPP tasks without any fine-tuning through zero- and few-shot
in-context learning (ICL). MolecularGPT exhibits competitive in-context
reasoning capabilities across 10 downstream evaluation datasets, setting new
benchmarks for few-shot molecular prediction tasks. More importantly, with just
two-shot examples, MolecularGPT can outperform standard supervised graph neural
network methods on 4 out of 7 datasets. It also excels state-of-the-art LLM
baselines by up to 15.7% increase on classification accuracy and decrease of
17.9 on regression metrics (e.g., RMSE) under zero-shot. This study
demonstrates the potential of LLMs as effective few-shot molecular property
predictors. The code is available at https://github.com/NYUSHCS/MolecularGPT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Fundamental Trade-off in Aligned Language Models and its Relation to
  Sampling Adaptors <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10203v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10203v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naaman Tan, Josef Valvoda, Tianyu Liu, Anej Svete, Yanxia Qin, Kan Min-Yen, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The relationship between the quality of a string, as judged by a human
reader, and its probability, $p(\boldsymbol{y})$ under a language model
undergirds the development of better language models. For example, many popular
algorithms for sampling from a language model have been conceived with the goal
of manipulating $p(\boldsymbol{y})$ to place higher probability on strings that
humans deem of high quality. In this article, we examine the
probability--quality relationship in language models explicitly aligned to
human preferences, e.g., through reinforcement learning through human feedback.
We show that, when sampling corpora from an aligned language model, there
exists a trade-off between the strings' average reward and average
log-likelihood under the prior language model, i.e., the same model before
alignment with human preferences. We provide a formal treatment of this
phenomenon and demonstrate how a choice of sampling adaptor allows for a
selection of how much likelihood we exchange for the reward.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entity Matching using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11244v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11244v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ralph Peeters, Aaron Steiner, Christian Bizer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity matching is the task of deciding whether two entity descriptions refer
to the same real-world entity. Entity matching is a central step in most data
integration pipelines. Many state-of-the-art entity matching methods rely on
pre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks
of these models for entity matching are that (i) the models require significant
amounts of task-specific training data and (ii) the fine-tuned models are not
robust concerning out-of-distribution entities. This paper investigates using
generative large language models (LLMs) as a less task-specific training
data-dependent and more robust alternative to PLM-based matchers. The study
covers hosted and open-source LLMs which can be run locally. We evaluate these
models in a zero-shot scenario and a scenario where task-specific training data
is available. We compare different prompt designs and the prompt sensitivity of
the models. We show that there is no single best prompt but that the prompt
needs to be tuned for each model/dataset combination. We further investigate
(i) the selection of in-context demonstrations, (ii) the generation of matching
rules, as well as (iii) fine-tuning LLMs using the same pool of training data.
Our experiments show that the best LLMs require no or only a few training
examples to perform comparably to PLMs that were fine-tuned using thousands of
examples. LLM-based matchers further exhibit higher robustness to unseen
entities. We show that GPT4 can generate structured explanations for matching
decisions and can automatically identify potential causes of matching errors by
analyzing explanations of wrong decisions. We demonstrate that the model can
generate meaningful textual descriptions of the identified error classes, which
can help data engineers to improve entity matching pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Proceedings of the 28th International Conference on
  Extending Database Technology (EDBT), 25th March-28th March, 2025, ISBN
  978-3-89318-098-1 on OpenProceedings.org</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 2D-TPE: Two-Dimensional Positional Encoding Enhances Table Understanding
  for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19700v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19700v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Nan Li, Jian Guan, Wei Wu, Zhengtao Yu, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tables are ubiquitous across various domains for concisely representing
structured information. Empowering large language models (LLMs) to reason over
tabular data represents an actively explored direction. However, since typical
LLMs only support one-dimensional~(1D) inputs, existing methods often flatten
the two-dimensional~(2D) table structure into a sequence of tokens, which can
severely disrupt the spatial relationships and result in an inevitable loss of
vital contextual information. In this paper, we first empirically demonstrate
the detrimental impact of such flattening operations on the performance of LLMs
in capturing the spatial information of tables through two elaborate proxy
tasks. Subsequently, we introduce a simple yet effective positional encoding
method, termed ``2D-TPE'' (Two-Dimensional Table Positional Encoding), to
address this challenge. 2D-TPE enables each attention head to dynamically
select a permutation order of tokens within the context for attending to them,
where each permutation represents a distinct traversal mode for the table, such
as column-wise or row-wise traversal. 2D-TPE effectively mitigates the risk of
losing essential spatial information while preserving computational efficiency,
thus better preserving the table structure. Extensive experiments across five
benchmarks demonstrate that 2D-TPE outperforms strong baselines, underscoring
the importance of preserving the table structure for accurate table
comprehension. Comprehensive analysis further reveals the substantially better
scalability of 2D-TPE to large tables than baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FAME: Towards Factual Multi-Task Model Editing <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10859v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10859v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zeng, Yingyu Shan, Zeming Liu, Jiashu Yao, Yuhang Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) embed extensive knowledge and utilize it to
perform exceptionally well across various tasks. Nevertheless, outdated
knowledge or factual errors within LLMs can lead to misleading or incorrect
responses, causing significant issues in practical applications. To rectify the
fatal flaw without the necessity for costly model retraining, various model
editing approaches have been proposed to correct inaccurate knowledge within
LLMs in a cost-efficient way. To evaluate these model editing methods, previous
work introduced a series of datasets. However, most of the previous datasets
only contain fabricated data in a single format, which diverges from real-world
model editing scenarios, raising doubts about their usability in practice. To
facilitate the application of model editing in real-world scenarios, we propose
the challenge of practicality. To resolve such challenges and effectively
enhance the capabilities of LLMs, we present FAME, an factual, comprehensive,
and multi-task dataset, which is designed to enhance the practicality of model
editing. We then propose SKEME, a model editing method that uses a novel
caching mechanism to ensure synchronization with the real world. The
experiments demonstrate that SKEME performs excellently across various tasks
and scenarios, confirming its practicality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures. This paper has been accepted by EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span> Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minsu Kim, Hyung-Il Kim, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Speech Recognition (VSR) aims to infer speech into text depending on
lip movements alone. As it focuses on visual information to model the speech,
its performance is inherently sensitive to personal lip appearances and
movements, and this makes the VSR models show degraded performance when they
are applied to unseen speakers. In this paper, to remedy the performance
degradation of the VSR model on unseen speakers, we propose prompt tuning
methods of Deep Neural Networks (DNNs) for speaker-adaptive VSR. Specifically,
motivated by recent advances in Natural Language Processing (NLP), we finetune
prompts on adaptation data of target speakers instead of modifying the
pre-trained model parameters. Different from the previous prompt tuning methods
mainly limited to Transformer variant architecture, we explore different types
of prompts, the addition, the padding, and the concatenation form prompts that
can be applied to the VSR model which is composed of CNN and Transformer in
general. With the proposed prompt tuning, we show that the performance of the
pre-trained VSR model on unseen speakers can be largely improved by using a
small amount of adaptation data (e.g., less than 5 minutes), even if the
pre-trained model is already developed with large speaker variations. Moreover,
by analyzing the performance and parameters of different types of prompts, we
investigate when the prompt tuning is preferred over the finetuning methods.
The effectiveness of the proposed method is evaluated on both word- and
sentence-level VSR databases, LRW-ID and GRID.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BANTH: A Multi-label Hate Speech Detection Dataset for Transliterated
  Bangla 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13281v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13281v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabiha Haider, Fariha Tanjim Shifat, Md Farhan Ishmam, Deeparghya Dutta Barua, Md Sakib Ul Rahman Sourove, Md Fahim, Md Farhad Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of transliterated texts in digital spaces has emphasized
the need for detecting and classifying hate speech in languages beyond English,
particularly in low-resource languages. As online discourse can perpetuate
discrimination based on target groups, e.g. gender, religion, and origin,
multi-label classification of hateful content can help in comprehending hate
motivation and enhance content moderation. While previous efforts have focused
on monolingual or binary hate classification tasks, no work has yet addressed
the challenge of multi-label hate speech classification in transliterated
Bangla. We introduce BanTH, the first multi-label transliterated Bangla hate
speech dataset comprising 37.3k samples. The samples are sourced from YouTube
comments, where each instance is labeled with one or more target groups,
reflecting the regional demographic. We establish novel transformer
encoder-based baselines by further pre-training on transliterated Bangla
corpus. We also propose a novel translation-based LLM prompting strategy for
transliterated text. Experiments reveal that our further pre-trained encoders
are achieving state-of-the-art performance on the BanTH dataset, while our
translation-based prompting outperforms other strategies in the zero-shot
setting. The introduction of BanTH not only fills a critical gap in hate speech
research for Bangla but also sets the stage for future exploration into
code-mixed and multi-label classification challenges in underrepresented
languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Likelihood Over-optimisation in Direct Alignment
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyan Shi, Sander Land, Acyr Locatelli, Matthieu Geist, Max Bartolo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Alignment Algorithms (DAAs), such as Direct Preference Optimisation
(DPO) and Identity Preference Optimisation (IPO), have emerged as alternatives
to online Reinforcement Learning from Human Feedback (RLHF) algorithms such as
Proximal Policy Optimisation (PPO) for aligning language models to human
preferences, without the need for explicit reward modelling. These methods
generally aim to increase the likelihood of generating better (preferred)
completions while discouraging worse (non-preferred) ones, while staying close
to the original model's behaviour. In this work, we explore the relationship
between completion likelihood and model performance in state-of-the-art DAAs,
and identify a critical issue of likelihood over-optimisation. Contrary to
expectations, we find that higher likelihood of better completions and larger
margins between better and worse completion likelihoods do not necessarily lead
to better performance, and may even degrade it. Our analysis reveals that while
higher likelihood correlates with better memorisation of factual knowledge
patterns, a slightly lower completion likelihood tends to improve output
diversity, thus leading to better generalisation to unseen scenarios. Moreover,
we identify two key indicators that signal when over-optimised output diversity
begins to harm performance: Decreasing Entropy over Top-k Tokens and
Diminishing Top-k Probability Mass. Our experimental results validate that
these indicators are reliable signs of declining performance under different
regularisations, helping prevent over-optimisation and improve alignment with
human preferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MaiBaam Annotation Guidelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Verena Blaschke, Barbara Kovačić, Siyao Peng, Barbara Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This document provides the annotation guidelines for MaiBaam, a Bavarian
corpus manually annotated with part-of-speech (POS) tags, syntactic
dependencies, and German lemmas. MaiBaam belongs to the Universal Dependencies
(UD) project, and our annotations elaborate on the general and German UD
version 2 guidelines. In this document, we detail how to preprocess and
tokenize Bavarian data, provide an overview of the POS tags and dependencies we
use, explain annotation decisions that would also apply to closely related
languages like German, and lastly we introduce and motivate decisions that are
specific to Bavarian grammar.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated for UD v2.15 (German lemmas added)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangru Zhu, Penglei Sun, Yaoxian Song, Yanghua Xiao, Zhixu Li, Chengyu Wang, Jun Huang, Bei Yang, Xiaoxiao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate interpretation and visualization of human instructions are crucial
for text-to-image (T2I) synthesis. However, current models struggle to capture
semantic variations from word order changes, and existing evaluations, relying
on indirect metrics like text-image similarity, fail to reliably assess these
challenges. This often obscures poor performance on complex or uncommon
linguistic patterns by the focus on frequent word combinations. To address
these deficiencies, we propose a novel metric called SemVarEffect and a
benchmark named SemVarBench, designed to evaluate the causality between
semantic variations in inputs and outputs in T2I synthesis. Semantic variations
are achieved through two types of linguistic permutations, while avoiding
easily predictable literal variations. Experiments reveal that the
CogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.
Semantic variations in object relations are less understood than attributes,
scoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in
UNet or Transformers plays a crucial role in handling semantic variations, a
factor previously overlooked by a focus on textual encoders. Our work
establishes an effective evaluation framework that advances the T2I synthesis
community's exploration of human instruction understanding. Our benchmark and
code are available at https://github.com/zhuxiangru/SemVarBench .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The only change in the current version update is the replacement of
  the template with a more precise one</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I run as fast as a rabbit, can you? A Multilingual Simile Dialogue
  Dataset <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05672v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05672v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longxuan Ma, Weinan Zhang, Shuhan Zhou, Churui Sun, Changxin Ke, Ting Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A simile is a figure of speech that compares two different things (called the
tenor and the vehicle) via shared properties. The tenor and the vehicle are
usually connected with comparator words such as "like" or "as". The simile
phenomena are unique and complex in a real-life dialogue scene where the tenor
and the vehicle can be verbal phrases or sentences, mentioned by different
speakers, exist in different sentences, or occur in reversed order. However,
the current simile research usually focuses on similes in a triplet tuple
(tenor, property, vehicle) or a single sentence where the tenor and vehicle are
usually entities or noun phrases, which could not reflect complex simile
phenomena in real scenarios. In this paper, we propose a novel and high-quality
multilingual simile dialogue (MSD) dataset to facilitate the study of complex
simile phenomena. The MSD is the largest manually annotated simile data
($\sim$20K) and it contains both English and Chinese data. Meanwhile, the MSD
data can also be used on dialogue tasks to test the ability of dialogue systems
when using similes. We design 3 simile tasks (recognition, interpretation, and
generation) and 2 dialogue tasks (retrieval and generation) with MSD. For each
task, we provide experimental results from strong pre-trained or
state-of-the-art models. The experiments demonstrate the challenge of MSD and
we have released the data/code on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 Pages, 1 Figure, 12 Tables, ACL 2023 findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Few-shot Work in Long-Context? Recycling the Context to Generate
  Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13632v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13632v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arie Cattan, Alon Jacovi, Alex Fabrikant, Jonathan Herzig, Roee Aharoni, Hannah Rashkin, Dror Marcus, Avinatan Hassidim, Yossi Matias, Idan Szpektor, Avi Caciularu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advancements in Large Language Models (LLMs), their
performance on tasks involving long contexts remains sub-optimal. In-Context
Learning (ICL) with few-shot examples may be an appealing solution to enhance
LLM performance in this scenario; However, na\"ively adding ICL examples with
long context introduces challenges, including substantial token overhead added
for each few-shot example and context mismatch between the demonstrations and
the target query. In this work, we propose to automatically generate few-shot
examples for long context QA tasks by recycling contexts. Specifically, given a
long input context (1-3k tokens) and a query, we generate additional
query-output pairs from the given context as few-shot examples, while
introducing the context only once. This ensures that the demonstrations are
leveraging the same context as the target query while only adding a small
number of tokens to the prompt. We further enhance each demonstration by
instructing the model to explicitly identify the relevant paragraphs before the
answer, which improves performance while providing fine-grained attribution to
the answer source. We apply our method on multiple LLMs and obtain substantial
improvements (+16 absolute points on average across models) on various QA
datasets with long context, especially when the answer lies within the middle
of the context. Surprisingly, despite introducing only single-hop ICL examples,
LLMs also successfully generalize to multi-hop long-context QA using our
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Verifiable Text Generation with Evolving Memory and
  Self-Reflection <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09075v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09075v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Sun, Hengyi Cai, Bo Wang, Yingyan Hou, Xiaochi Wei, Shuaiqiang Wang, Yan Zhang, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable ability of large language models (LLMs) in language
comprehension and generation, they often suffer from producing factually
incorrect information, also known as hallucination. A promising solution to
this issue is verifiable text generation, which prompts LLMs to generate
content with citations for accuracy verification. However, verifiable text
generation is non-trivial due to the focus-shifting phenomenon, the intricate
reasoning needed to align the claim with correct citations, and the dilemma
between the precision and breadth of retrieved documents. In this paper, we
present VTG, an innovative framework for Verifiable Text Generation with
evolving memory and self-reflection. VTG introduces evolving long short-term
memory to retain both valuable documents and recent documents. A two-tier
verifier equipped with an evidence finder is proposed to rethink and reflect on
the relationship between the claim and citations. Furthermore, active retrieval
and diverse query generation are utilized to enhance both the precision and
breadth of the retrieved documents. We conduct extensive experiments on five
datasets across three knowledge-intensive tasks and the results reveal that VTG
significantly outperforms baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing Webpage UIs for Text-Rich Visual Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13824v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13824v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, Xiang Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-rich visual understanding-the ability to process environments where
dense textual content is integrated with visuals-is crucial for multimodal
large language models (MLLMs) to interact effectively with structured
environments. To enhance this capability, we propose synthesizing general
multimodal instructions from webpage UIs using text-based large language models
(LLMs). Despite lacking direct visual input, text-based LLMs are able to
process structured text representations from webpage accessibility trees. These
instructions are then paired with UI screenshots to train multimodal models. We
introduce MultiUI, a dataset containing 7.3 million samples from 1 million
websites, covering diverse multimodal tasks and UI layouts. Models trained on
MultiUI not only excel in web UI tasks-achieving up to a 48% improvement on
VisualWebBench and a 19.1% boost in element accuracy on a web agent dataset
Mind2Web-but also generalize surprisingly well to non-web UI tasks and even to
non-UI domains, such as document understanding, OCR, and chart interpretation.
These results highlight the broad applicability of web UI data for advancing
text-rich visual understanding across various scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dating ancient manuscripts using radiocarbon and AI-based writing style
  analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12013v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12013v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mladen Popović, Maruf A. Dhali, Lambert Schomaker, Johannes van der Plicht, Kaare Lund Rasmussen, Jacopo La Nasa, Ilaria Degano, Maria Perla Colombini, Eibert Tigchelaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining the chronology of ancient handwritten manuscripts is essential
for reconstructing the evolution of ideas. For the Dead Sea Scrolls, this is
particularly important. However, there is an almost complete lack of
date-bearing manuscripts evenly distributed across the timeline and written in
similar scripts available for palaeographic comparison. Here, we present Enoch,
a state-of-the-art AI-based date-prediction model, trained on the basis of new
radiocarbon-dated samples of the scrolls. Enoch uses established
handwriting-style descriptors and applies Bayesian ridge regression. The
challenge of this study is that the number of radiocarbon-dated manuscripts is
small, while current machine learning requires an abundance of training data.
We show that by using combined angular and allographic writing style feature
vectors and applying Bayesian ridge regression, Enoch could predict the
radiocarbon-based dates from style, supported by leave-one-out validation, with
varied MAEs of 27.9 to 30.7 years relative to the radiocarbon dating. Enoch was
then used to estimate the dates of 135 unseen manuscripts, revealing that 79
per cent of the samples were considered 'realistic' upon palaeographic post-hoc
evaluation. We present a new chronology of the scrolls. The radiocarbon ranges
and Enoch's style-based predictions are often older than the traditionally
assumed palaeographic estimates. In the range of 300-50 BCE, Enoch's date
prediction provides an improved granularity. The study is in line with current
developments in multimodal machine-learning techniques, and the methods can be
used for date prediction in other partially-dated manuscript collections. This
research shows how Enoch's quantitative, probability-based approach can be a
tool for palaeographers and historians, re-dating ancient Jewish key texts and
contributing to current debates on Jewish and Christian origins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages of main article, 103 pages of supplementary materials; the
  first version of this article is originally prepared in July 2023 after the
  completion of all the experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conversational Recommender System and Large Language Model Are Made for
  Each Other in E-commerce Pre-sales Dialogue <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14626v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14626v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanxing Liu, Wei-Nan Zhang, Yifan Chen, Yuchi Zhang, Haopeng Bai, Fan Feng, Hengbin Cui, Yongbin Li, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  E-commerce pre-sales dialogue aims to understand and elicit user needs and
preferences for the items they are seeking so as to provide appropriate
recommendations. Conversational recommender systems (CRSs) learn user
representation and provide accurate recommendations based on dialogue context,
but rely on external knowledge. Large language models (LLMs) generate responses
that mimic pre-sales dialogues after fine-tuning, but lack domain-specific
knowledge for accurate recommendations. Intuitively, the strengths of LLM and
CRS in E-commerce pre-sales dialogues are complementary, yet no previous work
has explored this. This paper investigates the effectiveness of combining LLM
and CRS in E-commerce pre-sales dialogues, proposing two collaboration methods:
CRS assisting LLM and LLM assisting CRS. We conduct extensive experiments on a
real-world dataset of Ecommerce pre-sales dialogues. We analyze the impact of
two collaborative approaches with two CRSs and two LLMs on four tasks of
Ecommerce pre-sales dialogue. We find that collaborations between CRS and LLM
can be very effective in some cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unraveling and Mitigating Retriever Inconsistencies in
  Retrieval-Augmented Large Language Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20680v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20680v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingda Li, Xinyu Li, Yifan Chen, Wenfeng Xuan, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Retrieval-Augmented Large Language Models (RALMs) demonstrate their
superiority in terms of factuality, they do not consistently outperform the
original retrieval-free Language Models (LMs). Our experiments reveal that this
example-level performance inconsistency exists not only between
retrieval-augmented and retrieval-free LM but also among different retrievers.
To understand this phenomenon, we investigate the degeneration behavior of
RALMs and theoretically decompose it into four categories. Further analysis
based on our decomposition reveals that the innate difference in knowledge
sources and the unpredictable degeneration of the reader model contribute most
to the inconsistency. Drawing from our analysis, we introduce Ensemble of
Retrievers (EoR), a trainable framework that can adaptively retrieve from
different knowledge sources and effectively decrease unpredictable reader
errors. Our experiments on Open Domain Question Answering show that EoR
substantially improves performance over the RALM with a single retriever by
considerably reducing inconsistent behaviors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024 (findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement
  on Multilingual and Multi-Cultural Data <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishaan Watts, Varun Gumma, Aditya Yadavalli, Vivek Seshadri, Manohar Swaminathan, Sunayana Sitaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluation of multilingual Large Language Models (LLMs) is challenging due to
a variety of factors -- the lack of benchmarks with sufficient linguistic
diversity, contamination of popular benchmarks into LLM pre-training data and
the lack of local, cultural nuances in translated benchmarks. In this work, we
study human and LLM-based evaluation in a multilingual, multi-cultural setting.
We evaluate 30 models across 10 Indic languages by conducting 90K human
evaluations and 30K LLM-based evaluations and find that models such as GPT-4o
and Llama-3 70B consistently perform best for most Indic languages. We build
leaderboards for two evaluation settings - pairwise comparison and direct
assessment and analyze the agreement between humans and LLMs. We find that
humans and LLMs agree fairly well in the pairwise setting but the agreement
drops for direct assessment evaluation especially for languages such as Bengali
and Odia. We also check for various biases in human and LLM-based evaluation
and find evidence of self-bias in the GPT-based evaluator. Our work presents a
significant step towards scaling up multilingual evaluation of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Neural Network Enhanced Retrieval for Question Answering of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06572v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06572v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Li, Qingyan Guo, Jiawei Shao, Lei Song, Jiang Bian, Jun Zhang, Rui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval augmented generation has revolutionized large language model (LLM)
outputs by providing factual supports. Nevertheless, it struggles to capture
all the necessary knowledge for complex reasoning questions. Existing retrieval
methods typically divide reference documents into passages, treating them in
isolation. These passages, however, are often interrelated, such as passages
that are contiguous or share the same keywords. Therefore, it is crucial to
recognize such relatedness for enhancing the retrieval process. In this paper,
we propose a novel retrieval method, called GNN-Ret, which leverages graph
neural networks (GNNs) to enhance retrieval by exploiting the relatedness
between passages. Specifically, we first construct a graph of passages by
connecting passages that are structure-related or keyword-related. A graph
neural network (GNN) is then leveraged to exploit the relationships between
passages and improve the retrieval of supporting passages. Furthermore, we
extend our method to handle multi-hop reasoning questions using a recurrent
graph neural network (RGNN), named RGNN-Ret. At each step, RGNN-Ret integrates
the graphs of passages from previous steps, thereby enhancing the retrieval of
supporting passages. Extensive experiments on benchmark datasets demonstrate
that GNN-Ret achieves higher accuracy for question answering with a single
query of LLMs than strong baselines that require multiple queries, and RGNN-Ret
further improves accuracy and achieves state-of-the-art performance, with up to
10.4% accuracy improvement on the 2WikiMQA dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Use of Large Language Models to Generate Capability Ontologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17524v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17524v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Miguel Vieira da Silva, Aljosha Köcher, Felix Gehlhoff, Alexander Fay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capability ontologies are increasingly used to model functionalities of
systems or machines. The creation of such ontological models with all
properties and constraints of capabilities is very complex and can only be done
by ontology experts. However, Large Language Models (LLMs) have shown that they
can generate machine-interpretable models from natural language text input and
thus support engineers / ontology experts. Therefore, this paper investigates
how LLMs can be used to create capability ontologies. We present a study with a
series of experiments in which capabilities with varying complexities are
generated using different prompting techniques and with different LLMs. Errors
in the generated ontologies are recorded and compared. To analyze the quality
of the generated ontologies, a semi-automated approach based on RDF syntax
checking, OWL reasoning, and SHACL constraints is used. The results of this
study are very promising because even for complex capabilities, the generated
ontologies are almost free of errors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 2024 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward a Method to Generate Capability Ontologies from Natural Language
  Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07962v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07962v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Miguel Vieira da Silva, Aljosha Köcher, Felix Gehlhoff, Alexander Fay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve a flexible and adaptable system, capability ontologies are
increasingly leveraged to describe functions in a machine-interpretable way.
However, modeling such complex ontological descriptions is still a manual and
error-prone task that requires a significant amount of effort and ontology
expertise. This contribution presents an innovative method to automate
capability ontology modeling using Large Language Models (LLMs), which have
proven to be well suited for such tasks. Our approach requires only a natural
language description of a capability, which is then automatically inserted into
a predefined prompt using a few-shot prompting technique. After prompting an
LLM, the resulting capability ontology is automatically verified through
various steps in a loop with the LLM to check the overall correctness of the
capability ontology. First, a syntax check is performed, then a check for
contradictions, and finally a check for hallucinations and missing ontology
elements. Our method greatly reduces manual effort, as only the initial natural
language description and a final human review and possible correction are
necessary, thereby streamlining the capability ontology generation process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 2024 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model Based Generative Error Correction: A Challenge and
  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09785v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09785v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao-Han Huck Yang, Taejin Park, Yuan Gong, Yuanchao Li, Zhehuai Chen, Yen-Ting Lin, Chen Chen, Yuchen Hu, Kunal Dhawan, Piotr Żelasko, Chao Zhang, Yun-Nung Chen, Yu Tsao, Jagadeesh Balam, Boris Ginsburg, Sabato Marco Siniscalchi, Eng Siong Chng, Peter Bell, Catherine Lai, Shinji Watanabe, Andreas Stolcke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given recent advances in generative AI technology, a key question is how
large language models (LLMs) can enhance acoustic modeling tasks using text
decoding results from a frozen, pretrained automatic speech recognition (ASR)
model. To explore new capabilities in language modeling for speech processing,
we introduce the generative speech transcription error correction (GenSEC)
challenge. This challenge comprises three post-ASR language modeling tasks: (i)
post-ASR transcription correction, (ii) speaker tagging, and (iii) emotion
recognition. These tasks aim to emulate future LLM-based agents handling
voice-based interfaces while remaining accessible to a broad audience by
utilizing open pretrained language models or agent-based APIs. We also discuss
insights from baseline evaluations, as well as lessons learned for designing
future evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE SLT 2024. The initial draft version has been done in December
  2023. Post-ASR Text Processing and Understanding Community and LlaMA-7B
  pre-training correction model:
  https://huggingface.co/GenSEC-LLM/SLT-Task1-Llama2-7b-HyPo-baseline</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VLFeedback: A Large-Scale AI Feedback Dataset for Large <span class="highlight-title">Vision-Language</span>
  Models Alignment <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong, Qi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large vision-language models (LVLMs) evolve rapidly, the demand for
high-quality and diverse data to align these models becomes increasingly
crucial. However, the creation of such data with human supervision proves
costly and time-intensive. In this paper, we investigate the efficacy of AI
feedback to scale supervision for aligning LVLMs. We introduce VLFeedback, the
first large-scale vision-language feedback dataset, comprising over 82K
multi-modal instructions and comprehensive rationales generated by
off-the-shelf models without human annotations. To evaluate the effectiveness
of AI feedback for vision-language alignment, we train Silkie, an LVLM
fine-tuned via direct preference optimization on VLFeedback. Silkie showcases
exceptional performance regarding helpfulness, visual faithfulness, and safety
metrics. It outperforms its base model by 6.9\% and 9.5\% in perception and
cognition tasks, reduces hallucination issues on MMHal-Bench, and exhibits
enhanced resilience against red-teaming attacks. Furthermore, our analysis
underscores the advantage of AI feedback, particularly in fostering preference
diversity to deliver more comprehensive improvements. Our dataset, training
code and models are available at https://vlf-silkie.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main Conference camera-ready version (fixed small typos).
  This article supersedes arXiv:2312.10665</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WaterMax: breaking the LLM watermark detectability-robustness-quality
  trade-off 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04808v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04808v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eva Giboulot, Teddy Furon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking is a technical means to dissuade malfeasant usage of Large
Language Models. This paper proposes a novel watermarking scheme, so-called
WaterMax, that enjoys high detectability while sustaining the quality of the
generated text of the original LLM. Its new design leaves the LLM untouched (no
modification of the weights, logits, temperature, or sampling technique).
WaterMax balances robustness and complexity contrary to the watermarking
techniques of the literature inherently provoking a trade-off between quality
and robustness. Its performance is both theoretically proven and experimentally
validated. It outperforms all the SotA techniques under the most complete
benchmark suite. Code available at https://github.com/eva-giboulot/WaterMax.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM Critics Help Catch Bugs in Mathematics: Towards a Better
  Mathematical Verifier with Natural Language Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14024v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14024v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Dayiheng Liu, Chang Zhou, Wen Xiao, Junjie Hu, Tianyu Liu, Baobao Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent progress, mathematical verifiers have achieved success in
mathematical reasoning tasks by validating the correctness of solutions
generated by policy models. However, existing verifiers are trained with binary
classification labels, which are not informative enough for the model to
accurately assess the solutions. To mitigate the aforementioned insufficiency
of binary labels, we introduce step-wise natural language feedback as rationale
labels, that is, the correctness of each step and the detailed explanations. In
this paper, we propose Math-Minos, a natural language feedback-enhanced
verifier by constructing automatically generated training data and a two-stage
training paradigm for effective training and efficient inference. Our
experiments reveal that a small set of natural language feedback can
significantly boost the performance of the verifier in both verification and
reinforcement learning. We have released the code and data for further
exploration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PertEval: Unveiling Real Knowledge Capacity of LLMs with
  Knowledge-Invariant Perturbations <span class="chip">NeurIPS '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19740v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19740v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiatong Li, Renjun Hu, Kunzhe Huang, Yan Zhuang, Qi Liu, Mengxiao Zhu, Xing Shi, Wei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expert-designed close-ended benchmarks are indispensable in assessing the
knowledge capacity of large language models (LLMs). Despite their widespread
use, concerns have mounted regarding their reliability due to limited test
scenarios and an unavoidable risk of data contamination. To rectify this, we
present PertEval, a toolkit devised for in-depth probing of LLMs' knowledge
capacity through \textbf{knowledge-invariant perturbations}. These
perturbations employ human-like restatement techniques to generate on-the-fly
test samples from static benchmarks, meticulously retaining knowledge-critical
content while altering irrelevant details. Our toolkit further includes a suite
of \textbf{response consistency analyses} that compare performance on raw vs.
perturbed test sets to precisely assess LLMs' genuine knowledge capacity. Six
representative LLMs are re-evaluated using PertEval. Results reveal
significantly inflated performance of the LLMs on raw benchmarks, including an
absolute 25.8% overestimation for GPT-4. Additionally, through a nuanced
response pattern analysis, we discover that PertEval retains LLMs' uncertainty
to specious knowledge, and reveals their potential rote memorization to correct
options which leads to overestimated performance. We also find that the
detailed response consistency analyses by PertEval could illuminate various
weaknesses in existing LLMs' knowledge mastery and guide the development of
refinement. Our findings provide insights for advancing more robust and
genuinely knowledgeable LLMs. Our code is available at
\url{https://github.com/aigc-apps/PertEval}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS '24 D&B Spotlight; 28 pages, 15 figures, 14
  tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciAssess: Benchmarking LLM Proficiency in Scientific Literature
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01976v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01976v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengxing Cai, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Changxin Wang, Zhifeng Gao, Hongshuai Wang, Yongge Li, Mujie Lin, Shuwen Yang, Jiankun Wang, Mingjun Xu, Jin Huang, Xi Fang, Jiaxi Zhuang, Yuqi Yin, Yaqi Li, Changhong Chen, Zheng Cheng, Zifeng Zhao, Linfeng Zhang, Guolin Ke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in Large Language Models (LLMs) have revolutionized
scientific literature analysis. However, existing benchmarks fail to adequately
evaluate the proficiency of LLMs in this domain, particularly in scenarios
requiring higher-level abilities beyond mere memorization and the handling of
multimodal data. In response to this gap, we introduce SciAssess, a benchmark
specifically designed for the comprehensive evaluation of LLMs in scientific
literature analysis. It aims to thoroughly assess the efficacy of LLMs by
evaluating their capabilities in Memorization (L1), Comprehension (L2), and
Analysis \& Reasoning (L3). It encompasses a variety of tasks drawn from
diverse scientific fields, including biology, chemistry, material, and
medicine. To ensure the reliability of SciAssess, rigorous quality control
measures have been implemented, ensuring accuracy, anonymization, and
compliance with copyright standards. SciAssess evaluates 11 LLMs, highlighting
their strengths and areas for improvement. We hope this evaluation supports the
ongoing development of LLM applications in scientific literature analysis.
SciAssess and its resources are available at
\url{https://github.com/sci-assess/SciAssess}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural
  Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06833v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06833v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zhou, Taelin Karidi, Wanlong Liu, Nicolas Garneau, Yong Cao, Wenyu Chen, Haizhou Li, Daniel Hershcovich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have highlighted the presence of cultural biases in Large
Language Models (LLMs), yet often lack a robust methodology to dissect these
phenomena comprehensively. Our work aims to bridge this gap by delving into the
Food domain, a universally relevant yet culturally diverse aspect of human
life. We introduce FmLAMA, a multilingual dataset centered on food-related
cultural facts and variations in food practices. We analyze LLMs across various
architectures and configurations, evaluating their performance in both
monolingual and multilingual settings. By leveraging templates in six different
languages, we investigate how LLMs interact with language-specific and cultural
knowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias
towards food knowledge prevalent in the United States; (2) Incorporating
relevant cultural context significantly improves LLMs' ability to access
cultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is
highly dependent on the interplay between the probing language, the specific
model architecture, and the cultural context in question. This research
underscores the complexity of integrating cultural understanding into LLMs and
emphasizes the importance of culturally diverse datasets to mitigate biases and
enhance model performance across different cultural domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>cultural bias analysis, cultural knowledge probing, large language
  models, cultural NLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synergizing In-context Learning with Hints for End-to-end Task-oriented
  Dialog Systems <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15585v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15585v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishal Vivek Saley, Rocktim Jyoti Das, Dinesh Raghu,  Mausam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end Task-Oriented Dialog (TOD) systems typically require extensive
training datasets to perform well. In contrast, large language model (LLM)
based TOD systems can excel even with limited data due to their ability to
learn tasks through in-context exemplars. However, these models lack alignment
with the style of responses in training data and often generate comprehensive
responses, making it difficult for users to grasp the information quickly. In
response, we propose SyncTOD that synergizes LLMs with task-specific hints to
improve alignment in low-data settings. SyncTOD employs small auxiliary models
to provide hints and select exemplars for in-context prompts. With ChatGPT,
SyncTOD achieves superior performance compared to LLM-based baselines and SoTA
models in low-data settings, while retaining competitive performance in
full-data settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024 Camera-Ready Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hyper-multi-step: The Truth Behind Difficult Long-context Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04422v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04422v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijiong Yu, Ma Xiufa, Fang Jianwei, Zhi Xu, Su Guangyao, Wang Jiancheng, Yongfeng Huang, Zhixiao Qi, Wei Wang, Weifeng Liu, Ran Chen, Ji Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-context language models (LCLM), characterized by their extensive context
window, is becoming increasingly popular. Meanwhile, many long-context
benchmarks present challenging tasks that even the most advanced LCLMs struggle
to complete. However, the underlying sources of various challenging
long-context tasks have seldom been studied. To bridge this gap, we conduct
experiments to indicate their difficulty stems primarily from two basic issues:
"multi-matching retrieval," which requires the simultaneous retrieval of
multiple items, and "logic-based retrieval," which necessitates logical
judgment within retrieval criteria. These two problems, while seemingly
straightforward, actually exceed the capabilities of LCLMs because they are
proven to be hyper-multi-step (demanding numerous steps to solve) in nature.
This finding could explain why LLMs struggle with more advanced long-context
tasks, providing a more accurate perspective for rethinking solutions for them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is publicly available at
  https://github.com/yuyijiong/hard_retrieval_for_llm and the datasets is at
  https://huggingface.co/datasets/yuyijiong/difficult_retrieval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fisher Information-based Efficient Curriculum Federated Learning with
  Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Liu, Jiaxiang Ren, Ruoming Jin, Zijie Zhang, Yang Zhou, Patrick Valduriez, Dejing Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a promising paradigm to collaboratively train models with decentralized
data, Federated Learning (FL) can be exploited to fine-tune Large Language
Models (LLMs). While LLMs correspond to huge size, the scale of the training
data significantly increases, which leads to tremendous amounts of computation
and communication costs. The training data is generally non-Independent and
Identically Distributed (non-IID), which requires adaptive data processing
within each device. Although Low Rank Adaptation (LoRA) can significantly
reduce the scale of parameters to update in the fine-tuning process, it still
takes unaffordable time to transfer the low-rank parameters of all the layers
in LLMs. In this paper, we propose a Fisher Information-based Efficient
Curriculum Federated Learning framework (FibecFed) with two novel methods,
i.e., adaptive federated curriculum learning and efficient sparse parameter
update. First, we propose a fisher information-based method to adaptively
sample data within each device to improve the effectiveness of the FL
fine-tuning process. Second, we dynamically select the proper layers for global
aggregation and sparse parameters for local update with LoRA so as to improve
the efficiency of the FL fine-tuning process. Extensive experimental results
based on 10 datasets demonstrate that FibecFed yields excellent performance (up
to 45.35% in terms of accuracy) and superb fine-tuning speed (up to 98.61%
faster) compared with 17 baseline approaches).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 8 figures, 14 tables, to appear in EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QUIS: Question-guided Insights Generation for Automated Exploratory Data
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhijit Manatkar, Ashlesha Akella, Parthivi Gupta, Krishnasuri Narayanam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering meaningful insights from a large dataset, known as Exploratory
Data Analysis (EDA), is a challenging task that requires thorough exploration
and analysis of the data. Automated Data Exploration (ADE) systems use
goal-oriented methods with Large Language Models and Reinforcement Learning
towards full automation. However, these methods require human involvement to
anticipate goals that may limit insight extraction, while fully automated
systems demand significant computational resources and retraining for new
datasets. We introduce QUIS, a fully automated EDA system that operates in two
stages: insight generation (ISGen) driven by question generation (QUGen). The
QUGen module generates questions in iterations, refining them from previous
iterations to enhance coverage without human intervention or manually curated
examples. The ISGen module analyzes data to produce multiple relevant insights
in response to each question, requiring no prior training and enabling QUIS to
adapt to new datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for ENLP 2024 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15545v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15545v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihang Li, Jin Huang, Jiaxi Zhuang, Yaorui Shi, Xiaochen Cai, Mingjun Xu, Xiang Wang, Linfeng Zhang, Guolin Ke, Hengxing Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific literature understanding is crucial for extracting targeted
information and garnering insights, thereby significantly advancing scientific
discovery. Despite the remarkable success of Large Language Models (LLMs), they
face challenges in scientific literature understanding, primarily due to (1) a
lack of scientific knowledge and (2) unfamiliarity with specialized scientific
tasks.
  To develop an LLM specialized in scientific literature understanding, we
propose a hybrid strategy that integrates continual pre-training (CPT) and
supervised fine-tuning (SFT), to simultaneously infuse scientific domain
knowledge and enhance instruction-following capabilities for domain-specific
tasks.cIn this process, we identify two key challenges: (1) constructing
high-quality CPT corpora, and (2) generating diverse SFT instructions. We
address these challenges through a meticulous pipeline, including PDF text
extraction, parsing content error correction, quality filtering, and synthetic
instruction creation. Applying this strategy, we present a suite of LLMs:
SciLitLLM, specialized in scientific literature understanding. These models
demonstrate promising performance on scientific literature understanding
benchmarks.
  Our contributions are threefold: (1) We present an effective framework that
integrates CPT and SFT to adapt LLMs to scientific literature understanding,
which can also be easily adapted to other domains. (2) We propose an LLM-based
synthesis method to generate diverse and high-quality scientific instructions,
resulting in a new instruction set -- SciLitIns -- for supervised fine-tuning
in less-represented scientific domains. (3) SciLitLLM achieves promising
performance improvements on scientific literature understanding benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13276v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13276v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Hayden Kwok-Hay So, Ting Cao, Fan Yang, Mao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention is the cornerstone of modern Large Language Models (LLMs). Yet its
quadratic complexity limits the efficiency and scalability of LLMs, especially
for those with a long-context window. A promising approach addressing this
limitation is to leverage the sparsity in attention. However, existing
sparsity-based solutions predominantly rely on predefined patterns or
heuristics to approximate sparsity. This practice falls short to fully capture
the dynamic nature of attention sparsity in language-based tasks. This paper
argues that attention sparsity should be learned rather than predefined. To
this end, we design SeerAttention, a new Attention mechanism that augments the
conventional attention with a learnable gate that adaptively selects
significant blocks in an attention map and deems the rest blocks sparse. Such
block-level sparsity effectively balances accuracy and speedup. To enable
efficient learning of the gating network, we develop a customized
FlashAttention implementation that extracts the block-level ground truth of
attention map with minimum overhead. SeerAttention not only applies to
post-training, but also excels in long-context fine-tuning. Our results show
that at post-training stages, SeerAttention significantly outperforms
state-of-the-art static or heuristic-based sparse attention methods, while also
being more versatile and flexible to adapt to varying context lengths and
sparsity ratios. When applied to long-context fine-tuning with YaRN,
SeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context
length with minimal perplexity loss, offering a 5.67x speedup over
FlashAttention-2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ P3: A Policy-Driven, Pace-Adaptive, and Diversity-Promoted Framework for
  data pruning in LLM Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05541v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05541v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingxuan Yang, Huayi Wang, Muning Wen, Xiaoyun Mo, Qiuying Peng, Jun Wang, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly advancing field of Large Language Models (LLMs), effectively
leveraging existing datasets during fine-tuning to maximize the model's
potential is of paramount importance. This paper introduces P3, an adaptive
framework aimed at optimizing the task-specific fine-tuning process through
iterative data pruning. P3 consists of three key components: (1) Policy-driven
Difficulty Measurement, which dynamically assesses data difficulty based on the
model's real-time performance, replacing static metrics with adaptable
evaluations; (2) Pace-Adaptive Selection, leveraging self-paced learning to
progressively introduce more challenging data, thereby enhancing model
capability; (3) Diversity Promotion, incorporating Determinantal Point Process
(DPP) to ensure data diversity across epochs, enriching the learning process.
We validate P3 on the reasoning scenarios, APPS and MATH, demonstrating
significant improvements over traditional data pruning methods. By advancing
dynamic data selection and utilization strategies, P3 contributes both a
theoretical framework and concrete approach to fully exploit existing data for
LLMs' performance improvement, offering utility across diverse tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LatentExplainer: Explaining Latent Representations in Deep Generative
  Models with <span class="highlight-title">Multi-modal</span> Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14862v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14862v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengdan Zhu, Raasikh Kanjiani, Jiahui Lu, Andrew Choi, Qirui Ye, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models like VAEs and diffusion models have advanced various
generation tasks by leveraging latent variables to learn data distributions and
generate high-quality samples. Despite the field of explainable AI making
strides in interpreting machine learning models, understanding latent variables
in generative models remains challenging. This paper introduces
\textit{LatentExplainer}, a framework for automatically generating semantically
meaningful explanations of latent variables in deep generative models.
\textit{LatentExplainer} tackles three main challenges: inferring the meaning
of latent variables, aligning explanations with inductive biases, and handling
varying degrees of explainability. Our approach perturbs latent variables,
interpreting changes in generated data, and uses multi-modal large language
models (MLLMs) to produce human-understandable explanations. We evaluate our
proposed method on several real-world and synthetic datasets, and the results
demonstrate superior performance in generating high-quality explanations for
latent variables. The results highlight the effectiveness of incorporating
inductive biases and uncertainty quantification, significantly enhancing model
interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supervised <span class="highlight-title">Fine-Tuning</span> Achieve Rapid Task Adaption Via Alternating
  Attention Head Activation Patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhao, Li Du, Xiao Ding, Kai Xiong, Ting Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs' performance on complex tasks is still unsatisfactory. A key issue is
that presently LLMs learn in a data-driven schema, while the instructions about
these complex tasks are both scarce and hard to collect or construct. On the
contrary, a prominent phenomenon is that LLMs can learn rather fast on simpler
tasks with adequate prior knowledge captured during pretraining stage. Thus, if
the prerequisite and mechanism of such rapid generalization could be
elucidated, it could enhance the efficiency and effectiveness of the LLM's
ability to learn complex tasks. Thus, in this paper, we employ a gradient-based
method, to dissect the process that the SFT process adapts LLMs to downstream
tasks via the perspective of attention patterns. We find that: (1) LLMs
selectively activate task-specific attention heads during SFT; (2) activation
patterns for complex tasks are combinations of basic task patterns; and (3)
changes in a few parameters can significantly impact activation patterns after
SFT on a small number of samples.Based on these insights, experiments are
conducted to actually enhance the efficiency and effectiveness of SFT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Introspection to Best Practices: Principled Analysis of
  Demonstrations in <span class="highlight-title">Multimodal</span> In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Xu, Fei Wang, Sheng Zhang, Hoifung Poon, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by in-context learning (ICL) capabilities of Large Language models
(LLMs), multimodal LLMs with additional visual modality are also exhibited with
similar ICL abilities when multiple image-text pairs are provided as
demonstrations. However, relatively less work has been done to investigate the
principles behind how and why multimodal ICL works. We conduct a systematic and
principled evaluation of multimodal ICL for models of different scales on a
broad spectrum of new yet critical tasks. Through perturbations over different
modality information, we show that modalities matter differently across tasks
in multimodal ICL. Guided by task-specific modality impact, we recommend
modality-driven demonstration strategies to boost ICL performance. We also find
that models may follow inductive biases from multimodal ICL even if they are
rarely seen in or contradict semantic priors from pretraining data. Our
principled analysis provides a comprehensive way of understanding the role of
demonstrations in multimodal in-context learning, and sheds light on
effectively improving multimodal ICL on a wide range of tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Everything is Editable: Extend Knowledge Editing to Unstructured Data in
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingcheng Deng, Zihao Wei, Liang Pang, Hanxing Ding, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent knowledge editing methods have primarily focused on modifying
structured knowledge in large language models. However, this task setting
overlooks the fact that a significant portion of real-world knowledge is stored
in an unstructured format, characterized by long-form content, noise, and a
complex yet comprehensive nature. Techniques like local layer key-value storage
and term-driven optimization, as used in previous methods like MEMIT, are not
effective for handling unstructured knowledge. To address these challenges, we
propose a novel Unstructured Knowledge Editing method, namely UnKE, which
extends previous assumptions in the layer dimension and token dimension.
Firstly, in the layer dimension, we propose non-local block key-value storage
to replace local layer key-value storage, increasing the representation ability
of key-value pairs and incorporating attention layer knowledge. Secondly, in
the token dimension, we replace term-driven optimization with cause-driven
optimization, which edits the last token directly while preserving context,
avoiding the need to locate terms and preventing the loss of context
information. Results on newly proposed unstructured knowledge editing dataset
(UnKEBench) and traditional structured datasets demonstrate that UnKE achieves
remarkable performance, surpassing strong baselines. In addition, UnKE has
robust batch editing and sequential editing capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Amphista: Bi-directional Multi-head Decoding for Accelerating LLM
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13170v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13170v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeping Li, Xinlong Yang, Ziheng Gao, Ji Liu, Guanchen Li, Zhuang Liu, Dong Li, Jinzhang Peng, Lu Tian, Emad Barsoum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) inherently use autoregressive decoding, which
lacks parallelism in inference and results in significantly slow inference
speed. While methods such as Medusa constructs parallelized heads, they lack
adequate information interaction across different prediction positions. To
overcome this limitation, we introduce Amphista, an enhanced speculative
decoding framework that builds upon Medusa. Specifically, Amphista models an
Auto-embedding Block capable of parallel inference, incorporating
bi-directional attention to enable interaction between different drafting
heads. Additionally, Amphista integrates Staged Adaptation Layers, which ensure
a seamless transition of semantic information from the target model's
autoregressive inference to the drafting heads' non-autoregressive inference,
effectively achieving paradigm shift and feature fusion. Experimental results
on Vicuna models using MT-Bench and Spec-Bench demonstrate that Amphista
achieves substantial acceleration while maintaining generation quality. On
MT-Bench, Amphista delivers up to 2.75$\times$ speedup over vanilla
autoregressive decoding and 1.40$\times$ over Medusa on Vicuna 33B in
wall-clock time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16710v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16710v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed A Aly, Beidi Chen, Carole-Jean Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LayerSkip, an end-to-end solution to speed-up inference of large
language models (LLMs). First, during training we apply layer dropout, with low
dropout rates for earlier layers and higher dropout rates for later layers, and
an early exit loss where all transformer layers share the same exit. Second,
during inference, we show that this training recipe increases the accuracy of
early exit at earlier layers, without adding any auxiliary layers or modules to
the model. Third, we present a novel self-speculative decoding solution where
we exit at early layers and verify and correct with remaining layers of the
model. Our proposed self-speculative decoding approach has less memory
footprint than other speculative decoding approaches and benefits from shared
compute and activations of the draft and verification stages. We run
experiments on different Llama model sizes on different types of training:
pretraining from scratch, continual pretraining, finetuning on specific data
domain, and finetuning on specific task. We implement our inference solution
and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x
on coding, and 2.0x on TOPv2 semantic parsing task. We open source our code and
checkpoints at https://github.com/facebookresearch/LayerSkip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Tighter Complexity Analysis of SparseGPT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we improved the analysis of the running time of SparseGPT
[Frantar, Alistarh ICML 2023] from $O(d^{3})$ to $O(d^{\omega} + d^{2+a+o(1)} +
d^{1+\omega(1,1,a)-a})$ for any $a \in [0, 1]$, where $\omega$ is the exponent
of matrix multiplication. In particular, for the current $\omega \approx 2.371$
[Alman, Duan, Williams, Xu, Xu, Zhou 2024], our running time boils down to
$O(d^{2.53})$. This running time is due to the analysis of the lazy update
behavior in iterative maintenance problems such as [Deng, Song, Weinstein 2022;
Brand, Song, Zhou ICML 2024].
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Study of Multilingual Confidence Estimation on Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13606v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13606v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyang Xue, Hongru Wang, Rui Wang, Sheng Wang, Zezhong Wang, Yiming Du, Bin Liang, Kam-Fai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tendency of Large Language Models (LLMs) to generate hallucinations
raises concerns regarding their reliability. Therefore, confidence estimations
indicating the extent of trustworthiness of the generations become essential.
However, current LLM confidence estimations in languages other than English
remain underexplored. This paper addresses this gap by introducing a
comprehensive investigation of Multilingual Confidence estimation (MlingConf)
on LLMs, focusing on both language-agnostic (LA) and language-specific (LS)
tasks to explore the performance and language dominance effects of multilingual
confidence estimations on different tasks. The benchmark comprises four
meticulously checked and human-evaluate high-quality multilingual datasets for
LA tasks and one for the LS task tailored to specific social, cultural, and
geographical contexts of a language. Our experiments reveal that on LA tasks
English exhibits notable linguistic dominance in confidence estimations than
other languages, while on LS tasks, using question-related language to prompt
LLMs demonstrates better linguistic dominance in multilingual confidence
estimations. The phenomena inspire a simple yet effective native-tone prompting
strategy by employing language-specific prompts for LS tasks, effectively
improving LLMs' reliability and accuracy on LS tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comments: n pages; Previously this version appeared as
  arXiv:2410.12478 which was submitted as a new work by accident</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ExACT: Teaching AI Agents to Explore with Reflective-MCTS and
  Exploratory Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02052v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02052v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents have demonstrated significant potential in automating
complex multistep decision-making tasks. However, even state-of-the-art
vision-language models (VLMs), such as GPT-4o, still fall short of human-level
performance, particularly in intricate web environments and long-horizon tasks.
To address these limitations, we present ExACT, an approach to combine
test-time search and self-learning to build o1-like models for agentic
applications. We first introduce Reflective Monte Carlo Tree Search (R-MCTS), a
novel test time algorithm designed to enhance AI agents' ability to explore
decision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating
contrastive reflection, allowing agents to learn from past interactions and
dynamically improve their search efficiency; and 2) using multi-agent debate
for reliable state evaluation. Next, we introduce Exploratory Learning, a novel
learning strategy to teach agents to search at inference time without relying
on any external search algorithms. On the challenging VisualWebArena benchmark,
our GPT-4o based R-MCTS agent achieves a 6% to 30% relative improvement across
various tasks compared to the previous state-of-the-art. Additionally, we show
that the knowledge and experience gained from test-time search can be
effectively transferred back to GPT-4o via fine-tuning. After Exploratory
Learning, GPT-4o 1) demonstrates the ability to explore the environment,
evaluate a state, and backtrack to viable ones when it detects that the current
state cannot lead to success, and 2) matches 87% of R-MCTS's performance while
using significantly less compute. Notably, our work demonstrates the compute
scaling properties in both training - data collection with R-MCTS - and testing
time. These results suggest a promising research direction to enhance VLMs'
capabilities for agentic applications via test-time search and self-learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MlingConf: A Comprehensive Study of Multilingual Confidence Estimation
  on Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyang Xue, Hongru Wang, Rui Wang, Sheng Wang, Zezhong Wang, Yiming Du, Bin Liang, Kam-Fai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tendency of Large Language Models (LLMs) to generate hallucinations
raises concerns regarding their reliability. Therefore, confidence estimations
indicating the extent of trustworthiness of the generations become essential.
However, current LLM confidence estimations in languages other than English
remain underexplored. This paper addresses this gap by introducing a
comprehensive investigation of Multilingual Confidence estimation (MlingConf)
on LLMs, focusing on both language-agnostic (LA) and language-specific (LS)
tasks to explore the performance and language dominance effects of multilingual
confidence estimations on different tasks. The benchmark comprises four
meticulously checked and human-evaluate high-quality multilingual datasets for
LA tasks and one for the LS task tailored to specific social, cultural, and
geographical contexts of a language. Our experiments reveal that on LA tasks
English exhibits notable linguistic dominance in confidence estimations than
other languages, while on LS tasks, using question-related language to prompt
LLMs demonstrates better linguistic dominance in multilingual confidence
estimations. The phenomena inspire a simple yet effective native-tone prompting
strategy by employing language-specific prompts for LS tasks, effectively
improving LLMs' reliability and accuracy on LS tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comments: This work was intended as a replacement of arXiv:2402.13606
  and any subsequent updates will appear there</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction
  Diversity on Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04717v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04717v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Zhang, Justin Wang, Francois Charton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and accurately following instructions is critical for large
language models (LLMs) to be effective across diverse tasks. In this work, we
rigorously examine the key factors that enable models to generalize to unseen
instructions, providing insights to guide the collection of data for
instruction-tuning. Through controlled experiments, inspired by the
Turing-complete Markov algorithm, we demonstrate that such generalization
$\textbf{only emerges}$ when training data is diversified enough across
semantic domains. Our findings also reveal that merely diversifying within
limited domains fails to ensure robust generalization. In contrast,
cross-domain data diversification, even under constrained data budgets,
significantly enhances a model's adaptability. We further extend our analysis
to real-world scenarios, including fine-tuning of
$\textit{$\textbf{specialist}$}$ and $\textit{$\textbf{generalist}$}$ models.
In both cases, we demonstrate that 1) better performance can be achieved by
increasing the diversity of an established dataset while keeping the data size
constant, and 2) when scaling up the data, diversifying the semantics of
instructions is more effective than simply increasing the quantity of similar
data. Our research provides important insights for dataset collation,
particularly when optimizing model performance by expanding training data for
both specialist and generalist scenarios. We show that careful consideration of
data diversification is key: training specialist models with data extending
beyond their core domain leads to significant performance improvements, while
generalist models benefit from diverse data mixtures that enhance their overall
instruction-following capabilities across a wide range of applications. Our
results highlight the critical role of strategic diversification and offer
clear guidelines for improving data quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fix formatting issues</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BenTo: Benchmark Task Reduction with In-Context Transferability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13804v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13804v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Zhao, Ming Li, Lichao Sun, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating large language models (LLMs) is costly: it requires the generation
and examination of LLM outputs on a large-scale benchmark of various tasks.
This paper investigates how to efficiently reduce the tasks used to benchmark
LLMs without affecting the evaluation quality. Our study reveals that task
transferability and relevance provide critical information to identify the most
representative subset of tasks via optimizing a facility location function. We
propose a practically efficient metric for estimating the transferability
between two tasks via in-context learning (ICL). By analyzing the pairwise
transferability, we can reduce tasks in a modern LLM benchmark (e.g., MMLU or
FLAN) to 5% while inducing only a <4% difference to the evaluation on the
original benchmark. Compared to prior works, our method is training-free,
gradient-free, and highly efficient requiring ICL only.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/tianyi-lab/bento</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphInsight: Unlocking Insights in Large Language Models for Graph
  Structure Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukun Cao, Shuo Han, Zengyi Gao, Zezhong Ding, Xike Xie, S. Kevin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Large Language Models (LLMs) have demonstrated potential in
processing graphs, they struggle with comprehending graphical structure
information through prompts of graph description sequences, especially as the
graph size increases. We attribute this challenge to the uneven memory
performance of LLMs across different positions in graph description sequences,
known as ''positional biases''. To address this, we propose GraphInsight, a
novel framework aimed at improving LLMs' comprehension of both macro- and
micro-level graphical information. GraphInsight is grounded in two key
strategies: 1) placing critical graphical information in positions where LLMs
exhibit stronger memory performance, and 2) investigating a lightweight
external knowledge base for regions with weaker memory performance, inspired by
retrieval-augmented generation (RAG). Moreover, GraphInsight explores
integrating these two strategies into LLM agent processes for composite graph
tasks that require multi-step reasoning. Extensive empirical studies on
benchmarks with a wide range of evaluation tasks show that GraphInsight
significantly outperforms all other graph description methods (e.g., prompting
techniques and reordering strategies) in understanding graph structures of
varying sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoPal: Autonomous Adaptation to Users for Personal AI Companionship 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13960v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13960v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Cheng, Wenge Liu, Kaishuai Xu, Wenjun Hou, Yi Ouyang, Chak Tou Leong, Xian Wu, Yefeng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous research has demonstrated the potential of AI agents to act as
companions that can provide constant emotional support for humans. In this
paper, we emphasize the necessity of autonomous adaptation in personal AI
companionship, an underexplored yet promising direction. Such adaptability is
crucial as it can facilitate more tailored interactions with users and allow
the agent to evolve in response to users' changing needs. However, imbuing
agents with autonomous adaptability presents unique challenges, including
identifying optimal adaptations to meet users' expectations and ensuring a
smooth transition during the adaptation process. To address them, we devise a
hierarchical framework, AutoPal, that enables controllable and authentic
adjustments to the agent's persona based on user interactions. A
personamatching dataset is constructed to facilitate the learning of optimal
persona adaptations. Extensive experiments demonstrate the effectiveness of
AutoPal and highlight the importance of autonomous adaptability in AI
companionship.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficiently Quantifying and Mitigating Ripple Effects in Model Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07825v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07825v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianchen Wang, Zhouhong Gu, Xiaoxuan Zhu, Lin Zhang, Haoning Ye, Zhuozhi Xiong, Hongwei Feng, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have revolutionized numerous tasks with their
remarkable efficacy. However, editing these models, crucial for rectifying
outdated or erroneous information, often leads to a complex issue known as the
ripple effect in the hidden space. While difficult to detect, this effect can
significantly impede the efficacy of model editing tasks and deteriorate model
performance. This paper addresses this scientific challenge by proposing a
novel evaluation methodology, Graphical Impact Evaluation(GIE), which
quantitatively evaluates the adaptations of the model and the subsequent impact
of editing. Furthermore, we introduce the Selective Impact Revision(SIR), a
model editing method designed to mitigate this ripple effect. Our comprehensive
evaluations reveal that the ripple effect in the hidden space is a significant
issue in all current model editing methods. However, our proposed methods, GIE
and SIR, effectively identify and alleviate this issue, contributing to the
advancement of LLM editing techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoR: Mixture of Ranks for Low-Rank Adaptation Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13408v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13408v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyu Tang, Yilong Chen, Zhenyu Zhang, Junyuan Shang, Wenyuan Zhang, Yong Huang, Tingwen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) drives research to align its performance with full
fine-tuning. However, significant challenges remain: (1) Simply increasing the
rank size of LoRA does not effectively capture high-rank information, which
leads to a performance bottleneck.(2) MoE-style LoRA methods substantially
increase parameters and inference latency, contradicting the goals of efficient
fine-tuning and ease of application. To address these challenges, we introduce
Mixture of Ranks (MoR), which learns rank-specific information for different
tasks based on input and efficiently integrates multi-rank information. We
firstly propose a new framework that equates the integration of multiple LoRAs
to expanding the rank of LoRA. Moreover, we hypothesize that low-rank LoRA
already captures sufficient intrinsic information, and MoR can derive high-rank
information through mathematical transformations of the low-rank components.
Thus, MoR can reduces the learning difficulty of LoRA and enhances its
multi-task capabilities. MoR achieves impressive results, with MoR delivering a
1.31\% performance improvement while using only 93.93\% of the parameters
compared to baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniAutoML: A Human-Centered Framework for Unified Discriminative and
  Generative AutoML with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Guo, Zan Chen, Yingrui Ji, Liyun Zhang, Daqin Luo, Zhigang Li, Yiqin Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated Machine Learning (AutoML) has simplified complex ML processes such
as data pre-processing, model selection, and hyper-parameter searching.
However, traditional AutoML frameworks focus solely on discriminative tasks,
often falling short in tackling AutoML for generative models. Additionally,
these frameworks lack interpretability and user engagement during the training
process, primarily due to the absence of human-centered design. It leads to a
lack of transparency in final decision-making and limited user control,
potentially reducing trust and adoption of AutoML methods. To address these
limitations, we introduce UniAutoML, a human-centered AutoML framework that
leverages Large Language Models (LLMs) to unify AutoML for both discriminative
(e.g., Transformers and CNNs for classification or regression tasks) and
generative tasks (e.g., fine-tuning diffusion models or LLMs). The
human-centered design of UniAutoML innovatively features a conversational user
interface (CUI) that facilitates natural language interactions, providing users
with real-time guidance, feedback, and progress updates for better
interpretability. This design enhances transparency and user control throughout
the AutoML training process, allowing users to seamlessly break down or modify
the model being trained. To mitigate potential risks associated with LLM
generated content, UniAutoML incorporates a safety guardline that filters
inputs and censors outputs. We evaluated UniAutoML's performance and usability
through experiments on eight diverse datasets and user studies involving 25
participants, demonstrating that UniAutoML not only enhances performance but
also improves user control and trust. Our human-centered design bridges the gap
between AutoML capabilities and user understanding, making ML more accessible
to a broader audience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ACCEPT: Adaptive Codebook for Composite and Efficient <span class="highlight-title">Prompt</span> Tuning <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12847v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12847v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Chen Lin, Wei-Hua Li, Jun-Cheng Chen, Chu-Song Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt Tuning has been a popular Parameter-Efficient Fine-Tuning method
attributed to its remarkable performance with few updated parameters on various
large-scale pretrained Language Models (PLMs). Traditionally, each prompt has
been considered indivisible and updated independently, leading the parameters
increase proportionally as prompt length grows. To address this issue, we
propose Adaptive Codebook for Composite and Efficient Prompt Tuning (ACCEPT).
In our method, we refer to the concept of product quantization (PQ), allowing
all soft prompts to share a set of learnable codebook vectors in each subspace,
with each prompt differentiated by a set of adaptive weights. We achieve the
superior performance on 17 diverse natural language tasks including natural
language understanding (NLU) and question answering (QA) tasks by tuning only
0.3% of parameters of the PLMs. Our approach also excels in few-shot and large
model settings, highlighting its significant potential.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Subjective Uncertainty Quantification and Calibration in Natural
  Language Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Wang, Chris Holmes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applications of large language models often involve the generation of
free-form responses, in which case uncertainty quantification becomes
challenging. This is due to the need to identify task-specific uncertainties
(e.g., about the semantics) which appears difficult to define in general cases.
This work addresses these challenges from a perspective of Bayesian decision
theory, starting from the assumption that our utility is characterized by a
similarity measure that compares a generated response with a hypothetical true
response. We discuss how this assumption enables principled quantification of
the model's subjective uncertainty and its calibration. We further derive a
measure for epistemic uncertainty, based on a missing data perspective and its
characterization as an excess risk. The proposed methods can be applied to
black-box language models. We illustrate the methods on question answering and
machine translation tasks. Our experiments provide a principled evaluation of
task-specific calibration, and demonstrate that epistemic uncertainty offers a
promising deferral strategy for efficient data acquisition in in-context
learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Evolved Universal Transformer Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13166v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13166v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edoardo Cetin, Qi Sun, Tianyu Zhao, Yujin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior methods propose to offset the escalating costs of modern foundation
models by dropping specific parts of their contexts with hand-designed rules,
while attempting to preserve their original performance. We overcome this
trade-off with Neural Attention Memory Models (NAMMs), introducing a learned
network for memory management that improves both the performance and efficiency
of transformers. We evolve NAMMs atop pre-trained transformers to provide
different latent contexts focusing on the most relevant information for
individual layers and attention heads. NAMMs are universally applicable to any
model using self-attention as they condition exclusively on the values in the
produced attention matrices. Learning NAMMs on a small set of problems, we
achieve substantial performance improvements across multiple long-context
benchmarks while cutting the model's input contexts up to a fraction of the
original sizes. We show the generality of our conditioning enables zero-shot
transfer of NAMMs trained only on language to entirely new transformer
architectures even across input modalities, with their benefits carrying over
to vision and reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 14 figures. Preprint, under submission. Source code is
  available at https://github.com/SakanaAI/evo-memory</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">108</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BiGR: Harnessing Binary Latent Codes for Image Generation and Improved
  Visual Representation Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaozhe Hao, Xuantong Liu, Xianbiao Qi, Shihao Zhao, Bojia Zi, Rong Xiao, Kai Han, Kwan-Yee K. Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce BiGR, a novel conditional image generation model using compact
binary latent codes for generative training, focusing on enhancing both
generation and representation capabilities. BiGR is the first conditional
generative model that unifies generation and discrimination within the same
framework. BiGR features a binary tokenizer, a masked modeling mechanism, and a
binary transcoder for binary code prediction. Additionally, we introduce a
novel entropy-ordered sampling method to enable efficient image generation.
Extensive experiments validate BiGR's superior performance in generation
quality, as measured by FID-50k, and representation capabilities, as evidenced
by linear-probe accuracy. Moreover, BiGR showcases zero-shot generalization
across various vision tasks, enabling applications such as image inpainting,
outpainting, editing, interpolation, and enrichment, without the need for
structural modifications. Our findings suggest that BiGR unifies generative and
discriminative tasks effectively, paving the way for further advancements in
the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://haoosz.github.io/BiGR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NaturalBench: Evaluating <span class="highlight-title">Vision-Language</span> Models on Natural Adversarial
  Samples <span class="chip">NeurIPS 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, Deva Ramanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have made significant progress in recent
visual-question-answering (VQA) benchmarks that evaluate complex
visio-linguistic reasoning. However, are these models truly effective? In this
work, we show that VLMs still struggle with natural images and questions that
humans can easily answer, which we term natural adversarial samples. We also
find it surprisingly easy to generate these VQA samples from natural image-text
corpora using off-the-shelf models like CLIP and ChatGPT. We propose a
semi-automated approach to collect a new benchmark, NaturalBench, for reliably
evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a
$\textbf{vision-centric}$ design by pairing each question with two images that
yield different answers, preventing blind solutions from answering without
using the images. This makes NaturalBench more challenging than previous
benchmarks that can be solved with commonsense priors. We evaluate 53
state-of-the-art VLMs on NaturalBench, showing that models like
LLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o
lag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is
hard from two angles: (1) Compositionality: Solving NaturalBench requires
diverse visio-linguistic skills, including understanding attribute bindings,
object relationships, and advanced reasoning like logic and counting. To this
end, unlike prior work that uses a single tag per sample, we tag each
NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2)
Biases: NaturalBench exposes severe biases in VLMs, as models often choose the
same answer regardless of the image. Lastly, we apply our benchmark curation
method to diverse data sources, including long captions (over 100 words) and
non-English languages like Chinese and Hindi, highlighting its potential for
dynamic evaluations of VLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 24; We open-source our dataset at:
  https://huggingface.co/datasets/BaiqiL/NaturalBench; Project page at:
  https://linzhiqiu.github.io/papers/naturalbench/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallel Backpropagation for Inverse of a Convolution with Application
  to Normalizing Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandeep Nagar, Girish Varma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse of an invertible convolution is an important operation that comes up
in Normalizing Flows, Image Deblurring, etc. The naive algorithm for
backpropagation of this operation using Gaussian elimination has running time
$O(n^3)$ where $n$ is the number of pixels in the image. We give a fast
parallel backpropagation algorithm with running time $O(\sqrt{n})$ for a square
image and provide a GPU implementation of the same. Inverse Convolutions are
usually used in Normalizing Flows in the sampling pass, making them slow. We
propose to use Inverse Convolutions in the forward (image to latent vector)
pass of the Normalizing flow. Since the sampling pass is the inverse of the
forward pass, it will use convolutions only, resulting in efficient sampling
times. We use our parallel backpropagation algorithm for optimizing the inverse
convolution layer resulting in fast training times also. We implement this
approach in various Normalizing Flow backbones, resulting in our Inverse-Flow
models. We benchmark Inverse-Flow on standard datasets and show significantly
improved sampling times with similar bits per dimension compared to previous
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Swiss Army Knife: Synergizing Biases in Knowledge from Vision Foundation
  Models for Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Lu, Shengcao Cao, Yu-Xiong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Foundation Models (VFMs) have demonstrated outstanding performance on
numerous downstream tasks. However, due to their inherent representation biases
originating from different training paradigms, VFMs exhibit advantages and
disadvantages across distinct vision tasks. Although amalgamating the strengths
of multiple VFMs for downstream tasks is an intuitive strategy, effectively
exploiting these biases remains a significant challenge. In this paper, we
propose a novel and versatile "Swiss Army Knife" (SAK) solution, which
adaptively distills knowledge from a committee of VFMs to enhance multi-task
learning. Unlike existing methods that use a single backbone for knowledge
transfer, our approach preserves the unique representation bias of each teacher
by collaborating the lightweight Teacher-Specific Adapter Path modules with the
Teacher-Agnostic Stem. Through dynamic selection and combination of
representations with Mixture-of-Representations Routers, our SAK is capable of
synergizing the complementary strengths of multiple VFMs. Extensive experiments
show that our SAK remarkably outperforms prior state of the arts in multi-task
learning by 10% on the NYUD-v2 benchmark, while also providing a flexible and
robust framework that can readily accommodate more advanced model designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiOrg: A Multi-rater Organoid-detection Dataset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christina Bukas, Harshavardhan Subramanian, Fenja See, Carina Steinchen, Ivan Ezhov, Gowtham Boosarpu, Sara Asgharpour, Gerald Burgstaller, Mareike Lehmann, Florian Kofler, Marie Piraud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-throughput image analysis in the biomedical domain has gained
significant attention in recent years, driving advancements in drug discovery,
disease prediction, and personalized medicine. Organoids, specifically, are an
active area of research, providing excellent models for human organs and their
functions. Automating the quantification of organoids in microscopy images
would provide an effective solution to overcome substantial manual
quantification bottlenecks, particularly in high-throughput image analysis.
However, there is a notable lack of open biomedical datasets, in contrast to
other domains, such as autonomous driving, and, notably, only few of them have
attempted to quantify annotation uncertainty. In this work, we present MultiOrg
a comprehensive organoid dataset tailored for object detection tasks with
uncertainty quantification. This dataset comprises over 400 high-resolution 2d
microscopy images and curated annotations of more than 60,000 organoids. Most
importantly, it includes three label sets for the test data, independently
annotated by two experts at distinct time points. We additionally provide a
benchmark for organoid detection, and make the best model available through an
easily installable, interactive plugin for the popular image visualization tool
Napari, to perform organoid quantification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DRACO-DehazeNet: An Efficient Image Dehazing Network Combining Detail
  Recovery and a Novel Contrastive Learning Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gao Yu Lee, Tanmoy Dam, Md Meftahul Ferdaus, Daniel Puiu Poenar, Vu Duong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image dehazing is crucial for clarifying images obscured by haze or fog, but
current learning-based approaches is dependent on large volumes of training
data and hence consumed significant computational power. Additionally, their
performance is often inadequate under non-uniform or heavy haze. To address
these challenges, we developed the Detail Recovery And Contrastive DehazeNet,
which facilitates efficient and effective dehazing via a dense dilated inverted
residual block and an attention-based detail recovery network that tailors
enhancements to specific dehazed scene contexts. A major innovation is its
ability to train effectively with limited data, achieved through a novel
quadruplet loss-based contrastive dehazing paradigm. This approach distinctly
separates hazy and clear image features while also distinguish lower-quality
and higher-quality dehazed images obtained from each sub-modules of our
network, thereby refining the dehazing process to a larger extent. Extensive
tests on a variety of benchmarked haze datasets demonstrated the superiority of
our approach. The code repository for this work will be available soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to a journal and currently under review. Once the paper is
  accepted and published, the copyright will be transferred to the
  corresponding journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachel S. Y. Teo, Tan M. Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled
scalability in deep learning. SMoE has the potential to exponentially increase
parameter count while maintaining the efficiency of the model by only
activating a small subset of these parameters for a given sample. However, it
has been observed that SMoE suffers from unstable training and has difficulty
adapting to new distributions, leading to the model's lack of robustness to
data contamination. To overcome these limitations, we first establish a
connection between the dynamics of the expert representations in SMoEs and
gradient descent on a multi-objective optimization problem. Leveraging our
framework, we then integrate momentum into SMoE and propose a new family of
SMoEs named MomentumSMoE. We theoretically prove and numerically demonstrate
that MomentumSMoE is more stable and robust than SMoE. In particular, we verify
the advantages of MomentumSMoE over SMoE on a variety of practical tasks
including ImageNet-1K object recognition and WikiText-103 language modeling. We
demonstrate the applicability of MomentumSMoE to many types of SMoE models,
including those in the Sparse MoE model for vision (V-MoE) and the Generalist
Language Model (GLaM). We also show that other advanced momentum-based
optimization methods, such as Adam, can be easily incorporated into the
MomentumSMoE framework for designing new SMoE models with even better
performance, almost negligible additional computation cost, and simple
implementations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages in the main text. Published at NeurIPS 2024. The code is
  available at https://github.com/rachtsy/MomentumSMoE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multi-modal</span> Pose Diffuser: A <span class="highlight-title">Multimodal</span> Generative Conditional Pose
  Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Calvin-Khang Ta, Arindam Dutta, Rohit Kundu, Rohit Lal, Hannah Dela Cruz, Dripta S. Raychaudhuri, Amit Roy-Chowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Skinned Multi-Person Linear (SMPL) model plays a crucial role in 3D human
pose estimation, providing a streamlined yet effective representation of the
human body. However, ensuring the validity of SMPL configurations during tasks
such as human mesh regression remains a significant challenge , highlighting
the necessity for a robust human pose prior capable of discerning realistic
human poses. To address this, we introduce MOPED:
\underline{M}ulti-m\underline{O}dal \underline{P}os\underline{E}
\underline{D}iffuser. MOPED is the first method to leverage a novel multi-modal
conditional diffusion model as a prior for SMPL pose parameters. Our method
offers powerful unconditional pose generation with the ability to condition on
multi-modal inputs such as images and text. This capability enhances the
applicability of our approach by incorporating additional context often
overlooked in traditional pose priors. Extensive experiments across three
distinct tasks-pose estimation, pose denoising, and pose completion-demonstrate
that our multi-modal diffusion model-based prior significantly outperforms
existing methods. These results indicate that our model captures a broader
spectrum of plausible human poses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hybrid Feature Fusion Deep Learning Framework for Leukemia Cancer
  Detection in Microscopic Blood Sample Using Gated Recurrent Unit and
  Uncertainty Quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksuda Akter, Rabea Khatun, Md Manowarul Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acute lymphoblastic leukemia (ALL) is the most malignant form of leukemia and
the most common cancer in adults and children. Traditionally, leukemia is
diagnosed by analyzing blood and bone marrow smears under a microscope, with
additional cytochemical tests for confirmation. However, these methods are
expensive, time consuming, and highly dependent on expert knowledge. In recent
years, deep learning, particularly Convolutional Neural Networks (CNNs), has
provided advanced methods for classifying microscopic smear images, aiding in
the detection of leukemic cells. These approaches are quick, cost effective,
and not subject to human bias. However, most methods lack the ability to
quantify uncertainty, which could lead to critical misdiagnoses. In this
research, hybrid deep learning models (InceptionV3-GRU, EfficientNetB3-GRU,
MobileNetV2-GRU) were implemented to classify ALL. Bayesian optimization was
used to fine tune the model's hyperparameters and improve its performance.
Additionally, Deep Ensemble uncertainty quantification was applied to address
uncertainty during leukemia image classification. The proposed models were
trained on the publicly available datasets ALL-IDB1 and ALL-IDB2. Their results
were then aggregated at the score level using the sum rule. The parallel
architecture used in these models offers a high level of confidence in
differentiating between ALL and non-ALL cases. The proposed method achieved a
remarkable detection accuracy rate of 100% on the ALL-IDB1 dataset, 98.07% on
the ALL-IDB2 dataset, and 98.64% on the combined dataset, demonstrating its
potential for accurate and reliable leukemia diagnosis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Less is More: Selective Reduction of CT Data for Self-Supervised
  <span class="highlight-title">Pre-Train</span>ing of Deep Learning Models with Contrastive Learning Improves
  Downstream Classification Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Wolf, Tristan Payer, Catharina Silvia Lisson, Christoph Gerhard Lisson, Meinrad Beer, Michael Götz, Timo Ropinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised pre-training of deep learning models with contrastive
learning is a widely used technique in image analysis. Current findings
indicate a strong potential for contrastive pre-training on medical images.
However, further research is necessary to incorporate the particular
characteristics of these images. We hypothesize that the similarity of medical
images hinders the success of contrastive learning in the medical imaging
domain. To this end, we investigate different strategies based on deep
embedding, information theory, and hashing in order to identify and reduce
redundancy in medical pre-training datasets. The effect of these different
reduction strategies on contrastive learning is evaluated on two pre-training
datasets and several downstream classification tasks. In all of our
experiments, dataset reduction leads to a considerable performance gain in
downstream tasks, e.g., an AUC score improvement from 0.78 to 0.83 for the
COVID CT Classification Grand Challenge, 0.97 to 0.98 for the OrganSMNIST
Classification Challenge and 0.73 to 0.83 for a brain hemorrhage classification
task. Furthermore, pre-training is up to nine times faster due to the dataset
reduction. In conclusion, the proposed approach highlights the importance of
dataset quality and provides a transferable approach to improve contrastive
pre-training for classification downstream tasks on medical images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Computers in Biology and Medicine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">CLIP</span>-VAD: Exploiting <span class="highlight-title">Vision-Language</span> Models for Voice Activity Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Appiani, Cigdem Beyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice Activity Detection (VAD) is the process of automatically determining
whether a person is speaking and identifying the timing of their speech in an
audiovisual data. Traditionally, this task has been tackled by processing
either audio signals or visual data, or by combining both modalities through
fusion or joint learning. In our study, drawing inspiration from recent
advancements in visual-language models, we introduce a novel approach
leveraging Contrastive Language-Image Pretraining (CLIP) models. The CLIP
visual encoder analyzes video segments composed of the upper body of an
individual, while the text encoder handles textual descriptions automatically
generated through prompt engineering. Subsequently, embeddings from these
encoders are fused through a deep neural network to perform VAD. Our
experimental analysis across three VAD benchmarks showcases the superior
performance of our method compared to existing visual VAD approaches. Notably,
our approach outperforms several audio-visual methods despite its simplicity,
and without requiring pre-training on extensive audio-visual datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEAD: Latent Realignment for Human Motion Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nefeli Andreou, Xi Wang, Victoria Fernández Abrevaya, Marie-Paule Cani, Yiorgos Chrysanthou, Vicky Kalogeiton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our goal is to generate realistic human motion from natural language. Modern
methods often face a trade-off between model expressiveness and text-to-motion
alignment. Some align text and motion latent spaces but sacrifice
expressiveness; others rely on diffusion models producing impressive motions,
but lacking semantic meaning in their latent space. This may compromise
realism, diversity, and applicability. Here, we address this by combining
latent diffusion with a realignment mechanism, producing a novel, semantically
structured space that encodes the semantics of language. Leveraging this
capability, we introduce the task of textual motion inversion to capture novel
motion concepts from a few examples. For motion synthesis, we evaluate LEAD on
HumanML3D and KIT-ML and show comparable performance to the state-of-the-art in
terms of realism, diversity, and text-motion consistency. Our qualitative
analysis and user study reveal that our synthesized motions are sharper, more
human-like and comply better with the text compared to modern methods. For
motion textual inversion, our method demonstrates improved capacity in
capturing out-of-distribution characteristics in comparison to traditional
VAEs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Real-Time Recalibration for Infrared Multi-Camera Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benyamin Mehmandar, Reza Talakoob, Charalambos Poullis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, there are no learning-free or neural techniques for real-time
recalibration of infrared multi-camera systems. In this paper, we address the
challenge of real-time, highly-accurate calibration of multi-camera infrared
systems, a critical task for time-sensitive applications. Unlike traditional
calibration techniques that lack adaptability and struggle with on-the-fly
recalibrations, we propose a neural network-based method capable of dynamic
real-time calibration. The proposed method integrates a differentiable
projection model that directly correlates 3D geometries with their 2D image
projections and facilitates the direct optimization of both intrinsic and
extrinsic camera parameters. Key to our approach is the dynamic camera pose
synthesis with perturbations in camera parameters, emulating realistic
operational challenges to enhance model robustness. We introduce two model
variants: one designed for multi-camera systems with onboard processing of 2D
points, utilizing the direct 2D projections of 3D fiducials, and another for
image-based systems, employing color-coded projected points for implicitly
establishing correspondence. Through rigorous experimentation, we demonstrate
our method is more accurate than traditional calibration techniques with or
without perturbations while also being real-time, marking a significant leap in
the field of real-time multi-camera system calibration. The source code can be
found at https://github.com/theICTlab/neural-recalibration
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>real-time camera calibration, infrared camera, neural calibration</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Integrated Deep Learning Model for Skin Cancer Detection Using Hybrid
  Feature Fusion Technique 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksuda Akter, Rabea Khatun, Md. Alamin Talukder, Md. Manowarul Islam, Md. Ashraf Uddin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skin cancer is a serious and potentially fatal disease caused by DNA damage.
Early detection significantly increases survival rates, making accurate
diagnosis crucial. In this groundbreaking study, we present a hybrid framework
based on Deep Learning (DL) that achieves precise classification of benign and
malignant skin lesions. Our approach begins with dataset preprocessing to
enhance classification accuracy, followed by training two separate pre-trained
DL models, InceptionV3 and DenseNet121. By fusing the results of each model
using the weighted sum rule, our system achieves exceptional accuracy rates.
Specifically, we achieve a 92.27% detection accuracy rate, 92.33% sensitivity,
92.22% specificity, 90.81% precision, and 91.57% F1-score, outperforming
existing models and demonstrating the robustness and trustworthiness of our
hybrid approach. Our study represents a significant advance in skin cancer
diagnosis and provides a promising foundation for further research in the
field. With the potential to save countless lives through earlier detection,
our hybrid deep-learning approach is a game-changer in the fight against skin
cancer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Do Training Methods Influence the Utilization of Vision Models? <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Gavrikov, Shashank Agnihotri, Margret Keuper, Janis Keuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Not all learnable parameters (e.g., weights) contribute equally to a neural
network's decision function. In fact, entire layers' parameters can sometimes
be reset to random values with little to no impact on the model's decisions. We
revisit earlier studies that examined how architecture and task complexity
influence this phenomenon and ask: is this phenomenon also affected by how we
train the model? We conducted experimental evaluations on a diverse set of
ImageNet-1k classification models to explore this, keeping the architecture and
training data constant but varying the training pipeline. Our findings reveal
that the training method strongly influences which layers become critical to
the decision function for a given task. For example, improved training regimes
and self-supervised training increase the importance of early layers while
significantly under-utilizing deeper layers. In contrast, methods such as
adversarial training display an opposite trend. Our preliminary results extend
previous findings, offering a more nuanced understanding of the inner mechanics
of neural networks.
  Code: https://github.com/paulgavrikov/layer_criticality
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Interpretable AI: Past, Present and Future Workshop
  at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LUDVIG: Learning-free Uplifting of 2D Visual features to Gaussian
  Splatting scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juliette Marrie, Romain Ménégaux, Michael Arbel, Diane Larlus, Julien Mairal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the task of uplifting visual features or semantic masks from 2D
vision models to 3D scenes represented by Gaussian Splatting. Whereas common
approaches rely on iterative optimization-based procedures, we show that a
simple yet effective aggregation technique yields excellent results. Applied to
semantic masks from Segment Anything (SAM), our uplifting approach leads to
segmentation quality comparable to the state of the art. We then extend this
method to generic DINOv2 features, integrating 3D scene geometry through graph
diffusion, and achieve competitive segmentation results despite DINOv2 not
being trained on millions of annotated masks like SAM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Generalizing Visual Brain Decoding to Unseen Subjects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangtao Kong, Kexin Huang, Ping Li, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual brain decoding aims to decode visual information from human brain
activities. Despite the great progress, one critical limitation of current
brain decoding research lies in the lack of generalization capability to unseen
subjects. Prior works typically focus on decoding brain activity of individuals
based on the observation that different subjects exhibit different brain
activities, while it remains unclear whether brain decoding can be generalized
to unseen subjects. This study aims to answer this question. We first
consolidate an image-fMRI dataset consisting of stimulus-image and
fMRI-response pairs, involving 177 subjects in the movie-viewing task of the
Human Connectome Project (HCP). This dataset allows us to investigate the brain
decoding performance with the increase of participants. We then present a
learning paradigm that applies uniform processing across all subjects, instead
of employing different network heads or tokenizers for individuals as in
previous methods, which can accommodate a large number of subjects to explore
the generalization capability across different subjects. A series of
experiments are conducted and we have the following findings. First, the
network exhibits clear generalization capabilities with the increase of
training subjects. Second, the generalization capability is common to popular
network architectures (MLP, CNN and Transformer). Third, the generalization
performance is affected by the similarity between subjects. Our findings reveal
the inherent similarities in brain activities across individuals. With the
emerging of larger and more comprehensive datasets, it is possible to train a
brain decoding foundation model in the future.Codes and models can be found at
https://github.com/Xiangtaokong/TGBD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FashionR2R: Texture-preserving Rendered-to-Real Image Translation with
  Diffusion Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Hu, Qian He, Gaofeng He, Jiedong Zhuang, Huang Chen, Huafeng Liu, Huamin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling and producing lifelike clothed human images has attracted
researchers' attention from different areas for decades, with the complexity
from highly articulated and structured content. Rendering algorithms decompose
and simulate the imaging process of a camera, while are limited by the accuracy
of modeled variables and the efficiency of computation. Generative models can
produce impressively vivid human images, however still lacking in
controllability and editability. This paper studies photorealism enhancement of
rendered images, leveraging generative power from diffusion models on the
controlled basis of rendering. We introduce a novel framework to translate
rendered images into their realistic counterparts, which consists of two
stages: Domain Knowledge Injection (DKI) and Realistic Image Generation (RIG).
In DKI, we adopt positive (real) domain finetuning and negative (rendered)
domain embedding to inject knowledge into a pretrained Text-to-image (T2I)
diffusion model. In RIG, we generate the realistic image corresponding to the
input rendered image, with a Texture-preserving Attention Control (TAC) to
preserve fine-grained clothing textures, exploiting the decoupled features
encoded in the UNet structure. Additionally, we introduce SynFashion dataset,
featuring high-quality digital clothing images with diverse textures. Extensive
experimental results demonstrate the superiority and effectiveness of our
method in rendered-to-real image translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Deep Learning with Fundus and Optical Coherence Tomography
  for Cardiovascular Disease Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cynthia Maldonado-Garcia, Arezoo Zakeri, Alejandro F Frangi, Nishant Ravikumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early identification of patients at risk of cardiovascular diseases (CVD) is
crucial for effective preventive care, reducing healthcare burden, and
improving patients' quality of life. This study demonstrates the potential of
retinal optical coherence tomography (OCT) imaging combined with fundus
photographs for identifying future adverse cardiac events. We used data from
977 patients who experienced CVD within a 5-year interval post-image
acquisition, alongside 1,877 control participants without CVD, totaling 2,854
subjects. We propose a novel binary classification network based on a
Multi-channel Variational Autoencoder (MCVAE), which learns a latent embedding
of patients' fundus and OCT images to classify individuals into two groups:
those likely to develop CVD in the future and those who are not. Our model,
trained on both imaging modalities, achieved promising results (AUROC 0.78 +/-
0.02, accuracy 0.68 +/- 0.002, precision 0.74 +/- 0.02, sensitivity 0.73 +/-
0.02, and specificity 0.68 +/- 0.01), demonstrating its efficacy in identifying
patients at risk of future CVD events based on their retinal images. This study
highlights the potential of retinal OCT imaging and fundus photographs as
cost-effective, non-invasive alternatives for predicting cardiovascular disease
risk. The widespread availability of these imaging techniques in optometry
practices and hospitals further enhances their potential for large-scale CVD
risk screening. Our findings contribute to the development of standardized,
accessible methods for early CVD risk identification, potentially improving
preventive care strategies and patient outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Part of the book series: Lecture Notes in Computer Science
  ((LNCS,volume 15155))</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variable Aperture Bokeh Rendering via Customized Focal Plane Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kang Chen, Shijun Yan, Aiwen Jiang, Han Li, Zhifeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bokeh rendering is one of the most popular techniques in photography. It can
make photographs visually appealing, forcing users to focus their attentions on
particular area of image. However, achieving satisfactory bokeh effect usually
presents significant challenge, since mobile cameras with restricted optical
systems are constrained, while expensive high-end DSLR lens with large aperture
should be needed. Therefore, many deep learning-based computational photography
methods have been developed to mimic the bokeh effect in recent years.
Nevertheless, most of these methods were limited to rendering bokeh effect in
certain single aperture. There lacks user-friendly bokeh rendering method that
can provide precise focal plane control and customised bokeh generation. There
as well lacks authentic realistic bokeh dataset that can potentially promote
bokeh learning on variable apertures. To address these two issues, in this
paper, we have proposed an effective controllable bokeh rendering method, and
contributed a Variable Aperture Bokeh Dataset (VABD). In the proposed method,
user can customize focal plane to accurately locate concerned subjects and
select target aperture information for bokeh rendering. Experimental results on
public EBB! benchmark dataset and our constructed dataset VABD have
demonstrated that the customized focal plane together aperture prompt can
bootstrap model to simulate realistic bokeh effect. The proposed method has
achieved competitive state-of-the-art performance with only 4.4M parameters,
which is much lighter than mainstream computational bokeh models. The
contributed dataset and source codes will be released on github
https://github.com/MoTong-AI-studio/VABM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Negative Guidance of Diffusion Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Koulischer, Johannes Deleu, Gabriel Raya, Thomas Demeester, Luca Ambrogioni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Negative Prompting (NP) is widely utilized in diffusion models, particularly
in text-to-image applications, to prevent the generation of undesired features.
In this paper, we show that conventional NP is limited by the assumption of a
constant guidance scale, which may lead to highly suboptimal results, or even
complete failure, due to the non-stationarity and state-dependence of the
reverse process. Based on this analysis, we derive a principled technique
called Dynamic Negative Guidance, which relies on a near-optimal time and state
dependent modulation of the guidance without requiring additional training.
Unlike NP, negative guidance requires estimating the posterior class
probability during the denoising process, which is achieved with limited
additional computational overhead by tracking the discrete Markov Chain during
the generative process. We evaluate the performance of DNG class-removal on
MNIST and CIFAR10, where we show that DNG leads to higher safety, preservation
of class balance and image quality when compared with baseline methods.
Furthermore, we show that it is possible to use DNG with Stable Diffusion to
obtain more accurate and less invasive guidance than NP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper currently under review. Submitted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SurgeryV2: Bridging the Gap Between Model Merging and Multi-Task
  Learning with Deep Representation Surgery <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enneng Yang, Li Shen, Zhenyi Wang, Guibing Guo, Xingwei Wang, Xiaocun Cao, Jie Zhang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging-based multitask learning (MTL) offers a promising approach for
performing MTL by merging multiple expert models without requiring access to
raw training data. However, in this paper, we examine the merged model's
representation distribution and uncover a critical issue of "representation
bias". This bias arises from a significant distribution gap between the
representations of the merged and expert models, leading to the suboptimal
performance of the merged MTL model. To address this challenge, we first
propose a representation surgery solution called Surgery. Surgery is a
lightweight, task-specific module that aligns the final layer representations
of the merged model with those of the expert models, effectively alleviating
bias and improving the merged model's performance. Despite these improvements,
a performance gap remains compared to the traditional MTL method. Further
analysis reveals that representation bias phenomena exist at each layer of the
merged model, and aligning representations only in the last layer is
insufficient for fully reducing systemic bias because biases introduced at each
layer can accumulate and interact in complex ways. To tackle this, we then
propose a more comprehensive solution, deep representation surgery (also called
SurgeryV2), which mitigates representation bias across all layers, and thus
bridges the performance gap between model merging-based MTL and traditional
MTL. Finally, we design an unsupervised optimization objective to optimize both
the Surgery and SurgeryV2 modules. Our experimental results show that
incorporating these modules into state-of-the-art (SOTA) model merging schemes
leads to significant performance gains. Notably, our SurgeryV2 scheme reaches
almost the same level as individual expert models or the traditional MTL model.
The code is available at \url{https://github.com/EnnengYang/SurgeryV2}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is an extended version of our previous work
  [arXiv:2402.02705] presented at ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnomalyNCD: Towards Novel Anomaly Class Discovery in Industrial
  Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziming Huang, Xurui Li, Haotian Liu, Feng Xue, Yuzhe Wang, Yu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the industrial scenario, anomaly detection could locate but cannot
classify anomalies. To complete their capability, we study to automatically
discover and recognize visual classes of industrial anomalies. In terms of
multi-class anomaly classification, previous methods cluster anomalies
represented by frozen pre-trained models but often fail due to poor
discrimination. Novel class discovery (NCD) has the potential to tackle this.
However, it struggles with non-prominent and semantically weak anomalies that
challenge network learning focus. To address these, we introduce AnomalyNCD, a
multi-class anomaly classification framework compatible with existing anomaly
detection methods. This framework learns anomaly-specific features and
classifies anomalies in a self-supervised manner. Initially, a technique called
Main Element Binarization (MEBin) is first designed, which segments primary
anomaly regions into masks to alleviate the impact of incorrect detections on
learning. Subsequently, we employ mask-guided contrastive representation
learning to improve feature discrimination, which focuses network attention on
isolated anomalous regions and reduces the confusion of erroneous inputs
through re-corrected pseudo labels. Finally, to enable flexible classification
at both region and image levels during inference, we develop a region merging
strategy that determines the overall image category based on the classified
anomaly regions. Our method outperforms the state-of-the-art works on the MVTec
AD and MTD datasets. Compared with the current methods, AnomalyNCD combined
with zero-shot anomaly detection method achieves a 10.8% $F_1$ gain, 8.8% NMI
gain, and 9.5% ARI gain on MVTec AD, 12.8% $F_1$ gain, 5.7% NMI gain, and 10.8%
ARI gain on MTD. The source code is available at
https://github.com/HUST-SLOW/AnomalyNCD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of imperfect annotations on CNN training and performance for
  instance segmentation and classification in digital pathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Gálvez Jiménez, Christine Decaestecker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation and classification of large numbers of instances, such as cell
nuclei, are crucial tasks in digital pathology for accurate diagnosis. However,
the availability of high-quality datasets for deep learning methods is often
limited due to the complexity of the annotation process. In this work, we
investigate the impact of noisy annotations on the training and performance of
a state-of-the-art CNN model for the combined task of detecting, segmenting and
classifying nuclei in histopathology images. In this context, we investigate
the conditions for determining an appropriate number of training epochs to
prevent overfitting to annotation noise during training. Our results indicate
that the utilisation of a small, correctly annotated validation set is
instrumental in avoiding overfitting and maintaining model performance to a
large extent. Additionally, our findings underscore the beneficial role of
pre-training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 2D-3D Deformable Image Registration of Histology Slide and Micro-CT with
  ML-based Initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junan Chen, Matteo Ronchetti, Verena Stehl, Van Nguyen, Muhannad Al Kallaa, Mahesh Thalwaththe Gedara, Claudia Lölkes, Stefan Moser, Maximilian Seidl, Matthias Wieczorek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in the registration of histology and micro-computed
tomography ({\mu}CT) have broadened the perspective of pathological
applications such as virtual histology based on {\mu}CT. This topic remains
challenging because of the low image quality of soft tissue CT. Additionally,
soft tissue samples usually deform during the histology slide preparation,
making it difficult to correlate the structures between histology slide and
{\mu}CT. In this work, we propose a novel 2D-3D multi-modal deformable image
registration method. The method uses a machine learning (ML) based
initialization followed by the registration. The registration is finalized by
an analytical out-of-plane deformation refinement. The method is evaluated on
datasets acquired from tonsil and tumor tissues. {\mu}CTs of both
phase-contrast and conventional absorption modalities are investigated. The
registration results from the proposed method are compared with those from
intensity- and keypoint-based methods. The comparison is conducted using both
visual and fiducial-based evaluations. The proposed method demonstrates
superior performance compared to the other two methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-shot Action Localization via the Confidence of Large
  <span class="highlight-title">Vision-Language</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josiah Aklilu, Xiaohan Wang, Serena Yeung-Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise action localization in untrimmed video is vital for fields such as
professional sports and minimally invasive surgery, where the delineation of
particular motions in recordings can dramatically enhance analysis. But in many
cases, large scale datasets with video-label pairs for localization are
unavailable, limiting the opportunity to fine-tune video-understanding models.
Recent developments in large vision-language models (LVLM) address this need
with impressive zero-shot capabilities in a variety of video understanding
tasks. However, the adaptation of image-based LVLMs, with their powerful visual
question answering capabilities, to action localization in long-form video is
still relatively unexplored. To this end, we introduce a true ZEro-shot Action
Localization method (ZEAL). Specifically, we leverage the built-in action
knowledge of a large language model (LLM) to inflate actions into
highly-detailed descriptions of the archetypal start and end of the action.
These descriptions serve as queries to LVLM for generating frame-level
confidence scores which can be aggregated to produce localization outputs. The
simplicity and flexibility of our method lends it amenable to more capable
LVLMs as they are developed, and we demonstrate remarkable results in zero-shot
action localization on a challenging benchmark, without any training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the evaluators: Towards human-aligned metrics for missing
  markers reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taras Kucherenko, Derek Peristy, Judith Bütepage
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animation data is often obtained through optical motion capture systems,
which utilize a multitude of cameras to establish the position of optical
markers. However, system errors or occlusions can result in missing markers,
the manual cleaning of which can be time-consuming. This has sparked interest
in machine learning-based solutions for missing marker reconstruction in the
academic community. Most academic papers utilize a simplistic mean square error
as the main metric. In this paper, we show that this metric does not correlate
with subjective perception of the fill quality. We introduce and evaluate a set
of better-correlated metrics that can drive progress in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Croc: <span class="highlight-title">Pretrain</span>ing Large <span class="highlight-title">Multimodal</span> Models with Cross-Modal Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Xie, Kaicheng Yang, Ninghua Yang, Weimo Deng, Xiangzi Dai, Tiancheng Gu, Yumeng Wang, Xiang An, Yongle Zhao, Ziyong Feng, Jiankang Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have catalyzed the
development of Large Multimodal Models (LMMs). However, existing research
primarily focuses on tuning language and image instructions, ignoring the
critical pretraining phase where models learn to process textual and visual
modalities jointly. In this paper, we propose a new pretraining paradigm for
LMMs to enhance the visual comprehension capabilities of LLMs by introducing a
novel cross-modal comprehension stage. Specifically, we design a dynamically
learnable prompt token pool and employ the Hungarian algorithm to replace part
of the original visual tokens with the most relevant prompt tokens. Then, we
conceptualize visual tokens as analogous to a "foreign language" for the LLMs
and propose a mixed attention mechanism with bidirectional visual attention and
unidirectional textual attention to comprehensively enhance the understanding
of visual tokens. Meanwhile, we integrate a detailed caption generation task,
leveraging rich descriptions to further facilitate LLMs in understanding visual
semantic information. After pretraining on 1.5 million publicly accessible
data, we present a new foundation model called Croc. Experimental results
demonstrate that Croc achieves new state-of-the-art performance on massive
vision-language benchmarks. To support reproducibility and facilitate further
research, we release the training code and pre-trained model weights at
https://github.com/deepglint/Croc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast proxy centers for Jeffreys centroids: The Jeffreys-Fisher-Rao and
  the inductive Gauss-Bregman centers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frank Nielsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The symmetric Kullback-Leibler centroid also called the Jeffreys centroid of
a set of mutually absolutely continuous probability distributions on a measure
space provides a notion of centrality which has proven useful in many tasks
including information retrieval, information fusion, and clustering in image,
video and sound processing. However, the Jeffreys centroid is not available in
closed-form for sets of categorical or normal distributions, two widely used
statistical models, and thus need to be approximated numerically in practice.
In this paper, we first propose the new Jeffreys-Fisher-Rao center defined as
the Fisher-Rao midpoint of the sided Kullback-Leibler centroids as a plug-in
replacement of the Jeffreys centroid. This Jeffreys-Fisher-Rao center admits a
generic formula for uni-parameter exponential family distributions, and
closed-form formula for categorical and normal distributions, matches exactly
the Jeffreys centroid for same-mean normal distributions, and is experimentally
observed in practice to be close to the Jeffreys centroid. Second, we define a
new type of inductive centers generalizing the principle of Gauss
arithmetic-geometric double sequence mean for pairs of densities of any given
exponential family. This center is shown experimentally to approximate very
well the Jeffreys centroid and is suggested to use when the Jeffreys-Fisher-Rao
center is not available in closed form. Moreover, this Gauss-Bregman inductive
center always converges and matches the Jeffreys centroid for sets of same-mean
normal distributions. We report on our experiments demonstrating the use of the
Jeffreys-Fisher-Rao and Gauss-Bregman centers instead of the Jeffreys centroid.
Finally, we conclude this work by reinterpreting these fast proxy centers of
Jeffreys centroids under the lens of dually flat spaces in information
geometry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiCo: Hierarchical Controllable Diffusion Model for Layout-to-image
  Generation <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Cheng, Yuhang Ma, Liebucha Wu, Shanyuan Liu, Ao Ma, Xiaoyu Wu, Dawei Leng, Yuhui Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of layout-to-image generation involves synthesizing images based on
the captions of objects and their spatial positions. Existing methods still
struggle in complex layout generation, where common bad cases include object
missing, inconsistent lighting, conflicting view angles, etc. To effectively
address these issues, we propose a \textbf{Hi}erarchical \textbf{Co}ntrollable
(HiCo) diffusion model for layout-to-image generation, featuring object
seperable conditioning branch structure. Our key insight is to achieve spatial
disentanglement through hierarchical modeling of layouts. We use a multi branch
structure to represent hierarchy and aggregate them in fusion module. To
evaluate the performance of multi-objective controllable layout generation in
natural scenes, we introduce the HiCo-7K benchmark, derived from the GRIT-20M
dataset and manually cleaned. https://github.com/360CVGroup/HiCo_T2I.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advanced Underwater Image Quality Enhancement via Hybrid
  Super-Resolution Convolutional Neural Networks and Multi-Scale Retinex-Based
  Defogging Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yugandhar Reddy Gogireddy, Jithendra Reddy Gogireddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The difficulties of underwater image degradation due to light scattering,
absorption, and fog-like particles which lead to low resolution and poor
visibility are discussed in this study report. We suggest a sophisticated
hybrid strategy that combines Multi-Scale Retinex (MSR) defogging methods with
Super-Resolution Convolutional Neural Networks (SRCNN) to address these
problems. The Retinex algorithm mimics human visual perception to reduce uneven
lighting and fogging, while the SRCNN component improves the spatial resolution
of underwater photos.Through the combination of these methods, we are able to
enhance the clarity, contrast, and colour restoration of underwater images,
offering a reliable way to improve image quality in difficult underwater
conditions. The research conducts extensive experiments on real-world
underwater datasets to further illustrate the efficacy of the suggested
approach. In terms of sharpness, visibility, and feature retention,
quantitative evaluation which use metrics like the Structural Similarity Index
Measure (SSIM) and Peak Signal-to-Noise Ratio (PSNR) demonstrates notable
advances over conventional techniques.In real-time underwater applications like
marine exploration, underwater robotics, and autonomous underwater vehicles,
where clear and high-resolution imaging is crucial for operational success, the
combination of deep learning and conventional image processing techniques
offers a computationally efficient framework with superior results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Takin-ADA: Emotion Controllable Audio-Driven Animation with Canonical
  and Landmark Loss Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Lin, Yanzhen Yu, Jianhao Ye, Ruitao Lv, Yuguang Yang, Ruoye Xie, Pan Yu, Hongbin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing audio-driven facial animation methods face critical challenges,
including expression leakage, ineffective subtle expression transfer, and
imprecise audio-driven synchronization. We discovered that these issues stem
from limitations in motion representation and the lack of fine-grained control
over facial expressions. To address these problems, we present Takin-ADA, a
novel two-stage approach for real-time audio-driven portrait animation. In the
first stage, we introduce a specialized loss function that enhances subtle
expression transfer while reducing unwanted expression leakage. The second
stage utilizes an advanced audio processing technique to improve lip-sync
accuracy. Our method not only generates precise lip movements but also allows
flexible control over facial expressions and head motions. Takin-ADA achieves
high-resolution (512x512) facial animations at up to 42 FPS on an RTX 4090 GPU,
outperforming existing commercial solutions. Extensive experiments demonstrate
that our model significantly surpasses previous methods in video quality,
facial dynamics realism, and natural head movements, setting a new benchmark in
the field of audio-driven facial animation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ You Only Look Twice! for Failure Causes Identification of Drill Bits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asma Yamani, Nehal Al-Otaiby, Haifa Al-Shemmeri, Imane Boudellioua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient identification of the root causes of drill bit failure is crucial
due to potential impacts such as operational losses, safety threats, and
delays. Early recognition of these failures enables proactive maintenance,
reducing risks and financial losses associated with unforeseen breakdowns and
prolonged downtime. Thus, our study investigates various causes of drill bit
failure using images of different blades. The process involves annotating
cutters with their respective locations and damage types, followed by the
development of two YOLO Location and Damage Cutter Detection models, as well as
multi-class multi-label Decision Tree and Random Forests models to identify the
causes of failure by assessing the cutters' location and damage type.
Additionally, RRFCI is proposed for the classification of failure causes.
Notably, the cutter location detection model achieved a high score of 0.97 mPA,
and the cutter damage detection model yielded a 0.49 mPA. The rule-based
approach over-performed both DT and RF in failure cause identification,
achieving a macro-average F1-score of 0.94 across all damage causes. The
integration of the complete automated pipeline successfully identified 100\% of
the 24 failure causes when tested on independent sets of ten drill bits,
showcasing its potential to efficiently assist experts in identifying the root
causes of drill bit damages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ClearSR: Latent Low-Resolution Image Embeddings Help Diffusion-Based
  Real-World Super Resolution Models See Clearer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Wan, Peng-Tao Jiang, Qibin Hou, Hao Zhang, Jinwei Chen, Ming-Ming Cheng, Bo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ClearSR, a new method that can better take advantage of latent
low-resolution image (LR) embeddings for diffusion-based real-world image
super-resolution (Real-ISR). Previous Real-ISR models mostly focus on how to
activate more generative priors of text-to-image diffusion models to make the
output high-resolution (HR) images look better. However, since these methods
rely too much on the generative priors, the content of the output images is
often inconsistent with the input LR ones. To mitigate the above issue, in this
work, we explore using latent LR embeddings to constrain the control signals
from ControlNet, and extract LR information at both detail and structure
levels. We show that the proper use of latent LR embeddings can produce
higher-quality control signals, which enables the super-resolution results to
be more consistent with the LR image and leads to clearer visual results. In
addition, we also show that latent LR embeddings can be used to control the
inference stage, allowing for the improvement of fidelity and generation
ability simultaneously. Experiments demonstrate that our model can achieve
better performance across multiple metrics on several test sets and generate
more consistent SR results with LR images than existing methods. Our code will
be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HYPNOS : Highly Precise Foreground-focused Diffusion Finetuning for
  Inanimate Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliverio Theophilus Nathanael, Jonathan Samuel Lumentut, Nicholas Hans Muliawan, Edbert Valencio Angky, Felix Indra Kurniadi, Alfi Yusrotis Zakiyyah, Jeklin Harefa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, personalized diffusion-based text-to-image generative tasks
have been a hot topic in computer vision studies. A robust diffusion model is
determined by its ability to perform near-perfect reconstruction of certain
product outcomes given few related input samples. Unfortunately, the current
prominent diffusion-based finetuning technique falls short in maintaining the
foreground object consistency while being constrained to produce diverse
backgrounds in the image outcome. In the worst scenario, the overfitting issue
may occur, meaning that the foreground object is less controllable due to the
condition above, for example, the input prompt information is transferred
ambiguously to both foreground and background regions, instead of the supposed
background region only. To tackle the issues above, we proposed Hypnos, a
highly precise foreground-focused diffusion finetuning technique. On the image
level, this strategy works best for inanimate object generation tasks, and to
do so, Hypnos implements two main approaches, namely: (i) a content-centric
prompting strategy and (ii) the utilization of our additional
foreground-focused discriminative module. The utilized module is connected with
the diffusion model and finetuned with our proposed set of supervision
mechanism. Combining the strategies above yielded to the foreground-background
disentanglement capability of the diffusion model. Our experimental results
showed that the proposed strategy gave a more robust performance and visually
pleasing results compared to the former technique. For better elaborations, we
also provided extensive studies to assess the fruitful outcomes above, which
reveal how personalization behaves in regard to several training conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 12 figures, to appear on the Rich Media with Generative AI
  workshop in conjunction with Asian Conference on Computer Vision (ACCV) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Vision-Language</span> Navigation with Energy-Based Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Liu, Wenguan Wang, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language navigation (VLN) requires an agent to execute actions
following human instructions. Existing VLN models are optimized through expert
demonstrations by supervised behavioural cloning or incorporating manual reward
engineering. While straightforward, these efforts overlook the accumulation of
errors in the Markov decision process, and struggle to match the distribution
of the expert policy. Going beyond this, we propose an Energy-based Navigation
Policy (ENP) to model the joint state-action distribution using an energy-based
model. At each step, low energy values correspond to the state-action pairs
that the expert is most likely to perform, and vice versa. Theoretically, the
optimization objective is equivalent to minimizing the forward divergence
between the occupancy measure of the expert and ours. Consequently, ENP learns
to globally align with the expert policy by maximizing the likelihood of the
actions and modeling the dynamics of the navigation states in a collaborative
manner. With a variety of VLN architectures, ENP achieves promising
performances on R2R, REVERIE, RxR, and R2R-CE, unleashing the power of existing
VLN models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ERDDCI: Exact Reversible Diffusion via Dual-Chain Inversion for
  High-Quality Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jimin Dai, Yingzhen Zhang, Shuo Chen, Jian Yang, Lei Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) have been successfully applied to real image editing.
These models typically invert images into latent noise vectors used to
reconstruct the original images (known as inversion), and then edit them during
the inference process. However, recent popular DMs often rely on the assumption
of local linearization, where the noise injected during the inversion process
is expected to approximate the noise removed during the inference process.
While DM efficiently generates images under this assumption, it can also
accumulate errors during the diffusion process due to the assumption,
ultimately negatively impacting the quality of real image reconstruction and
editing. To address this issue, we propose a novel method, referred to as
ERDDCI (Exact Reversible Diffusion via Dual-Chain Inversion). ERDDCI uses the
new Dual-Chain Inversion (DCI) for joint inference to derive an exact
reversible diffusion process. By using DCI, our method effectively avoids the
cumbersome optimization process in existing inversion approaches and achieves
high-quality image editing. Additionally, to accommodate image operations under
high guidance scales, we introduce a dynamic control strategy that enables more
refined image reconstruction and editing. Our experiments demonstrate that
ERDDCI significantly outperforms state-of-the-art methods in a 50-step
diffusion process. It achieves rapid and precise image reconstruction with an
SSIM of 0.999 and an LPIPS of 0.001, and also delivers competitive results in
image editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PReP: Efficient context-based shape retrieval for missing parts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vlassis Fotis, Ioannis Romanelis, Georgios Mylonas, Athanasios Kalogeras, Konstantinos Moustakas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we study the problem of shape part retrieval in the point cloud
domain. Shape retrieval methods in the literature rely on the presence of an
existing query object, but what if the part we are looking for is not
available? We present Part Retrieval Pipeline (PReP), a pipeline that
creatively utilizes metric learning techniques along with a trained
classification model to measure the suitability of potential replacement parts
from a database, as part of an application scenario targeting circular economy.
Through an innovative training procedure with increasing difficulty, it is able
to learn to recognize suitable parts relying only on shape context. Thanks to
its low parameter size and computational requirements, it can be used to sort
through a warehouse of potentially tens of thousand of spare parts in just a
few seconds. We also establish an alternative baseline approach to compare
against, and extensively document the unique challenges associated with this
task, as well as identify the design choices to solve them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pseudo-label Refinement for Improving Self-Supervised Learning Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Zia-ur-Rehman, Arif Mahmood, Wenxiong Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning systems have gained significant attention in recent
years by leveraging clustering-based pseudo-labels to provide supervision
without the need for human annotations. However, the noise in these
pseudo-labels caused by the clustering methods poses a challenge to the
learning process leading to degraded performance. In this work, we propose a
pseudo-label refinement (SLR) algorithm to address this issue. The cluster
labels from the previous epoch are projected to the current epoch
cluster-labels space and a linear combination of the new label and the
projected label is computed as a soft refined label containing the information
from the previous epoch clusters as well as from the current epoch. In contrast
to the common practice of using the maximum value as a cluster/class indicator,
we employ hierarchical clustering on these soft pseudo-labels to generate
refined hard-labels. This approach better utilizes the information embedded in
the soft labels, outperforming the simple maximum value approach for hard label
generation. The effectiveness of the proposed SLR algorithm is evaluated in the
context of person re-identification (Re-ID) using unsupervised domain
adaptation (UDA). Experimental results demonstrate that the modified Re-ID
baseline, incorporating the SLR algorithm, achieves significantly improved mean
Average Precision (mAP) performance in various UDA tasks, including
real-to-synthetic, synthetic-to-real, and different real-to-real scenarios.
These findings highlight the efficacy of the SLR algorithm in enhancing the
performance of self-supervised learning systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Storyboard guided Alignment for Fine-grained Video Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enqi Liu, Liyuan Pan, Yan Yang, Yiran Zhong, Zhijing Wu, Xinxiao Wu, Liu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-grained video action recognition can be conceptualized as a video-text
matching problem. Previous approaches often rely on global video semantics to
consolidate video embeddings, which can lead to misalignment in video-text
pairs due to a lack of understanding of action semantics at an atomic
granularity level. To tackle this challenge, we propose a multi-granularity
framework based on two observations: (i) videos with different global semantics
may share similar atomic actions or appearances, and (ii) atomic actions within
a video can be momentary, slow, or even non-directly related to the global
video semantics. Inspired by the concept of storyboarding, which disassembles a
script into individual shots, we enhance global video semantics by generating
fine-grained descriptions using a pre-trained large language model. These
detailed descriptions capture common atomic actions depicted in videos. A
filtering metric is proposed to select the descriptions that correspond to the
atomic actions present in both the videos and the descriptions. By employing
global semantics and fine-grained descriptions, we can identify key frames in
videos and utilize them to aggregate embeddings, thereby making the embedding
more accurate. Extensive experiments on various video action recognition
datasets demonstrate superior performance of our proposed method in supervised,
few-shot, and zero-shot settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MambaSCI: Efficient Mamba-UNet for Quad-Bayer Patterned Video Snapshot
  Compressive Imaging <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenghao Pan, Haijin Zeng, Jiezhang Cao, Yongyong Chen, Kai Zhang, Yong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Color video snapshot compressive imaging (SCI) employs computational imaging
techniques to capture multiple sequential video frames in a single
Bayer-patterned measurement. With the increasing popularity of quad-Bayer
pattern in mainstream smartphone cameras for capturing high-resolution videos,
mobile photography has become more accessible to a wider audience. However,
existing color video SCI reconstruction algorithms are designed based on the
traditional Bayer pattern. When applied to videos captured by quad-Bayer
cameras, these algorithms often result in color distortion and ineffective
demosaicing, rendering them impractical for primary equipment. To address this
challenge, we propose the MambaSCI method, which leverages the Mamba and UNet
architectures for efficient reconstruction of quad-Bayer patterned color video
SCI. To the best of our knowledge, our work presents the first algorithm for
quad-Bayer patterned SCI reconstruction, and also the initial application of
the Mamba model to this task. Specifically, we customize Residual-Mamba-Blocks,
which residually connect the Spatial-Temporal Mamba (STMamba),
Edge-Detail-Reconstruction (EDR) module, and Channel Attention (CA) module.
Respectively, STMamba is used to model long-range spatial-temporal dependencies
with linear complexity, EDR is for better edge-detail reconstruction, and CA is
used to compensate for the missing channel information interaction in Mamba
model. Experiments demonstrate that MambaSCI surpasses state-of-the-art methods
with lower computational and memory costs. PyTorch style pseudo-code for the
core modules is provided in the supplementary materials.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shape Transformation Driven by Active Contour for Class-Imbalanced
  Semi-Supervised Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuliang Gu, Yepeng Liu, Zhichao Sun, Jinchi Zhu, Yongchao Xu, Laurent Najman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annotating 3D medical images demands expert knowledge and is time-consuming.
As a result, semi-supervised learning (SSL) approaches have gained significant
interest in 3D medical image segmentation. The significant size differences
among various organs in the human body lead to imbalanced class distribution,
which is a major challenge in the real-world application of these SSL
approaches. To address this issue, we develop a novel Shape Transformation
driven by Active Contour (STAC), that enlarges smaller organs to alleviate
imbalanced class distribution across different organs. Inspired by curve
evolution theory in active contour methods, STAC employs a signed distance
function (SDF) as the level set function, to implicitly represent the shape of
organs, and deforms voxels in the direction of the steepest descent of SDF
(i.e., the normal vector). To ensure that the voxels far from expansion organs
remain unchanged, we design an SDF-based weight function to control the degree
of deformation for each voxel. We then use STAC as a data-augmentation process
during the training stage. Experimental results on two benchmark datasets
demonstrate that the proposed method significantly outperforms some
state-of-the-art methods. Source code is publicly available at
https://github.com/GuGuLL123/STAC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-to-Image Representativity Fairness Evaluation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asma Yamani, Malak Baslyman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Image generative systems are progressing rapidly to be a source of
advertisement and media and could soon serve as image searches or artists.
However, there is a significant concern about the representativity bias these
models embody and how these biases can propagate in the social fabric after
fine-tuning them. Therefore, continuously monitoring and evaluating these
models for fairness is important. To address this issue, we propose
Text-to-Image (TTI) Representativity Fairness Evaluation Framework. In this
framework, we evaluate three aspects of a TTI system; diversity, inclusion, and
quality. For each aspect, human-based and model-based approaches are proposed
and evaluated for their ability to capture the bias and whether they can
substitute each other. The framework starts by suggesting the prompts for
generating the images for the evaluation based on the context and the sensitive
attributes under study. Then the three aspects are evaluated using the proposed
approaches. Based on the evaluation, a decision is made regarding the
representativity bias within the TTI system. The evaluation of our framework on
Stable Diffusion shows that the framework can effectively capture the bias in
TTI systems. The results also confirm that our proposed model based-approaches
can substitute human-based approaches in three out of four components with high
correlation, which could potentially reduce costs and automate the process. The
study suggests that continual learning of the model on more inclusive data
across disadvantaged minorities such as Indians and Middle Easterners is
essential to mitigate current stereotyping and lack of inclusiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E3D-GPT: Enhanced 3D Visual Foundation for Medical <span class="highlight-title">Vision-Language</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Lai, Zihang Jiang, Qingsong Yao, Rongsheng Wang, Zhiyang He, Xiaodong Tao, Wei Wei, Weifu Lv, S. Kevin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of 3D medical vision-language models holds significant
potential for disease diagnosis and patient treatment. However, compared to 2D
medical images, 3D medical images, such as CT scans, face challenges related to
limited training data and high dimension, which severely restrict the progress
of 3D medical vision-language models. To address these issues, we collect a
large amount of unlabeled 3D CT data and utilize self-supervised learning to
construct a 3D visual foundation model for extracting 3D visual features. Then,
we apply 3D spatial convolutions to aggregate and project high-level image
features, reducing computational complexity while preserving spatial
information. We also construct two instruction-tuning datasets based on BIMCV-R
and CT-RATE to fine-tune the 3D vision-language model. Our model demonstrates
superior performance compared to existing methods in report generation, visual
question answering, and disease diagnosis. Code and data will be made publicly
available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Transformer for Long Contextual Histopathology Whole Slide
  Image Analysis <span class="chip">NeurIPS-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honglin Li, Yunlong Zhang, Pingyi Chen, Zhongyi Shui, Chenglu Zhu, Lin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Histopathology Whole Slide Image (WSI) analysis serves as the gold standard
for clinical cancer diagnosis in the daily routines of doctors. To develop
computer-aided diagnosis model for WSIs, previous methods typically employ
Multi-Instance Learning to enable slide-level prediction given only slide-level
labels. Among these models, vanilla attention mechanisms without pairwise
interactions have traditionally been employed but are unable to model
contextual information. More recently, self-attention models have been utilized
to address this issue. To alleviate the computational complexity of long
sequences in large WSIs, methods like HIPT use region-slicing, and TransMIL
employs approximation of full self-attention. Both approaches suffer from
suboptimal performance due to the loss of key information. Moreover, their use
of absolute positional embedding struggles to effectively handle long
contextual dependencies in shape-varying WSIs. In this paper, we first analyze
how the low-rank nature of the long-sequence attention matrix constrains the
representation ability of WSI modelling. Then, we demonstrate that the rank of
attention matrix can be improved by focusing on local interactions via a local
attention mask. Our analysis shows that the local mask aligns with the
attention patterns in the lower layers of the Transformer. Furthermore, the
local attention mask can be implemented during chunked attention calculation,
reducing the quadratic computational complexity to linear with a small local
bandwidth. Building on this, we propose a local-global hybrid Transformer for
both computational acceleration and local-global information interactions
modelling. Our method, Long-contextual MIL (LongMIL), is evaluated through
extensive experiments on various WSI tasks to validate its superiority. Our
code will be available at github.com/invoker-LL/Long-MIL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS-2024. arXiv admin note: text overlap with arXiv:2311.12885</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Signed Distance Function Inference through Splatting 3D Gaussians
  Pulled on Zero-Level Set <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyuan Zhang, Yu-Shen Liu, Zhizhong Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is vital to infer a signed distance function (SDF) in multi-view based
surface reconstruction. 3D Gaussian splatting (3DGS) provides a novel
perspective for volume rendering, and shows advantages in rendering efficiency
and quality. Although 3DGS provides a promising neural rendering option, it is
still hard to infer SDFs for surface reconstruction with 3DGS due to the
discreteness, the sparseness, and the off-surface drift of 3D Gaussians. To
resolve these issues, we propose a method that seamlessly merge 3DGS with the
learning of neural SDFs. Our key idea is to more effectively constrain the SDF
inference with the multi-view consistency. To this end, we dynamically align 3D
Gaussians on the zero-level set of the neural SDF using neural pulling, and
then render the aligned 3D Gaussians through the differentiable rasterization.
Meanwhile, we update the neural SDF by pulling neighboring space to the pulled
3D Gaussians, which progressively refine the signed distance field near the
surface. With both differentiable pulling and splatting, we jointly optimize 3D
Gaussians and the neural SDF with both RGB and geometry constraints, which
recovers more accurate, smooth, and complete surfaces with more geometry
details. Our numerical and visual comparisons show our superiority over the
state-of-the-art results on the widely used benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. Project page:
  https://wen-yuan-zhang.github.io/GS-Pull/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiChartQA: Benchmarking <span class="highlight-title">Vision-Language</span> Models on Multi-Chart
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifeng Zhu, Mengzhao Jia, Zhihan Zhang, Lang Li, Meng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have demonstrated impressive
abilities across various tasks, including visual question answering and chart
comprehension, yet existing benchmarks for chart-related tasks fall short in
capturing the complexity of real-world multi-chart scenarios. Current
benchmarks primarily focus on single-chart tasks, neglecting the multi-hop
reasoning required to extract and integrate information from multiple charts,
which is essential in practical applications. To fill this gap, we introduce
MultiChartQA, a benchmark that evaluates MLLMs' capabilities in four key areas:
direct question answering, parallel question answering, comparative reasoning,
and sequential reasoning. Our evaluation of a wide range of MLLMs reveals
significant performance gaps compared to humans. These results highlight the
challenges in multi-chart comprehension and the potential of MultiChartQA to
drive advancements in this field. Our code and data are available at
https://github.com/Zivenzhu/Multi-chart-QA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature Augmentation based Test-Time Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Younggeol Cho, Youngrae Kim, Junho Yoon, Seunghoon Hong, Dongman Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time adaptation (TTA) allows a model to be adapted to an unseen domain
without accessing the source data. Due to the nature of practical environments,
TTA has a limited amount of data for adaptation. Recent TTA methods further
restrict this by filtering input data for reliability, making the effective
data size even smaller and limiting adaptation potential. To address this
issue, We propose Feature Augmentation based Test-time Adaptation (FATA), a
simple method that fully utilizes the limited amount of input data through
feature augmentation. FATA employs Normalization Perturbation to augment
features and adapts the model using the FATA loss, which makes the outputs of
the augmented and original features similar. FATA is model-agnostic and can be
seamlessly integrated into existing models without altering the model
architecture. We demonstrate the effectiveness of FATA on various models and
scenarios on ImageNet-C and Office-Home, validating its superiority in diverse
real-world conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning autonomous driving from aerial imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varun Murali, Guy Rosman, Sertac Karaman, Daniela Rus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we consider the problem of learning end to end perception to
control for ground vehicles solely from aerial imagery. Photogrammetric
simulators allow the synthesis of novel views through the transformation of
pre-generated assets into novel views.However, they have a large setup cost,
require careful collection of data and often human effort to create usable
simulators. We use a Neural Radiance Field (NeRF) as an intermediate
representation to synthesize novel views from the point of view of a ground
vehicle. These novel viewpoints can then be used for several downstream
autonomous navigation applications. In this work, we demonstrate the utility of
novel view synthesis though the application of training a policy for end to end
learning from images and depth data. In a traditional real to sim to real
framework, the collected data would be transformed into a visual simulator
which could then be used to generate novel views. In contrast, using a NeRF
allows a compact representation and the ability to optimize over the parameters
of the visual simulator as more data is gathered in the environment. We
demonstrate the efficacy of our method in a custom built mini-city environment
through the deployment of imitation policies on robotic cars. We additionally
consider the task of place localization and demonstrate that our method is able
to relocalize the car in the real world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DaRePlane: Direction-aware Representations for Dynamic Scene
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ange Lou, Benjamin Planche, Zhongpai Gao, Yamin Li, Tianyu Luan, Hao Ding, Meng Zheng, Terrence Chen, Ziyan Wu, Jack Noble
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous recent approaches to modeling and re-rendering dynamic scenes
leverage plane-based explicit representations, addressing slow training times
associated with models like neural radiance fields (NeRF) and Gaussian
splatting (GS). However, merely decomposing 4D dynamic scenes into multiple 2D
plane-based representations is insufficient for high-fidelity re-rendering of
scenes with complex motions. In response, we present DaRePlane, a novel
direction-aware representation approach that captures scene dynamics from six
different directions. This learned representation undergoes an inverse
dual-tree complex wavelet transformation (DTCWT) to recover plane-based
information. Within NeRF pipelines, DaRePlane computes features for each
space-time point by fusing vectors from these recovered planes, then passed to
a tiny MLP for color regression. When applied to Gaussian splatting, DaRePlane
computes the features of Gaussian points, followed by a tiny multi-head MLP for
spatial-time deformation prediction. Notably, to address redundancy introduced
by the six real and six imaginary direction-aware wavelet coefficients, we
introduce a trainable masking approach, mitigating storage issues without
significant performance decline. To demonstrate the generality and efficiency
of DaRePlane, we test it on both regular and surgical dynamic scenes, for both
NeRF and GS systems. Extensive experiments show that DaRePlane yields
state-of-the-art performance in novel view synthesis for various complex
dynamic scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2403.02265</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal DLT-based Solutions for the Perspective-n-Point 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sébastien Henry, John A. Christian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a modified normalized direct linear transform (DLT) algorithm for
solving the perspective-n-point (PnP) problem with much better behavior than
the conventional DLT. The modification consists of analytically weighting the
different measurements in the linear system with a negligible increase in
computational load. Our approach exhibits clear improvements -- in both
performance and runtime -- when compared to popular methods such as EPnP, CPnP,
RPnP, and OPnP. Our new non-iterative solution approaches that of the true
optimal found via Gauss-Newton optimization, but at a fraction of the
computational cost. Our optimal DLT (oDLT) implementation, as well as the
experiments, are released in open source.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlabeled Action Quality Assessment Based on Multi-dimensional Adaptive
  Constrained Dynamic Time Warping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renguang Chen, Guolong Zheng, Xu Yang, Zhide Chen, Jiwu Shu, Wencheng Yang, Kexin Zhu, Chen Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing popularity of online sports and exercise necessitates effective
methods for evaluating the quality of online exercise executions. Previous
action quality assessment methods, which relied on labeled scores from motion
videos, exhibited slightly lower accuracy and discriminability. This limitation
hindered their rapid application to newly added exercises. To address this
problem, this paper presents an unlabeled Multi-Dimensional Exercise Distance
Adaptive Constrained Dynamic Time Warping (MED-ACDTW) method for action quality
assessment. Our approach uses an athletic version of DTW to compare features
from template and test videos, eliminating the need for score labels during
training. The result shows that utilizing both 2D and 3D spatial dimensions,
along with multiple human body features, improves the accuracy by 2-3% compared
to using either 2D or 3D pose estimation alone. Additionally, employing MED for
score calculation enhances the precision of frame distance matching, which
significantly boosts overall discriminability. The adaptive constraint scheme
enhances the discriminability of action quality assessment by approximately
30%. Furthermore, to address the absence of a standardized perspective in
sports class evaluations, we introduce a new dataset called BGym.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing Open-world Forgetting in Generative Image Model Customization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Héctor Laria, Alex Gomez-Villa, Imad Eddine Marouf, Kai Wang, Bogdan Raducanu, Joost van de Weijer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in diffusion models have significantly enhanced image
generation capabilities. However, customizing these models with new classes
often leads to unintended consequences that compromise their reliability. We
introduce the concept of open-world forgetting to emphasize the vast scope of
these unintended alterations, contrasting it with the well-studied closed-world
forgetting, which is measurable by evaluating performance on a limited set of
classes or skills. Our research presents the first comprehensive investigation
into open-world forgetting in diffusion models, focusing on semantic and
appearance drift of representations. We utilize zero-shot classification to
analyze semantic drift, revealing that even minor model adaptations lead to
unpredictable shifts affecting areas far beyond newly introduced concepts, with
dramatic drops in zero-shot classification of up to 60%. Additionally, we
observe significant changes in texture and color of generated content when
analyzing appearance drift. To address these issues, we propose a mitigation
strategy based on functional regularization, designed to preserve original
capabilities while accommodating new concepts. Our study aims to raise
awareness of unintended changes due to model customization and advocates for
the analysis of open-world forgetting in future research on model customization
and finetuning methods. Furthermore, we provide insights for developing more
robust adaptation methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://hecoding.github.io/open-world-forgetting/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in
  <span class="highlight-title">Vision-Language</span> Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhang Cui, An Zhang, Yiyang Zhou, Zhaorun Chen, Gelei Deng, Huaxiu Yao, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancements in large language models (LLMs) and pre-trained
vision models have accelerated the development of vision-language large models
(VLLMs), enhancing the interaction between visual and linguistic modalities.
Despite their notable success across various domains, VLLMs face challenges in
modality alignment, which can lead to issues like hallucinations and unsafe
content generation. Current alignment techniques often rely on coarse feedback
and external datasets, limiting scalability and performance. In this paper, we
propose FiSAO (Fine-Grained Self-Alignment Optimization), a novel
self-alignment method that utilizes the model's own visual encoder as a
fine-grained verifier to improve vision-language alignment without the need for
additional data. By leveraging token-level feedback from the vision encoder,
FiSAO significantly improves vision-language alignment, even surpassing
traditional preference tuning methods that require additional data. Through
both theoretical analysis and experimental validation, we demonstrate that
FiSAO effectively addresses the misalignment problem in VLLMs, marking the
first instance of token-level rewards being applied to such models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preview-based Category Contrastive Learning for Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhe Ding, Jianlong Wu, Xue Dong, Xiaojie Li, Pengda Qin, Tian Gan, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation is a mainstream algorithm in model compression by
transferring knowledge from the larger model (teacher) to the smaller model
(student) to improve the performance of student. Despite many efforts, existing
methods mainly investigate the consistency between instance-level feature
representation or prediction, which neglects the category-level information and
the difficulty of each sample, leading to undesirable performance. To address
these issues, we propose a novel preview-based category contrastive learning
method for knowledge distillation (PCKD). It first distills the structural
knowledge of both instance-level feature correspondence and the relation
between instance features and category centers in a contrastive learning
fashion, which can explicitly optimize the category representation and explore
the distinct correlation between representations of instances and categories,
contributing to discriminative category centers and better classification
results. Besides, we introduce a novel preview strategy to dynamically
determine how much the student should learn from each sample according to their
difficulty. Different from existing methods that treat all samples equally and
curriculum learning that simply filters out hard samples, our method assigns a
small weight for hard instances as a preview to better guide the student
training. Extensive experiments on several challenging datasets, including
CIFAR-100 and ImageNet, demonstrate the superiority over state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures, Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProReason: <span class="highlight-title">Multi-Modal</span> Proactive Reasoning with Decoupled Eyesight and
  Wisdom 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingqi Zhou, Sheng Wang, Jingwei Dong, Lei Li, Jiahui Gao, Lingpeng Kong, Chuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) have witnessed significant progress on
visual understanding tasks. However, they often prioritize language knowledge
over image information on visual reasoning tasks, incurring performance
degradation. To tackle this issue, we first identify the drawbacks of existing
solutions (i.e., insufficient and irrelevant visual descriptions, and limited
multi-modal capacities). We then decompose visual reasoning process into two
stages: visual perception (i.e., eyesight) and textual reasoning (i.e.,
wisdom), and introduce a novel visual reasoning framework named ProReason. This
framework features multi-run proactive perception and decoupled
vision-reasoning capabilities. Briefly, given a multi-modal question, ProReason
iterates proactive information collection and reasoning until the answer can be
concluded with necessary and sufficient visual descriptions. Notably, the
disassociation of capabilities allows seamless integration of existing large
language models (LLMs) to compensate for the reasoning deficits of LVLMs. Our
extensive experiments demonstrate that ProReason outperforms both existing
multi-step reasoning frameworks and passive peer methods on a wide range of
benchmarks for both open-source and closed-source models. In addition, with the
assistance of LLMs, ProReason achieves a performance improvement of up to 15%
on MMMU benchmark. Our insights into existing solutions and the decoupled
perspective for feasible integration of LLMs illuminate future research on
visual reasoning techniques, especially LLM-assisted ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViConsFormer: Constituting Meaningful Phrases of Scene Texts using
  Transformer-based Method in Vietnamese Text-based Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nghia Hieu Nguyen, Tho Thanh Quan, Ngan Luu-Thuy Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-based VQA is a challenging task that requires machines to use scene
texts in given images to yield the most appropriate answer for the given
question. The main challenge of text-based VQA is exploiting the meaning and
information from scene texts. Recent studies tackled this challenge by
considering the spatial information of scene texts in images via embedding 2D
coordinates of their bounding boxes. In this study, we follow the definition of
meaning from linguistics to introduce a novel method that effectively exploits
the information from scene texts written in Vietnamese. Experimental results
show that our proposed method obtains state-of-the-art results on two
large-scale Vietnamese Text-based VQA datasets. The implementation can be found
at this link.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning Applications in Medical Image Analysis: Advancements,
  Challenges, and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aimina Ali Eli, Abida Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image analysis has emerged as an essential element of contemporary
healthcare, facilitating physicians in achieving expedited and precise
diagnosis. Recent breakthroughs in deep learning, a subset of artificial
intelligence, have markedly revolutionized the analysis of medical pictures,
improving the accuracy and efficiency of clinical procedures. Deep learning
algorithms, especially convolutional neural networks (CNNs), have demonstrated
remarkable proficiency in autonomously learning features from multidimensional
medical pictures, including MRI, CT, and X-ray scans, without the necessity for
manual feature extraction. These models have been utilized across multiple
medical disciplines, including pathology, radiology, ophthalmology, and
cardiology, where they aid in illness detection, classification, and
segmentation tasks......
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extreme Precipitation Nowcasting using Multi-Task Latent Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Chaorong, Ling Xudong, Yang Qiang, Qin Fengqing, Huang Yuanyuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models have made remarkable strides in precipitation
prediction, yet they continue to struggle with capturing the spatial details of
the features of radar images, particularly over high precipitation intensity
areas. This shortcoming is evident in the form of low forecast accuracy in the
spatial positioning of radar echo images across varying precipitation intensity
regions. To address this challenge, we introduce the multi-task latent
diffusion model(MTLDM), a novel approach for precipitation prediction. The
basic concept of the MTLDM is based on the understanding that the radar image
representing precipitation is the result of multiple factors. Therefore, we
adopt a divide-and-conquer approach, that is, we decompose the radar image
using decomposition technology and then predict the decomposed sub-images
separately. We conceptualize the precipitation image as a composition of
various components corresponding to different precipitation intensities. The
MTLDM decomposes the precipitation image into these distinct components and
employs a dedicated task to predict each one. This method enables
spatiotemporally consistent prediction of real-world precipitation areas up to
5-80 min in advance, outperforming existing state-of-the-art techniques across
multiple evaluation metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing In-vehicle Multiple Object Tracking Systems with Embeddable
  Ising Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kosuke Tatsumura, Yohei Hamakawa, Masaya Yamasaki, Koji Oya, Hiroshi Fujimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A cognitive function of tracking multiple objects, needed in autonomous
mobile vehicles, comprises object detection and their temporal association.
While great progress owing to machine learning has been recently seen for
elaborating the similarity matrix between the objects that have been recognized
and the objects detected in a current video frame, less for the assignment
problem that finally determines the temporal association, which is a
combinatorial optimization problem. Here we show an in-vehicle multiple object
tracking system with a flexible assignment function for tracking through
multiple long-term occlusion events. To solve the flexible assignment problem
formulated as a nondeterministic polynomial time-hard problem, the system
relies on an embeddable Ising machine based on a quantum-inspired algorithm
called simulated bifurcation. Using a vehicle-mountable computing platform, we
demonstrate a realtime system-wide throughput (23 frames per second on average)
with the enhanced functionality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Generative Interactive Environments By Trained Agent
  Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06445v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06445v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naser Kazemi, Nedko Savov, Danda Paudel, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models are increasingly pivotal in interpreting and simulating the
rules and actions of complex environments. Genie, a recent model, excels at
learning from visually diverse environments but relies on costly
human-collected data. We observe that their alternative method of using random
agents is too limited to explore the environment. We propose to improve the
model by employing reinforcement learning based agents for data generation.
This approach produces diverse datasets that enhance the model's ability to
adapt and perform well across various scenarios and realistic actions within
the environment. In this paper, we first release the model GenieRedux - an
implementation based on Genie. Additionally, we introduce GenieRedux-G, a
variant that uses the agent's readily available actions to factor out action
prediction uncertainty during validation. Our evaluation, including a
replication of the Coinrun case study, shows that GenieRedux-G achieves
superior visual fidelity and controllability using the trained agent
exploration. The proposed approach is reproducable, scalable and adaptable to
new types of environments. Our codebase is available at
https://github.com/insait-institute/GenieRedux .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01804v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01804v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Mai, Peter Hedman, George Kopanas, Dor Verbin, David Futschik, Qiangeng Xu, Falko Kuester, Jonathan T. Barron, Yinda Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Exact Volumetric Ellipsoid Rendering (EVER), a method for
real-time differentiable emission-only volume rendering. Unlike recent
rasterization based approach by 3D Gaussian Splatting (3DGS), our primitive
based representation allows for exact volume rendering, rather than alpha
compositing 3D Gaussian billboards. As such, unlike 3DGS our formulation does
not suffer from popping artifacts and view dependent density, but still
achieves frame rates of $\sim\!30$ FPS at 720p on an NVIDIA RTX4090. Since our
approach is built upon ray tracing it enables effects such as defocus blur and
camera distortion (e.g. such as from fisheye cameras), which are difficult to
achieve by rasterization. We show that our method is more accurate with fewer
blending issues than 3DGS and follow-up work on view-consistent rendering,
especially on the challenging large-scale scenes from the Zip-NeRF dataset
where it achieves sharpest results among real-time techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://half-potato.gitlab.io/posts/ever</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Movie101v2: Improved Movie Narration Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Yue, Yepeng Zhang, Ziheng Wang, Qin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic movie narration aims to generate video-aligned plot descriptions to
assist visually impaired audiences. Unlike standard video captioning, it
involves not only describing key visual details but also inferring plots that
unfold across multiple movie shots, presenting distinct and complex challenges.
To advance this field, we introduce Movie101v2, a large-scale, bilingual
dataset with enhanced data quality specifically designed for movie narration.
Revisiting the task, we propose breaking down the ultimate goal of automatic
movie narration into three progressive stages, offering a clear roadmap with
corresponding evaluation metrics. Based on our new benchmark, we baseline a
range of large vision-language models, including GPT-4V, and conduct an
in-depth analysis of the challenges in narration generation. Our findings
highlight that achieving applicable movie narration generation is a fascinating
goal that requires significant research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing Shared Relations via <span class="highlight-title">Multimodal</span> Mixup Contrastive Learning
  for <span class="highlight-title">Multimodal</span> Classification <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raja Kumar, Raghav Singhal, Pranamya Kulkarni, Deval Mehta, Kshitij Jadhav
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep multimodal learning has shown remarkable success by leveraging
contrastive learning to capture explicit one-to-one relations across
modalities. However, real-world data often exhibits shared relations beyond
simple pairwise associations. We propose M3CoL, a Multimodal Mixup Contrastive
Learning approach to capture nuanced shared relations inherent in multimodal
data. Our key contribution is a Mixup-based contrastive loss that learns robust
representations by aligning mixed samples from one modality with their
corresponding samples from other modalities thereby capturing shared relations
between them. For multimodal classification tasks, we introduce a framework
that integrates a fusion module with unimodal prediction modules for auxiliary
supervision during training, complemented by our proposed Mixup-based
contrastive loss. Through extensive experiments on diverse datasets (N24News,
ROSMAP, BRCA, and Food-101), we demonstrate that M3CoL effectively captures
shared multimodal relations and generalizes across domains. It outperforms
state-of-the-art methods on N24News, ROSMAP, and BRCA, while achieving
comparable performance on Food-101. Our work highlights the significance of
learning shared relations for robust multimodal learning, opening up promising
avenues for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RK and RS contributed equally to this work, 20 Pages, 8 Figures, 9
  Tables. Another version of the paper accepted at NeurIPS 2024 Workshop on
  Unifying Representations in Neural Models (UniReps)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IncEventGS: Pose-Free Gaussian Splatting from a Single Event Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Huang, Chengrui Dong, Peidong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit neural representation and explicit 3D Gaussian Splatting (3D-GS) for
novel view synthesis have achieved remarkable progress with frame-based camera
(e.g. RGB and RGB-D cameras) recently. Compared to frame-based camera, a novel
type of bio-inspired visual sensor, i.e. event camera, has demonstrated
advantages in high temporal resolution, high dynamic range, low power
consumption and low latency. Due to its unique asynchronous and irregular data
capturing process, limited work has been proposed to apply neural
representation or 3D Gaussian splatting for an event camera. In this work, we
present IncEventGS, an incremental 3D Gaussian Splatting reconstruction
algorithm with a single event camera. To recover the 3D scene representation
incrementally, we exploit the tracking and mapping paradigm of conventional
SLAM pipelines for IncEventGS. Given the incoming event stream, the tracker
firstly estimates an initial camera motion based on prior reconstructed 3D-GS
scene representation. The mapper then jointly refines both the 3D scene
representation and camera motion based on the previously estimated motion
trajectory from the tracker. The experimental results demonstrate that
IncEventGS delivers superior performance compared to prior NeRF-based methods
and other related baselines, even we do not have the ground-truth camera poses.
Furthermore, our method can also deliver better performance compared to
state-of-the-art event visual odometry methods in terms of camera motion
estimation. Code is publicly available at:
https://github.com/wu-cvgl/IncEventGS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code Page: https://github.com/wu-cvgl/IncEventGS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Drift Monitoring in Medical Imaging AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jameson Merkow, Felix J. Dorfner, Xiyu Yang, Alexander Ersoy, Giridhar Dasegowda, Mannudeep Kalra, Matthew P. Lungren, Christopher P. Bridge, Ivan Tarapov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of artificial intelligence (AI) into medical imaging has
advanced clinical diagnostics but poses challenges in managing model drift and
ensuring long-term reliability. To address these challenges, we develop MMC+,
an enhanced framework for scalable drift monitoring, building upon the
CheXstray framework that introduced real-time drift detection for medical
imaging AI models using multi-modal data concordance. This work extends the
original framework's methodologies, providing a more scalable and adaptable
solution for real-world healthcare settings and offers a reliable and
cost-effective alternative to continuous performance monitoring addressing
limitations of both continuous and periodic monitoring methods. MMC+ introduces
critical improvements to the original framework, including more robust handling
of diverse data streams, improved scalability with the integration of
foundation models like MedImageInsight for high-dimensional image embeddings
without site-specific training, and the introduction of uncertainty bounds to
better capture drift in dynamic clinical environments. Validated with
real-world data from Massachusetts General Hospital during the COVID-19
pandemic, MMC+ effectively detects significant data shifts and correlates them
with model performance changes. While not directly predicting performance
degradation, MMC+ serves as an early warning system, indicating when AI systems
may deviate from acceptable performance bounds and enabling timely
interventions. By emphasizing the importance of monitoring diverse data streams
and evaluating data shifts alongside model performance, this work contributes
to the broader adoption and integration of AI solutions in clinical settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fundus to Fluorescein Angiography Video Generation as a Retinal
  Generative Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13242v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13242v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiyi Zhang, Jiancheng Yang, Ruoyu Chen, Siyu Huang, Pusheng Xu, Xiaolan Chen, Shanfu Lu, Hongyu Cao, Mingguang He, Danli Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fundus fluorescein angiography (FFA) is crucial for diagnosing and monitoring
retinal vascular issues but is limited by its invasive nature and restricted
accessibility compared to color fundus (CF) imaging. Existing methods that
convert CF images to FFA are confined to static image generation, missing the
dynamic lesional changes. We introduce Fundus2Video, an autoregressive
generative adversarial network (GAN) model that generates dynamic FFA videos
from single CF images. Fundus2Video excels in video generation, achieving an
FVD of 1497.12 and a PSNR of 11.77. Clinical experts have validated the
fidelity of the generated videos. Additionally, the model's generator
demonstrates remarkable downstream transferability across ten external public
datasets, including blood vessel segmentation, retinal disease diagnosis,
systemic disease prediction, and multimodal retrieval, showcasing impressive
zero-shot and few-shot capabilities. These findings position Fundus2Video as a
powerful, non-invasive alternative to FFA exams and a versatile retinal
generative foundation model that captures both static and temporal retinal
features, enabling the representation of complex inter-modality relationships.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-XL: Extra-Long <span class="highlight-title">Vision Language</span> Model for Hour-Scale Video
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14485v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14485v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Shu, Peitian Zhang, Zheng Liu, Minghao Qin, Junjie Zhou, Tiejun Huang, Bo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although current Multi-modal Large Language Models (MLLMs) demonstrate
promising results in video understanding, processing extremely long videos
remains an ongoing challenge. Typically, MLLMs struggle with handling thousands
of visual tokens that exceed the maximum context length, and they suffer from
the information decay due to token aggregation. Another challenge is the high
computational cost stemming from the large number of video tokens. To tackle
these issues, we propose Video-XL, an extra-long vision language model designed
for efficient hour-scale video understanding. Specifically, we argue that LLMs
can be adapted as effective visual condensers and propose Visual Context Latent
Summarization which condenses visual contexts into highly compact forms.
Extensive experiments demonstrate that our model achieves promising results on
popular long video understanding benchmarks. For example, Video-XL outperforms
the current state-of-the-art method on VNBench by nearly 10\% in accuracy.
Moreover, Video-XL presents an impressive balance between efficiency and
effectiveness, processing 2048 frames on a single 80GB GPU while achieving
nearly 95% accuracy in the Needle-in-a-Haystack evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Efficient Variants of Segment Anything Model: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04960v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04960v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaorui Sun, Jun Liu, Heng Tao Shen, Xiaofeng Zhu, Ping Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Segment Anything Model (SAM) is a foundational model for image
segmentation tasks, known for its strong generalization across diverse
applications. However, its impressive performance comes with significant
computational and resource demands, making it challenging to deploy in
resource-limited environments such as edge devices. To address this, a variety
of SAM variants have been proposed to enhance efficiency while keeping
accuracy. This survey provides the first comprehensive review of these
efficient SAM variants. We begin by exploring the motivations driving this
research. We then present core techniques used in SAM and model acceleration.
This is followed by a detailed exploration of SAM acceleration strategies,
categorized by approach, and a discussion of several future research
directions. Finally, we offer a unified and extensive evaluation of these
methods across various hardware, assessing their efficiency and accuracy on
representative benchmarks, and providing a clear comparison of their overall
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Implicit Optimization for Robust and Flexible Image Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07361v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07361v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Jena, Pratik Chaudhari, James C. Gee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning in Image Registration (DLIR) methods have been tremendously
successful in image registration due to their speed and ability to incorporate
weak label supervision at training time. However, DLIR methods forego many of
the benefits of classical optimization-based methods. The functional nature of
deep networks do not guarantee that the predicted transformation is a local
minima of the registration objective, the representation of the transformation
(displacement/velocity field/affine) is fixed, and the networks are not robust
to domain shift. Our method aims to bridge this gap between classical and
learning methods by incorporating optimization as a layer in a deep network. A
deep network is trained to predict multi-scale dense feature images that are
registered using a black box iterative optimization solver. This optimal warp
is then used to minimize image and label alignment errors. By implicitly
differentiating end-to-end through an iterative optimization solver, our
learned features are registration and label-aware, and the warp functions are
guaranteed to be local minima of the registration objective in the feature
space. Our framework shows excellent performance on in-domain datasets, and is
agnostic to domain shift such as anisotropy and varying intensity profiles. For
the first time, our method allows switching between arbitrary transformation
representations (free-form to diffeomorphic) at test time with zero retraining.
End-to-end feature learning also facilitates interpretability of features, and
out-of-the-box promptability using additional label-fidelity terms at
inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantization Effects on Neural Networks Perception: How would
  quantization change the perceptual field of vision models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Amine Kerkouri, Marouane Tliba, Aladine Chetouani, Alessandro Bruno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network quantization is a critical technique for deploying models on
resource-limited devices. Despite its widespread use, the impact of
quantization on model perceptual fields, particularly in relation to class
activation maps (CAMs), remains underexplored. This study investigates how
quantization influences the spatial recognition abilities of vision models by
examining the alignment between CAMs and visual salient objects maps across
various architectures. Utilizing a dataset of 10,000 images from ImageNet, we
conduct a comprehensive evaluation of six diverse CNN architectures: VGG16,
ResNet50, EfficientNet, MobileNet, SqueezeNet, and DenseNet. Through the
systematic application of quantization techniques, we identify subtle changes
in CAMs and their alignment with Salient object maps. Our results demonstrate
the differing sensitivities of these architectures to quantization and
highlight its implications for model performance and interpretability in
real-world applications. This work primarily contributes to a deeper
understanding of neural network quantization, offering insights essential for
deploying efficient and interpretable models in practical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted & presented at IPTA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MicroDreamer: Efficient 3D Generation in $\sim$20 Seconds by Score-based
  Iterative Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.19525v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.19525v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luxi Chen, Zhengyi Wang, Zihan Zhou, Tingting Gao, Hang Su, Jun Zhu, Chongxuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimization-based approaches, such as score distillation sampling (SDS),
show promise in zero-shot 3D generation but suffer from low efficiency,
primarily due to the high number of function evaluations (NFEs) required for
each sample and the limitation of optimization confined to latent space. This
paper introduces score-based iterative reconstruction (SIR), an efficient and
general algorithm mimicking a differentiable 3D reconstruction process to
reduce the NFEs and enable optimization in pixel space. Given a single set of
images sampled from a multi-view score-based diffusion model, SIR repeatedly
optimizes 3D parameters, unlike the single-step optimization in SDS. With other
improvements in training, we present an efficient approach called MicroDreamer
that generally applies to various 3D representations and 3D generation tasks.
In particular, MicroDreamer is 5-20 times faster than SDS in generating neural
radiance field while retaining a comparable performance and takes about 20
seconds to create meshes from 3D Gaussian splatting on a single A100 GPU,
halving the time of the fastest optimization-based baseline DreamGaussian with
significantly superior performance compared to the measurement standard
deviation. Our code is available at https://github.com/ML-GSAI/MicroDreamer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Similarity and Quality Metrics for MR Image-To-Image Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.08431v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.08431v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melanie Dohmen, Mark Klemens, Ivo Baltruschat, Tuan Truong, Matthias Lenga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-to-image translation can create large impact in medical imaging, as
images can be synthetically transformed to other modalities, sequence types,
higher resolutions or lower noise levels. To ensure patient safety, these
methods should be validated by human readers, which requires a considerable
amount of time and costs. Quantitative metrics can effectively complement such
studies and provide reproducible and objective assessment of synthetic images.
If a reference is available, the similarity of MR images is frequently
evaluated by SSIM and PSNR metrics, even though these metrics are not or too
sensitive regarding specific distortions. When reference images to compare with
are not available, non-reference quality metrics can reliably detect specific
distortions, such as blurriness. To provide an overview on distortion
sensitivity, we quantitatively analyze 11 similarity (reference) and 12 quality
(non-reference) metrics for assessing synthetic images. We additionally include
a metric on a downstream segmentation task. We investigate the sensitivity
regarding 11 kinds of distortions and typical MR artifacts, and analyze the
influence of different normalization methods on each metric and distortion.
Finally, we derive recommendations for effective usage of the analyzed
similarity and quality metrics for evaluation of image-to-image translation
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 8 figures, supplement with 16 pages, 10 figures, submitted
  to Nature Scientific Reports</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LED: Light Enhanced Depth Estimation at Night 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon de Moreau, Yasser Almehio, Andrei Bursuc, Hafid El-Idrissi, Bogdan Stanciulescu, Fabien Moutarde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nighttime camera-based depth estimation is a highly challenging task,
especially for autonomous driving applications, where accurate depth perception
is essential for ensuring safe navigation. We aim to improve the reliability of
perception systems at night time, where models trained on daytime data often
fail in the absence of precise but costly LiDAR sensors. In this work, we
introduce Light Enhanced Depth (LED), a novel cost-effective approach that
significantly improves depth estimation in low-light environments by harnessing
a pattern projected by high definition headlights available in modern vehicles.
LED leads to significant performance boosts across multiple depth-estimation
architectures (encoder-decoder, Adabins, DepthFormer) both on synthetic and
real datasets. Furthermore, increased performances beyond illuminated areas
reveal a holistic enhancement in scene understanding. Finally, we release the
Nighttime Synthetic Drive Dataset, a new synthetic and photo-realistic
nighttime dataset, which comprises 49,990 comprehensively annotated images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Code and dataset available on the project page :
  https://simondemoreau.github.io/LED/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiTFastAttn: Attention Compression for Diffusion Transformer Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08552v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08552v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihang Yuan, Hanling Zhang, Pu Lu, Xuefei Ning, Linfeng Zhang, Tianchen Zhao, Shengen Yan, Guohao Dai, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Transformers (DiT) excel at image and video generation but face
computational challenges due to the quadratic complexity of self-attention
operators. We propose DiTFastAttn, a post-training compression method to
alleviate the computational bottleneck of DiT. We identify three key
redundancies in the attention computation during DiT inference: (1) spatial
redundancy, where many attention heads focus on local information; (2) temporal
redundancy, with high similarity between the attention outputs of neighboring
steps; (3) conditional redundancy, where conditional and unconditional
inferences exhibit significant similarity. We propose three techniques to
reduce these redundancies: (1) Window Attention with Residual Sharing to reduce
spatial redundancy; (2) Attention Sharing across Timesteps to exploit the
similarity between steps; (3) Attention Sharing across CFG to skip redundant
computations during conditional generation. We apply DiTFastAttn to DiT,
PixArt-Sigma for image generation tasks, and OpenSora for video generation
tasks. Our results show that for image generation, our method reduces up to 76%
of the attention FLOPs and achieves up to 1.8x end-to-end speedup at
high-resolution (2k x 2k) generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span> Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minsu Kim, Hyung-Il Kim, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Speech Recognition (VSR) aims to infer speech into text depending on
lip movements alone. As it focuses on visual information to model the speech,
its performance is inherently sensitive to personal lip appearances and
movements, and this makes the VSR models show degraded performance when they
are applied to unseen speakers. In this paper, to remedy the performance
degradation of the VSR model on unseen speakers, we propose prompt tuning
methods of Deep Neural Networks (DNNs) for speaker-adaptive VSR. Specifically,
motivated by recent advances in Natural Language Processing (NLP), we finetune
prompts on adaptation data of target speakers instead of modifying the
pre-trained model parameters. Different from the previous prompt tuning methods
mainly limited to Transformer variant architecture, we explore different types
of prompts, the addition, the padding, and the concatenation form prompts that
can be applied to the VSR model which is composed of CNN and Transformer in
general. With the proposed prompt tuning, we show that the performance of the
pre-trained VSR model on unseen speakers can be largely improved by using a
small amount of adaptation data (e.g., less than 5 minutes), even if the
pre-trained model is already developed with large speaker variations. Moreover,
by analyzing the performance and parameters of different types of prompts, we
investigate when the prompt tuning is preferred over the finetuning methods.
The effectiveness of the proposed method is evaluated on both word- and
sentence-level VSR databases, LRW-ID and GRID.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing Webpage UIs for Text-Rich Visual Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13824v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13824v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, Xiang Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-rich visual understanding-the ability to process environments where
dense textual content is integrated with visuals-is crucial for multimodal
large language models (MLLMs) to interact effectively with structured
environments. To enhance this capability, we propose synthesizing general
multimodal instructions from webpage UIs using text-based large language models
(LLMs). Despite lacking direct visual input, text-based LLMs are able to
process structured text representations from webpage accessibility trees. These
instructions are then paired with UI screenshots to train multimodal models. We
introduce MultiUI, a dataset containing 7.3 million samples from 1 million
websites, covering diverse multimodal tasks and UI layouts. Models trained on
MultiUI not only excel in web UI tasks-achieving up to a 48% improvement on
VisualWebBench and a 19.1% boost in element accuracy on a web agent dataset
Mind2Web-but also generalize surprisingly well to non-web UI tasks and even to
non-UI domains, such as document understanding, OCR, and chart interpretation.
These results highlight the broad applicability of web UI data for advancing
text-rich visual understanding across various scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dating ancient manuscripts using radiocarbon and AI-based writing style
  analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12013v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12013v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mladen Popović, Maruf A. Dhali, Lambert Schomaker, Johannes van der Plicht, Kaare Lund Rasmussen, Jacopo La Nasa, Ilaria Degano, Maria Perla Colombini, Eibert Tigchelaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining the chronology of ancient handwritten manuscripts is essential
for reconstructing the evolution of ideas. For the Dead Sea Scrolls, this is
particularly important. However, there is an almost complete lack of
date-bearing manuscripts evenly distributed across the timeline and written in
similar scripts available for palaeographic comparison. Here, we present Enoch,
a state-of-the-art AI-based date-prediction model, trained on the basis of new
radiocarbon-dated samples of the scrolls. Enoch uses established
handwriting-style descriptors and applies Bayesian ridge regression. The
challenge of this study is that the number of radiocarbon-dated manuscripts is
small, while current machine learning requires an abundance of training data.
We show that by using combined angular and allographic writing style feature
vectors and applying Bayesian ridge regression, Enoch could predict the
radiocarbon-based dates from style, supported by leave-one-out validation, with
varied MAEs of 27.9 to 30.7 years relative to the radiocarbon dating. Enoch was
then used to estimate the dates of 135 unseen manuscripts, revealing that 79
per cent of the samples were considered 'realistic' upon palaeographic post-hoc
evaluation. We present a new chronology of the scrolls. The radiocarbon ranges
and Enoch's style-based predictions are often older than the traditionally
assumed palaeographic estimates. In the range of 300-50 BCE, Enoch's date
prediction provides an improved granularity. The study is in line with current
developments in multimodal machine-learning techniques, and the methods can be
used for date prediction in other partially-dated manuscript collections. This
research shows how Enoch's quantitative, probability-based approach can be a
tool for palaeographers and historians, re-dating ancient Jewish key texts and
contributing to current debates on Jewish and Christian origins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages of main article, 103 pages of supplementary materials; the
  first version of this article is originally prepared in July 2023 after the
  completion of all the experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distribution Guidance Network for Weakly Supervised Point Cloud Semantic
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08091v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08091v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyi Pan, Wei Gao, Shan Liu, Ge Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite alleviating the dependence on dense annotations inherent to fully
supervised methods, weakly supervised point cloud semantic segmentation suffers
from inadequate supervision signals. In response to this challenge, we
introduce a novel perspective that imparts auxiliary constraints by regulating
the feature space under weak supervision. Our initial investigation identifies
which distributions accurately characterize the feature space, subsequently
leveraging this priori to guide the alignment of the weakly supervised
embeddings. Specifically, we analyze the superiority of the mixture of von
Mises-Fisher distributions (moVMF) among several common distribution
candidates. Accordingly, we develop a Distribution Guidance Network (DGNet),
which comprises a weakly supervised learning branch and a distribution
alignment branch. Leveraging reliable clustering initialization derived from
the weakly supervised learning branch, the distribution alignment branch
alternately updates the parameters of the moVMF and the network, ensuring
alignment with the moVMF-defined latent space. Extensive experiments validate
the rationality and effectiveness of our distribution choice and network
design. Consequently, DGNet achieves state-of-the-art performance under
multiple datasets and various weakly supervised settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SatSwinMAE: Efficient Autoencoding for Multiscale Time-series Satellite
  Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yohei Nakayama, Jiawei Su, Luis M. Pazos-Outón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in foundation models have significantly impacted various
fields, including natural language processing, computer vision, and multi-modal
tasks. One area that stands to benefit greatly is Earth observation, where
these models can efficiently process large-scale, unlabeled geospatial data. In
this work we extend the SwinMAE model to integrate temporal information for
satellite time-series data. The architecture employs a hierarchical 3D Masked
Autoencoder (MAE) with Video Swin Transformer blocks to effectively capture
multi-scale spatio-temporal dependencies in satellite imagery. To enhance
transfer learning, we incorporate both encoder and decoder pretrained weights,
along with skip connections to preserve scale-specific information. This forms
an architecture similar to SwinUNet with an additional temporal component. Our
approach shows significant performance improvements over existing
state-of-the-art foundation models for all the evaluated downstream tasks: land
cover segmentation, building density prediction, flood mapping, wildfire scar
mapping and multi-temporal crop segmentation. Particularly, in the land cover
segmentation task of the PhilEO Bench dataset, it outperforms other geospatial
foundation models with a 10.4% higher accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TotalVibeSegmentator: Full Body MRI Segmentation for the NAKO and UK
  Biobank 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00125v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00125v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Graf, Paul-Sören Platzek, Evamaria Olga Riedel, Constanze Ramschütz, Sophie Starck, Hendrik Kristian Möller, Matan Atad, Henry Völzke, Robin Bülow, Carsten Oliver Schmidt, Julia Rüdebusch, Matthias Jung, Marco Reisert, Jakob Weiss, Maximilian Löffler, Fabian Bamberg, Bene Wiestler, Johannes C. Paetzold, Daniel Rueckert, Jan Stefan Kirschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objectives: To present a publicly available torso segmentation network for
large epidemiology datasets on volumetric interpolated breath-hold examination
(VIBE) images. Materials & Methods: We extracted preliminary segmentations from
TotalSegmentator, spine, and body composition networks for VIBE images, then
improved them iteratively and retrained a nnUNet network. Using subsets of NAKO
(85 subjects) and UK Biobank (16 subjects), we evaluated with Dice-score on a
holdout set (12 subjects) and existing organ segmentation approach (1000
subjects), generating 71 semantic segmentation types for VIBE images. We
provide an additional network for the vertebra segments 22 individual vertebra
types. Results: We achieved an average Dice score of 0.89 +- 0.07 overall 71
segmentation labels. We scored > 0.90 Dice-score on the abdominal organs except
for the pancreas with a Dice of 0.70. Conclusion: Our work offers a detailed
and refined publicly available full torso segmentation on VIBE images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/robert-graf/TotalVibeSegmentator</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced <span class="highlight-title">Prompt</span>-leveraged Weakly Supervised Cancer Segmentation based on
  Segment Anything 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13621v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13621v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joonhyeon Song, Seohwan Yun, Seongho Yoon, Joohyeok Kim, Sangmin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a novel approach beyond supervised learning for effective
pathological image analysis, addressing the challenge of limited robust labeled
data. Pathological diagnosis of diseases like cancer has conventionally relied
on the evaluation of morphological features by physicians and pathologists.
However, recent advancements in compute-aided diagnosis (CAD) systems are
gaining significant attention as diagnostic support tools. Although the
advancement of deep learning has improved CAD significantly, segmentation
models typically require large pixel-level annotated dataset, and such labeling
is expensive. Existing studies not based on supervised approaches still
struggle with limited generalization, and no practical approach has emerged
yet. To address this issue, we present a weakly supervised semantic
segmentation (WSSS) model by combining class activation map and Segment
Anything Model (SAM)-based pseudo-labeling. For effective pretraining, we adopt
the SAM-a foundation model that is pretrained on large datasets and operates in
zero-shot configurations using only coarse prompts. The proposed approach
transfer enhanced Attention Dropout Layer's knowledge to SAM, thereby
generating pseudo-labels. To demonstrate the superiority of the proposed
method, experimental studies are conducted on histopathological breast cancer
datasets. The proposed method outperformed other WSSS methods across three
datasets, demonstrating its efficiency by achieving this with only 12GB of GPU
memory during training. Our code is available at :
https://github.com/QI-NemoSong/EPLC-SAM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded
  Egocentric Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.05822v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.05822v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junxiao Shen, John Dudley, Per Ola Kristensson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We depend on our own memory to encode, store, and retrieve our experiences.
However, memory lapses can occur. One promising avenue for achieving memory
augmentation is through the use of augmented reality head-mounted displays to
capture and preserve egocentric videos, a practice commonly referred to as
lifelogging. However, a significant challenge arises from the sheer volume of
video data generated through lifelogging, as the current technology lacks the
capability to encode and store such large amounts of data efficiently. Further,
retrieving specific information from extensive video archives requires
substantial computational power, further complicating the task of quickly
accessing desired content. To address these challenges, we propose a memory
augmentation agent that involves leveraging natural language encoding for video
data and storing them in a vector database. This approach harnesses the power
of large vision language models to perform the language encoding process.
Additionally, we propose using large language models to facilitate natural
language querying. Our agent underwent extensive evaluation using the QA-Ego4D
dataset and achieved state-of-the-art results with a BLEU score of 8.3,
outperforming conventional machine learning models that scored between 3.4 and
5.8. Additionally, we conducted a user study in which participants interacted
with the human memory augmentation agent through episodic memory and open-ended
questions. The results of this study show that the agent results in
significantly better recall performance on episodic memory tasks compared to
human participants. The results also highlight the agent's practical
applicability and user acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Object Pose Estimation via the Aggregation of Diffusion Features <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18791v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18791v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianfu Wang, Guosheng Hu, Hongguang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the pose of objects from images is a crucial task of 3D scene
understanding, and recent approaches have shown promising results on very large
benchmarks. However, these methods experience a significant performance drop
when dealing with unseen objects. We believe that it results from the limited
generalizability of image features. To address this problem, we have an
in-depth analysis on the features of diffusion models, e.g. Stable Diffusion,
which hold substantial potential for modeling unseen objects. Based on this
analysis, we then innovatively introduce these diffusion features for object
pose estimation. To achieve this, we propose three distinct architectures that
can effectively capture and aggregate diffusion features of different
granularity, greatly improving the generalizability of object pose estimation.
Our approach outperforms the state-of-the-art methods by a considerable margin
on three popular benchmark datasets, LM, O-LM, and T-LESS. In particular, our
method achieves higher accuracy than the previous best arts on unseen objects:
97.9% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the
strong generalizability of our method. Our code is released at
https://github.com/Tianfu18/diff-feats-pose.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2024, fix typo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VLFeedback: A Large-Scale AI Feedback Dataset for Large <span class="highlight-title">Vision-Language</span>
  Models Alignment <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong, Qi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large vision-language models (LVLMs) evolve rapidly, the demand for
high-quality and diverse data to align these models becomes increasingly
crucial. However, the creation of such data with human supervision proves
costly and time-intensive. In this paper, we investigate the efficacy of AI
feedback to scale supervision for aligning LVLMs. We introduce VLFeedback, the
first large-scale vision-language feedback dataset, comprising over 82K
multi-modal instructions and comprehensive rationales generated by
off-the-shelf models without human annotations. To evaluate the effectiveness
of AI feedback for vision-language alignment, we train Silkie, an LVLM
fine-tuned via direct preference optimization on VLFeedback. Silkie showcases
exceptional performance regarding helpfulness, visual faithfulness, and safety
metrics. It outperforms its base model by 6.9\% and 9.5\% in perception and
cognition tasks, reduces hallucination issues on MMHal-Bench, and exhibits
enhanced resilience against red-teaming attacks. Furthermore, our analysis
underscores the advantage of AI feedback, particularly in fostering preference
diversity to deliver more comprehensive improvements. Our dataset, training
code and models are available at https://vlf-silkie.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main Conference camera-ready version (fixed small typos).
  This article supersedes arXiv:2312.10665</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ X-Fi: A Modality-Invariant Foundation Model for <span class="highlight-title">Multimodal</span> Human Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyan Chen, Jianfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human sensing, which employs various sensors and advanced deep learning
technologies to accurately capture and interpret human body information, has
significantly impacted fields like public security and robotics. However,
current human sensing primarily depends on modalities such as cameras and
LiDAR, each of which has its own strengths and limitations. Furthermore,
existing multi-modal fusion solutions are typically designed for fixed modality
combinations, requiring extensive retraining when modalities are added or
removed for diverse scenarios. In this paper, we propose a modality-invariant
foundation model for all modalities, X-Fi, to address this issue. X-Fi enables
the independent or combinatory use of sensor modalities without additional
training by utilizing a transformer structure to accommodate variable input
sizes and incorporating a novel "X-fusion" mechanism to preserve
modality-specific features during multimodal integration. This approach not
only enhances adaptability but also facilitates the learning of complementary
features across modalities. Extensive experiments conducted on the MM-Fi and
XRF55 datasets, employing six distinct modalities, demonstrate that X-Fi
achieves state-of-the-art performance in human pose estimation (HPE) and human
activity recognition (HAR) tasks. The findings indicate that our proposed model
can efficiently support a wide range of human sensing applications, ultimately
contributing to the evolution of scalable, multimodal sensing technologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Suppress Content Shift: Better Diffusion Features via Off-the-Shelf
  Generation Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06719v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06719v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benyuan Meng, Qianqian Xu, Zitai Wang, Zhiyong Yang, Xiaochun Cao, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are powerful generative models, and this capability can also
be applied to discrimination. The inner activations of a pre-trained diffusion
model can serve as features for discriminative tasks, namely, diffusion
feature. We discover that diffusion feature has been hindered by a hidden yet
universal phenomenon that we call content shift. To be specific, there are
content differences between features and the input image, such as the exact
shape of a certain object. We locate the cause of content shift as one inherent
characteristic of diffusion models, which suggests the broad existence of this
phenomenon in diffusion feature. Further empirical study also indicates that
its negative impact is not negligible even when content shift is not visually
perceivable. Hence, we propose to suppress content shift to enhance the overall
quality of diffusion features. Specifically, content shift is related to the
information drift during the process of recovering an image from the noisy
input, pointing out the possibility of turning off-the-shelf generation
techniques into tools for content shift suppression. We further propose a
practical guideline named GATE to efficiently evaluate the potential benefit of
a technique and provide an implementation of our methodology. Despite the
simplicity, the proposed approach has achieved superior results on various
tasks and datasets, validating its potential as a generic booster for diffusion
features. Our code is available at
https://github.com/Darkbblue/diffusion-content-shift.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2410.03558</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Not All Diffusion Model Activations Have Been Evaluated as
  Discriminative Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03558v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03558v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benyuan Meng, Qianqian Xu, Zitai Wang, Xiaochun Cao, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are initially designed for image generation. Recent research
shows that the internal signals within their backbones, named activations, can
also serve as dense features for various discriminative tasks such as semantic
segmentation. Given numerous activations, selecting a small yet effective
subset poses a fundamental problem. To this end, the early study of this field
performs a large-scale quantitative comparison of the discriminative ability of
the activations. However, we find that many potential activations have not been
evaluated, such as the queries and keys used to compute attention scores.
Moreover, recent advancements in diffusion architectures bring many new
activations, such as those within embedded ViT modules. Both combined,
activation selection remains unresolved but overlooked. To tackle this issue,
this paper takes a further step with a much broader range of activations
evaluated. Considering the significant increase in activations, a full-scale
quantitative comparison is no longer operational. Instead, we seek to
understand the properties of these activations, such that the activations that
are clearly inferior can be filtered out in advance via simple qualitative
evaluation. After careful analysis, we discover three properties universal
among diffusion models, enabling this study to go beyond specific models. On
top of this, we present effective feature selection solutions for several
popular diffusion models. Finally, the experiments across multiple
discriminative tasks validate the superiority of our method over the SOTA
competitors. Our code is available at
https://github.com/Darkbblue/generic-diffusion-feature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniG: Modelling Unitary 3D Gaussians for View-consistent 3D
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiamin Wu, Kenkun Liu, Yukai Shi, Xiaoke Jiang, Yuan Yao, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present UniG, a view-consistent 3D reconstruction and novel
view synthesis model that generates a high-fidelity representation of 3D
Gaussians from sparse images. Existing 3D Gaussians-based methods usually
regress Gaussians per-pixel of each view, create 3D Gaussians per view
separately, and merge them through point concatenation. Such a view-independent
reconstruction approach often results in a view inconsistency issue, where the
predicted positions of the same 3D point from different views may have
discrepancies. To address this problem, we develop a DETR (DEtection
TRansformer)-like framework, which treats 3D Gaussians as decoder queries and
updates their parameters layer by layer by performing multi-view
cross-attention (MVDFA) over multiple input images. In this way, multiple views
naturally contribute to modeling a unitary representation of 3D Gaussians,
thereby making 3D reconstruction more view-consistent. Moreover, as the number
of 3D Gaussians used as decoder queries is irrespective of the number of input
views, allow an arbitrary number of input images without causing memory
explosion. Extensive experiments validate the advantages of our approach,
showcasing superior performance over existing methods quantitatively (improving
PSNR by 4.2 dB when trained on Objaverse and tested on the GSO benchmark) and
qualitatively. The code will be released at https://github.com/jwubz123/UNIG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViLCo-Bench: VIdeo Language COntinual learning Benchmark <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianqi Tang, Shohreh Deldari, Hao Xue, Celso De Melo, Flora D. Salim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video language continual learning involves continuously adapting to
information from video and text inputs, enhancing a model's ability to handle
new tasks while retaining prior knowledge. This field is a relatively
under-explored area, and establishing appropriate datasets is crucial for
facilitating communication and research in this field. In this study, we
present the first dedicated benchmark, ViLCo-Bench, designed to evaluate
continual learning models across a range of video-text tasks. The dataset
comprises ten-minute-long videos and corresponding language queries collected
from publicly available datasets. Additionally, we introduce a novel
memory-efficient framework that incorporates self-supervised learning and
mimics long-term and short-term memory effects. This framework addresses
challenges including memory complexity from long video clips, natural language
complexity from open queries, and text-video misalignment. We posit that
ViLCo-Bench, with greater complexity compared to existing continual learning
benchmarks, would serve as a critical tool for exploring the video-language
domain, extending beyond conventional class-incremental tasks, and addressing
complex and limited annotation issues. The curated data, evaluations, and our
novel method are available at https://github.com/cruiseresearchgroup/ViLCo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures, 8 tables, Accepted at NeurIPS Dataset and
  Benchmark Track 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MK-SGN: A Spiking Graph Convolutional Network with <span class="highlight-title">Multimodal</span> Fusion and
  Knowledge Distillation for Skeleton-based Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10210v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10210v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naichuan Zheng, Hailun Xia, Zeyu Liang, Yuanyuan Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, skeleton-based action recognition, leveraging multimodal
Graph Convolutional Networks (GCN), has achieved remarkable results. However,
due to their deep structure and reliance on continuous floating-point
operations, GCN-based methods are energy-intensive. We propose an innovative
Spiking Graph Convolutional Network with Multimodal Fusion and Knowledge
Distillation (MK-SGN) to address this issue. By merging the energy efficiency
of Spiking Neural Network (SNN) with the graph representation capability of
GCN, the proposed MK-SGN reduces energy consumption while maintaining
recognition accuracy. Firstly, we convert Graph Convolutional Networks (GCN)
into Spiking Graph Convolutional Networks (SGN) establishing a new benchmark
and paving the way for future research exploration. During this process, we
introduce a spiking attention mechanism and design a Spiking-Spatio Graph
Convolution module with a Spatial Global Spiking Attention mechanism (SA-SGC),
enhancing feature learning capability. Secondly, we propose a Spiking
Multimodal Fusion module (SMF), leveraging mutual information to process
multimodal data more efficiently. Lastly, we delve into knowledge distillation
methods from multimodal GCN to SGN and propose a novel, integrated method that
simultaneously focuses on both intermediate layer distillation and soft label
distillation to improve the performance of SGN. MK-SGN outperforms the
state-of-the-art GCN-like frameworks on three challenging datasets for
skeleton-based action recognition in reducing energy consumption. It also
outperforms the state-of-the-art SNN frameworks in accuracy. Specifically, our
method reduces energy consumption by more than 98% compared to typical
GCN-based methods, while maintaining competitive accuracy on the NTU-RGB+D 60
cross-subject split using 4-time steps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hard Region Aware Network for Remote Sensing Change Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenglai Li, Chang Tang, Xinwang Liu, Xingchen Hu, Xianju Li, Ning Li, Changdong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Change detection (CD) is essential for various real-world applications, such
as urban management and disaster assessment. Numerous CD methods have been
proposed, and considerable results have been achieved recently. However,
detecting changes in hard regions, i.e., the change boundary and irrelevant
pseudo changes caused by background clutters, remains difficult for these
methods, since they pose equal attention for all regions in bi-temporal images.
This paper proposes a novel change detection network, termed as HRANet, which
provides accurate change maps via hard region mining. Specifically, an online
hard region estimation branch is constructed to model the pixel-wise hard
samples, supervised by the error between predicted change maps and
corresponding ground truth during the training process. A cross-layer knowledge
review module is introduced to distill temporal change information from
low-level to high-level features, thereby enhancing the feature representation
capabilities. Finally, the hard region aware features extracted from the online
hard region estimation branch and multi-level temporal difference features are
aggregated into a unified feature representation to improve the accuracy of CD.
Experimental results on two benchmark datasets demonstrate the superior
performance of HRANet in the CD task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Signal-SGN: A Spiking Graph Convolutional Network for Skeletal Action
  Recognition via Learning Temporal-Frequency Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01701v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01701v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naichuan Zheng, Hailun Xia, Dapeng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In skeletal-based action recognition, Graph Convolutional Networks (GCNs)
based methods face limitations due to their complexity and high energy
consumption. Spiking Neural Networks (SNNs) have gained attention in recent
years for their low energy consumption, but existing methods combining GCNs and
SNNs fail to fully utilize the temporal characteristics of skeletal sequences,
leading to increased storage and computational costs. To address this issue, we
propose a Signal-SGN(Spiking Graph Convolutional Network), which leverages the
temporal dimension of skeletal sequences as the spiking timestep and treats
features as discrete stochastic signals. The core of the network consists of a
1D Spiking Graph Convolutional Network (1D-SGN) and a Frequency Spiking
Convolutional Network (FSN). The SGN performs graph convolution on single
frames and incorporates spiking network characteristics to capture inter-frame
temporal relationships, while the FSN uses Fast Fourier Transform (FFT) and
complex convolution to extract temporal-frequency features. We also introduce a
multi-scale wavelet transform feature fusion module(MWTF) to capture spectral
features of temporal signals, enhancing the model's classification capability.
We propose a pluggable temporal-frequency spatial semantic feature extraction
module(TFSM) to enhance the model's ability to distinguish features without
increasing inference-phase consumption. Our numerous experiments on the NTU
RGB+D, NTU RGB+D 120, and NW-UCLA datasets demonstrate that the proposed models
not only surpass existing SNN-based methods in accuracy but also reduce
computational and storage costs during training. Furthermore, they achieve
competitive accuracy compared to corresponding GCN-based methods, which is
quite remarkable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scene Prior Filtering for Depth Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13876v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13876v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxue Wang, Zhiqiang Yan, Ming-Hsuan Yang, Jinshan Pan, Guangwei Gao, Ying Tai, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal fusion is vital to the success of super-resolution of depth maps.
However, commonly used fusion strategies, such as addition and concatenation,
fall short of effectively bridging the modal gap. As a result, guided image
filtering methods have been introduced to mitigate this issue. Nevertheless, it
is observed that their filter kernels usually encounter significant texture
interference and edge inaccuracy. To tackle these two challenges, we introduce
a Scene Prior Filtering network, SPFNet, which utilizes the priors surface
normal and semantic map from large-scale models. Specifically, we design an
All-in-one Prior Propagation that computes the similarity between multi-modal
scene priors, i.e., RGB, normal, semantic, and depth, to reduce the texture
interference. In addition, we present a One-to-one Prior Embedding that
continuously embeds each single-modal prior into depth using Mutual Guided
Filtering, further alleviating the texture interference while enhancing edges.
Our SPFNet has been extensively evaluated on both real and synthetic datasets,
achieving state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LatentExplainer: Explaining Latent Representations in Deep Generative
  Models with <span class="highlight-title">Multi-modal</span> Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14862v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14862v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengdan Zhu, Raasikh Kanjiani, Jiahui Lu, Andrew Choi, Qirui Ye, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models like VAEs and diffusion models have advanced various
generation tasks by leveraging latent variables to learn data distributions and
generate high-quality samples. Despite the field of explainable AI making
strides in interpreting machine learning models, understanding latent variables
in generative models remains challenging. This paper introduces
\textit{LatentExplainer}, a framework for automatically generating semantically
meaningful explanations of latent variables in deep generative models.
\textit{LatentExplainer} tackles three main challenges: inferring the meaning
of latent variables, aligning explanations with inductive biases, and handling
varying degrees of explainability. Our approach perturbs latent variables,
interpreting changes in generated data, and uses multi-modal large language
models (MLLMs) to produce human-understandable explanations. We evaluate our
proposed method on several real-world and synthetic datasets, and the results
demonstrate superior performance in generating high-quality explanations for
latent variables. The results highlight the effectiveness of incorporating
inductive biases and uncertainty quantification, significantly enhancing model
interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Introspection to Best Practices: Principled Analysis of
  Demonstrations in <span class="highlight-title">Multimodal</span> In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Xu, Fei Wang, Sheng Zhang, Hoifung Poon, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by in-context learning (ICL) capabilities of Large Language models
(LLMs), multimodal LLMs with additional visual modality are also exhibited with
similar ICL abilities when multiple image-text pairs are provided as
demonstrations. However, relatively less work has been done to investigate the
principles behind how and why multimodal ICL works. We conduct a systematic and
principled evaluation of multimodal ICL for models of different scales on a
broad spectrum of new yet critical tasks. Through perturbations over different
modality information, we show that modalities matter differently across tasks
in multimodal ICL. Guided by task-specific modality impact, we recommend
modality-driven demonstration strategies to boost ICL performance. We also find
that models may follow inductive biases from multimodal ICL even if they are
rarely seen in or contradict semantic priors from pretraining data. Our
principled analysis provides a comprehensive way of understanding the role of
demonstrations in multimodal in-context learning, and sheds light on
effectively improving multimodal ICL on a wide range of tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAPL-SLAM: Principal Axis-Anchored Monocular Point-Line SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12324v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12324v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanghao Li, Yu Cao, Qi Chen, Yifan Yang, Jian Pu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In point-line SLAM systems, the utilization of line structural information
and the optimization of lines are two significant problems. The former is
usually addressed through structural regularities, while the latter typically
involves using minimal parameter representations of lines in optimization.
However, separating these two steps leads to the loss of constraint information
to each other. We anchor lines with similar directions to a principal axis and
optimize them with $n+2$ parameters for $n$ lines, solving both problems
together. Our method considers scene structural information, which can be
easily extended to different world hypotheses while significantly reducing the
number of line parameters to be optimized, enabling rapid and accurate mapping
and tracking. To further enhance the system's robustness and avoid mismatch, we
have modeled the line-axis probabilistic data association and provided the
algorithm for axis creation, updating, and optimization. Additionally,
considering that most real-world scenes conform to the Atlanta World
hypothesis, we provide a structural line detection strategy based on vertical
priors and vanishing points. Experimental results and ablation studies on
various indoor and outdoor datasets demonstrate the effectiveness of our
system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PredFormer: Transformers Are Effective Spatial-Temporal Predictive
  Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04733v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04733v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujin Tang, Lu Qi, Fei Xie, Xiangtai Li, Chao Ma, Ming-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatiotemporal predictive learning methods generally fall into two
categories: recurrent-based approaches, which face challenges in
parallelization and performance, and recurrent-free methods, which employ
convolutional neural networks (CNNs) as encoder-decoder architectures. These
methods benefit from strong inductive biases but often at the expense of
scalability and generalization. This paper proposes PredFormer, a pure
transformer-based framework for spatiotemporal predictive learning. Motivated
by the Vision Transformers (ViT) design, PredFormer leverages carefully
designed Gated Transformer blocks, following a comprehensive analysis of 3D
attention mechanisms, including full-, factorized-, and
interleaved-spatial-temporal attention. With its recurrent-free,
transformer-based design, PredFormer is both simple and efficient,
significantly outperforming previous methods by large margins. Extensive
experiments on synthetic and real-world datasets demonstrate that PredFormer
achieves state-of-the-art performance. On Moving MNIST, PredFormer achieves a
51.3% reduction in MSE relative to SimVP. For TaxiBJ, the model decreases MSE
by 33.1% and boosts FPS from 533 to 2364. Additionally, on WeatherBench, it
reduces MSE by 11.1% while enhancing FPS from 196 to 404. These performance
gains in both accuracy and efficiency demonstrate PredFormer's potential for
real-world applications. The source code will be released at
https://github.com/yyyujintang/PredFormer .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Biometric Authentication Based on Enhanced Remote Photoplethysmography
  Signal Morphology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04127v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04127v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaodong Sun, Xiaobai Li, Jukka Komulainen, Guoying Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote photoplethysmography (rPPG) is a non-contact method for measuring
cardiac signals from facial videos, offering a convenient alternative to
contact photoplethysmography (cPPG) obtained from contact sensors. Recent
studies have shown that each individual possesses a unique cPPG signal
morphology that can be utilized as a biometric identifier, which has inspired
us to utilize the morphology of rPPG signals extracted from facial videos for
person authentication. Since the facial appearance and rPPG are mixed in the
facial videos, we first de-identify facial videos to remove facial appearance
while preserving the rPPG information, which protects facial privacy and
guarantees that only rPPG is used for authentication. The de-identified videos
are fed into an rPPG model to get the rPPG signal morphology for
authentication. In the first training stage, unsupervised rPPG training is
performed to get coarse rPPG signals. In the second training stage, an
rPPG-cPPG hybrid training is performed by incorporating external cPPG datasets
to achieve rPPG biometric authentication and enhance rPPG signal morphology.
Our approach needs only de-identified facial videos with subject IDs to train
rPPG authentication models. The experimental results demonstrate that rPPG
signal morphology hidden in facial videos can be used for biometric
authentication. The code is available at
https://github.com/zhaodongsun/rppg_biometrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by IJCB 2024, Best Paper Runner-Up Award</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework
  for Talking Head Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13726v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13726v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanbo Cheng, Limin Lin, Chenyu Liu, Pengcheng Xia, Pengfei Hu, Jiefeng Ma, Jun Du, Jia Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Talking head generation intends to produce vivid and realistic talking head
videos from a single portrait and speech audio clip. Although significant
progress has been made in diffusion-based talking head generation, almost all
methods rely on autoregressive strategies, which suffer from limited context
utilization beyond the current generation step, error accumulation, and slower
generation speed. To address these challenges, we present DAWN (Dynamic frame
Avatar With Non-autoregressive diffusion), a framework that enables all-at-once
generation of dynamic-length video sequences. Specifically, it consists of two
main components: (1) audio-driven holistic facial dynamics generation in the
latent motion space, and (2) audio-driven head pose and blink generation.
Extensive experiments demonstrate that our method generates authentic and vivid
videos with precise lip motions, and natural pose/blink movements.
Additionally, with a high generation speed, DAWN possesses strong extrapolation
capabilities, ensuring the stable production of high-quality long videos. These
results highlight the considerable promise and potential impact of DAWN in the
field of talking head video generation. Furthermore, we hope that DAWN sparks
further exploration of non-autoregressive approaches in diffusion models. Our
code will be publicly available at https://github.com/Hanbo-Cheng/DAWN-pytorch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning
  via Image-Guided Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13674v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13674v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijun Liang, Shweta Bhardwaj, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-quality or scarce data has posed significant challenges for training deep
neural networks in practice. While classical data augmentation cannot
contribute very different new data, diffusion models opens up a new door to
build self-evolving AI by generating high-quality and diverse synthetic data
through text-guided prompts. However, text-only guidance cannot control
synthetic images' proximity to the original images, resulting in
out-of-distribution data detrimental to the model performance. To overcome the
limitation, we study image guidance to achieve a spectrum of interpolations
between synthetic and real images. With stronger image guidance, the generated
images are similar to the training data but hard to learn. While with weaker
image guidance, the synthetic images will be easier for model but contribute to
a larger distribution gap with the original data. The generated full spectrum
of data enables us to build a novel "Diffusion Curriculum (DisCL)". DisCL
adjusts the image guidance level of image synthesis for each training stage: It
identifies and focuses on hard samples for the model and assesses the most
effective guidance level of synthetic images to improve hard data learning. We
apply DisCL to two challenging tasks: long-tail (LT) classification and
learning from low-quality data. It focuses on lower-guidance images of
high-quality to learn prototypical features as a warm-up of learning
higher-guidance images that might be weak on diversity or quality. Extensive
experiments showcase a gain of 2.7% and 2.1% in OOD and ID macro-accuracy when
applying DisCL to iWildCam dataset. On ImageNet-LT, DisCL improves the base
model's tail-class accuracy from 4.4% to 23.64% and leads to a 4.02%
improvement in all-class accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, including references and appendix. Code is available at
  http://github.com/tianyi-lab/DisCL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ExACT: Teaching AI Agents to Explore with Reflective-MCTS and
  Exploratory Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02052v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02052v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents have demonstrated significant potential in automating
complex multistep decision-making tasks. However, even state-of-the-art
vision-language models (VLMs), such as GPT-4o, still fall short of human-level
performance, particularly in intricate web environments and long-horizon tasks.
To address these limitations, we present ExACT, an approach to combine
test-time search and self-learning to build o1-like models for agentic
applications. We first introduce Reflective Monte Carlo Tree Search (R-MCTS), a
novel test time algorithm designed to enhance AI agents' ability to explore
decision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating
contrastive reflection, allowing agents to learn from past interactions and
dynamically improve their search efficiency; and 2) using multi-agent debate
for reliable state evaluation. Next, we introduce Exploratory Learning, a novel
learning strategy to teach agents to search at inference time without relying
on any external search algorithms. On the challenging VisualWebArena benchmark,
our GPT-4o based R-MCTS agent achieves a 6% to 30% relative improvement across
various tasks compared to the previous state-of-the-art. Additionally, we show
that the knowledge and experience gained from test-time search can be
effectively transferred back to GPT-4o via fine-tuning. After Exploratory
Learning, GPT-4o 1) demonstrates the ability to explore the environment,
evaluate a state, and backtrack to viable ones when it detects that the current
state cannot lead to success, and 2) matches 87% of R-MCTS's performance while
using significantly less compute. Notably, our work demonstrates the compute
scaling properties in both training - data collection with R-MCTS - and testing
time. These results suggest a promising research direction to enhance VLMs'
capabilities for agentic applications via test-time search and self-learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Double-Condensing Attention Condenser: Leveraging Attention in Deep
  Learning to Detect Skin Cancer from Skin Lesion Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi-en Amy Tai, Elizabeth Janes, Chris Czarnecki, Alexander Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skin cancer is the most common type of cancer in the United States and is
estimated to affect one in five Americans. Recent advances have demonstrated
strong performance on skin cancer detection, as exemplified by state of the art
performance in the SIIM-ISIC Melanoma Classification Challenge; however these
solutions leverage ensembles of complex deep neural architectures requiring
immense storage and compute costs, and therefore may not be tractable. A recent
movement for TinyML applications is integrating Double-Condensing Attention
Condensers (DC-AC) into a self-attention neural network backbone architecture
to allow for faster and more efficient computation. This paper explores
leveraging an efficient self-attention structure to detect skin cancer in skin
lesion images and introduces a deep neural network design with DC-AC customized
for skin cancer detection from skin lesion images. The final model is publicly
available as a part of a global open-source initiative dedicated to
accelerating advancement in machine learning to aid clinicians in the fight
against cancer. Future work of this research includes iterating on the design
of the selected network architecture and refining the approach to generalize to
other forms of cancer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Residual-INR: Communication Efficient On-Device Learning Using Implicit
  Neural Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05617v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05617v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqiu Chen, Xuebin Yao, Pradeep Subedi, Cong Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge computing is a distributed computing paradigm that collects and
processes data at or near the source of data generation. The on-device learning
at edge relies on device-to-device wireless communication to facilitate
real-time data sharing and collaborative decision-making among multiple
devices. This significantly improves the adaptability of the edge computing
system to the changing environments. However, as the scale of the edge
computing system is getting larger, communication among devices is becoming the
bottleneck because of the limited bandwidth of wireless communication leads to
large data transfer latency. To reduce the amount of device-to-device data
transmission and accelerate on-device learning, in this paper, we propose
Residual-INR, a fog computing-based communication-efficient on-device learning
framework by utilizing implicit neural representation (INR) to compress
images/videos into neural network weights. Residual-INR enhances data transfer
efficiency by collecting JPEG images from edge devices, compressing them into
INR format at the fog node, and redistributing them for on-device learning. By
using a smaller INR for full image encoding and a separate object INR for
high-quality object region reconstruction through residual encoding, our
technique can reduce the encoding redundancy while maintaining the object
quality. Residual-INR is a promising solution for edge on-device learning
because it reduces data transmission by up to 5.16 x across a network of 10
edge devices. It also facilitates CPU-free accelerated on-device learning,
achieving up to 2.9 x speedup without sacrificing accuracy. Our code is
available at: https://github.com/sharclab/Residual-INR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by ICCAD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embedded <span class="highlight-title">Prompt</span> Tuning: Towards Enhanced Calibration of <span class="highlight-title">Pretrain</span>ed
  Models for Medical Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01003v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01003v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqiang Zu, Shenghao Xie, Qing Zhao, Guoqi Li, Lei Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models pre-trained on large-scale data have been widely witnessed
to achieve success in various natural imaging downstream tasks.
Parameter-efficient fine-tuning (PEFT) methods aim to adapt foundation models
to new domains by updating only a small portion of parameters in order to
reduce computational overhead. However, the effectiveness of these PEFT
methods, especially in cross-domain few-shot scenarios, e.g., medical image
analysis, has not been fully explored. In this work, we facilitate the study of
the performance of PEFT when adapting foundation models to medical image
classification tasks. Furthermore, to alleviate the limitations of prompt
introducing ways and approximation capabilities on Transformer architectures of
mainstream prompt tuning methods, we propose the Embedded Prompt Tuning (EPT)
method by embedding prompt tokens into the expanded channels. We also find that
there are anomalies in the feature space distribution of foundation models
during pre-training process, and prompt tuning can help mitigate this negative
impact. To explain this phenomenon, we also introduce a novel perspective to
understand prompt tuning: Prompt tuning is a distribution calibrator. And we
support it by analyzing patch-wise scaling and feature separation operations
contained in EPT. Our experiments show that EPT outperforms several
state-of-the-art fine-tuning methods by a significant margin on few-shot
medical image classification tasks, and completes the fine-tuning process
within highly competitive time, indicating EPT is an effective PEFT method. The
source code is available at github.com/zuwenqiang/EPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures. arXiv admin note: text overlap with
  arXiv:2306.09579, arXiv:2203.12119 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Polyhedral Complex Derivation from Piecewise Trilinear Networks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10403v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10403v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin-Hwa Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in visualizing deep neural networks provide insights into
their structures and mesh extraction from Continuous Piecewise Affine (CPWA)
functions. Meanwhile, developments in neural surface representation learning
incorporate non-linear positional encoding, addressing issues like spectral
bias; however, this poses challenges in applying mesh extraction techniques
based on CPWA functions. Focusing on trilinear interpolating methods as
positional encoding, we present theoretical insights and an analytical mesh
extraction, showing the transformation of hypersurfaces to flat planes within
the trilinear region under the eikonal constraint. Moreover, we introduce a
method for approximating intersecting points among three hypersurfaces
contributing to broader applications. We empirically validate correctness and
parsimony through chamfer distance and efficiency, and angular distance, while
examining the correlation between the eikonal loss and the planarity of the
hypersurfaces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024. Updated with the camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MOS: Model Synergy for Test-Time Adaptation on LiDAR-Based 3D Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14878v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14878v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoxiao Chen, Junjie Meng, Mahsa Baktashmotlagh, Yonggang Zhang, Zi Huang, Yadan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR-based 3D object detection is crucial for various applications but often
experiences performance degradation in real-world deployments due to domain
shifts. While most studies focus on cross-dataset shifts, such as changes in
environments and object geometries, practical corruptions from sensor
variations and weather conditions remain underexplored. In this work, we
propose a novel online test-time adaptation framework for 3D detectors that
effectively tackles these shifts, including a challenging cross-corruption
scenario where cross-dataset shifts and corruptions co-occur. By leveraging
long-term knowledge from previous test batches, our approach mitigates
catastrophic forgetting and adapts effectively to diverse shifts. Specifically,
we propose a Model Synergy (MOS) strategy that dynamically selects historical
checkpoints with diverse knowledge and assembles them to best accommodate the
current test batch. This assembly is directed by our proposed Synergy Weights
(SW), which perform a weighted averaging of the selected checkpoints,
minimizing redundancy in the composite model. The SWs are computed by
evaluating the similarity of predicted bounding boxes on the test data and the
independence of features between checkpoint pairs in the model bank. To
maintain an efficient and informative model bank, we discard checkpoints with
the lowest average SW scores, replacing them with newly updated models. Our
method was rigorously tested against existing test-time adaptation strategies
across three datasets and eight types of corruptions, demonstrating superior
adaptability to dynamic scenes and conditions. Notably, it achieved a 67.3%
improvement in a challenging cross-corruption scenario, offering a more
comprehensive benchmark for adaptation. The source code will be made publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Action Selection Learning for Multi-label Multi-view Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03302v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03302v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trung Thanh Nguyen, Yasutomo Kawanishi, Takahiro Komamizu, Ichiro Ide
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-label multi-view action recognition aims to recognize multiple
concurrent or sequential actions from untrimmed videos captured by multiple
cameras. Existing work has focused on multi-view action recognition in a narrow
area with strong labels available, where the onset and offset of each action
are labeled at the frame-level. This study focuses on real-world scenarios
where cameras are distributed to capture a wide-range area with only weak
labels available at the video-level. We propose the method named Multi-view
Action Selection Learning (MultiASL), which leverages action selection learning
to enhance view fusion by selecting the most useful information from different
viewpoints. The proposed method includes a Multi-view Spatial-Temporal
Transformer video encoder to extract spatial and temporal features from
multi-viewpoint videos. Action Selection Learning is employed at the
frame-level, using pseudo ground-truth obtained from weak labels at the
video-level, to identify the most relevant frames for action recognition.
Experiments in a real-world office environment using the MM-Office dataset
demonstrate the superior performance of the proposed method compared to
existing methods. The source code is available at
https://github.com/thanhhff/MultiASL/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Multimedia Asia 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIMformer: Single-Layer Vanilla Transformer Can Learn Free-Space
  Trajectory Similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuang Yang, Renhe Jiang, Xiaohang Xu, Chuan Xiao, Kaoru Sezaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Free-space trajectory similarity calculation, e.g., DTW, Hausdorff, and
Frechet, often incur quadratic time complexity, thus learning-based methods
have been proposed to accelerate the computation. The core idea is to train an
encoder to transform trajectories into representation vectors and then compute
vector similarity to approximate the ground truth. However, existing methods
face dual challenges of effectiveness and efficiency: 1) they all utilize
Euclidean distance to compute representation similarity, which leads to the
severe curse of dimensionality issue -- reducing the distinguishability among
representations and significantly affecting the accuracy of subsequent
similarity search tasks; 2) most of them are trained in triplets manner and
often necessitate additional information which downgrades the efficiency; 3)
previous studies, while emphasizing the scalability in terms of efficiency,
overlooked the deterioration of effectiveness when the dataset size grows. To
cope with these issues, we propose a simple, yet accurate, fast, scalable model
that only uses a single-layer vanilla transformer encoder as the feature
extractor and employs tailored representation similarity functions to
approximate various ground truth similarity measures. Extensive experiments
demonstrate our model significantly mitigates the curse of dimensionality issue
and outperforms the state-of-the-arts in effectiveness, efficiency, and
scalability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing AI Accessibility in Veterinary Medicine: Linking Classifiers
  and Electronic Health Records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chun Yin Kong, Picasso Vasquez, Makan Farhoodimoghadam, Chris Brandt, Titus C. Brown, Krystle L. Reagan, Allison Zwingenberger, Stefan M. Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving landscape of veterinary healthcare, integrating
machine learning (ML) clinical decision-making tools with electronic health
records (EHRs) promises to improve diagnostic accuracy and patient care.
However, the seamless integration of ML classifiers into existing EHRs in
veterinary medicine is frequently hindered by the rigidity of EHR systems or
the limited availability of IT resources. To address this shortcoming, we
present Anna, a freely-available software solution that provides ML classifier
results for EHR laboratory data in real-time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiSCo Meets LLMs: A Unified Approach for Sparse Retrieval and Contextual
  Distillation in Conversational Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Lupart, Mohammad Aliannejadi, Evangelos Kanoulas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Search (CS) is the task of retrieving relevant documents from
a corpus within a conversational context, combining retrieval with
conversational context modeling. With the explosion of Large Language Models
(LLMs), the CS field has seen major improvements with LLMs rewriting user
queries, accounting for conversational context. However, engaging LLMs at
inference time harms efficiency. Current methods address this by distilling
embeddings from human-rewritten queries to learn the context modeling task.
Yet, these approaches predominantly focus on context modeling, and only treat
the contrastive component of the retrieval task within a
distillation-independent loss term. To address these limitations, we propose a
new distillation method, as a relaxation of the previous objective, unifying
retrieval and context modeling. We relax the existing training objectives by
distilling similarity scores between conversations and documents, rather than
relying solely on representation learning. Our proposed distillation objective
allows for more freedom in the representation space and leverages the
contrastive nature of document relevance. Through experiments on Learned Sparse
Retrieval (LSR) across 5 CS datasets, our approach demonstrates substantial
improvements in both in-domain and out-of-domain retrieval performance,
outperforming state-of-the-art with gains of up to 6 points in recall for
out-of-domain datasets. Additionally, through the relaxation of the objective,
we propose a multi-teacher distillation, using multiple LLMs as teachers,
yielding additional gains, and outperforming the teachers themselves in
in-domain experiments. Finally, analysis of the sparsity of the models reveals
that our distillation allows for better control over the sparsity of the
trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAG-ConfusionQA: A Benchmark for Evaluating LLMs on Confusing Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Peng, Jinming Nian, Alexandre Evfimievski, Yi Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational AI agents use Retrieval Augmented Generation (RAG) to provide
verifiable document-grounded responses to user inquiries. However, many natural
questions do not have good answers: about 25\% contain false
assumptions~\cite{Yu2023:CREPE}, and over 50\% are
ambiguous~\cite{Min2020:AmbigQA}. RAG agents need high-quality data to improve
their responses to confusing questions. This paper presents a novel synthetic
data generation method to efficiently create a diverse set of context-grounded
confusing questions from a given document corpus. We conduct an empirical
comparative evaluation of several large language models as RAG agents to
measure the accuracy of confusion detection and appropriate response
generation. We contribute a benchmark dataset to the public domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPFresh: Incremental In-Place Update for Billion-Scale Vector Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuming Xu, Hengyu Liang, Jin Li, Shuotao Xu, Qi Chen, Qianxi Zhang, Cheng Li, Ziyue Yang, Fan Yang, Yuqing Yang, Peng Cheng, Mao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate Nearest Neighbor Search (ANNS) is now widely used in various
applications, ranging from information retrieval, question answering, and
recommendation, to search for similar high-dimensional vectors. As the amount
of vector data grows continuously, it becomes important to support updates to
vector index, the enabling technique that allows for efficient and accurate
ANNS on vectors. Because of the curse of high dimensionality, it is often
costly to identify the right neighbors of a single new vector, a necessary
process for index update. To amortize update costs, existing systems maintain a
secondary index to accumulate updates, which are merged by the main index by
global rebuilding the entire index periodically. However, this approach has
high fluctuations of search latency and accuracy, not even to mention that it
requires substantial resources and is extremely time-consuming for rebuilds. We
introduce SPFresh, a system that supports in-place vector updates. At the heart
of SPFresh is LIRE, a lightweight incremental rebalancing protocol to split
vector partitions and reassign vectors in the nearby partitions to adapt to
data distribution shift. LIRE achieves low-overhead vector updates by only
reassigning vectors at the boundary between partitions, where in a high-quality
vector index the amount of such vectors are deemed small. With LIRE, SPFresh
provides superior query latency and accuracy to solutions based on global
rebuild, with only 1% of DRAM and less than 10% cores needed at the peak
compared to the state-of-the-art, in a billion scale vector index with 1% of
daily vector update rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SOSP 23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChartifyText: Automated Chart Generation from Data-Involved Texts via
  LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songheng Zhang, Lei Wang, Toby Jia-Jun Li, Qiaomu Shen, Yixin Cao, Yong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text documents with numerical values involved are widely used in various
applications such as scientific research, economy, public health and
journalism. However, it is difficult for readers to quickly interpret such
data-involved texts and gain deep insights. To fill this research gap, this
work aims to automatically generate charts to accurately convey the underlying
data and ideas to readers, which is essentially a challenging task. The
challenges originate from text ambiguities, intrinsic sparsity and uncertainty
of data in text documents, and subjective sentiment differences. Specifically,
we propose ChartifyText, a novel fully-automated approach that leverages Large
Language Models (LLMs) to convert complex data-involved texts to expressive
charts. It consists of two major modules: tabular data inference and expressive
chart generation. The tabular data inference module employs systematic prompt
engineering to guide the LLM (e.g., GPT-4) to infer table data, where data
ranges, uncertainties, missing data values and corresponding subjective
sentiments are explicitly considered. The expressive chart generation module
augments standard charts with intuitive visual encodings and concise texts to
accurately convey the underlying data and insights. We extensively evaluate the
effectiveness of ChartifyText on real-world data-involved text documents
through case studies, in-depth interviews with three visualization experts, and
a carefully-designed user study with 15 participants. The results demonstrate
the usefulness and effectiveness of ChartifyText in helping readers efficiently
and effectively make sense of data-involved texts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Neural Patching for Cold-Start Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Chen, Yu Yang, Yuanchen Bei, Zefan Wang, Yue Xu, Feiran Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The cold start problem in recommender systems remains a critical challenge.
Current solutions often train hybrid models on auxiliary data for both cold and
warm users/items, potentially degrading the experience for the latter. This
drawback limits their viability in practical scenarios where the satisfaction
of existing warm users/items is paramount. Although graph neural networks
(GNNs) excel at warm recommendations by effective collaborative signal
modeling, they haven't been effectively leveraged for the cold-start issue
within a user-item graph, which is largely due to the lack of initial
connections for cold user/item entities. Addressing this requires a GNN adept
at cold-start recommendations without sacrificing performance for existing
ones. To this end, we introduce Graph Neural Patching for Cold-Start
Recommendations (GNP), a customized GNN framework with dual functionalities:
GWarmer for modeling collaborative signal on existing warm users/items and
Patching Networks for simulating and enhancing GWarmer's performance on
cold-start recommendations. Extensive experiments on three benchmark datasets
confirm GNP's superiority in recommending both warm and cold users/items.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, accepted by Australasian Database Conference 2024. arXiv
  admin note: substantial text overlap with arXiv:2209.12215</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Image Generation with Large <span class="highlight-title">Multimodal</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyan Xu, Wenjie Wang, Yang Zhang, Tang Biao, Peng Yan, Fuli Feng, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized content filtering, such as recommender systems, has become a
critical infrastructure to alleviate information overload. However, these
systems merely filter existing content and are constrained by its limited
diversity, making it difficult to meet users' varied content needs. To address
this limitation, personalized content generation has emerged as a promising
direction with broad applications. Nevertheless, most existing research focuses
on personalized text generation, with relatively little attention given to
personalized image generation. The limited work in personalized image
generation faces challenges in accurately capturing users' visual preferences
and needs from noisy user-interacted images and complex multimodal
instructions. Worse still, there is a lack of supervised data for training
personalized image generation models.
  To overcome the challenges, we propose a Personalized Image Generation
Framework named Pigeon, which adopts exceptional large multimodal models with
three dedicated modules to capture users' visual preferences and needs from
noisy user history and multimodal instructions. To alleviate the data scarcity,
we introduce a two-stage preference alignment scheme, comprising masked
preference reconstruction and pairwise preference alignment, to align Pigeon
with the personalized image generation task. We apply Pigeon to personalized
sticker and movie poster generation, where extensive quantitative results and
human evaluation highlight its superiority over various generative baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Retrieval-Augmented Generation with Elasticsearch for
  Enhanced Question-Answering Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajing Chen, Runyuan Bao, Hongye Zheng, Zhen Qi, Jianjun Wei, Jiacheng Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study aims to improve the accuracy and quality of large-scale language
models (LLMs) in answering questions by integrating Elasticsearch into the
Retrieval Augmented Generation (RAG) framework. The experiment uses the
Stanford Question Answering Dataset (SQuAD) version 2.0 as the test dataset and
compares the performance of different retrieval methods, including traditional
methods based on keyword matching or semantic similarity calculation, BM25-RAG
and TF-IDF- RAG, and the newly proposed ES-RAG scheme. The results show that
ES-RAG not only has obvious advantages in retrieval efficiency but also
performs well in key indicators such as accuracy, which is 0.51 percentage
points higher than TF-IDF-RAG. In addition, Elasticsearch's powerful search
capabilities and rich configuration options enable the entire
question-answering system to better handle complex queries and provide more
flexible and efficient responses based on the diverse needs of users. Future
research directions can further explore how to optimize the interaction
mechanism between Elasticsearch and LLM, such as introducing higher-level
semantic understanding and context-awareness capabilities, to achieve a more
intelligent and humanized question-answering experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust Transcription: Exploring Noise Injection Strategies for
  Training Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghyun Kim, Alexander Lerch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Automatic Piano Transcription (APT) have significantly
improved system performance, but the impact of noisy environments on the system
performance remains largely unexplored. This study investigates the impact of
white noise at various Signal-to-Noise Ratio (SNR) levels on state-of-the-art
APT models and evaluates the performance of the Onsets and Frames model when
trained on noise-augmented data. We hope this research provides valuable
insights as preliminary work toward developing transcription models that
maintain consistent performance across a range of acoustic conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Late-Breaking Demo Session of the 25th International
  Society for Music Information Retrieval (ISMIR) Conference, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EasyRec: Simple yet Effective Language Models for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08821v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08821v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xubin Ren, Chao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have become a powerful technique for learning
representations from user-item interaction data in collaborative filtering (CF)
for recommender systems. However, many existing methods heavily rely on unique
user and item IDs, which limits their ability to perform well in practical
zero-shot learning scenarios where sufficient training data may be unavailable.
Inspired by the success of language models (LMs) and their strong
generalization capabilities, a crucial question arises: How can we harness the
potential of language models to empower recommender systems and elevate its
generalization capabilities to new heights? In this study, we propose EasyRec -
an effective and easy-to-use approach that seamlessly integrates text-based
semantic understanding with collaborative signals. EasyRec employs a
text-behavior alignment framework, which combines contrastive learning with
collaborative language model tuning, to ensure a strong alignment between the
text-enhanced semantic space and the collaborative behavior information.
Extensive empirical evaluations across diverse real-world datasets demonstrate
the superior performance of EasyRec compared to state-of-the-art alternative
models, particularly in the challenging text-based zero-shot recommendation
scenarios. Furthermore, the study highlights the potential of seamlessly
integrating EasyRec as a plug-and-play component into text-enhanced
collaborative filtering frameworks, thereby empowering existing recommender
systems to elevate their recommendation performance and adapt to the evolving
user preferences in dynamic environments. For better result reproducibility of
our EasyRec framework, the model implementation details, source code, and
datasets are available at the link: https://github.com/HKUDS/EasyRec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Retrieval in Sponsored Search by Leveraging Query Context
  Signals <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14346v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14346v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akash Kumar Mohankumar, Gururaj K, Gagan Madan, Amit Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately retrieving relevant bid keywords for user queries is critical in
Sponsored Search but remains challenging, particularly for short, ambiguous
queries. Existing dense and generative retrieval models often fail to capture
nuanced user intent in these cases. To address this, we propose an approach to
enhance query understanding by augmenting queries with rich contextual signals
derived from web search results and large language models, stored in an online
cache. Specifically, we use web search titles and snippets to ground queries in
real-world information and utilize GPT-4 to generate query rewrites and
explanations that clarify user intent. These signals are efficiently integrated
through a Fusion-in-Decoder based Unity architecture, enabling both dense and
generative retrieval with serving costs on par with traditional context-free
models. To address scenarios where context is unavailable in the cache, we
introduce context glancing, a curriculum learning strategy that improves model
robustness and performance even without contextual signals during inference.
Extensive offline experiments demonstrate that our context-aware approach
substantially outperforms context-free models. Furthermore, online A/B testing
on a prominent search engine across 160+ countries shows significant
improvements in user engagement and revenue.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting BPR: A Replicability Study of a Common Recommender System
  Baseline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14217v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14217v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Milogradskii, Oleg Lashinin, Alexander P, Marina Ananyeva, Sergey Kolesnikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian Personalized Ranking (BPR), a collaborative filtering approach based
on matrix factorization, frequently serves as a benchmark for recommender
systems research. However, numerous studies often overlook the nuances of BPR
implementation, claiming that it performs worse than newly proposed methods
across various tasks. In this paper, we thoroughly examine the features of the
BPR model, indicating their impact on its performance, and investigate
open-source BPR implementations. Our analysis reveals inconsistencies between
these implementations and the original BPR paper, leading to a significant
decrease in performance of up to 50% for specific implementations. Furthermore,
through extensive experiments on real-world datasets under modern evaluation
settings, we demonstrate that with proper tuning of its hyperparameters, the
BPR model can achieve performance levels close to state-of-the-art methods on
the top-n recommendation tasks and even outperform them on specific datasets.
Specifically, on the Million Song Dataset, the BPR model with hyperparameters
tuning statistically significantly outperforms Mult-VAE by 10% in NDCG@100 with
binary relevance function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted at the Reproducibility track of the ACM RecSys
  '24 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generate and Instantiate What You Prefer: Text-Guided Diffusion for
  Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoqing Hu, Zhangyi Yang, Zhibo Cai, An Zhang, Xiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in generative recommendation systems, particularly in the
realm of sequential recommendation tasks, have shown promise in enhancing
generalization to new items. Among these approaches, diffusion-based generative
recommendation has emerged as an effective tool, leveraging its ability to
capture data distributions and generate high-quality samples. Despite
effectiveness, two primary challenges have been identified: 1) the lack of
consistent modeling of data distribution for oracle items; and 2) the
difficulty in scaling to more informative control signals beyond historical
interactions. These issues stem from the uninformative nature of ID embeddings,
which necessitate random initialization and limit the incorporation of
additional control signals. To address these limitations, we propose iDreamRec
to involve more concrete prior knowledge to establish item embeddings,
particularly through detailed item text descriptions and advanced Text
Embedding Models (TEM). More importantly, by converting item descriptions into
embeddings aligned with TEM, we enable the integration of intention
instructions as control signals to guide the generation of oracle items.
Experimental results on four datasets demonstrate that iDreamRec not only
outperforms existing diffusion-based generative recommenders but also
facilitates the incorporation of intention instructions for more precise and
effective recommendation generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conversational Recommender System and Large Language Model Are Made for
  Each Other in E-commerce Pre-sales Dialogue <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14626v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14626v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanxing Liu, Wei-Nan Zhang, Yifan Chen, Yuchi Zhang, Haopeng Bai, Fan Feng, Hengbin Cui, Yongbin Li, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  E-commerce pre-sales dialogue aims to understand and elicit user needs and
preferences for the items they are seeking so as to provide appropriate
recommendations. Conversational recommender systems (CRSs) learn user
representation and provide accurate recommendations based on dialogue context,
but rely on external knowledge. Large language models (LLMs) generate responses
that mimic pre-sales dialogues after fine-tuning, but lack domain-specific
knowledge for accurate recommendations. Intuitively, the strengths of LLM and
CRS in E-commerce pre-sales dialogues are complementary, yet no previous work
has explored this. This paper investigates the effectiveness of combining LLM
and CRS in E-commerce pre-sales dialogues, proposing two collaboration methods:
CRS assisting LLM and LLM assisting CRS. We conduct extensive experiments on a
real-world dataset of Ecommerce pre-sales dialogues. We analyze the impact of
two collaborative approaches with two CRSs and two LLMs on four tasks of
Ecommerce pre-sales dialogue. We find that collaborations between CRS and LLM
can be very effective in some cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Starbucks: Improved Training for 2D Matryoshka Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyao Zhuang, Shuai Wang, Bevan Koopman, Guido Zuccon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective approaches that can scale embedding model depth (i.e. layers) and
embedding size allow for the creation of models that are highly scalable across
different computational resources and task requirements. While the recently
proposed 2D Matryoshka training approach can efficiently produce a single
embedding model such that its sub-layers and sub-dimensions can measure text
similarity, its effectiveness is significantly worse than if smaller models
were trained separately. To address this issue, we propose Starbucks, a new
training strategy for Matryoshka-like embedding models, which encompasses both
the fine-tuning and pre-training phases. For the fine-tuning phase, we discover
that, rather than sampling a random sub-layer and sub-dimensions for each
training steps, providing a fixed list of layer-dimension pairs, from small
size to large sizes, and computing the loss across all pairs significantly
improves the effectiveness of 2D Matryoshka embedding models, bringing them on
par with their separately trained counterparts. To further enhance performance,
we introduce a new pre-training strategy, which applies masked autoencoder
language modelling to sub-layers and sub-dimensions during pre-training,
resulting in a stronger backbone for subsequent fine-tuning of the embedding
model. Experimental results on both semantic text similarity and retrieval
benchmarks demonstrate that the proposed pre-training and fine-tuning
strategies significantly improved the effectiveness over 2D Matryoshka models,
enabling Starbucks models to perform more efficiently and effectively than
separately trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Neural Network Enhanced Retrieval for Question Answering of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06572v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06572v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Li, Qingyan Guo, Jiawei Shao, Lei Song, Jiang Bian, Jun Zhang, Rui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval augmented generation has revolutionized large language model (LLM)
outputs by providing factual supports. Nevertheless, it struggles to capture
all the necessary knowledge for complex reasoning questions. Existing retrieval
methods typically divide reference documents into passages, treating them in
isolation. These passages, however, are often interrelated, such as passages
that are contiguous or share the same keywords. Therefore, it is crucial to
recognize such relatedness for enhancing the retrieval process. In this paper,
we propose a novel retrieval method, called GNN-Ret, which leverages graph
neural networks (GNNs) to enhance retrieval by exploiting the relatedness
between passages. Specifically, we first construct a graph of passages by
connecting passages that are structure-related or keyword-related. A graph
neural network (GNN) is then leveraged to exploit the relationships between
passages and improve the retrieval of supporting passages. Furthermore, we
extend our method to handle multi-hop reasoning questions using a recurrent
graph neural network (RGNN), named RGNN-Ret. At each step, RGNN-Ret integrates
the graphs of passages from previous steps, thereby enhancing the retrieval of
supporting passages. Extensive experiments on benchmark datasets demonstrate
that GNN-Ret achieves higher accuracy for question answering with a single
query of LLMs than strong baselines that require multiple queries, and RGNN-Ret
further improves accuracy and achieves state-of-the-art performance, with up to
10.4% accuracy improvement on the 2WikiMQA dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FINED: Feed Instance-Wise Information Need with Essential and
  Disentangled Parametric Knowledge from the Past 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kounianhua Du, Jizheng Chen, Jianghao Lin, Menghui Zhu, Bo Chen, Shuai Li, Yong Yu, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender models play a vital role in various industrial scenarios, while
often faced with the catastrophic forgetting problem caused by the fast
shifting data distribution. To alleviate this problem, a common approach is to
reuse knowledge from the historical data. However, preserving the vast and
fast-accumulating data is hard, which causes dramatic storage overhead.
Memorizing old data through a parametric knowledge base is then proposed, which
compresses the vast amount of raw data into model parameters. Despite the
flexibility, how to improve the memorization and generalization capabilities of
the parametric knowledge base and suit the flexible information need of each
instance are challenging. In this paper, we propose FINED to Feed INstance-wise
information need with Essential and Disentangled parametric knowledge from past
data for recommendation enhancement. Concretely, we train a knowledge extractor
that extracts knowledge patterns of arbitrary order from past data and a
knowledge encoder that memorizes the arbitrary order patterns, which serves as
the retrieval key generator and memory network respectively in the following
knowledge reusing phase. The whole process is regularized by the proposed two
constraints, which improve the capabilities of the parametric knowledge base
without increasing the size of it. The essential principle helps to compress
the input into representative vectors that capture the task-relevant
information and filter out the noisy information. The disentanglement principle
reduces the redundancy of stored information and pushes the knowledge base to
focus on capturing the disentangled invariant patterns. These two rules
together promote rational compression of information for robust and generalized
knowledge representations. Extensive experiments on two datasets justify the
effectiveness of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-supervised contrastive learning performs non-linear system
  identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo González Laiz, Tobias Schmidt, Steffen Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) approaches have brought tremendous success
across many tasks and domains. It has been argued that these successes can be
attributed to a link between SSL and identifiable representation learning:
Temporal structure and auxiliary variables ensure that latent representations
are related to the true underlying generative factors of the data. Here, we
deepen this connection and show that SSL can perform system identification in
latent space. We propose DynCL, a framework to uncover linear, switching linear
and non-linear dynamics under a non-linear observation model, give theoretical
guarantees and validate them empirically.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decomposing The Dark Matter of Sparse Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Engels, Logan Riggs, Max Tegmark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse autoencoders (SAEs) are a promising technique for decomposing language
model activations into interpretable linear features. However, current SAEs
fall short of completely explaining model performance, resulting in "dark
matter": unexplained variance in activations. This work investigates dark
matter as an object of study in its own right. Surprisingly, we find that much
of SAE dark matter--about half of the error vector itself and >90% of its
norm--can be linearly predicted from the initial activation vector.
Additionally, we find that the scaling behavior of SAE error norms at a per
token level is remarkably predictable: larger SAEs mostly struggle to
reconstruct the same contexts as smaller SAEs. We build on the linear
representation hypothesis to propose models of activations that might lead to
these observations, including postulating a new type of "introduced error";
these insights imply that the part of the SAE error vector that cannot be
linearly predicted ("nonlinear" error) might be fundamentally different from
the linearly predictable component. To validate this hypothesis, we empirically
analyze nonlinear SAE error and show that 1) it contains fewer not yet learned
features, 2) SAEs trained on it are quantitatively worse, 3) it helps predict
SAE per-token scaling behavior, and 4) it is responsible for a proportional
amount of the downstream increase in cross entropy loss when SAE activations
are inserted into the model. Finally, we examine two methods to reduce
nonlinear SAE error at a fixed sparsity: inference time gradient pursuit, which
leads to a very slight decrease in nonlinear error, and linear transformations
from earlier layer SAE outputs, which leads to a larger reduction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code at https://github.com/JoshEngels/SAE-Dark-Matter</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Gradient Descent Jittering for Inverse Problems: Alleviating
  the Accuracy-Robustness Tradeoff 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peimeng Guan, Mark A. Davenport
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse problems aim to reconstruct unseen data from corrupted or perturbed
measurements. While most work focuses on improving reconstruction quality,
generalization accuracy and robustness are equally important, especially for
safety-critical applications. Model-based architectures (MBAs), such as loop
unrolling methods, are considered more interpretable and achieve better
reconstructions. Empirical evidence suggests that MBAs are more robust to
perturbations than black-box solvers, but the accuracy-robustness tradeoff in
MBAs remains underexplored. In this work, we propose a simple yet effective
training scheme for MBAs, called SGD jittering, which injects noise
iteration-wise during reconstruction. We theoretically demonstrate that SGD
jittering not only generalizes better than the standard mean squared error
training but is also more robust to average-case attacks. We validate SGD
jittering using denoising toy examples, seismic deconvolution, and single-coil
MRI reconstruction. The proposed method achieves cleaner reconstructions for
out-of-distribution data and demonstrates enhanced robustness to adversarial
attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie
  Character-Aware Discourse Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maitreya Prafulla Chitale, Uday Bindal, Rajakrishnan Rajkumar, Rahul Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Summarizing movie screenplays presents a unique set of challenges compared to
standard document summarization. Screenplays are not only lengthy, but also
feature a complex interplay of characters, dialogues, and scenes, with numerous
direct and subtle relationships and contextual nuances that are difficult for
machine learning models to accurately capture and comprehend. Recent attempts
at screenplay summarization focus on fine-tuning transformer-based pre-trained
models, but these models often fall short in capturing long-term dependencies
and latent relationships, and frequently encounter the "lost in the middle"
issue. To address these challenges, we introduce DiscoGraMS, a novel resource
that represents movie scripts as a movie character-aware discourse graph (CaD
Graph). This approach is well-suited for various downstream tasks, such as
summarization, question-answering, and salience detection. The model aims to
preserve all salient information, offering a more comprehensive and faithful
representation of the screenplay's content. We further explore a baseline
method that combines the CaD Graph with the corresponding movie script through
a late fusion of graph and text modalities, and we present very initial
promising results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Reinforcement Learning with Passive Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anay Pattanaik, Lav R. Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers an online reinforcement learning algorithm that
leverages pre-collected data (passive memory) from the environment for online
interaction. We show that using passive memory improves performance and further
provide theoretical guarantees for regret that turns out to be near-minimax
optimal. Results show that the quality of passive memory determines
sub-optimality of the incurred regret. The proposed approach and results hold
in both continuous and discrete state-action spaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Large Language Model-Driven Reward Design Framework via Dynamic
  Feedback for Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengjie Sun, Runze Liu, Jiafei Lyu, Jing-Wen Yang, Liangpeng Zhang, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown significant potential in designing
reward functions for Reinforcement Learning (RL) tasks. However, obtaining
high-quality reward code often involves human intervention, numerous LLM
queries, or repetitive RL training. To address these issues, we propose CARD, a
LLM-driven Reward Design framework that iteratively generates and improves
reward function code. Specifically, CARD includes a Coder that generates and
verifies the code, while a Evaluator provides dynamic feedback to guide the
Coder in improving the code, eliminating the need for human feedback. In
addition to process feedback and trajectory feedback, we introduce Trajectory
Preference Evaluation (TPE), which evaluates the current reward function based
on trajectory preferences. If the code fails the TPE, the Evaluator provides
preference feedback, avoiding RL training at every iteration and making the
reward function better aligned with the task objective. Empirical results on
Meta-World and ManiSkill2 demonstrate that our method achieves an effective
balance between task performance and token efficiency, outperforming or
matching the baselines across all tasks. On 10 out of 12 tasks, CARD shows
better or comparable performance to policies trained with expert-designed
rewards, and our method even surpasses the oracle on 3 tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing Causality in Reinforcement Learning With Bagged Decision
  Times 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daiqi Gao, Hsin-Yu Lai, Predrag Klasnja, Susan A. Murphy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider reinforcement learning (RL) for a class of problems with bagged
decision times. A bag contains a finite sequence of consecutive decision times.
The transition dynamics are non-Markovian and non-stationary within a bag.
Further, all actions within a bag jointly impact a single reward, observed at
the end of the bag. Our goal is to construct an online RL algorithm to maximize
the discounted sum of the bag-specific rewards. To handle non-Markovian
transitions within a bag, we utilize an expert-provided causal directed acyclic
graph (DAG). Based on the DAG, we construct the states as a dynamical Bayesian
sufficient statistic of the observed history, which results in Markovian state
transitions within and across bags. We then frame this problem as a periodic
Markov decision process (MDP) that allows non-stationarity within a period. An
online RL algorithm based on Bellman-equations for stationary MDPs is
generalized to handle periodic MDPs. To justify the proposed RL algorithm, we
show that our constructed state achieves the maximal optimal value function
among all state constructions for a periodic MDP. Further we prove the Bellman
optimality equations for periodic MDPs. We evaluate the proposed method on
testbed variants, constructed with real data from a mobile health clinical
trial.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated
  Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhepeng Cen, Yao Liu, Siliang Zeng, Pratik Chaudhar, Huzefa Rangwala, George Karypis, Rasool Fakoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models are often trained to maximize the likelihood of the next
token given past tokens in the training dataset. However, during inference
time, they are utilized differently, generating text sequentially and
auto-regressively by using previously generated tokens as input to predict the
next one. Marginal differences in predictions at each step can cascade over
successive steps, resulting in different distributions from what the models
were trained for and potentially leading to unpredictable behavior. This paper
proposes two simple approaches based on model own generation to address this
discrepancy between the training and inference time. Our first approach is
Batch-Scheduled Sampling, where, during training, we stochastically choose
between the ground-truth token from the dataset and the model's own generated
token as input to predict the next token. This is done in an offline manner,
modifying the context window by interleaving ground-truth tokens with those
generated by the model. Our second approach is Reference-Answer-based
Correction, where we explicitly incorporate a self-correction capability into
the model during training. This enables the model to effectively self-correct
the gaps between the generated sequences and the ground truth data without
relying on an external oracle model. By incorporating our proposed strategies
during training, we have observed an overall improvement in performance
compared to baseline methods, as demonstrated by our extensive experiments
using summarization, general question-answering, and math question-answering
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Sieberling, Denis Kuznedelev, Eldar Kurtic, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high computational costs of large language models (LLMs) have led to a
flurry of research on LLM compression, via methods such as quantization,
sparsification, or structured pruning. A new frontier in this area is given by
\emph{dynamic, non-uniform} compression methods, which adjust the compression
levels (e.g., sparsity) per-block or even per-layer in order to minimize
accuracy loss, while guaranteeing a global compression threshold. Yet, current
methods rely on heuristics for identifying the "importance" of a given layer
towards the loss, based on assumptions such as \emph{error monotonicity}, i.e.
that the end-to-end model compression error is proportional to the sum of
layer-wise errors. In this paper, we revisit this area, and propose a new and
general approach for dynamic compression that is provably optimal in a given
input range. We begin from the motivating observation that, in general,
\emph{error monotonicity does not hold for LLMs}: compressed models with lower
sum of per-layer errors can perform \emph{worse} than models with higher error
sums. To address this, we propose a new general evolutionary framework for
dynamic LLM compression called EvoPress, which has provable convergence, and
low sample and evaluation complexity. We show that these theoretical guarantees
lead to highly competitive practical performance for dynamic compression of
Llama, Mistral and Phi models. Via EvoPress, we set new state-of-the-art
results across all compression approaches: structural pruning (block/layer
dropping), unstructured sparsity, as well as quantization with dynamic
bitwidths. Our code is available at https://github.com/IST-DASLab/EvoPress.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HR-Bandit: Human-AI Collaborated Linear Recourse Bandit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Cao, Ruijiang Gao, Esmaeil Keyvanshokooh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human doctors frequently recommend actionable recourses that allow patients
to modify their conditions to access more effective treatments. Inspired by
such healthcare scenarios, we propose the Recourse Linear UCB
($\textsf{RLinUCB}$) algorithm, which optimizes both action selection and
feature modifications by balancing exploration and exploitation. We further
extend this to the Human-AI Linear Recourse Bandit ($\textsf{HR-Bandit}$),
which integrates human expertise to enhance performance. $\textsf{HR-Bandit}$
offers three key guarantees: (i) a warm-start guarantee for improved initial
performance, (ii) a human-effort guarantee to minimize required human
interactions, and (iii) a robustness guarantee that ensures sublinear regret
even when human decisions are suboptimal. Empirical results, including a
healthcare case study, validate its superior performance against existing
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergence of Manifold Filter-Combine Networks <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David R. Johnson, Joyce Chew, Siddharth Viswanath, Edward De Brouwer, Deanna Needell, Smita Krishnaswamy, Michael Perlmutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to better understand manifold neural networks (MNNs), we introduce
Manifold Filter-Combine Networks (MFCNs). The filter-combine framework
parallels the popular aggregate-combine paradigm for graph neural networks
(GNNs) and naturally suggests many interesting families of MNNs which can be
interpreted as the manifold analog of various popular GNNs. We then propose a
method for implementing MFCNs on high-dimensional point clouds that relies on
approximating the manifold by a sparse graph. We prove that our method is
consistent in the sense that it converges to a continuum limit as the number of
data points tends to infinity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS Workshop on Symmetry and Geometry in Neural
  Representations (Extended Abstract Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallel Backpropagation for Inverse of a Convolution with Application
  to Normalizing Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandeep Nagar, Girish Varma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse of an invertible convolution is an important operation that comes up
in Normalizing Flows, Image Deblurring, etc. The naive algorithm for
backpropagation of this operation using Gaussian elimination has running time
$O(n^3)$ where $n$ is the number of pixels in the image. We give a fast
parallel backpropagation algorithm with running time $O(\sqrt{n})$ for a square
image and provide a GPU implementation of the same. Inverse Convolutions are
usually used in Normalizing Flows in the sampling pass, making them slow. We
propose to use Inverse Convolutions in the forward (image to latent vector)
pass of the Normalizing flow. Since the sampling pass is the inverse of the
forward pass, it will use convolutions only, resulting in efficient sampling
times. We use our parallel backpropagation algorithm for optimizing the inverse
convolution layer resulting in fast training times also. We implement this
approach in various Normalizing Flow backbones, resulting in our Inverse-Flow
models. We benchmark Inverse-Flow on standard datasets and show significantly
improved sampling times with similar bits per dimension compared to previous
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Regularization of Learnable Embeddings for Time Series Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Butera, Giovanni De Felice, Andrea Cini, Cesare Alippi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In processing multiple time series, accounting for the individual features of
each sequence can be challenging. To address this, modern deep learning methods
for time series analysis combine a shared (global) model with local layers,
specific to each time series, often implemented as learnable embeddings.
Ideally, these local embeddings should encode meaningful representations of the
unique dynamics of each sequence. However, when these are learned end-to-end as
parameters of a forecasting model, they may end up acting as mere sequence
identifiers. Shared processing blocks may then become reliant on such
identifiers, limiting their transferability to new contexts. In this paper, we
address this issue by investigating methods to regularize the learning of local
learnable embeddings for time series processing. Specifically, we perform the
first extensive empirical study on the subject and show how such
regularizations consistently improve performance in widely adopted
architectures. Furthermore, we show that methods preventing the co-adaptation
of local and global parameters are particularly effective in this context. This
hypothesis is validated by comparing several methods preventing the downstream
models from relying on sequence identifiers, going as far as completely
resetting the embeddings during training. The obtained results provide an
important contribution to understanding the interplay between learnable local
parameters and shared processing layers: a key challenge in modern time series
processing models and a step toward developing effective foundation models for
time series.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIMformer: Single-Layer Vanilla Transformer Can Learn Free-Space
  Trajectory Similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuang Yang, Renhe Jiang, Xiaohang Xu, Chuan Xiao, Kaoru Sezaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Free-space trajectory similarity calculation, e.g., DTW, Hausdorff, and
Frechet, often incur quadratic time complexity, thus learning-based methods
have been proposed to accelerate the computation. The core idea is to train an
encoder to transform trajectories into representation vectors and then compute
vector similarity to approximate the ground truth. However, existing methods
face dual challenges of effectiveness and efficiency: 1) they all utilize
Euclidean distance to compute representation similarity, which leads to the
severe curse of dimensionality issue -- reducing the distinguishability among
representations and significantly affecting the accuracy of subsequent
similarity search tasks; 2) most of them are trained in triplets manner and
often necessitate additional information which downgrades the efficiency; 3)
previous studies, while emphasizing the scalability in terms of efficiency,
overlooked the deterioration of effectiveness when the dataset size grows. To
cope with these issues, we propose a simple, yet accurate, fast, scalable model
that only uses a single-layer vanilla transformer encoder as the feature
extractor and employs tailored representation similarity functions to
approximate various ground truth similarity measures. Extensive experiments
demonstrate our model significantly mitigates the curse of dimensionality issue
and outperforms the state-of-the-arts in effectiveness, efficiency, and
scalability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing AI Accessibility in Veterinary Medicine: Linking Classifiers
  and Electronic Health Records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chun Yin Kong, Picasso Vasquez, Makan Farhoodimoghadam, Chris Brandt, Titus C. Brown, Krystle L. Reagan, Allison Zwingenberger, Stefan M. Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving landscape of veterinary healthcare, integrating
machine learning (ML) clinical decision-making tools with electronic health
records (EHRs) promises to improve diagnostic accuracy and patient care.
However, the seamless integration of ML classifiers into existing EHRs in
veterinary medicine is frequently hindered by the rigidity of EHR systems or
the limited availability of IT resources. To address this shortcoming, we
present Anna, a freely-available software solution that provides ML classifier
results for EHR laboratory data in real-time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ syren-new: Precise formulae for the linear and nonlinear matter power
  spectra with massive neutrinos and dynamical dark energy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ce Sui, Deaglan J. Bartlett, Shivam Pandey, Harry Desmond, Pedro G. Ferreira, Benjamin D. Wandelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current and future large scale structure surveys aim to constrain the
neutrino mass and the equation of state of dark energy. We aim to construct
accurate and interpretable symbolic approximations to the linear and nonlinear
matter power spectra as a function of cosmological parameters in extended
$\Lambda$CDM models which contain massive neutrinos and non-constant equations
of state for dark energy. This constitutes an extension of the syren-halofit
emulators to incorporate these two effects, which we call syren-new
(SYmbolic-Regression-ENhanced power spectrum emulator with NEutrinos and
$W_0-w_a$). We also obtain a simple approximation to the derived parameter
$\sigma_8$ as a function of the cosmological parameters for these models. Our
results for the linear power spectrum are designed to emulate CLASS, whereas
for the nonlinear case we aim to match the results of EuclidEmulator2. We
compare our results to existing emulators and $N$-body simulations. Our
analytic emulators for $\sigma_8$, the linear and nonlinear power spectra
achieve root mean squared errors of 0.1%, 0.3% and 1.3%, respectively, across a
wide range of cosmological parameters, redshifts and wavenumbers. We verify
that emulator-related discrepancies are subdominant compared to observational
errors and other modelling uncertainties when computing shear power spectra for
LSST-like surveys. Our expressions have similar accuracy to existing
(numerical) emulators, but are at least an order of magnitude faster, both on a
CPU and GPU. Our work greatly improves the accuracy, speed and range of
applicability of current symbolic approximations to the linear and nonlinear
matter power spectra. We provide publicly available code for all symbolic
approximations found.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JAMUN: Transferable Molecular Conformational Ensemble Generation with
  Walk-Jump Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ameya Daigavane, Bodhi P. Vani, Saeed Saremi, Joseph Kleinhenz, Joshua Rackers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conformational ensembles of protein structures are immensely important both
to understanding protein function, and for drug discovery in novel modalities
such as cryptic pockets. Current techniques for sampling ensembles are
computationally inefficient, or do not transfer to systems outside their
training data. We present walk-Jump Accelerated Molecular ensembles with
Universal Noise (JAMUN), a step towards the goal of efficiently sampling the
Boltzmann distribution of arbitrary proteins. By extending Walk-Jump Sampling
to point clouds, JAMUN enables ensemble generation at orders of magnitude
faster rates than traditional molecular dynamics or state-of-the-art ML
methods. Further, JAMUN is able to predict the stable basins of small peptides
that were not seen during training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Deep Reinforcement Learning for Navigation in Denied Sensor
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariusz Wisniewski, Paraskevas Chatzithanos, Weisi Guo, Antonios Tsourdos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement learning (DRL) is used to enable autonomous navigation in
unknown environments. Most research assume perfect sensor data, but real-world
environments may contain natural and artificial sensor noise and denial. Here,
we present a benchmark of both well-used and emerging DRL algorithms in a
navigation task with configurable sensor denial effects. In particular, we are
interested in comparing how different DRL methods (e.g. model-free PPO vs.
model-based DreamerV3) are affected by sensor denial. We show that DreamerV3
outperforms other methods in the visual end-to-end navigation task with a
dynamic goal - and other methods are not able to learn this. Furthermore,
DreamerV3 generally outperforms other methods in sensor-denied environments. In
order to improve robustness, we use adversarial training and demonstrate an
improved performance in denied environments, although this generally comes with
a performance cost on the vanilla environments. We anticipate this benchmark of
different DRL methods and the usage of adversarial training to be a starting
point for the development of more elaborate navigation strategies that are
capable of dealing with uncertain and denied sensor readings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 19 figures. For associated code, see
  https://github.com/mazqtpopx/cranfield-navigation-gym</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asymptotically Optimal Change Detection for Unnormalized Pre- and
  Post-Change Distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arman Adibi, Sanjeev Kulkarni, H. Vincent Poor, Taposh Banerjee, Vahid Tarokh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of detecting changes when only unnormalized
pre- and post-change distributions are accessible. This situation happens in
many scenarios in physics such as in ferromagnetism, crystallography,
magneto-hydrodynamics, and thermodynamics, where the energy models are
difficult to normalize.
  Our approach is based on the estimation of the Cumulative Sum (CUSUM)
statistics, which is known to produce optimal performance. We first present an
intuitively appealing approximation method. Unfortunately, this produces a
biased estimator of the CUSUM statistics and may cause performance degradation.
We then propose the Log-Partition Approximation Cumulative Sum (LPA-CUSUM)
algorithm based on thermodynamic integration (TI) in order to estimate the
log-ratio of normalizing constants of pre- and post-change distributions. It is
proved that this approach gives an unbiased estimate of the log-partition
function and the CUSUM statistics, and leads to an asymptotically optimal
performance. Moreover, we derive a relationship between the required sample
size for thermodynamic integration and the desired detection delay performance,
offering guidelines for practical parameter selection. Numerical studies are
provided demonstrating the efficacy of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Streaming Deep Reinforcement Learning Finally Works 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Elsayed, Gautham Vasan, A. Rupam Mahmood
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural intelligence processes experience as a continuous stream, sensing,
acting, and learning moment-by-moment in real time. Streaming learning, the
modus operandi of classic reinforcement learning (RL) algorithms like
Q-learning and TD, mimics natural learning by using the most recent sample
without storing it. This approach is also ideal for resource-constrained,
communication-limited, and privacy-sensitive applications. However, in deep RL,
learners almost always use batch updates and replay buffers, making them
computationally expensive and incompatible with streaming learning. Although
the prevalence of batch deep RL is often attributed to its sample efficiency, a
more critical reason for the absence of streaming deep RL is its frequent
instability and failure to learn, which we refer to as stream barrier. This
paper introduces the stream-x algorithms, the first class of deep RL algorithms
to overcome stream barrier for both prediction and control and match sample
efficiency of batch RL. Through experiments in Mujoco Gym, DM Control Suite,
and Atari Games, we demonstrate stream barrier in existing algorithms and
successful stable learning with our stream-x algorithms: stream Q, stream AC,
and stream TD, achieving the best model-free performance in DM Control Dog
environments. A set of common techniques underlies the stream-x algorithms,
enabling their success with a single set of hyperparameters and allowing for
easy extension to other algorithms, thereby reviving streaming RL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Control the Smoothness of Graph Convolutional Network
  Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shih-Hsin Wang, Justin Baker, Cory Hauck, Bao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pioneering work of Oono and Suzuki [ICLR, 2020] and Cai and Wang
[arXiv:2006.13318] initializes the analysis of the smoothness of graph
convolutional network (GCN) features. Their results reveal an intricate
empirical correlation between node classification accuracy and the ratio of
smooth to non-smooth feature components. However, the optimal ratio that favors
node classification is unknown, and the non-smooth features of deep GCN with
ReLU or leaky ReLU activation function diminish. In this paper, we propose a
new strategy to let GCN learn node features with a desired smoothness --
adapting to data and tasks -- to enhance node classification. Our approach has
three key steps: (1) We establish a geometric relationship between the input
and output of ReLU or leaky ReLU. (2) Building on our geometric insights, we
augment the message-passing process of graph convolutional layers (GCLs) with a
learnable term to modulate the smoothness of node features with computational
efficiency. (3) We investigate the achievable ratio between smooth and
non-smooth feature components for GCNs with the augmented message-passing
scheme. Our extensive numerical results show that the augmented message-passing
schemes significantly improve node classification for GCN and some related
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Does Data Diversity Shape the Weight Landscape of Neural Networks? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Ba, Michelle V. Mancenido, Rong Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To enhance the generalization of machine learning models to unseen data,
techniques such as dropout, weight decay ($L_2$ regularization), and noise
augmentation are commonly employed. While regularization methods (i.e., dropout
and weight decay) are geared toward adjusting model parameters to prevent
overfitting, data augmentation increases the diversity of the input training
set, a method purported to improve accuracy and calibration error. In this
paper, we investigate the impact of each of these techniques on the parameter
space of neural networks, with the goal of understanding how they alter the
weight landscape in transfer learning scenarios. To accomplish this, we employ
Random Matrix Theory to analyze the eigenvalue distributions of pre-trained
models, fine-tuned using these techniques but using different levels of data
diversity, for the same downstream tasks. We observe that diverse data
influences the weight landscape in a similar fashion as dropout. Additionally,
we compare commonly used data augmentation methods with synthetic data created
by generative models. We conclude that synthetic data can bring more diversity
into real input data, resulting in a better performance on out-of-distribution
test instances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contractivity and linear convergence in bilinear saddle-point problems:
  An operator-theoretic approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Colin Dirren, Mattia Bianchi, Panagiotis D. Grontas, John Lygeros, Florian Dörfler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the convex-concave bilinear saddle-point problem $\min_x \max_y f(x)
+ y^\top Ax - g(y)$, where both, only one, or none of the functions $f$ and $g$
are strongly convex, and suitable rank conditions on the matrix $A$ hold. The
solution of this problem is at the core of many machine learning tasks. By
employing tools from operator theory, we systematically prove the contractivity
(in turn, the linear convergence) of several first-order primal-dual
algorithms, including the Chambolle-Pock method. Our approach results in
concise and elegant proofs, and it yields new convergence guarantees and
tighter bounds compared to known results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Lipschitz spaces view of infinitely wide shallow neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesca Bartolucci, Marcello Carioni, José A. Iglesias, Yury Korolev, Emanuele Naldi, Stefano Vigogna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisit the mean field parametrization of shallow neural networks, using
signed measures on unbounded parameter spaces and duality pairings that take
into account the regularity and growth of activation functions. This setting
directly leads to the use of unbalanced Kantorovich-Rubinstein norms defined by
duality with Lipschitz functions, and of spaces of measures dual to those of
continuous functions with controlled growth. These allow to make transparent
the need for total variation and moment bounds or penalization to obtain
existence of minimizers of variational formulations, under which we prove a
compactness result in strong Kantorovich-Rubinstein norm, and in the absence of
which we show several examples demonstrating undesirable behavior. Further, the
Kantorovich-Rubinstein setting enables us to combine the advantages of a
completely linear parametrization and ensuing reproducing kernel Banach space
framework with optimal transport insights. We showcase this synergy with
representer theorems and uniform large data limits for empirical risk
minimization, and in proposed formulations for distillation and fusion
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning With Multi-Group Guarantees For Clusterable Subpopulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jessica Dai, Nika Haghtalab, Eric Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A canonical desideratum for prediction problems is that performance
guarantees should hold not just on average over the population, but also for
meaningful subpopulations within the overall population. But what constitutes a
meaningful subpopulation? In this work, we take the perspective that relevant
subpopulations should be defined with respect to the clusters that naturally
emerge from the distribution of individuals for which predictions are being
made. In this view, a population refers to a mixture model whose components
constitute the relevant subpopulations. We suggest two formalisms for capturing
per-subgroup guarantees: first, by attributing each individual to the component
from which they were most likely drawn, given their features; and second, by
attributing each individual to all components in proportion to their relative
likelihood of having been drawn from each component. Using online calibration
as a case study, we study a \variational algorithm that provides guarantees for
each of these formalisms by handling all plausible underlying subpopulation
structures simultaneously, and achieve an $O(T^{1/2})$ rate even when the
subpopulations are not well-separated. In comparison, the more natural
cluster-then-predict approach that first recovers the structure of the
subpopulations and then makes predictions suffers from a $O(T^{2/3})$ rate and
requires the subpopulations to be separable. Along the way, we prove that
providing per-subgroup calibration guarantees for underlying clusters can be
easier than learning the clusters: separation between median subgroup features
is required for the latter but not the former.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuro-Symbolic Traders: Assessing the Wisdom of AI Crowds in Markets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Namid R. Stillman, Rory Baggott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models are becoming increasingly used as tools for financial
analysis. However, it is unclear how these models will influence financial
markets, especially when they infer financial value in a semi-autonomous way.
In this work, we explore the interplay between deep generative models and
market dynamics. We develop a form of virtual traders that use deep generative
models to make buy/sell decisions, which we term neuro-symbolic traders, and
expose them to a virtual market. Under our framework, neuro-symbolic traders
are agents that use vision-language models to discover a model of the
fundamental value of an asset. Agents develop this model as a stochastic
differential equation, calibrated to market data using gradient descent. We
test our neuro-symbolic traders on both synthetic data and real financial time
series, including an equity stock, commodity, and a foreign exchange pair. We
then expose several groups of neuro-symbolic traders to a virtual market
environment. This market environment allows for feedback between the traders
belief of the underlying value to the observed price dynamics. We find that
this leads to price suppression compared to the historical data, highlighting a
future risk to market stability. Our work is a first step towards quantifying
the effect of deep generative agents on markets dynamics and sets out some of
the potential risks and benefits of this approach in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, ACM format</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Combinatorial Clustered Bandits for Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baran Atalar, Carlee Joe-Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the contextual combinatorial bandit setting where in each round,
the learning agent, e.g., a recommender system, selects a subset of "arms,"
e.g., products, and observes rewards for both the individual base arms, which
are a function of known features (called "context"), and the super arm (the
subset of arms), which is a function of the base arm rewards. The agent's goal
is to simultaneously learn the unknown reward functions and choose the
highest-reward arms. For example, the "reward" may represent a user's
probability of clicking on one of the recommended products. Conventional bandit
models, however, employ restrictive reward function models in order to obtain
performance guarantees. We make use of deep neural networks to estimate and
learn the unknown reward functions and propose Neural UCB Clustering
(NeUClust), which adopts a clustering approach to select the super arm in every
round by exploiting underlying structure in the context space. Unlike prior
neural bandit works, NeUClust uses a neural network to estimate the super arm
reward and select the super arm, thus eliminating the need for a known
optimization oracle. We non-trivially extend prior neural combinatorial bandit
works to prove that NeUClust achieves
$\widetilde{O}\left(\widetilde{d}\sqrt{T}\right)$ regret, where $\widetilde{d}$
is the effective dimension of a neural tangent kernel matrix, $T$ the number of
rounds. Experiments on real world recommendation datasets show that NeUClust
achieves better regret and reward than other contextual combinatorial and
neural bandit algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Attention with Mirror Descent: Generalized Max-Margin Token
  Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Alvarado Kristanto Julistiono, Davoud Ataee Tarzanagh, Navid Azizan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention mechanisms have revolutionized several domains of artificial
intelligence, such as natural language processing and computer vision, by
enabling models to selectively focus on relevant parts of the input data. While
recent work has characterized the optimization dynamics of gradient descent
(GD) in attention-based models and the structural properties of its preferred
solutions, less is known about more general optimization algorithms such as
mirror descent (MD). In this paper, we investigate the convergence properties
and implicit biases of a family of MD algorithms tailored for softmax attention
mechanisms, with the potential function chosen as the $p$-th power of the
$\ell_p$-norm. Specifically, we show that these algorithms converge in
direction to a generalized hard-margin SVM with an $\ell_p$-norm objective when
applied to a classification problem using a softmax attention model. Notably,
our theoretical results reveal that the convergence rate is comparable to that
of traditional GD in simpler models, despite the highly nonlinear and nonconvex
nature of the present problem. Additionally, we delve into the joint
optimization dynamics of the key-query matrix and the decoder, establishing
conditions under which this complex joint optimization converges to their
respective hard-margin SVM solutions. Lastly, our numerical experiments on real
data demonstrate that MD algorithms improve generalization over standard GD and
excel in optimal token selection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Unsupervised Validation of Anomaly-Detection Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lihi Idan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised validation of anomaly-detection models is a highly challenging
task. While the common practices for model validation involve a labeled
validation set, such validation sets cannot be constructed when the underlying
datasets are unlabeled. The lack of robust and efficient unsupervised
model-validation techniques presents an acute challenge in the implementation
of automated anomaly-detection pipelines, especially when there exists no prior
knowledge of the model's performance on similar datasets. This work presents a
new paradigm to automated validation of anomaly-detection models, inspired by
real-world, collaborative decision-making mechanisms. We focus on two
commonly-used, unsupervised model-validation tasks -- model selection and model
evaluation -- and provide extensive experimental results that demonstrate the
accuracy and robustness of our approach on both tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Are Overparameterized Text Encoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thennal D K, Tim Fischer, Chris Biemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate strong performance as text embedding
models when finetuned with supervised contrastive training. However, their
large size balloons inference time and memory requirements. In this paper, we
show that by pruning the last $p\%$ layers of an LLM before supervised training
for only 1000 steps, we can achieve a proportional reduction in memory and
inference time. We evaluate four different state-of-the-art LLMs on text
embedding tasks and find that our method can prune up to 30\% of layers with
negligible impact on performance and up to 80\% with only a modest drop. With
only three lines of code, our method is easily implemented in any pipeline for
transforming LLMs to text encoders. We also propose $\text{L}^3 \text{Prune}$,
a novel layer-pruning strategy based on the model's initial loss that provides
two optimal pruning configurations: a large variant with negligible performance
loss and a small variant for resource-constrained settings. On average, the
large variant prunes 21\% of the parameters with a $-0.3$ performance drop, and
the small variant only suffers from a $-5.1$ decrease while pruning 74\% of the
model. We consider these results strong evidence that LLMs are
overparameterized for text embedding tasks, and can be easily pruned.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages of content + 1 for limitations and ethical considerations, 14
  pages in total including references and appendix, 5+1 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachel S. Y. Teo, Tan M. Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled
scalability in deep learning. SMoE has the potential to exponentially increase
parameter count while maintaining the efficiency of the model by only
activating a small subset of these parameters for a given sample. However, it
has been observed that SMoE suffers from unstable training and has difficulty
adapting to new distributions, leading to the model's lack of robustness to
data contamination. To overcome these limitations, we first establish a
connection between the dynamics of the expert representations in SMoEs and
gradient descent on a multi-objective optimization problem. Leveraging our
framework, we then integrate momentum into SMoE and propose a new family of
SMoEs named MomentumSMoE. We theoretically prove and numerically demonstrate
that MomentumSMoE is more stable and robust than SMoE. In particular, we verify
the advantages of MomentumSMoE over SMoE on a variety of practical tasks
including ImageNet-1K object recognition and WikiText-103 language modeling. We
demonstrate the applicability of MomentumSMoE to many types of SMoE models,
including those in the Sparse MoE model for vision (V-MoE) and the Generalist
Language Model (GLaM). We also show that other advanced momentum-based
optimization methods, such as Adam, can be easily incorporated into the
MomentumSMoE framework for designing new SMoE models with even better
performance, almost negligible additional computation cost, and simple
implementations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages in the main text. Published at NeurIPS 2024. The code is
  available at https://github.com/rachtsy/MomentumSMoE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Trust in Black-box Optimization: A Comprehensive Framework for
  Explainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nazanin Nezami, Hadis Anahideh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimizing costly black-box functions within a constrained evaluation budget
presents significant challenges in many real-world applications. Surrogate
Optimization (SO) is a common resolution, yet its proprietary nature introduced
by the complexity of surrogate models and the sampling core (e.g., acquisition
functions) often leads to a lack of explainability and transparency. While
existing literature has primarily concentrated on enhancing convergence to
global optima, the practical interpretation of newly proposed strategies
remains underexplored, especially in batch evaluation settings. In this paper,
we propose \emph{Inclusive} Explainability Metrics for Surrogate Optimization
(IEMSO), a comprehensive set of model-agnostic metrics designed to enhance the
transparency, trustworthiness, and explainability of the SO approaches. Through
these metrics, we provide both intermediate and post-hoc explanations to
practitioners before and after performing expensive evaluations to gain trust.
We consider four primary categories of metrics, each targeting a specific
aspect of the SO process: Sampling Core Metrics, Batch Properties Metrics,
Optimization Process Metrics, and Feature Importance. Our experimental
evaluations demonstrate the significant potential of the proposed metrics
across different benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the difficulty of low-precision post-training quantization
  of large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifei Xu, Sayeh Sharify, Wanzin Yazar, Tristan Webb, Xin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models of high parameter counts are computationally expensive,
yet can be made much more efficient by compressing their weights to very low
numerical precision. This can be achieved either through post-training
quantization by minimizing local, layer-wise quantization errors, or through
quantization-aware fine-tuning by minimizing the global loss function. In this
study, we discovered that, under the same data constraint, the former approach
nearly always fared worse than the latter, a phenomenon particularly prominent
when the numerical precision is very low. We further showed that this
difficulty of post-training quantization arose from stark misalignment between
optimization of the local and global objective functions. Our findings explains
limited utility in minimization of local quantization error and the importance
of direct quantization-aware fine-tuning, in the regime of large models at very
low precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring Diversity: Axioms and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikhail Mironov, Liudmila Prokhorenkova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The concept of diversity is widely used in various applications: from image
or molecule generation to recommender systems. Thus, being able to properly
measure diversity is important. This paper addresses the problem of quantifying
diversity for a set of objects. First, we make a systematic review of existing
diversity measures and explore their undesirable behavior in some cases. Based
on this review, we formulate three desirable properties (axioms) of a reliable
diversity measure: monotonicity, uniqueness, and continuity. We show that none
of the existing measures has all three properties and thus these measures are
not suitable for quantifying diversity. Then, we construct two examples of
measures that have all the desirable properties, thus proving that the list of
axioms is not self-contradicting. Unfortunately, the constructed examples are
too computationally complex for practical use, thus we pose an open problem of
constructing a diversity measure that has all the listed properties and can be
computed in practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting K-means for Big Data by Fusing Data Streaming with Global
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ravil Mussabayev, Rustam Mussabayev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  K-means clustering is a cornerstone of data mining, but its efficiency
deteriorates when confronted with massive datasets. To address this limitation,
we propose a novel heuristic algorithm that leverages the Variable Neighborhood
Search (VNS) metaheuristic to optimize K-means clustering for big data. Our
approach is based on the sequential optimization of the partial objective
function landscapes obtained by restricting the Minimum Sum-of-Squares
Clustering (MSSC) formulation to random samples from the original big dataset.
Within each landscape, systematically expanding neighborhoods of the currently
best (incumbent) solution are explored by reinitializing all degenerate and a
varying number of additional centroids. Extensive and rigorous experimentation
on a large number of real-world datasets reveals that by transforming the
traditional local search into a global one, our algorithm significantly
enhances the accuracy and efficiency of K-means clustering in big data
environments, becoming the new state of the art in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion-based Semi-supervised Spectral Algorithm for Regression on
  Manifolds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weichun Xia, Jiaxin Jiang, Lei Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel diffusion-based spectral algorithm to tackle regression
analysis on high-dimensional data, particularly data embedded within
lower-dimensional manifolds. Traditional spectral algorithms often fall short
in such contexts, primarily due to the reliance on predetermined kernel
functions, which inadequately address the complex structures inherent in
manifold-based data. By employing graph Laplacian approximation, our method
uses the local estimation property of heat kernel, offering an adaptive,
data-driven approach to overcome this obstacle. Another distinct advantage of
our algorithm lies in its semi-supervised learning framework, enabling it to
fully use the additional unlabeled data. This ability enhances the performance
by allowing the algorithm to dig the spectrum and curvature of the data
manifold, providing a more comprehensive understanding of the dataset.
Moreover, our algorithm performs in an entirely data-driven manner, operating
directly within the intrinsic manifold structure of the data, without requiring
any predefined manifold information. We provide a convergence analysis of our
algorithm. Our findings reveal that the algorithm achieves a convergence rate
that depends solely on the intrinsic dimension of the underlying manifold,
thereby avoiding the curse of dimensionality associated with the higher ambient
dimension.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing Differentiable and Dynamic Ray Tracing: Introducing the
  Multipath Lifetime Map 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jérome Eertmans, Enrico Maria Vittuci, Vittorio Degli Esposti, Laurent Jacques, Claude Oestges
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing presence of dynamic scenarios, such as Vehicle-to-Vehicle
communications, radio propagation modeling tools must adapt to the rapidly
changing nature of the radio channel. Recently, both Differentiable and Dynamic
Ray Tracing frameworks have emerged to address these challenges. However, there
is often confusion about how these approaches differ and which one should be
used in specific contexts. In this paper, we provide an overview of these two
techniques and a comparative analysis against two state-of-the-art tools:
3DSCAT from UniBo and Sionna from NVIDIA. To provide a more precise
characterization of the scope of these methods, we introduce a novel
simulation-based metric, the Multipath Lifetime Map, which enables the
evaluation of spatial and temporal coherence in radio channels only based on
the geometrical description of the environment. Finally, our metrics are
evaluated on a classic urban street canyon scenario, yielding similar results
to those obtained from measurement campaigns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures, 1 table, submitted to EuCAP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Traveling Bandit: A Framework for Bayesian Optimization with
  Movement Costs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyuan Chen, Raed Al Kontar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a framework for Bayesian Optimization (BO) with metric
movement costs, addressing a critical challenge in practical applications where
input alterations incur varying costs. Our approach is a convenient plug-in
that seamlessly integrates with the existing literature on batched algorithms,
where designs within batches are observed following the solution of a Traveling
Salesman Problem. The proposed method provides a theoretical guarantee of
convergence in terms of movement costs for BO. Empirically, our method
effectively reduces average movement costs over time while maintaining
comparable regret performance to conventional BO methods. This framework also
shows promise for broader applications in various bandit settings with movement
costs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Sentiment and Technical Analysis to Predict Bitcoin with Machine
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Emanuel de Oliveira Carosia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cryptocurrencies have gained significant attention in recent years due to
their decentralized nature and potential for financial innovation. Thus, the
ability to accurately predict its price has become a subject of great interest
for investors, traders, and researchers. Some works in the literature show how
Bitcoin's market sentiment correlates with its price fluctuations in the
market. However, papers that consider the sentiment of the market associated
with financial Technical Analysis indicators in order to predict Bitcoin's
price are still scarce. In this paper, we present a novel approach for
predicting Bitcoin price movements by combining the Fear & Greedy Index, a
measure of market sentiment, Technical Analysis indicators, and the potential
of Machine Learning algorithms. This work represents a preliminary study on the
importance of sentiment metrics in cryptocurrency forecasting. Our initial
experiments demonstrate promising results considering investment returns,
surpassing the Buy & Hold baseline, and offering valuable insights about the
combination of indicators of sentiment and market in a cryptocurrency
prediction model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Adaptive Safety Filters via Deep Operator Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lakshmideepakreddy Manda, Shaoru Chen, Mahyar Fazlyab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based approaches for constructing Control Barrier Functions (CBFs)
are increasingly being explored for safety-critical control systems. However,
these methods typically require complete retraining when applied to unseen
environments, limiting their adaptability. To address this, we propose a
self-supervised deep operator learning framework that learns the mapping from
environmental parameters to the corresponding CBF, rather than learning the CBF
directly. Our approach leverages the residual of a parametric Partial
Differential Equation (PDE), where the solution defines a parametric CBF
approximating the maximal control invariant set. This framework accommodates
complex safety constraints, higher relative degrees, and actuation limits. We
demonstrate the effectiveness of the method through numerical experiments on
navigation tasks involving dynamic obstacles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>63rd IEEE Conference on Decision and Control (CDC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Distance Metrics for Counterfactual Explainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Nathaniel Williams, Anurag Katakkar, Hoda Heidari, J. Zico Kolter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual explanations have been a popular method of post-hoc
explainability for a variety of settings in Machine Learning. Such methods
focus on explaining classifiers by generating new data points that are similar
to a given reference, while receiving a more desirable prediction. In this
work, we investigate a framing for counterfactual generation methods that
considers counterfactuals not as independent draws from a region around the
reference, but as jointly sampled with the reference from the underlying data
distribution. Through this framing, we derive a distance metric, tailored for
counterfactual similarity that can be applied to a broad range of settings.
Through both quantitative and qualitative analyses of counterfactual generation
methods, we show that this framing allows us to express more nuanced
dependencies among the covariates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Annotator Reliability Assessment and Sample Weighting for
  Knowledge-Based Misinformation Detection on Social Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Owen Cook, Charlie Grimshaw, Ben Wu, Sophie Dillon, Jack Hicks, Luke Jones, Thomas Smith, Matyas Szert, Xingyi Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Misinformation spreads rapidly on social media, confusing the truth and
targetting potentially vulnerable people. To effectively mitigate the negative
impact of misinformation, it must first be accurately detected before applying
a mitigation strategy, such as X's community notes, which is currently a manual
process. This study takes a knowledge-based approach to misinformation
detection, modelling the problem similarly to one of natural language
inference. The EffiARA annotation framework is introduced, aiming to utilise
inter- and intra-annotator agreement to understand the reliability of each
annotator and influence the training of large language models for
classification based on annotator reliability. In assessing the EffiARA
annotation framework, the Russo-Ukrainian Conflict Knowledge-Based
Misinformation Classification Dataset (RUC-MCD) was developed and made publicly
available. This study finds that sample weighting using annotator reliability
performs the best, utilising both inter- and intra-annotator agreement and
soft-label training. The highest classification performance achieved using
Llama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, 3 tables. Code available here:
  https://github.com/MiniEggz/ruc-misinfo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Integrated Deep Learning Model for Skin Cancer Detection Using Hybrid
  Feature Fusion Technique 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksuda Akter, Rabea Khatun, Md. Alamin Talukder, Md. Manowarul Islam, Md. Ashraf Uddin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skin cancer is a serious and potentially fatal disease caused by DNA damage.
Early detection significantly increases survival rates, making accurate
diagnosis crucial. In this groundbreaking study, we present a hybrid framework
based on Deep Learning (DL) that achieves precise classification of benign and
malignant skin lesions. Our approach begins with dataset preprocessing to
enhance classification accuracy, followed by training two separate pre-trained
DL models, InceptionV3 and DenseNet121. By fusing the results of each model
using the weighted sum rule, our system achieves exceptional accuracy rates.
Specifically, we achieve a 92.27% detection accuracy rate, 92.33% sensitivity,
92.22% specificity, 90.81% precision, and 91.57% F1-score, outperforming
existing models and demonstrating the robustness and trustworthiness of our
hybrid approach. Our study represents a significant advance in skin cancer
diagnosis and provides a promising foundation for further research in the
field. With the potential to save countless lives through earlier detection,
our hybrid deep-learning approach is a game-changer in the fight against skin
cancer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ANT: Adaptive Noise Schedule for Time Series Diffusion Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seunghan Lee, Kibok Lee, Taeyoung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in diffusion models for generative artificial intelligence have
recently propagated to the time series (TS) domain, demonstrating
state-of-the-art performance on various tasks. However, prior works on TS
diffusion models often borrow the framework of existing works proposed in other
domains without considering the characteristics of TS data, leading to
suboptimal performance. In this work, we propose Adaptive Noise schedule for
Time series diffusion models (ANT), which automatically predetermines proper
noise schedules for given TS datasets based on their statistics representing
non-stationarity. Our intuition is that an optimal noise schedule should
satisfy the following desiderata: 1) It linearly reduces the non-stationarity
of TS data so that all diffusion steps are equally meaningful, 2) the data is
corrupted to the random noise at the final step, and 3) the number of steps is
sufficiently large. The proposed method is practical for use in that it
eliminates the necessity of finding the optimal noise schedule with a small
additional cost to compute the statistics for given datasets, which can be done
offline before training. We validate the effectiveness of our method across
various tasks, including TS forecasting, refinement, and generation, on
datasets from diverse domains. Code is available at this repository:
https://github.com/seunghan96/ANT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers and
  Fully-Connected Neural Networks for Causally Constrained Predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew J. Vowels, Mathieu Rochat, Sina Akbari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Neural Networks (ANNs), including fully-connected networks and
transformers, are highly flexible and powerful function approximators, widely
applied in fields like computer vision and natural language processing.
However, their inability to inherently respect causal structures can limit
their robustness, making them vulnerable to covariate shift and difficult to
interpret/explain. This poses significant challenges for their reliability in
real-world applications. In this paper, we introduce Causal Fully-Connected
Neural Networks (CFCNs) and Causal Transformers (CaTs), two general model
families designed to operate under predefined causal constraints, as specified
by a Directed Acyclic Graph (DAG). These models retain the powerful function
approximation abilities of traditional neural networks while adhering to the
underlying structural constraints, improving robustness, reliability, and
interpretability at inference time. This approach opens new avenues for
deploying neural networks in more demanding, real-world scenarios where
robustness and explainability is critical.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transfer Reinforcement Learning in Heterogeneous Action Spaces using
  Subgoal Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kavinayan P. Sivakumar, Yan Zhang, Zachary Bell, Scott Nivison, Michael M. Zavlanos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider a transfer reinforcement learning problem
involving agents with different action spaces. Specifically, for any new unseen
task, the goal is to use a successful demonstration of this task by an expert
agent in its action space to enable a learner agent learn an optimal policy in
its own different action space with fewer samples than those required if the
learner was learning on its own. Existing transfer learning methods across
different action spaces either require handcrafted mappings between those
action spaces provided by human experts, which can induce bias in the learning
procedure, or require the expert agent to share its policy parameters with the
learner agent, which does not generalize well to unseen tasks. In this work, we
propose a method that learns a subgoal mapping between the expert agent policy
and the learner agent policy. Since the expert agent and the learner agent have
different action spaces, their optimal policies can have different subgoal
trajectories. We learn this subgoal mapping by training a Long Short Term
Memory (LSTM) network for a distribution of tasks and then use this mapping to
predict the learner subgoal sequence for unseen tasks, thereby improving the
speed of learning by biasing the agent's policy towards the predicted learner
subgoal sequence. Through numerical experiments, we demonstrate that the
proposed learning scheme can effectively find the subgoal mapping underlying
the given distribution of tasks. Moreover, letting the learner agent imitate
the expert agent's policy with the learnt subgoal mapping can significantly
improve the sample efficiency and training time of the learner agent in unseen
new tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spectral Representations for Accurate Causal Uncertainty Quantification
  with Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugh Dance, Peter Orbanz, Arthur Gretton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate uncertainty quantification for causal effects is essential for
robust decision making in complex systems, but remains challenging in
non-parametric settings. One promising framework represents conditional
distributions in a reproducing kernel Hilbert space and places Gaussian process
priors on them to infer posteriors on causal effects, but requires restrictive
nuclear dominant kernels and approximations that lead to unreliable uncertainty
estimates. In this work, we introduce a method, IMPspec, that addresses these
limitations via a spectral representation of the Hilbert space. We show that
posteriors in this model can be obtained explicitly, by extending a result in
Hilbert space regression theory. We also learn the spectral representation to
optimise posterior calibration. Our method achieves state-of-the-art
performance in uncertainty quantification and causal Bayesian optimisation
across simulations and a healthcare application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Backdoored Retrievers for <span class="highlight-title">Prompt</span> Injection Attacks on Retrieval
  Augmented Generation of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cody Clop, Yannick Teglia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities in
generating coherent text but remain limited by the static nature of their
training data. Retrieval Augmented Generation (RAG) addresses this issue by
combining LLMs with up-to-date information retrieval, but also expand the
attack surface of the system. This paper investigates prompt injection attacks
on RAG, focusing on malicious objectives beyond misinformation, such as
inserting harmful links, promoting unauthorized services, and initiating
denial-of-service behaviors. We build upon existing corpus poisoning techniques
and propose a novel backdoor attack aimed at the fine-tuning process of the
dense retriever component. Our experiments reveal that corpus poisoning can
achieve significant attack success rates through the injection of a small
number of compromised documents into the retriever corpus. In contrast,
backdoor attacks demonstrate even higher success rates but necessitate a more
complex setup, as the victim must fine-tune the retriever using the attacker
poisoned dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Laplace Transform Based Low-Complexity Learning of Continuous Markov
  Semigroups 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimir R. Kostic, Karim Lounici, Hélène Halconruy, Timothée Devergne, Pietro Novelli, Massimiliano Pontil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Markov processes serve as a universal model for many real-world random
processes. This paper presents a data-driven approach for learning these models
through the spectral decomposition of the infinitesimal generator (IG) of the
Markov semigroup. The unbounded nature of IGs complicates traditional methods
such as vector-valued regression and Hilbert-Schmidt operator analysis.
Existing techniques, including physics-informed kernel regression, are
computationally expensive and limited in scope, with no recovery guarantees for
transfer operator methods when the time-lag is small. We propose a novel method
that leverages the IG's resolvent, characterized by the Laplace transform of
transfer operators. This approach is robust to time-lag variations, ensuring
accurate eigenvalue learning even for small time-lags. Our statistical analysis
applies to a broader class of Markov processes than current methods while
reducing computational complexity from quadratic to linear in the state
dimension. Finally, we illustrate the behaviour of our method in two
experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Cryptocurrency Market Forecasting: Advanced Machine Learning
  Techniques and Industrial Engineering Contributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jannatun Nayeem Pinky, Ramya Akula
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cryptocurrencies, as decentralized digital assets, have experienced rapid
growth and adoption, with over 23,000 cryptocurrencies and a market
capitalization nearing \$1.1 trillion (about \$3,400 per person in the US) as
of 2023. This dynamic market presents significant opportunities and risks,
highlighting the need for accurate price prediction models to manage
volatility. This chapter comprehensively reviews machine learning (ML)
techniques applied to cryptocurrency price prediction from 2014 to 2024. We
explore various ML algorithms, including linear models, tree-based approaches,
and advanced deep learning architectures such as transformers and large
language models. Additionally, we examine the role of sentiment analysis in
capturing market sentiment from textual data like social media posts and news
articles to anticipate price fluctuations. With expertise in optimizing complex
systems and processes, industrial engineers are pivotal in enhancing these
models. They contribute by applying principles of process optimization,
efficiency, and risk mitigation to improve computational performance and data
management. This chapter highlights the evolving landscape of cryptocurrency
price prediction, the integration of emerging technologies, and the significant
role of industrial engineers in refining predictive models. By addressing
current limitations and exploring future research directions, this chapter aims
to advance the development of more accurate and robust prediction systems,
supporting better-informed investment decisions and more stable market
behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>63 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Do Training Methods Influence the Utilization of Vision Models? <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Gavrikov, Shashank Agnihotri, Margret Keuper, Janis Keuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Not all learnable parameters (e.g., weights) contribute equally to a neural
network's decision function. In fact, entire layers' parameters can sometimes
be reset to random values with little to no impact on the model's decisions. We
revisit earlier studies that examined how architecture and task complexity
influence this phenomenon and ask: is this phenomenon also affected by how we
train the model? We conducted experimental evaluations on a diverse set of
ImageNet-1k classification models to explore this, keeping the architecture and
training data constant but varying the training pipeline. Our findings reveal
that the training method strongly influences which layers become critical to
the decision function for a given task. For example, improved training regimes
and self-supervised training increase the importance of early layers while
significantly under-utilizing deeper layers. In contrast, methods such as
adversarial training display an opposite trend. Our preliminary results extend
previous findings, offering a more nuanced understanding of the inner mechanics
of neural networks.
  Code: https://github.com/paulgavrikov/layer_criticality
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Interpretable AI: Past, Present and Future Workshop
  at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flow-based Sampling for Entanglement Entropy and the Machine Learning of
  Defects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Bulgarelli, Elia Cellini, Karl Jansen, Stefan Kühn, Alessandro Nada, Shinichi Nakajima, Kim A. Nicoli, Marco Panero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel technique to numerically calculate R\'enyi entanglement
entropies in lattice quantum field theory using generative models. We describe
how flow-based approaches can be combined with the replica trick using a custom
neural-network architecture around a lattice defect connecting two replicas.
Numerical tests for the $\phi^4$ scalar field theory in two and three
dimensions demonstrate that our technique outperforms state-of-the-art Monte
Carlo calculations, and exhibit a promising scaling with the defect size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Electrocardiogram-Language Model for Few-Shot Question Answering with
  Meta Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialu Tang, Tong Xia, Yuan Lu, Cecilia Mascolo, Aaqib Saeed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electrocardiogram (ECG) interpretation requires specialized expertise, often
involving synthesizing insights from ECG signals with complex clinical queries
posed in natural language. The scarcity of labeled ECG data coupled with the
diverse nature of clinical inquiries presents a significant challenge for
developing robust and adaptable ECG diagnostic systems. This work introduces a
novel multimodal meta-learning method for few-shot ECG question answering,
addressing the challenge of limited labeled data while leveraging the rich
knowledge encoded within large language models (LLMs). Our LLM-agnostic
approach integrates a pre-trained ECG encoder with a frozen LLM (e.g., LLaMA
and Gemma) via a trainable fusion module, enabling the language model to reason
about ECG data and generate clinically meaningful answers. Extensive
experiments demonstrate superior generalization to unseen diagnostic tasks
compared to supervised baselines, achieving notable performance even with
limited ECG leads. For instance, in a 5-way 5-shot setting, our method using
LLaMA-3.1-8B achieves accuracy of 84.6%, 77.3%, and 69.6% on single verify,
choose and query question types, respectively. These results highlight the
potential of our method to enhance clinical ECG interpretation by combining
signal processing with the nuanced language understanding capabilities of LLMs,
particularly in data-constrained scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Propensity for Density in Feed-forward Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nandi Schoots, Alex Jackson, Ali Kholmovaia, Peter McBurney, Murray Shanahan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Does the process of training a neural network to solve a task tend to use all
of the available weights even when the task could be solved with fewer weights?
To address this question we study the effects of pruning fully connected,
convolutional and residual models while varying their widths. We find that the
proportion of weights that can be pruned without degrading performance is
largely invariant to model size. Increasing the width of a model has little
effect on the density of the pruned model relative to the increase in absolute
size of the pruned network. In particular, we find substantial prunability
across a large range of model sizes, where our biggest model is 50 times as
wide as our smallest model. We explore three hypotheses that could explain
these findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to refine domain knowledge for biological network inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiwen Li, Menghua Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perturbation experiments allow biologists to discover causal relationships
between variables of interest, but the sparsity and high dimensionality of
these data pose significant challenges for causal structure learning
algorithms. Biological knowledge graphs can bootstrap the inference of causal
structures in these situations, but since they compile vastly diverse
information, they can bias predictions towards well-studied systems.
Alternatively, amortized causal structure learning algorithms encode inductive
biases through data simulation and train supervised models to recapitulate
these synthetic graphs. However, realistically simulating biology is arguably
even harder than understanding a specific system. In this work, we take
inspiration from both strategies and propose an amortized algorithm for
refining domain knowledge, based on data observations. On real and synthetic
datasets, we show that our approach outperforms baselines in recovering ground
truth causal graphs and identifying errors in the prior knowledge with limited
interventional data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Bioinformatic Approach Validated Utilizing Machine Learning Algorithms
  to Identify Relevant Biomarkers and Crucial Pathways in Gallbladder Cancer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rabea Khatun, Wahia Tasnim, Maksuda Akter, Md Manowarul Islam, Md. Ashraf Uddin, Md. Zulfiker Mahmud, Saurav Chandra Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gallbladder cancer (GBC) is the most frequent cause of disease among biliary
tract neoplasms. Identifying the molecular mechanisms and biomarkers linked to
GBC progression has been a significant challenge in scientific research. Few
recent studies have explored the roles of biomarkers in GBC. Our study aimed to
identify biomarkers in GBC using machine learning (ML) and bioinformatics
techniques. We compared GBC tumor samples with normal samples to identify
differentially expressed genes (DEGs) from two microarray datasets (GSE100363,
GSE139682) obtained from the NCBI GEO database. A total of 146 DEGs were found,
with 39 up-regulated and 107 down-regulated genes. Functional enrichment
analysis of these DEGs was performed using Gene Ontology (GO) terms and
REACTOME pathways through DAVID. The protein-protein interaction network was
constructed using the STRING database. To identify hub genes, we applied three
ranking algorithms: Degree, MNC, and Closeness Centrality. The intersection of
hub genes from these algorithms yielded 11 hub genes. Simultaneously, two
feature selection methods (Pearson correlation and recursive feature
elimination) were used to identify significant gene subsets. We then developed
ML models using SVM and RF on the GSE100363 dataset, with validation on
GSE139682, to determine the gene subset that best distinguishes GBC samples.
The hub genes outperformed the other gene subsets. Finally, NTRK2, COL14A1,
SCN4B, ATP1A2, SLC17A7, SLIT3, COL7A1, CLDN4, CLEC3B, ADCYAP1R1, and MFAP4 were
identified as crucial genes, with SLIT3, COL7A1, and CLDN4 being strongly
linked to GBC development and prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FashionR2R: Texture-preserving Rendered-to-Real Image Translation with
  Diffusion Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Hu, Qian He, Gaofeng He, Jiedong Zhuang, Huang Chen, Huafeng Liu, Huamin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling and producing lifelike clothed human images has attracted
researchers' attention from different areas for decades, with the complexity
from highly articulated and structured content. Rendering algorithms decompose
and simulate the imaging process of a camera, while are limited by the accuracy
of modeled variables and the efficiency of computation. Generative models can
produce impressively vivid human images, however still lacking in
controllability and editability. This paper studies photorealism enhancement of
rendered images, leveraging generative power from diffusion models on the
controlled basis of rendering. We introduce a novel framework to translate
rendered images into their realistic counterparts, which consists of two
stages: Domain Knowledge Injection (DKI) and Realistic Image Generation (RIG).
In DKI, we adopt positive (real) domain finetuning and negative (rendered)
domain embedding to inject knowledge into a pretrained Text-to-image (T2I)
diffusion model. In RIG, we generate the realistic image corresponding to the
input rendered image, with a Texture-preserving Attention Control (TAC) to
preserve fine-grained clothing textures, exploiting the decoupled features
encoded in the UNet structure. Additionally, we introduce SynFashion dataset,
featuring high-quality digital clothing images with diverse textures. Extensive
experimental results demonstrate the superiority and effectiveness of our
method in rendered-to-real image translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting time-varying flux and balance in metabolic systems using
  structured neural-ODE processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santanu Rathod, Pietro Lio, Xiao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a novel data-driven framework as an alternative to dynamic flux
balance analysis, bypassing the demand for deep domain knowledge and manual
efforts to formulate the optimization problem. The proposed framework is
end-to-end, which trains a structured neural ODE process (SNODEP) model to
estimate flux and balance samples using gene-expression time-series data.
SNODEP is designed to circumvent the limitations of the standard neural ODE
process model, including restricting the latent and decoder sampling
distributions to be normal and lacking structure between context points for
calculating the latent, thus more suitable for modeling the underlying dynamics
of a metabolic system. Through comprehensive experiments ($156$ in total), we
demonstrate that SNODEP not only predicts the unseen time points of real-world
gene-expression data and the flux and balance estimates well but can even
generalize to more challenging unseen knockout configurations and irregular
data sampling scenarios, all essential for metabolic pathway analysis. We hope
our work can serve as a catalyst for building more scalable and powerful models
for genome-scale metabolic analysis. Our code is available at:
\url{https://github.com/TrustMLRG/SNODEP}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Deep Learning with Fundus and Optical Coherence Tomography
  for Cardiovascular Disease Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cynthia Maldonado-Garcia, Arezoo Zakeri, Alejandro F Frangi, Nishant Ravikumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early identification of patients at risk of cardiovascular diseases (CVD) is
crucial for effective preventive care, reducing healthcare burden, and
improving patients' quality of life. This study demonstrates the potential of
retinal optical coherence tomography (OCT) imaging combined with fundus
photographs for identifying future adverse cardiac events. We used data from
977 patients who experienced CVD within a 5-year interval post-image
acquisition, alongside 1,877 control participants without CVD, totaling 2,854
subjects. We propose a novel binary classification network based on a
Multi-channel Variational Autoencoder (MCVAE), which learns a latent embedding
of patients' fundus and OCT images to classify individuals into two groups:
those likely to develop CVD in the future and those who are not. Our model,
trained on both imaging modalities, achieved promising results (AUROC 0.78 +/-
0.02, accuracy 0.68 +/- 0.002, precision 0.74 +/- 0.02, sensitivity 0.73 +/-
0.02, and specificity 0.68 +/- 0.01), demonstrating its efficacy in identifying
patients at risk of future CVD events based on their retinal images. This study
highlights the potential of retinal OCT imaging and fundus photographs as
cost-effective, non-invasive alternatives for predicting cardiovascular disease
risk. The widespread availability of these imaging techniques in optometry
practices and hospitals further enhances their potential for large-scale CVD
risk screening. Our findings contribute to the development of standardized,
accessible methods for early CVD risk identification, potentially improving
preventive care strategies and patient outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Part of the book series: Lecture Notes in Computer Science
  ((LNCS,volume 15155))</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asymptotic non-linear shrinkage formulas for weighted sample covariance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benoit Oriol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We compute asymptotic non-linear shrinkage formulas for covariance and
precision matrix estimators for weighted sample covariances, in the spirit of
Ledoit and P\'ech\'e. We detail explicitly the formulas for
exponentially-weighted sample covariances. Those new tools pave a way for
applying non-linear shrinkage methods on weighted sample covariance. We show
experimentally the performance of the asymptotic shrinkage formulas. Finally,
we test the robustness of the theory to a heavy-tailed distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An explainable machine learning approach for energy forecasting at the
  household level 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pauline Béraud, Margaux Rioux, Michel Babany, Philippe de La Chevasnerie, Damien Theis, Giacomo Teodori, Chloé Pinguet, Romane Rigaud, François Leclerc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electricity forecasting has been a recurring research topic, as it is key to
finding the right balance between production and consumption. While most papers
are focused on the national or regional scale, few are interested in the
household level. Desegregated forecast is a common topic in Machine Learning
(ML) literature but lacks explainability that household energy forecasts
require. This paper specifically targets the challenges of forecasting
electricity use at the household level. This paper confronts common Machine
Learning algorithms to electricity household forecasts, weighing the pros and
cons, including accuracy and explainability with well-known key metrics.
Furthermore, we also confront them in this paper with the business challenges
specific to this sector such as explainability or outliers resistance. We
introduce a custom decision tree, aiming at providing a fair estimate of the
energy consumption, while being explainable and consistent with human
intuition. We show that this novel method allows greater explainability without
sacrificing much accuracy. The custom tree methodology can be used in various
business use cases but is subject to limitations, such as a lack of resilience
with outliers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WeSpeR: Population spectrum retrieval and spectral density estimation of
  weighted sample covariance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benoit Oriol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The spectrum of the weighted sample covariance shows a asymptotic non random
behavior when the dimension grows with the number of samples. In this setting,
we prove that the asymptotic spectral distribution $F$ of the weighted sample
covariance has a continuous density on $\mathbb{R}^*$. We address then the
practical problem of numerically finding this density. We propose a procedure
to compute it, to determine the support of $F$ and define an efficient grid on
it. We use this procedure to design the $\textit{WeSpeR}$ algorithm, which
estimates the spectral density and retrieves the true spectral covariance
spectrum. Empirical tests confirm the good properties of the $\textit{WeSpeR}$
algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SNAC: Multi-Scale Neural Audio Codec 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hubert Siuzdak, Florian Grötschla, Luca A. Lanzendörfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural audio codecs have recently gained popularity because they can
represent audio signals with high fidelity at very low bitrates, making it
feasible to use language modeling approaches for audio generation and
understanding. Residual Vector Quantization (RVQ) has become the standard
technique for neural audio compression using a cascade of VQ codebooks. This
paper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ
where the quantizers can operate at different temporal resolutions. By applying
a hierarchy of quantizers at variable frame rates, the codec adapts to the
audio structure across multiple timescales. This leads to more efficient
compression, as demonstrated by extensive objective and subjective evaluations.
The code and model weights are open-sourced at
https://github.com/hubertsiuzdak/snac.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debug Smarter, Not Harder: AI Agents for Error Resolution in
  Computational Notebooks <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Grotov, Artem Borzilov, Maksim Krivobok, Timofey Bryksin, Yaroslav Zharov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational notebooks became indispensable tools for research-related
development, offering unprecedented interactivity and flexibility in the
development process. However, these benefits come at the cost of
reproducibility and an increased potential for bugs. With the rise of
code-fluent Large Language Models empowered with agentic techniques, smart
bug-fixing tools with a high level of autonomy have emerged. However, those
tools are tuned for classical script programming and still struggle with
non-linear computational notebooks. In this paper, we present an AI agent
designed specifically for error resolution in a computational notebook. We have
developed an agentic system capable of exploring a notebook environment by
interacting with it -- similar to how a user would -- and integrated the system
into the JetBrains service for collaborative data science called Datalore. We
evaluate our approach against the pre-existing single-action solution by
comparing costs and conducting a user study. Users rate the error resolution
capabilities of the agentic system higher but experience difficulties with UI.
We share the results of the study and consider them valuable for further
improving user-agent collaboration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 System Demonstrations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalizing Low-Rank Bayesian Neural Networks Via Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boning Zhang, Dongzhu Liu, Osvaldo Simeone, Guanchu Wang, Dimitrios Pezaros, Guangxu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To support real-world decision-making, it is crucial for models to be
well-calibrated, i.e., to assign reliable confidence estimates to their
predictions. Uncertainty quantification is particularly important in
personalized federated learning (PFL), as participating clients typically have
small local datasets, making it difficult to unambiguously determine optimal
model parameters. Bayesian PFL (BPFL) methods can potentially enhance
calibration, but they often come with considerable computational and memory
requirements due to the need to track the variances of all the individual model
parameters. Furthermore, different clients may exhibit heterogeneous
uncertainty levels owing to varying local dataset sizes and distributions. To
address these challenges, we propose LR-BPFL, a novel BPFL method that learns a
global deterministic model along with personalized low-rank Bayesian
corrections. To tailor the local model to each client's inherent uncertainty
level, LR-BPFL incorporates an adaptive rank selection mechanism. We evaluate
LR-BPFL across a variety of datasets, demonstrating its advantages in terms of
calibration, accuracy, as well as computational and memory requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SurgeryV2: Bridging the Gap Between Model Merging and Multi-Task
  Learning with Deep Representation Surgery <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enneng Yang, Li Shen, Zhenyi Wang, Guibing Guo, Xingwei Wang, Xiaocun Cao, Jie Zhang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging-based multitask learning (MTL) offers a promising approach for
performing MTL by merging multiple expert models without requiring access to
raw training data. However, in this paper, we examine the merged model's
representation distribution and uncover a critical issue of "representation
bias". This bias arises from a significant distribution gap between the
representations of the merged and expert models, leading to the suboptimal
performance of the merged MTL model. To address this challenge, we first
propose a representation surgery solution called Surgery. Surgery is a
lightweight, task-specific module that aligns the final layer representations
of the merged model with those of the expert models, effectively alleviating
bias and improving the merged model's performance. Despite these improvements,
a performance gap remains compared to the traditional MTL method. Further
analysis reveals that representation bias phenomena exist at each layer of the
merged model, and aligning representations only in the last layer is
insufficient for fully reducing systemic bias because biases introduced at each
layer can accumulate and interact in complex ways. To tackle this, we then
propose a more comprehensive solution, deep representation surgery (also called
SurgeryV2), which mitigates representation bias across all layers, and thus
bridges the performance gap between model merging-based MTL and traditional
MTL. Finally, we design an unsupervised optimization objective to optimize both
the Surgery and SurgeryV2 modules. Our experimental results show that
incorporating these modules into state-of-the-art (SOTA) model merging schemes
leads to significant performance gains. Notably, our SurgeryV2 scheme reaches
almost the same level as individual expert models or the traditional MTL model.
The code is available at \url{https://github.com/EnnengYang/SurgeryV2}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is an extended version of our previous work
  [arXiv:2402.02705] presented at ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unscrambling disease progression at scale: fast inference of event
  permutations with optimal transport <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter A. Wijeratne, Daniel C. Alexander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disease progression models infer group-level temporal trajectories of change
in patients' features as a chronic degenerative condition plays out. They
provide unique insight into disease biology and staging systems with
individual-level clinical utility. Discrete models consider disease progression
as a latent permutation of events, where each event corresponds to a feature
becoming measurably abnormal. However, permutation inference using traditional
maximum likelihood approaches becomes prohibitive due to combinatoric
explosion, severely limiting model dimensionality and utility. Here we leverage
ideas from optimal transport to model disease progression as a latent
permutation matrix of events belonging to the Birkhoff polytope, facilitating
fast inference via optimisation of the variational lower bound. This enables a
factor of 1000 times faster inference than the current state of the art and,
correspondingly, supports models with several orders of magnitude more features
than the current state of the art can consider. Experiments demonstrate the
increase in speed, accuracy and robustness to noise in simulation. Further
experiments with real-world imaging data from two separate datasets, one from
Alzheimer's disease patients, the other age-related macular degeneration,
showcase, for the first time, pixel-level disease progression events in the
brain and eye, respectively. Our method is low compute, interpretable and
applicable to any progressive condition and data modality, giving it broad
potential clinical utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print of version accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating the Capabilities of Deep Learning for Processing and
  Interpreting One-Shot Multi-offset GPR Data: A Numerical Case Study for Lunar
  and Martian Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14386v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14386v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iraklis Giannakis, Craig Warren, Antonios Giannopoulos, Georgios Leontidis, Yan Su, Feng Zhou, Javier Martin-Torres, Nectaria Diamanti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ground-penetrating radar (GPR) is a mature geophysical method that has gained
increasing popularity in planetary science over the past decade. GPR has been
utilised both for Lunar and Martian missions providing pivotal information
regarding the near surface geology of Terrestrial planets. Within that context,
numerous processing pipelines have been suggested to address the unique
challenges present in planetary setups. These processing pipelines often
require manual tuning resulting to ambiguous outputs open to non-unique
interpretations. These pitfalls combined with the large number of planetary GPR
data (kilometers in magnitude), highlight the necessity for automatic,
objective and advanced processing and interpretation schemes. The current paper
investigates the potential of deep learning for interpreting and processing GPR
data. The one-shot multi-offset configuration is investigated via a coherent
numerical case study, showcasing the potential of deep learning for A)
reconstructing the dielectric distribution of the the near surface of
Terrestrial planets, and B) filling missing or bad-quality traces. Special care
was taken for the numerical data to be both realistic and challenging.
Moreover, the generated synthetic data are properly labelled and made publicly
available for training future data-driven pipelines and contributing towards
developing pre-trained foundation models for GPR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-Label LearningWith Irregularly Present Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqian Li, Qiao Han, Yiteng Zhai, Ruifeng Li, Yao Yang, Hongyang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multi-task learning, we often encounter the case when the presence of
labels across samples exhibits irregular patterns: samples can be fully
labeled, partially labeled or unlabeled. Taking drug analysis as an example,
multiple toxicity properties of a drug molecule may not be concurrently
available due to experimental limitations. It triggers a demand for a new
training and inference mechanism that could accommodate irregularly present
labels and maximize the utility of any available label information. In this
work, we focus on the two-label learning task, and propose a novel training and
inference framework, Dual-Label Learning (DLL). The DLL framework formulates
the problem into a dual-function system, in which the two functions should
simultaneously satisfy standard supervision, structural duality and
probabilistic duality. DLL features a dual-tower model architecture that
explicitly captures the information exchange between labels, aimed at
maximizing the utility of partially available labels in understanding label
correlation. During training, label imputation for missing labels is conducted
as part of the forward propagation process, while during inference, labels are
regarded as unknowns of a bivariate system of equations and are solved jointly.
Theoretical analysis guarantees the feasibility of DLL, and extensive
experiments are conducted to verify that by explicitly modeling label
correlation and maximizing the utility of available labels, our method makes
consistently better predictions than baseline approaches by up to a 10% gain in
F1-score or MAPE. Remarkably, our method provided with data at a label missing
rate as high as 60% can achieve similar or even better results than baseline
approaches at a label missing rate of only 10%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Fine-Tuning</span> <span class="highlight-title">Pre-train</span>ed Language Models for Robust Causal Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialin Yu, Yuxiang Zhou, Yulan He, Nevin L. Zhang, Ricardo Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fine-tuning of pre-trained language models (PLMs) has been shown to be
effective across various domains. By using domain-specific supervised data, the
general-purpose representation derived from PLMs can be transformed into a
domain-specific representation. However, these methods often fail to generalize
to out-of-domain (OOD) data due to their reliance on non-causal
representations, often described as spurious features. Existing methods either
make use of adjustments with strong assumptions about lack of hidden common
causes, or mitigate the effect of spurious features using multi-domain data. In
this work, we investigate how fine-tuned pre-trained language models aid
generalizability from single-domain scenarios under mild assumptions, targeting
more general and practical real-world scenarios. We show that a robust
representation can be derived through a so-called causal front-door adjustment,
based on a decomposition assumption, using fine-tuned representations as a
source of data augmentation. Comprehensive experiments in both synthetic and
real-world settings demonstrate the superior generalizability of the proposed
method compared to existing approaches. Our work thus sheds light on the domain
generalization problem by introducing links between fine-tuning and causal
mechanisms into representation learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Scientific Machine Learning Approach for Predicting and Forecasting
  Battery Degradation in Electric Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharv Murgai, Hrishikesh Bhagwat, Raj Abhijit Dandekar, Rajat Dandekar, Sreedath Panat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Carbon emissions are rising at an alarming rate, posing a significant threat
to global efforts to mitigate climate change. Electric vehicles have emerged as
a promising solution, but their reliance on lithium-ion batteries introduces
the critical challenge of battery degradation. Accurate prediction and
forecasting of battery degradation over both short and long time spans are
essential for optimizing performance, extending battery life, and ensuring
effective long-term energy management. This directly influences the
reliability, safety, and sustainability of EVs, supporting their widespread
adoption and aligning with key UN SDGs. In this paper, we present a novel
approach to the prediction and long-term forecasting of battery degradation
using Scientific Machine Learning framework which integrates domain knowledge
with neural networks, offering more interpretable and scientifically grounded
solutions for both predicting short-term battery health and forecasting
degradation over extended periods. This hybrid approach captures both known and
unknown degradation dynamics, improving predictive accuracy while reducing data
requirements. We incorporate ground-truth data to inform our models, ensuring
that both the predictions and forecasts reflect practical conditions. The model
achieved MSE of 9.90 with the UDE and 11.55 with the NeuralODE, in experimental
data, a loss of 1.6986 with the UDE, and a MSE of 2.49 in the NeuralODE,
demonstrating the enhanced precision of our approach. This integration of
data-driven insights with SciML's strengths in interpretability and scalability
allows for robust battery management. By enhancing battery longevity and
minimizing waste, our approach contributes to the sustainability of energy
systems and accelerates the global transition toward cleaner, more responsible
energy solutions, aligning with the UN's SDG agenda.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the evaluators: Towards human-aligned metrics for missing
  markers reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taras Kucherenko, Derek Peristy, Judith Bütepage
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animation data is often obtained through optical motion capture systems,
which utilize a multitude of cameras to establish the position of optical
markers. However, system errors or occlusions can result in missing markers,
the manual cleaning of which can be time-consuming. This has sparked interest
in machine learning-based solutions for missing marker reconstruction in the
academic community. Most academic papers utilize a simplistic mean square error
as the main metric. In this paper, we show that this metric does not correlate
with subjective perception of the fill quality. We introduce and evaluate a set
of better-correlated metrics that can drive progress in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast proxy centers for Jeffreys centroids: The Jeffreys-Fisher-Rao and
  the inductive Gauss-Bregman centers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frank Nielsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The symmetric Kullback-Leibler centroid also called the Jeffreys centroid of
a set of mutually absolutely continuous probability distributions on a measure
space provides a notion of centrality which has proven useful in many tasks
including information retrieval, information fusion, and clustering in image,
video and sound processing. However, the Jeffreys centroid is not available in
closed-form for sets of categorical or normal distributions, two widely used
statistical models, and thus need to be approximated numerically in practice.
In this paper, we first propose the new Jeffreys-Fisher-Rao center defined as
the Fisher-Rao midpoint of the sided Kullback-Leibler centroids as a plug-in
replacement of the Jeffreys centroid. This Jeffreys-Fisher-Rao center admits a
generic formula for uni-parameter exponential family distributions, and
closed-form formula for categorical and normal distributions, matches exactly
the Jeffreys centroid for same-mean normal distributions, and is experimentally
observed in practice to be close to the Jeffreys centroid. Second, we define a
new type of inductive centers generalizing the principle of Gauss
arithmetic-geometric double sequence mean for pairs of densities of any given
exponential family. This center is shown experimentally to approximate very
well the Jeffreys centroid and is suggested to use when the Jeffreys-Fisher-Rao
center is not available in closed form. Moreover, this Gauss-Bregman inductive
center always converges and matches the Jeffreys centroid for sets of same-mean
normal distributions. We report on our experiments demonstrating the use of the
Jeffreys-Fisher-Rao and Gauss-Bregman centers instead of the Jeffreys centroid.
Finally, we conclude this work by reinterpreting these fast proxy centers of
Jeffreys centroids under the lens of dually flat spaces in information
geometry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debiasing Mini-Batch Quadratics for Applications in Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Tatzel, Bálint Mucsányi, Osane Hackel, Philipp Hennig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quadratic approximations form a fundamental building block of machine
learning methods. E.g., second-order optimizers try to find the Newton step
into the minimum of a local quadratic proxy to the objective function; and the
second-order approximation of a network's loss function can be used to quantify
the uncertainty of its outputs via the Laplace approximation. When computations
on the entire training set are intractable - typical for deep learning - the
relevant quantities are computed on mini-batches. This, however, distorts and
biases the shape of the associated stochastic quadratic approximations in an
intricate way with detrimental effects on applications. In this paper, we (i)
show that this bias introduces a systematic error, (ii) provide a theoretical
explanation for it, (iii) explain its relevance for second-order optimization
and uncertainty quantification via the Laplace approximation in deep learning,
and (iv) develop and evaluate debiasing strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main text (including references): 13 pages, 6 figures; Supplements:
  25 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing importance weighting in the presence of sub-population shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Floris Holstege, Bram Wouters, Noud van Giersbergen, Cees Diks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A distribution shift between the training and test data can severely harm
performance of machine learning models. Importance weighting addresses this
issue by assigning different weights to data points during training. We argue
that existing heuristics for determining the weights are suboptimal, as they
neglect the increase of the variance of the estimated model due to the finite
sample size of the training data. We interpret the optimal weights in terms of
a bias-variance trade-off, and propose a bi-level optimization procedure in
which the weights and model parameters are optimized simultaneously. We apply
this optimization to existing importance weighting techniques for last-layer
retraining of deep neural networks in the presence of sub-population shifts and
show empirically that optimizing weights significantly improves generalization
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Currently under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PTR: A <span class="highlight-title">Pre-train</span>ed Language Model for Trajectory Recovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tonglong Wei, Yan Lin, Youfang Lin, Shengnan Guo, Jilin Hu, Gao Cong, Huaiyu Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatiotemporal trajectory data is vital for web-of-things services and is
extensively collected and analyzed by web-based hardware and platforms.
However, issues such as service interruptions and network instability often
lead to sparsely recorded trajectories, resulting in a loss of detailed
movement data. As a result, recovering these trajectories to restore missing
information becomes essential. Despite progress, several challenges remain
unresolved. First, the lack of large-scale dense trajectory data hampers the
performance of existing deep learning methods, which rely heavily on abundant
data for supervised training. Second, current methods struggle to generalize
across sparse trajectories with varying sampling intervals, necessitating
separate re-training for each interval and increasing computational costs.
Third, external factors crucial for the recovery of missing points are not
fully incorporated.
  To address these challenges, we propose a framework called PTR. This
framework mitigates the issue of limited dense trajectory data by leveraging
the capabilities of pre-trained language models (PLMs). PTR incorporates an
explicit trajectory prompt and is trained on datasets with multiple sampling
intervals, enabling it to generalize effectively across different intervals in
sparse trajectories. To capture external factors, we introduce an implicit
trajectory prompt that models road conditions, providing richer information for
recovering missing points. Additionally, we present a trajectory embedder that
encodes trajectory points and transforms the embeddings of both observed and
missing points into a format comprehensible to PLMs. Experimental results on
two public trajectory datasets with three sampling intervals demonstrate the
efficacy and scalability of PTR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Quasi-Newton Optimization in Large Dimensions Including Deep
  Network Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uttam Suman, Mariya Mamajiwala, Mukul Saxena, Ankit Tyagi, Debasish Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our proposal is on a new stochastic optimizer for non-convex and possibly
non-smooth objective functions typically defined over large dimensional design
spaces. Towards this, we have tried to bridge noise-assisted global search and
faster local convergence, the latter being the characteristic feature of a
Newton-like search. Our specific scheme -- acronymed FINDER (Filtering Informed
Newton-like and Derivative-free Evolutionary Recursion), exploits the nonlinear
stochastic filtering equations to arrive at a derivative-free update that has
resemblance with the Newton search employing the inverse Hessian of the
objective function. Following certain simplifications of the update to enable a
linear scaling with dimension and a few other enhancements, we apply FINDER to
a range of problems, starting with some IEEE benchmark objective functions to a
couple of archetypal data-driven problems in deep networks to certain cases of
physics-informed deep networks. The performance of the new method vis-\'a-vis
the well-known Adam and a few others bears evidence to its promise and
potentialities for large dimensional optimization problems of practical
interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On time series clustering with k-means 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Holder, Anthony Bagnall, Jason Lines
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a long history of research into time series clustering using
distance-based partitional clustering. Many of the most popular algorithms
adapt k-means (also known as Lloyd's algorithm) to exploit time dependencies in
the data by specifying a time series distance function. However, these
algorithms are often presented with k-means configured in various ways,
altering key parameters such as the initialisation strategy. This variability
makes it difficult to compare studies because k-means is known to be highly
sensitive to its configuration. To address this, we propose a standard
Lloyd's-based model for TSCL that adopts an end-to-end approach, incorporating
a specialised distance function not only in the assignment step but also in the
initialisation and stopping criteria. By doing so, we create a unified
structure for comparing seven popular Lloyd's-based TSCL algorithms. This
common framework enables us to more easily attribute differences in clustering
performance to the distance function itself, rather than variations in the
k-means configuration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoDification: Mixture of Depths Made Easy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zhang, Meizhi Zhong, Qimeng Wang, Xuantao Lu, Zheyu Ye, Chengqiang Lu, Yan Gao, Yao Hu, Kehai Chen, Min Zhang, Dawei Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-context efficiency has recently become a trending topic in serving large
language models (LLMs). And mixture of depths (MoD) is proposed as a perfect
fit to bring down both latency and memory. In this paper, however, we discover
that MoD can barely transform existing LLMs without costly training over an
extensive number of tokens. To enable the transformations from any LLMs to MoD
ones, we showcase top-k operator in MoD should be promoted to threshold-p
operator, and refinement to architecture and data should also be crafted along.
All these designs form our method termed MoDification. Through a comprehensive
set of experiments covering model scales from 3B to 70B, we exhibit
MoDification strikes an excellent balance between efficiency and effectiveness.
MoDification can achieve up to ~1.2x speedup in latency and ~1.8x reduction in
memory compared to original LLMs especially in long-context applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures, 5 tables, work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting SLO and Goodput Metrics in LLM Serving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhibin Wang, Shipeng Li, Yuhang Zhou, Xue Li, Rong Gu, Nguyen Cam-Tu, Chen Tian, Sheng Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved remarkable performance and are
widely deployed in various applications, while the serving of LLM inference has
raised concerns about user experience and serving throughput. Accordingly,
service level objectives (SLOs) and goodput-the number of requests that meet
SLOs per second-are introduced to evaluate the performance of LLM serving.
However, existing metrics fail to capture the nature of user experience. We
observe two ridiculous phenomena in existing metrics: 1) delaying token
delivery can smooth the tail time between tokens (tail TBT) of a request and 2)
dropping the request that fails to meet the SLOs midway can improve goodput.
  In this paper, we revisit SLO and goodput metrics in LLM serving and propose
a unified metric framework smooth goodput including SLOs and goodput to reflect
the nature of user experience in LLM serving. The framework can adapt to
specific goals of different tasks by setting parameters. We re-evaluate the
performance of different LLM serving systems under multiple workloads based on
this unified framework and provide possible directions for future optimization
of existing strategies. We hope that this framework can provide a unified
standard for evaluating LLM serving and foster researches in the field of LLM
serving optimization to move in a cohesive direction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAZOR: Refining Accuracy by Zeroing Out Redundancies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Riccio, Genoveffa Tortora, Mara Sangiovanni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many application domains, the proliferation of sensors and devices is
generating vast volumes of data, imposing significant pressure on existing data
analysis and data mining techniques. Nevertheless, an increase in data volume
does not inherently imply an increase in informational content, as a
substantial portion may be redundant or represent noise. This challenge is
particularly evident in the deep learning domain, where the utility of
additional data is contingent on its informativeness. In the absence of such,
larger datasets merely exacerbate the computational cost and complexity of the
learning process. To address these challenges, we propose RAZOR, a novel
instance selection technique designed to extract a significantly smaller yet
sufficiently informative subset from a larger set of instances without
compromising the learning process. RAZOR has been specifically engineered to be
robust, efficient, and scalable, making it suitable for large-scale datasets.
Unlike many techniques in the literature, RAZOR is capable of operating in both
supervised and unsupervised settings. Experimental results demonstrate that
RAZOR outperforms recent state-of-the-art techniques in terms of both
effectiveness and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pseudo-label Refinement for Improving Self-Supervised Learning Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Zia-ur-Rehman, Arif Mahmood, Wenxiong Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning systems have gained significant attention in recent
years by leveraging clustering-based pseudo-labels to provide supervision
without the need for human annotations. However, the noise in these
pseudo-labels caused by the clustering methods poses a challenge to the
learning process leading to degraded performance. In this work, we propose a
pseudo-label refinement (SLR) algorithm to address this issue. The cluster
labels from the previous epoch are projected to the current epoch
cluster-labels space and a linear combination of the new label and the
projected label is computed as a soft refined label containing the information
from the previous epoch clusters as well as from the current epoch. In contrast
to the common practice of using the maximum value as a cluster/class indicator,
we employ hierarchical clustering on these soft pseudo-labels to generate
refined hard-labels. This approach better utilizes the information embedded in
the soft labels, outperforming the simple maximum value approach for hard label
generation. The effectiveness of the proposed SLR algorithm is evaluated in the
context of person re-identification (Re-ID) using unsupervised domain
adaptation (UDA). Experimental results demonstrate that the modified Re-ID
baseline, incorporating the SLR algorithm, achieves significantly improved mean
Average Precision (mAP) performance in various UDA tasks, including
real-to-synthetic, synthetic-to-real, and different real-to-real scenarios.
These findings highlight the efficacy of the SLR algorithm in enhancing the
performance of self-supervised learning systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in
  Dynamical Systems Reconstruction <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Brenner, Christoph Jürgen Hemmer, Zahra Monfared, Daniel Durstewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamical systems (DS) theory is fundamental for many areas of science and
engineering. It can provide deep insights into the behavior of systems evolving
in time, as typically described by differential or recursive equations. A
common approach to facilitate mathematical tractability and interpretability of
DS models involves decomposing nonlinear DS into multiple linear DS separated
by switching manifolds, i.e. piecewise linear (PWL) systems. PWL models are
popular in engineering and a frequent choice in mathematics for analyzing the
topological properties of DS. However, hand-crafting such models is tedious and
only possible for very low-dimensional scenarios, while inferring them from
data usually gives rise to unnecessarily complex representations with very many
linear subregions. Here we introduce Almost-Linear Recurrent Neural Networks
(AL-RNNs) which automatically and robustly produce most parsimonious PWL
representations of DS from time series data, using as few PWL nonlinearities as
possible. AL-RNNs can be efficiently trained with any SOTA algorithm for
dynamical systems reconstruction (DSR), and naturally give rise to a symbolic
encoding of the underlying DS that provably preserves important topological
properties. We show that for the Lorenz and R\"ossler systems, AL-RNNs
discover, in a purely data-driven way, the known topologically minimal PWL
representations of the corresponding chaotic attractors. We further illustrate
on two challenging empirical datasets that interpretable symbolic encodings of
the dynamics can be achieved, tremendously facilitating mathematical and
computational analysis of the underlying systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Conference on Neural Information Processing Systems (NeurIPS
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Convergence Analysis for Score-Based Diffusion Models with
  Deterministic Samplers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runjia Li, Qiwei Di, Quanquan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Score-based diffusion models have emerged as powerful techniques for
generating samples from high-dimensional data distributions. These models
involve a two-phase process: first, injecting noise to transform the data
distribution into a known prior distribution, and second, sampling to recover
the original data distribution from noise. Among the various sampling methods,
deterministic samplers stand out for their enhanced efficiency. However,
analyzing these deterministic samplers presents unique challenges, as they
preclude the use of established techniques such as Girsanov's theorem, which
are only applicable to stochastic samplers. Furthermore, existing analysis for
deterministic samplers usually focuses on specific examples, lacking a
generalized approach for general forward processes and various deterministic
samplers. Our paper addresses these limitations by introducing a unified
convergence analysis framework. To demonstrate the power of our framework, we
analyze the variance-preserving (VP) forward process with the exponential
integrator (EI) scheme, achieving iteration complexity of $\tilde
O(d^2/\epsilon)$. Additionally, we provide a detailed analysis of Denoising
Diffusion Implicit Models (DDIM)-type samplers, which have been underexplored
in previous research, achieving polynomial iteration complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>68 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ G-NeuroDAVIS: A Neural Network model for generalized embedding, data
  visualization and sample generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chayan Maitra, Rajat K. De
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visualizing high-dimensional datasets through a generalized embedding has
been a challenge for a long time. Several methods have shown up for the same,
but still, they have not been able to generate a generalized embedding, which
not only can reveal the hidden patterns present in the data but also generate
realistic high-dimensional samples from it. Motivated by this aspect, in this
study, a novel generative model, called G-NeuroDAVIS, has been developed, which
is capable of visualizing high-dimensional data through a generalized
embedding, and thereby generating new samples. The model leverages advanced
generative techniques to produce high-quality embedding that captures the
underlying structure of the data more effectively than existing methods.
G-NeuroDAVIS can be trained in both supervised and unsupervised settings. We
rigorously evaluated our model through a series of experiments, demonstrating
superior performance in classification tasks, which highlights the robustness
of the learned representations. Furthermore, the conditional sample generation
capability of the model has been described through qualitative assessments,
revealing a marked improvement in generating realistic and diverse samples.
G-NeuroDAVIS has outperformed the Variational Autoencoder (VAE) significantly
in multiple key aspects, including embedding quality, classification
performance, and sample generation capability. These results underscore the
potential of our generative model to serve as a powerful tool in various
applications requiring high-quality data generation and representation
learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Formal Explanations for Neuro-Symbolic AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sushmita Paul, Jinqiang Yu, Jip J. Dekker, Alexey Ignatiev, Peter J. Stuckey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the practical success of Artificial Intelligence (AI), current neural
AI algorithms face two significant issues. First, the decisions made by neural
architectures are often prone to bias and brittleness. Second, when a chain of
reasoning is required, neural systems often perform poorly. Neuro-symbolic
artificial intelligence is a promising approach that tackles these (and other)
weaknesses by combining the power of neural perception and symbolic reasoning.
Meanwhile, the success of AI has made it critical to understand its behaviour,
leading to the development of explainable artificial intelligence (XAI). While
neuro-symbolic AI systems have important advantages over purely neural AI, we
still need to explain their actions, which are obscured by the interactions of
the neural and symbolic components. To address the issue, this paper proposes a
formal approach to explaining the decisions of neuro-symbolic systems. The
approach hinges on the use of formal abductive explanations and on solving the
neuro-symbolic explainability problem hierarchically. Namely, it first computes
a formal explanation for the symbolic component of the system, which serves to
identify a subset of the individual parts of neural information that needs to
be explained. This is followed by explaining only those individual neural
inputs, independently of each other, which facilitates succinctness of
hierarchical formal explanations and helps to increase the overall performance
of the approach. Experimental results for a few complex reasoning tasks
demonstrate practical efficiency of the proposed approach, in comparison to
purely neural systems, from the perspective of explanation size, explanation
time, training time, model sizes, and the quality of explanations reported.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Evaluation of Clustered Federated Learning Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Ben Ali, Omar El-Rifai, Imen Megdiche, André Peninou, Olivier Teste
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over recent years, Federated Learning (FL) has proven to be one of the most
promising methods of distributed learning which preserves data privacy. As the
method evolved and was confronted to various real-world scenarios, new
challenges have emerged. One such challenge is the presence of highly
heterogeneous (often referred as non-IID) data distributions among participants
of the FL protocol. A popular solution to this hurdle is Clustered Federated
Learning (CFL), which aims to partition clients into groups where the
distribution are homogeneous. In the literature, state-of-the-art CFL
algorithms are often tested using a few cases of data heterogeneities, without
systematically justifying the choices. Further, the taxonomy used for
differentiating the different heterogeneity scenarios is not always
straightforward. In this paper, we explore the performance of two
state-of-theart CFL algorithms with respect to a proposed taxonomy of data
heterogeneities in federated learning (FL). We work with three image
classification datasets and analyze the resulting clusters against the
heterogeneity classes using extrinsic clustering metrics. Our objective is to
provide a clearer understanding of the relationship between CFL performances
and data heterogeneity scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Montessori-Instruct: Generate Influential Training Data Tailored for
  Student Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochuan Li, Zichun Yu, Chenyan Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data has been widely used to train large language models, but their
generative nature inevitably introduces noisy, non-informative, and misleading
learning signals. In this paper, we propose Montessori-Instruct, a novel data
synthesis framework that tailors the data synthesis ability of the teacher
language model toward the student language model's learning process.
Specifically, we utilize local data influence of synthetic training data points
on students to characterize students' learning preferences. Then, we train the
teacher model with Direct Preference Optimization (DPO) to generate synthetic
data tailored toward student learning preferences. Experiments with
Llama3-8B-Instruct (teacher) and Llama3-8B (student) on Alpaca Eval and
MT-Bench demonstrate that Montessori-Instruct significantly outperforms
standard synthesis methods by 18.35\% and 46.24\% relatively. Our method also
beats data synthesized by a stronger teacher model, GPT-4o. Further analysis
confirms the benefits of teacher's learning to generate more influential
training data in the student's improved learning, the advantages of local data
influence in accurately measuring student preferences, and the robustness of
Montessori-Instruct across different student models. Our code and data are
open-sourced at https://github.com/cxcscmu/Montessori-Instruct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Codes and data are open-sourced at
  https://github.com/cxcscmu/Montessori-Instruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flexi-Fuzz least squares SVM for Alzheimer's diagnosis: Tackling noise,
  outliers, and class imbalance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mushir Akhtar, A. Quadir, M. Tanveer, Mohd. Arshad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's disease (AD) is a leading neurodegenerative condition and the
primary cause of dementia, characterized by progressive cognitive decline and
memory loss. Its progression, marked by shrinkage in the cerebral cortex, is
irreversible. Numerous machine learning algorithms have been proposed for the
early diagnosis of AD. However, they often struggle with the issues of noise,
outliers, and class imbalance. To tackle the aforementioned limitations, in
this article, we introduce a novel, robust, and flexible membership scheme
called Flexi-Fuzz. This scheme integrates a novel flexible weighting mechanism,
class probability, and imbalance ratio. The proposed flexible weighting
mechanism assigns the maximum weight to samples within a specific proximity to
the center, with a gradual decrease in weight beyond a certain threshold. This
approach ensures that samples near the class boundary still receive significant
weight, maintaining their influence in the classification process. Class
probability is used to mitigate the impact of noisy samples, while the
imbalance ratio addresses class imbalance. Leveraging this, we incorporate the
proposed Flexi-Fuzz membership scheme into the least squares support vector
machines (LSSVM) framework, resulting in a robust and flexible model termed
Flexi-Fuzz-LSSVM. We determine the class-center using two methods: the
conventional mean approach and an innovative median approach, leading to two
model variants, Flexi-Fuzz-LSSVM-I and Flexi-Fuzz-LSSVM-II. To validate the
effectiveness of the proposed Flexi-Fuzz-LSSVM models, we evaluated them on
benchmark UCI and KEEL datasets, both with and without label noise.
Additionally, we tested the models on the Alzheimer's Disease Neuroimaging
Initiative (ADNI) dataset for AD diagnosis. Experimental results demonstrate
the superiority of the Flexi-Fuzz-LSSVM models over baseline models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ xPerT: Extended Persistence Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sehun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A persistence diagram provides a compact summary of persistent homology,
which captures the topological features of a space at different scales.
However, due to its nature as a set, incorporating it as a feature into a
machine learning framework is challenging. Several methods have been proposed
to use persistence diagrams as input for machine learning models, but they
often require complex preprocessing steps and extensive hyperparameter tuning.
In this paper, we propose a novel transformer architecture called the
\textit{Extended Persistence Transformer (xPerT)}, which is highly scalable
than the compared to Persformer, an existing transformer for persistence
diagrams. xPerT reduces GPU memory usage by over 90\% and improves accuracy on
multiple datasets. Additionally, xPerT does not require complex preprocessing
steps or extensive hyperparameter tuning, making it easy to use in practice.
Our code is available at https://github.com/sehunfromdaegu/ECG_JEPA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoran Zhang, Yongxiang Li, Zijian Kan, Keyuan Cheng, Lijie Hu, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The locate-then-edit paradigm has shown significant promise for knowledge
editing (KE) in Large Language Models (LLMs). While previous methods perform
well on single-hop fact recall tasks, they consistently struggle with multi-hop
factual recall tasks involving newly edited knowledge. In this paper,
leveraging tools in mechanistic interpretability, we first identify that in
multi-hop tasks, LLMs tend to retrieve implicit subject knowledge from deeper
MLP layers, unlike single-hop tasks, which rely on earlier layers. This
distinction explains the poor performance of current methods in multi-hop
queries, as they primarily focus on editing shallow layers, leaving deeper
layers unchanged. To address this, we propose IFMET, a novel locate-then-edit
KE approach designed to edit both shallow and deep MLP layers. IFMET employs
multi-hop editing prompts and supplementary sets to locate and modify knowledge
across different reasoning stages. Experimental results demonstrate that IFMET
significantly improves performance on multi-hop factual recall tasks,
effectively overcoming the limitations of previous locate-then-edit methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Distance-based Anomaly Detection Framework for Deep Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.09889v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.09889v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongming Zhang, Ke Sun, Bo Xu, Linglong Kong, Martin Müller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In deep reinforcement learning (RL) systems, abnormal states pose significant
risks by potentially triggering unpredictable behaviors and unsafe actions,
thus impeding the deployment of RL systems in real-world scenarios. It is
crucial for reliable decision-making systems to have the capability to cast an
alert whenever they encounter unfamiliar observations that they are not
equipped to handle. In this paper, we propose a novel Mahalanobis
distance-based (MD) anomaly detection framework, called \textit{MDX}, for deep
RL algorithms. MDX simultaneously addresses random, adversarial, and
out-of-distribution (OOD) state outliers in both offline and online settings.
It utilizes Mahalanobis distance within class-conditional distributions for
each action and operates within a statistical hypothesis testing framework
under the Gaussian assumption. We further extend it to robust and
distribution-free versions by incorporating Robust MD and conformal inference
techniques. Through extensive experiments on classical control environments,
Atari games, and autonomous driving scenarios, we demonstrate the effectiveness
of our MD-based detection framework. MDX offers a simple, unified, and
practical anomaly detection tool for enhancing the safety and reliability of RL
systems in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Liger Kernel: Efficient Triton Kernels for LLM Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10989v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10989v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni, Haowen Ning, Yanning Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training Large Language Models (LLMs) efficiently at scale presents a
formidable challenge, driven by their ever-increasing computational demands and
the need for enhanced performance. In this work, we introduce Liger-Kernel, an
open-sourced set of Triton kernels developed specifically for LLM training.
With kernel optimization techniques like kernel operation fusing and input
chunking, our kernels achieve on average a 20% increase in training throughput
and a 60% reduction in GPU memory usage for popular LLMs compared to
HuggingFace implementations. In addition, Liger-Kernel is designed with
modularity, accessibility, and adaptability in mind, catering to both casual
and expert users. Comprehensive benchmarks and integration tests are built in
to ensure compatibility, performance, correctness, and convergence across
diverse computing environments and model architectures.
  The source code is available under a permissive license at:
github.com/linkedin/Liger-Kernel.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Linear Attention in Polynomial Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morris Yau, Ekin Akyürek, Jiayuan Mao, Joshua B. Tenenbaum, Stefanie Jegelka, Jacob Andreas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous research has explored the computational expressivity of Transformer
models in simulating Boolean circuits or Turing machines. However, the
learnability of these simulators from observational data has remained an open
question. Our study addresses this gap by providing the first polynomial-time
learnability results (specifically strong, agnostic PAC learning) for
single-layer Transformers with linear attention. We show that linear attention
may be viewed as a linear predictor in a suitably defined RKHS. As a
consequence, the problem of learning any linear transformer may be converted
into the problem of learning an ordinary linear predictor in an expanded
feature space, and any such predictor may be converted back into a multiheaded
linear transformer. Moving to generalization, we show how to efficiently
identify training datasets for which every empirical risk minimizer is
equivalent (up to trivial symmetries) to the linear Transformer that generated
the data, thereby guaranteeing the learned model will correctly generalize
across all inputs. Finally, we provide examples of computations expressible via
linear attention and therefore polynomial-time learnable, including associative
memories, finite automata, and a class of Universal Turing Machine (UTMs) with
polynomially bounded computation histories. We empirically validate our
theoretical findings on three tasks: learning random linear attention networks,
key--value associations, and learning to execute finite automata. Our findings
bridge a critical gap between theoretical expressivity and learnability of
Transformers, and show that flexible and general models of computation are
efficiently learnable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One size doesn't fit all: Predicting the Number of Examples for
  In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06402v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06402v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manish Chandra, Debasis Ganguly, Iadh Ounis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) refers to the process of adding a small number of
localized examples (ones that are semantically similar to the input) from a
training set of labelled data to an LLM's prompt with an objective to
effectively control the generative process seeking to improve the downstream
task performance. Existing ICL approaches use an identical number of examples
(a pre-configured hyper-parameter) for each data instance. Our work alleviates
the limitations of this 'one fits all' approach by dynamically predicting the
number of examples for each data instance to be used in few-shot inference with
LLMs. In particular, we employ a multi-label classifier, the parameters of
which are fitted using a training set, where the label for each instance in the
training set indicates if using a specific value of k (number of most similar
examples from 0 up to a maximum value) leads to correct k-shot downstream
predictions. Our experiments on a number of text classification benchmarks show
that AICL substantially outperforms standard ICL by up to 17%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modular Boundaries in Recurrent Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.20601v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.20601v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Tanner, Sina Mansour L., Ludovico Coletta, Alessandro Gozzi, Richard F. Betzel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent theoretical and experimental work in neuroscience has focused on the
representational and dynamical character of neural manifolds --subspaces in
neural activity space wherein many neurons coactivate. Importantly, neural
populations studied under this "neural manifold hypothesis" are continuous and
not cleanly divided into separate neural populations. This perspective clashes
with the "modular hypothesis" of brain organization, wherein neural elements
maintain an "all-or-nothing" affiliation with modules. In line with this
modular hypothesis, recent research on recurrent neural networks suggests that
multi-task networks become modular across training, such that different modules
specialize for task-general dynamical motifs. If the modular hypothesis is
true, then it would be important to use a dimensionality reduction technique
that captures modular structure. Here, we investigate the features of such a
method. We leverage RNNs as a model system to study the character of modular
neural populations, using a community detection method from network science
known as modularity maximization to partition neurons into distinct modules.
These partitions allow us to ask the following question: do these modular
boundaries matter to the system? ...
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TGB 2.0: A Benchmark for Learning on Temporal Knowledge Graphs and
  Heterogeneous Graphs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09639v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09639v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julia Gastinger, Shenyang Huang, Mikhail Galkin, Erfan Loghmani, Ali Parviz, Farimah Poursafaei, Jacob Danovitch, Emanuele Rossi, Ioannis Koutis, Heiner Stuckenschmidt, Reihaneh Rabbany, Guillaume Rabusseau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-relational temporal graphs are powerful tools for modeling real-world
data, capturing the evolving and interconnected nature of entities over time.
Recently, many novel models are proposed for ML on such graphs intensifying the
need for robust evaluation and standardized benchmark datasets. However, the
availability of such resources remains scarce and evaluation faces added
complexity due to reproducibility issues in experimental protocols. To address
these challenges, we introduce Temporal Graph Benchmark 2.0 (TGB 2.0), a novel
benchmarking framework tailored for evaluating methods for predicting future
links on Temporal Knowledge Graphs and Temporal Heterogeneous Graphs with a
focus on large-scale datasets, extending the Temporal Graph Benchmark. TGB 2.0
facilitates comprehensive evaluations by presenting eight novel datasets
spanning five domains with up to 53 million edges. TGB 2.0 datasets are
significantly larger than existing datasets in terms of number of nodes, edges,
or timestamps. In addition, TGB 2.0 provides a reproducible and realistic
evaluation pipeline for multi-relational temporal graphs. Through extensive
experimentation, we observe that 1) leveraging edge-type information is crucial
to obtain high performance, 2) simple heuristic baselines are often competitive
with more complex methods, 3) most methods fail to run on our largest datasets,
highlighting the need for research on more scalable methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures, 11 tables, accepted at NeurIPS 2024 Track on
  Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Drift Monitoring in Medical Imaging AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jameson Merkow, Felix J. Dorfner, Xiyu Yang, Alexander Ersoy, Giridhar Dasegowda, Mannudeep Kalra, Matthew P. Lungren, Christopher P. Bridge, Ivan Tarapov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of artificial intelligence (AI) into medical imaging has
advanced clinical diagnostics but poses challenges in managing model drift and
ensuring long-term reliability. To address these challenges, we develop MMC+,
an enhanced framework for scalable drift monitoring, building upon the
CheXstray framework that introduced real-time drift detection for medical
imaging AI models using multi-modal data concordance. This work extends the
original framework's methodologies, providing a more scalable and adaptable
solution for real-world healthcare settings and offers a reliable and
cost-effective alternative to continuous performance monitoring addressing
limitations of both continuous and periodic monitoring methods. MMC+ introduces
critical improvements to the original framework, including more robust handling
of diverse data streams, improved scalability with the integration of
foundation models like MedImageInsight for high-dimensional image embeddings
without site-specific training, and the introduction of uncertainty bounds to
better capture drift in dynamic clinical environments. Validated with
real-world data from Massachusetts General Hospital during the COVID-19
pandemic, MMC+ effectively detects significant data shifts and correlates them
with model performance changes. While not directly predicting performance
degradation, MMC+ serves as an early warning system, indicating when AI systems
may deviate from acceptable performance bounds and enabling timely
interventions. By emphasizing the importance of monitoring diverse data streams
and evaluating data shifts alongside model performance, this work contributes
to the broader adoption and integration of AI solutions in clinical settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning diffusion at lightspeed <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12616v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12616v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Terpin, Nicolas Lanzetti, Martin Gadea, Florian Dörfler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion regulates numerous natural processes and the dynamics of many
successful generative models. Existing models to learn the diffusion terms from
observational data rely on complex bilevel optimization problems and model only
the drift of the system. We propose a new simple model, JKOnet*, which bypasses
the complexity of existing architectures while presenting significantly
enhanced representational capabilities: JKOnet* recovers the potential,
interaction, and internal energy components of the underlying diffusion
process. JKOnet* minimizes a simple quadratic loss and outperforms other
baselines in terms of sample efficiency, computational complexity, and
accuracy. Additionally, JKOnet* provides a closed-form optimal solution for
linearly parametrized functionals, and, when applied to predict the evolution
of cellular processes from real-world data, it achieves state-of-the-art
accuracy at a fraction of the computational cost of all existing methods. Our
methodology is based on the interpretation of diffusion processes as
energy-minimizing trajectories in the probability space via the so-called JKO
scheme, which we study via its first-order optimality conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at, and publication in the proceedings of,
  the 38th Conference on Neural Information Processing Systems (NeurIPS 2024,
  oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ English offensive text detection using CNN based Bi-GRU model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15652v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15652v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tonmoy Roy, Md Robiul Islam, Asif Ahammad Miazee, Anika Antara, Al Amin, Sunjim Hossain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the years, the number of users of social media has increased
drastically. People frequently share their thoughts through social platforms,
and this leads to an increase in hate content. In this virtual community,
individuals share their views, express their feelings, and post photos, videos,
blogs, and more. Social networking sites like Facebook and Twitter provide
platforms to share vast amounts of content with a single click. However, these
platforms do not impose restrictions on the uploaded content, which may include
abusive language and explicit images unsuitable for social media. To resolve
this issue, a new idea must be implemented to divide the inappropriate content.
Numerous studies have been done to automate the process. In this paper, we
propose a new Bi-GRU-CNN model to classify whether the text is offensive or
not. The combination of the Bi-GRU and CNN models outperforms the existing
model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages and 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retraining with Predicted Hard Labels Provably Increases Model Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11206v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11206v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rudrajit Das, Inderjit S. Dhillon, Alessandro Epasto, Adel Javanmard, Jieming Mao, Vahab Mirrokni, Sujay Sanghavi, Peilin Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of a model trained with \textit{noisy labels} is often
improved by simply \textit{retraining} the model with its own predicted
\textit{hard} labels (i.e., $1$/$0$ labels). Yet, a detailed theoretical
characterization of this phenomenon is lacking. In this paper, we theoretically
analyze retraining in a linearly separable setting with randomly corrupted
labels given to us and prove that retraining can improve the population
accuracy obtained by initially training with the given (noisy) labels. To the
best of our knowledge, this is the first such theoretical result. Retraining
finds application in improving training with local label differential privacy
(DP) which involves training with noisy labels. We empirically show that
retraining selectively on the samples for which the predicted label matches the
given label significantly improves label DP training at \textit{no extra
privacy cost}; we call this \textit{consensus-based retraining}. As an example,
when training ResNet-18 on CIFAR-100 with $\epsilon=3$ label DP, we obtain
$6.4\%$ improvement in accuracy with consensus-based retraining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clustering of timed sequences -- Application to the analysis of care
  pathways 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15379v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15379v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Guyet, Pierre Pinson, Enoal Gesny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving the future of healthcare starts by better understanding the current
actual practices in hospital settings. This motivates the objective of
discovering typical care pathways from patient data. Revealing typical care
pathways can be achieved through clustering. The difficulty in clustering care
pathways, represented by sequences of timestamped events, lies in defining a
semantically appropriate metric and clustering algorithms. In this article, we
adapt two methods developed for time series to the clustering of timed
sequences: the drop-DTW metric and the DBA approach for the construction of
averaged time sequences. These methods are then applied in clustering
algorithms to propose original and sound clustering algorithms for timed
sequences. This approach is experimented with and evaluated on synthetic and
real-world data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Debiasing Text Embeddings Through Context Injection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12874v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12874v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Uriot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current advances in Natural Language Processing (NLP) have made it
increasingly feasible to build applications leveraging textual data. Generally,
the core of these applications rely on having a good semantic representation of
text into vectors, via embedding models. However, it has been shown that these
embeddings capture and perpetuate biases already present in text. While a few
techniques have been proposed to debias embeddings, they do not take advantage
of the recent advances in context understanding of modern embedding models. In
this paper, we fill this gap by conducting a review of 19 embedding models by
quantifying their biases and how well they respond to context injection as a
mean of debiasing. We show that higher performing models are more prone to
capturing biases, but are also better at incorporating context. Surprisingly,
we find that while models can easily embed affirmative semantics, they fail at
embedding neutral semantics. Finally, in a retrieval task, we show that biases
in embeddings can lead to non-desirable outcomes. We use our new-found insights
to design a simple algorithm for top $k$ retrieval, where $k$ is dynamically
selected. We show that our algorithm is able to retrieve all relevant gendered
and neutral chunks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inferring Change Points in High-Dimensional Regression via Approximate
  Message Passing <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07864v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07864v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Arpino, Xiaoqi Liu, Julia Gontarek, Ramji Venkataramanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of localizing change points in a generalized linear
model (GLM), a model that covers many widely studied problems in statistical
learning including linear, logistic, and rectified linear regression. We
propose a novel and computationally efficient Approximate Message Passing (AMP)
algorithm for estimating both the signals and the change point locations, and
rigorously characterize its performance in the high-dimensional limit where the
number of parameters $p$ is proportional to the number of samples $n$. This
characterization is in terms of a state evolution recursion, which allows us to
precisely compute performance measures such as the asymptotic Hausdorff error
of our change point estimates, and allows us to tailor the algorithm to take
advantage of any prior structural information on the signals and change points.
Moreover, we show how our AMP iterates can be used to efficiently compute a
Bayesian posterior distribution over the change point locations in the
high-dimensional limit. We validate our theory via numerical experiments, and
demonstrate the favorable performance of our estimators on both synthetic and
real data in the settings of linear, logistic, and rectified linear regression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 9 figures. A preliminary version of this paper appeared in
  ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kernel Density Estimators in Large Dimensions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05807v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05807v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulio Biroli, Marc Mézard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies Kernel Density Estimation for a high-dimensional
distribution $\rho(x)$. Traditional approaches have focused on the limit of
large number of data points $n$ and fixed dimension $d$. We analyze instead the
regime where both the number $n$ of data points $y_i$ and their dimensionality
$d$ grow with a fixed ratio $\alpha=(\log n)/d$. Our study reveals three
distinct statistical regimes for the kernel-based estimate of the density $\hat
\rho_h^{\mathcal {D}}(x)=\frac{1}{n h^d}\sum_{i=1}^n
K\left(\frac{x-y_i}{h}\right)$, depending on the bandwidth $h$: a classical
regime for large bandwidth where the Central Limit Theorem (CLT) holds, which
is akin to the one found in traditional approaches. Below a certain value of
the bandwidth, $h_{CLT}(\alpha)$, we find that the CLT breaks down. The
statistics of $\hat\rho_h^{\mathcal {D}}(x)$ for a fixed $x$ drawn from
$\rho(x)$ is given by a heavy-tailed distribution (an alpha-stable
distribution). In particular below a value $h_G(\alpha)$, we find that
$\hat\rho_h^{\mathcal {D}}(x)$ is governed by extreme value statistics: only a
few points in the database matter and give the dominant contribution to the
density estimator. We provide a detailed analysis for high-dimensional
multivariate Gaussian data. We show that the optimal bandwidth threshold based
on Kullback-Leibler divergence lies in the new statistical regime identified in
this paper. As known by practitioners, when decreasing the bandwidth a
Kernel-estimated estimated changes from a smooth curve to a collections of
peaks centred on the data points. Our findings reveal that this general
phenomenon is related to sharp transitions between phases characterized by
different statistical properties, and offer new insights for Kernel density
estimation in high-dimensional settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Implicit Optimization for Robust and Flexible Image Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07361v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07361v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Jena, Pratik Chaudhari, James C. Gee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning in Image Registration (DLIR) methods have been tremendously
successful in image registration due to their speed and ability to incorporate
weak label supervision at training time. However, DLIR methods forego many of
the benefits of classical optimization-based methods. The functional nature of
deep networks do not guarantee that the predicted transformation is a local
minima of the registration objective, the representation of the transformation
(displacement/velocity field/affine) is fixed, and the networks are not robust
to domain shift. Our method aims to bridge this gap between classical and
learning methods by incorporating optimization as a layer in a deep network. A
deep network is trained to predict multi-scale dense feature images that are
registered using a black box iterative optimization solver. This optimal warp
is then used to minimize image and label alignment errors. By implicitly
differentiating end-to-end through an iterative optimization solver, our
learned features are registration and label-aware, and the warp functions are
guaranteed to be local minima of the registration objective in the feature
space. Our framework shows excellent performance on in-domain datasets, and is
agnostic to domain shift such as anisotropy and varying intensity profiles. For
the first time, our method allows switching between arbitrary transformation
representations (free-form to diffeomorphic) at test time with zero retraining.
End-to-end feature learning also facilitates interpretability of features, and
out-of-the-box promptability using additional label-fidelity terms at
inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Overcoming Slow Decision Frequencies in Continuous Control: Model-Based
  Sequence Reinforcement Learning for Model-Free Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08979v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08979v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devdhar Patel, Hava Siegelmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) is rapidly reaching and surpassing human-level
control capabilities. However, state-of-the-art RL algorithms often require
timesteps and reaction times significantly faster than human capabilities,
which is impractical in real-world settings and typically necessitates
specialized hardware. Such speeds are difficult to achieve in the real world
and often requires specialized hardware. We introduce Sequence Reinforcement
Learning (SRL), an RL algorithm designed to produce a sequence of actions for a
given input state, enabling effective control at lower decision frequencies.
SRL addresses the challenges of learning action sequences by employing both a
model and an actor-critic architecture operating at different temporal scales.
We propose a "temporal recall" mechanism, where the critic uses the model to
estimate intermediate states between primitive actions, providing a learning
signal for each individual action within the sequence. Once training is
complete, the actor can generate action sequences independently of the model,
achieving model-free control at a slower frequency. We evaluate SRL on a suite
of continuous control tasks, demonstrating that it achieves performance
comparable to state-of-the-art algorithms while significantly reducing actor
sample complexity. To better assess performance across varying decision
frequencies, we introduce the Frequency-Averaged Score (FAS) metric. Our
results show that SRL significantly outperforms traditional RL algorithms in
terms of FAS, making it particularly suitable for applications requiring
variable decision frequencies. Additionally, we compare SRL with model-based
online planning, showing that SRL achieves superior FAS while leveraging the
same model during training that online planners use for planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample Compression Scheme Reductions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idan Attias, Steve Hanneke, Arvind Ramaswami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present novel reductions from sample compression schemes in multiclass
classification, regression, and adversarially robust learning settings to
binary sample compression schemes. Assuming we have a compression scheme for
binary classes of size $f(d_\mathrm{VC})$, where $d_\mathrm{VC}$ is the VC
dimension, then we have the following results: (1) If the binary compression
scheme is a majority-vote or a stable compression scheme, then there exists a
multiclass compression scheme of size $O(f(d_\mathrm{G}))$, where
$d_\mathrm{G}$ is the graph dimension. Moreover, for general binary compression
schemes, we obtain a compression of size $O(f(d_\mathrm{G})\log|Y|)$, where $Y$
is the label space. (2) If the binary compression scheme is a majority-vote or
a stable compression scheme, then there exists an $\epsilon$-approximate
compression scheme for regression over $[0,1]$-valued functions of size
$O(f(d_\mathrm{P}))$, where $d_\mathrm{P}$ is the pseudo-dimension. For general
binary compression schemes, we obtain a compression of size
$O(f(d_\mathrm{P})\log(1/\epsilon))$. These results would have significant
implications if the sample compression conjecture, which posits that any binary
concept class with a finite VC dimension admits a binary compression scheme of
size $O(d_\mathrm{VC})$, is resolved (Littlestone and Warmuth, 1986; Floyd and
Warmuth, 1995; Warmuth, 2003). Our results would then extend the proof of the
conjecture immediately to other settings. We establish similar results for
adversarially robust learning and also provide an example of a concept class
that is robustly learnable but has no bounded-size compression scheme,
demonstrating that learnability is not equivalent to having a compression
scheme independent of the sample size, unlike in binary classification, where
compression of size $2^{O(d_\mathrm{VC})}$ is attainable (Moran and Yehudayoff,
2016).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BlackDAN: A Black-Box Multi-Objective Approach for Effective and
  Contextual Jailbreaking of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09804v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09804v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Wang, Victor Shea-Jay Huang, Renmiao Chen, Hao Wang, Chengwei Pan, Lei Sha, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) exhibit remarkable capabilities across
various tasks, they encounter potential security risks such as jailbreak
attacks, which exploit vulnerabilities to bypass security measures and generate
harmful outputs. Existing jailbreak strategies mainly focus on maximizing
attack success rate (ASR), frequently neglecting other critical factors,
including the relevance of the jailbreak response to the query and the level of
stealthiness. This narrow focus on single objectives can result in ineffective
attacks that either lack contextual relevance or are easily recognizable. In
this work, we introduce BlackDAN, an innovative black-box attack framework with
multi-objective optimization, aiming to generate high-quality prompts that
effectively facilitate jailbreaking while maintaining contextual relevance and
minimizing detectability. BlackDAN leverages Multiobjective Evolutionary
Algorithms (MOEAs), specifically the NSGA-II algorithm, to optimize jailbreaks
across multiple objectives including ASR, stealthiness, and semantic relevance.
By integrating mechanisms like mutation, crossover, and Pareto-dominance,
BlackDAN provides a transparent and interpretable process for generating
jailbreaks. Furthermore, the framework allows customization based on user
preferences, enabling the selection of prompts that balance harmfulness,
relevance, and other factors. Experimental results demonstrate that BlackDAN
outperforms traditional single-objective methods, yielding higher success rates
and improved robustness across various LLMs and multimodal LLMs, while ensuring
jailbreak responses are both relevant and less detectable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IncidentResponseGPT: Generating Traffic Incident Response Plans with
  Generative Artificial Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18550v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18550v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artur Grigorev, Adriana-Simona Mihaita Khaled Saleh, Yuming Ou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proposed IncidentResponseGPT framework - a novel system that applies
generative artificial intelligence (AI) to potentially enhance the efficiency
and effectiveness of traffic incident response. This model allows for synthesis
of region-specific incident response guidelines and generates incident response
plans adapted to specific area, aiming to expedite decision-making for traffic
management authorities. This approach aims to accelerate incident resolution
times by suggesting various recommendations (e.g. optimal rerouting strategies,
estimating resource needs) to minimize the overall impact on the urban traffic
network. The system suggests specific actions, including dynamic lane closures,
optimized rerouting and dispatching appropriate emergency resources. We utilize
the Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) to
rank generated response plans based on criteria like impact minimization and
resource efficiency based on their proximity to an human-proposed solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Internals-based Answer Attribution for Trustworthy
  Retrieval-Augmented Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13663v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13663v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jirui Qi, Gabriele Sarti, Raquel Fernández, Arianna Bisazza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the verifiability of model answers is a fundamental challenge for
retrieval-augmented generation (RAG) in the question answering (QA) domain.
Recently, self-citation prompting was proposed to make large language models
(LLMs) generate citations to supporting documents along with their answers.
However, self-citing LLMs often struggle to match the required format, refer to
non-existent sources, and fail to faithfully reflect LLMs' context usage
throughout the generation. In this work, we present MIRAGE --Model
Internals-based RAG Explanations -- a plug-and-play approach using model
internals for faithful answer attribution in RAG applications. MIRAGE detects
context-sensitive answer tokens and pairs them with retrieved documents
contributing to their prediction via saliency methods. We evaluate our proposed
approach on a multilingual extractive QA dataset, finding high agreement with
human answer attribution. On open-ended QA, MIRAGE achieves citation quality
and efficiency comparable to self-citation while also allowing for a
finer-grained control of attribution parameters. Our qualitative evaluation
highlights the faithfulness of MIRAGE's attributions and underscores the
promising application of model internals for RAG answer attribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 Main Conference. Code and data released at
  https://github.com/Betswish/MIRAGE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hip Fracture Patient Pathways and Agent-based Modelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12804v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12804v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alison N. O'Connor, Stephen E. Ryan, Gauri Vaidya, Paul Harford, Meghana Kshirsagar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Increased healthcare demand is significantly straining European services.
Digital solutions including advanced modelling techniques offer a promising
solution to optimising patient flow without impacting day-to-day healthcare
provision. In this work we outline an ongoing project that aims to optimise
healthcare resources using agent-based simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are High-Degree Representations Really Unnecessary in Equivariant Graph
  Neural Networks? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11443v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11443v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Cen, Anyi Li, Ning Lin, Yuxiang Ren, Zihe Wang, Wenbing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Equivariant Graph Neural Networks (GNNs) that incorporate E(3) symmetry have
achieved significant success in various scientific applications. As one of the
most successful models, EGNN leverages a simple scalarization technique to
perform equivariant message passing over only Cartesian vectors (i.e.,
1st-degree steerable vectors), enjoying greater efficiency and efficacy
compared to equivariant GNNs using higher-degree steerable vectors. This
success suggests that higher-degree representations might be unnecessary. In
this paper, we disprove this hypothesis by exploring the expressivity of
equivariant GNNs on symmetric structures, including $k$-fold rotations and
regular polyhedra. We theoretically demonstrate that equivariant GNNs will
always degenerate to a zero function if the degree of the output
representations is fixed to 1 or other specific values. Based on this
theoretical insight, we propose HEGNN, a high-degree version of EGNN to
increase the expressivity by incorporating high-degree steerable vectors while
maintaining EGNN's efficiency through the scalarization trick. Our extensive
experiments demonstrate that HEGNN not only aligns with our theoretical
analyses on toy datasets consisting of symmetric structures, but also shows
substantial improvements on more complicated datasets such as $N$-body and
MD17. Our theoretical findings and empirical results potentially open up new
possibilities for the research of equivariant GNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI:
  The First Romanian Natural Language Inference Corpus <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11877v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11877v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduard Poesina, Cornelia Caragea, Radu Tudor Ionescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language inference (NLI), the task of recognizing the entailment
relationship in sentence pairs, is an actively studied topic serving as a proxy
for natural language understanding. Despite the relevance of the task in
building conversational agents and improving text classification, machine
translation and other NLP tasks, to the best of our knowledge, there is no
publicly available NLI corpus for the Romanian language. To this end, we
introduce the first Romanian NLI corpus (RoNLI) comprising 58K training
sentence pairs, which are obtained via distant supervision, and 6K validation
and test sentence pairs, which are manually annotated with the correct labels.
We conduct experiments with multiple machine learning methods based on distant
learning, ranging from shallow models based on word embeddings to
transformer-based neural networks, to establish a set of competitive baselines.
Furthermore, we improve on the best model by employing a new curriculum
learning strategy based on data cartography. Our dataset and code to reproduce
the baselines are available at https://github.com/Eduard6421/RONLI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2024 (Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Plug-and-Play Posterior Sampling under Mismatched Measurement and Prior
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03546v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03546v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marien Renaud, Jiaming Liu, Valentin de Bortoli, Andrés Almansa, Ulugbek S. Kamilov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Posterior sampling has been shown to be a powerful Bayesian approach for
solving imaging inverse problems. The recent plug-and-play unadjusted Langevin
algorithm (PnP-ULA) has emerged as a promising method for Monte Carlo sampling
and minimum mean squared error (MMSE) estimation by combining physical
measurement models with deep-learning priors specified using image denoisers.
However, the intricate relationship between the sampling distribution of
PnP-ULA and the mismatched data-fidelity and denoiser has not been
theoretically analyzed. We address this gap by proposing a posterior-L2
pseudometric and using it to quantify an explicit error bound for PnP-ULA under
mismatched posterior distribution. We numerically validate our theory on
several inverse problems such as sampling from Gaussian mixture models and
image deblurring. Our results suggest that the sensitivity of the sampling
distribution of PnP-ULA to a mismatch in the measurement model and the denoiser
can be precisely characterized.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-LLM QA with Embodied Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10918v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10918v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhrij Patel, Vishnu Sashank Dorbala, Amrit Singh Bedi, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have grown in popularity due to their natural
language interface and pre trained knowledge, leading to rapidly increasing
success in question-answering (QA) tasks. More recently, multi-agent systems
with LLM-based agents (Multi-LLM) have been utilized increasingly more for QA.
In these scenarios, the models may each answer the question and reach a
consensus or each model is specialized to answer different domain questions.
However, most prior work dealing with Multi-LLM QA has focused on scenarios
where the models are asked in a zero-shot manner or are given information
sources to extract the answer. For question answering of an unknown
environment, embodied exploration of the environment is first needed to answer
the question. This skill is necessary for personalizing embodied AI to
environments such as households. There is a lack of insight into whether a
Multi-LLM system can handle question-answering based on observations from
embodied exploration. In this work, we address this gap by investigating the
use of Multi-Embodied LLM Explorers (MELE) for QA in an unknown environment.
Multiple LLM-based agents independently explore and then answer queries about a
household environment. We analyze different aggregation methods to generate a
single, final answer for each query: debating, majority voting, and training a
central answer module (CAM). Using CAM, we observe a $46\%$ higher accuracy
compared against the other non-learning-based aggregation methods. We provide
code and the query dataset for further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 Figures, 5 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Social Cost Functions for Human-Aware Path Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10547v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10547v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Eirale, Matteo Leonetti, Marcello Chiaberge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving social acceptance is one of the main goals of Social Robotic
Navigation. Despite this topic has received increasing interest in recent
years, most of the research has focused on driving the robotic agent along
obstacle-free trajectories, planning around estimates of future human motion to
respect personal distances and optimize navigation. However, social
interactions in everyday life are also dictated by norms that do not strictly
depend on movement, such as when standing at the end of a queue rather than
cutting it. In this paper, we propose a novel method to recognize common social
scenarios and modify a traditional planner's cost function to adapt to them.
This solution enables the robot to carry out different social navigation
behaviors that would not arise otherwise, maintaining the robustness of
traditional navigation. Our approach allows the robot to learn different social
norms with a single learned model, rather than having different modules for
each task. As a proof of concept, we consider the tasks of queuing and respect
interaction spaces of groups of people talking to one another, but the method
can be extended to other human activities that do not involve motion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An algorithm for clustering with confidence-based must-link and
  cannot-link constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.14437v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.14437v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Baumann, Dorit S. Hochbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study here the semi-supervised $k$-clustering problem where information is
available on whether pairs of objects are in the same or in different clusters.
This information is either available with certainty or with a limited level of
confidence. We introduce the PCCC (Pairwise-Confidence-Constraints-Clustering)
algorithm, which iteratively assigns objects to clusters while accounting for
the information provided on the pairs of objects. Our algorithm uses integer
programming for the assignment of objects which allows to include relationships
as hard constraints that are guaranteed to be satisfied or as soft constraints
that can be violated subject to a penalty. This flexibility distinguishes our
algorithm from the state-of-the-art in which all pairwise constraints are
either considered hard, or all are considered soft. We developed an enhanced
multi-start approach and a model-size reduction technique for the integer
program that contributes to the effectiveness and the efficiency of the
algorithm. Unlike existing algorithms, our algorithm scales to large-scale
instances with up to 60,000 objects, 100 clusters, and millions of cannot-link
constraints (which are the most challenging constraints to incorporate). We
compare the PCCC algorithm with state-of-the-art approaches in an extensive
computational study. Even though the PCCC algorithm is more general than the
state-of-the-art approaches in its applicability, it outperforms the
state-of-the-art approaches on instances with all hard or all soft constraints
both in terms of runtime and various metrics of solution quality. The code of
the PCCC algorithm is publicly available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in INFORMS Journal on Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explaining Modern Gated-Linear RNNs via a Unified Implicit Attention
  Formulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16504v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16504v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Itamar Zimerman, Ameen Ali, Lior Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in efficient sequence modeling have led to attention-free
layers, such as Mamba, RWKV, and various gated RNNs, all featuring
sub-quadratic complexity in sequence length and excellent scaling properties,
enabling the construction of a new type of foundation models. In this paper, we
present a unified view of these models, formulating such layers as implicit
causal self-attention layers. The formulation includes most of their
sub-components and is not limited to a specific part of the architecture. The
framework compares the underlying mechanisms on similar grounds for different
layers and provides a direct means for applying explainability methods. Our
experiments show that our attention matrices and attribution method outperform
an alternative and a more limited formulation that was recently proposed for
Mamba. For the other architectures for which our method is the first to provide
such a view, our method is effective and competitive in the relevant metrics
compared to the results obtained by state-of-the-art Transformer explainability
methods. Our code is publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular
  Property Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyan Liu, Sirui Ding, Sheng Zhou, Wenqi Fan, Qiaoyu Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Molecular property prediction (MPP) is a fundamental and crucial task in drug
discovery. However, prior methods are limited by the requirement for a large
number of labeled molecules and their restricted ability to generalize for
unseen and new tasks, both of which are essential for real-world applications.
To address these challenges, we present MolecularGPT for few-shot MPP. From a
perspective on instruction tuning, we fine-tune large language models (LLMs)
based on curated molecular instructions spanning over 1000 property prediction
tasks. This enables building a versatile and specialized LLM that can be
adapted to novel MPP tasks without any fine-tuning through zero- and few-shot
in-context learning (ICL). MolecularGPT exhibits competitive in-context
reasoning capabilities across 10 downstream evaluation datasets, setting new
benchmarks for few-shot molecular prediction tasks. More importantly, with just
two-shot examples, MolecularGPT can outperform standard supervised graph neural
network methods on 4 out of 7 datasets. It also excels state-of-the-art LLM
baselines by up to 15.7% increase on classification accuracy and decrease of
17.9 on regression metrics (e.g., RMSE) under zero-shot. This study
demonstrates the potential of LLMs as effective few-shot molecular property
predictors. The code is available at https://github.com/NYUSHCS/MolecularGPT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Timeseria: an object-oriented time series processing library 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09567v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09567v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Alberto Russo, Giuliano Taffoni, Luca Bortolussi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Timeseria is an object-oriented time series processing library implemented in
Python, which aims at making it easier to manipulate time series data and to
build statistical and machine learning models on top of it. Unlike common data
analysis frameworks, it builds up from well defined and reusable logical units
(objects), which can be easily combined together in order to ensure a high
level of consistency. Thanks to this approach, Timeseria can address by design
several non-trivial issues often underestimated, such as handling data losses,
non-uniform sampling rates, differences between aggregated data and punctual
observations, time zones, daylight saving times, and more. Timeseria comes with
a comprehensive set of base data structures, common data manipulation
operations, and extensible models for data reconstruction, forecasting and
anomaly detection. It also integrates a powerful plotting engine capable of
handling even millions of data points.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectral and Rhythm Features for Audio Classification with Deep
  Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06927v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06927v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Friedrich Wolf-Monheim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) are widely used in computer vision. They
can be used not only for conventional digital image material to recognize
patterns, but also for feature extraction from digital imagery representing
spectral and rhythm features extracted from time-domain digital audio signals
for the acoustic classification of sounds. Different spectral and rhythm
feature representations like mel-scaled spectrograms, mel-frequency cepstral
coefficients (MFCCs), cyclic tempograms, short-time Fourier transform (STFT)
chromagrams, constant-Q transform (CQT) chromagrams and chroma energy
normalized statistics (CENS) chromagrams are investigated in terms of the audio
classification performance using a deep convolutional neural network. It can be
clearly shown that the mel-scaled spectrograms and the mel-frequency cepstral
coefficients (MFCCs) perform significantly better than the other spectral and
rhythm features investigated in this research for audio classification tasks
using deep CNNs. The experiments were carried out with the aid of the ESC-50
dataset with 2,000 labeled environmental audio recordings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributionally and Adversarially Robust Logistic Regression via
  Intersecting Wasserstein Balls 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aras Selvi, Eleonora Kreacic, Mohsen Ghassemi, Vamsi Potluru, Tucker Balch, Manuela Veloso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarially robust optimization (ARO) has become the de facto standard for
training models to defend against adversarial attacks during testing. However,
despite their robustness, these models often suffer from severe overfitting. To
mitigate this issue, several successful approaches have been proposed,
including replacing the empirical distribution in training with: (i) a
worst-case distribution within an ambiguity set, leading to a distributionally
robust (DR) counterpart of ARO; or (ii) a mixture of the empirical distribution
with one derived from an auxiliary dataset (e.g., synthetic, external, or
out-of-domain). Building on the first approach, we explore the Wasserstein DR
counterpart of ARO for logistic regression and show it admits a tractable
convex optimization reformulation. Adopting the second approach, we enhance the
DR framework by intersecting its ambiguity set with one constructed from an
auxiliary dataset, which yields significant improvements when the Wasserstein
distance between the data-generating and auxiliary distributions can be
estimated. We analyze the resulting optimization problem, develop efficient
solutions, and show that our method outperforms benchmark approaches on
standard datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 3 color figures, under review at a conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting Accurate Lagrangian Multipliers for Mixed Integer Linear
  Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14659v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14659v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Demelas, Joseph Le Roux, Mathieu Lacroix, Axel Parmentier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lagrangian relaxation stands among the most efficient approaches for solving
a Mixed Integer Linear Programs (MILP) with difficult constraints. Given any
duals for these constraints, called Lagrangian Multipliers (LMs), it returns a
bound on the optimal value of the MILP, and Lagrangian methods seek the LMs
giving the best such bound. But these methods generally rely on iterative
algorithms resembling gradient descent to maximize the concave piecewise linear
dual function: the computational burden grows quickly with the number of
relaxed constraints. We introduce a deep learning approach that bypasses the
descent, effectively amortizing the local, per instance, optimization. A
probabilistic encoder based on a graph convolutional network computes
high-dimensional representations of relaxed constraints in MILP instances. A
decoder then turns these representations into LMs. We train the encoder and
decoder jointly by directly optimizing the bound obtained from the predicted
multipliers. Numerical experiments show that our approach closes up to 85~\% of
the gap between the continuous relaxation and the best Lagrangian bound, and
provides a high quality warm-start for descent based Lagrangian methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3-D Magnetotelluric Deep Learning Inversion Guided by Pseudo-Physical
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09388v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09388v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peifan Jiang, Xuben Wang, Shuang Wang, Fei Deng, Kunpeng Wang, Bin Wang, Yuhan Yang, Islam Fadel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetotelluric deep learning (DL) inversion methods based on joint
data-driven and physics-driven have become a hot topic in recent years. When
mapping observation data (or forward modeling data) to the resistivity model
using neural networks (NNs), incorporating the error (loss) term of the
inversion resistivity's forward modeling response--which introduces physical
information about electromagnetic field propagation--can significantly enhance
the inversion accuracy. To efficiently achieve data-physical dual-driven MT
deep learning inversion for large-scale 3-D MT data, we propose using DL
forward modeling networks to compute this portion of the loss. This approach
introduces pseudo-physical information through the forward modeling of NN
simulation, further guiding the inversion network fitting. Specifically, we
first pre-train the forward modeling networks as fixed forward modeling
operators, then transfer and integrate them into the inversion network
training, and finally optimize the inversion network by minimizing the
multinomial loss. Theoretical experimental results indicate that despite some
simulation errors in DL forward modeling, the introduced pseudo-physical
information still enhances inversion accuracy and significantly mitigates the
overfitting problem during training. Additionally, we propose a new input mode
that involves masking and adding noise to the data, simulating the field data
environment of 3-D MT inversion, thereby making the method more flexible and
effective for practical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Survey of Mamba 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01129v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01129v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haohao Qu, Liangbo Ning, Rui An, Wenqi Fan, Tyler Derr, Hui Liu, Xin Xu, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As one of the most representative DL techniques, Transformer architecture has
empowered numerous advanced models, especially the large language models (LLMs)
that comprise billions of parameters, becoming a cornerstone in deep learning.
Despite the impressive achievements, Transformers still face inherent
limitations, particularly the time-consuming inference resulting from the
quadratic computation complexity of attention calculation. Recently, a novel
architecture named Mamba, drawing inspiration from classical state space models
(SSMs), has emerged as a promising alternative for building foundation models,
delivering comparable modeling abilities to Transformers while preserving
near-linear scalability concerning sequence length. This has sparked an
increasing number of studies actively exploring Mamba's potential to achieve
impressive performance across diverse domains. Given such rapid evolution,
there is a critical need for a systematic review that consolidates existing
Mamba-empowered models, offering a comprehensive understanding of this emerging
model architecture. In this survey, we therefore conduct an in-depth
investigation of recent Mamba-associated studies, covering three main aspects:
the advancements of Mamba-based models, the techniques of adapting Mamba to
diverse data, and the applications where Mamba can excel. Specifically, we
first review the foundational knowledge of various representative deep learning
models and the details of Mamba-1&2 as preliminaries. Then, to showcase the
significance of Mamba for AI, we comprehensively review the related studies
focusing on Mamba models' architecture design, data adaptability, and
applications. Finally, we present a discussion of current limitations and
explore various promising research directions to provide deeper insights for
future investigations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimization Dynamics of Equivariant and Augmented Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13458v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13458v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oskar Nordenfors, Fredrik Ohlsson, Axel Flinth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the optimization of neural networks on symmetric data, and
compare the strategy of constraining the architecture to be equivariant to that
of using data augmentation. Our analysis reveals that that the relative
geometry of the admissible and the equivariant layers, respectively, plays a
key role. Under natural assumptions on the data, network, loss, and group of
symmetries, we show that compatibility of the spaces of admissible layers and
equivariant layers, in the sense that the corresponding orthogonal projections
commute, implies that the sets of equivariant stationary points are identical
for the two strategies. If the linear layers of the network also are given a
unitary parametrization, the set of equivariant layers is even invariant under
the gradient flow for augmented models. Our analysis however also reveals that
even in the latter situation, stationary points may be unstable for augmented
training although they are stable for the manifestly equivariant models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v4: Some discussions added, along with an updated experiment section.
  v3: Completely revised manuscript: New framework for neural nets, new main
  result (involving compability condition), new experiments, new author. v2:
  Revised manuscript. Mostly small edits, apart from new experiments (see
  Appendix E)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entity Matching using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11244v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11244v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ralph Peeters, Aaron Steiner, Christian Bizer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity matching is the task of deciding whether two entity descriptions refer
to the same real-world entity. Entity matching is a central step in most data
integration pipelines. Many state-of-the-art entity matching methods rely on
pre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks
of these models for entity matching are that (i) the models require significant
amounts of task-specific training data and (ii) the fine-tuned models are not
robust concerning out-of-distribution entities. This paper investigates using
generative large language models (LLMs) as a less task-specific training
data-dependent and more robust alternative to PLM-based matchers. The study
covers hosted and open-source LLMs which can be run locally. We evaluate these
models in a zero-shot scenario and a scenario where task-specific training data
is available. We compare different prompt designs and the prompt sensitivity of
the models. We show that there is no single best prompt but that the prompt
needs to be tuned for each model/dataset combination. We further investigate
(i) the selection of in-context demonstrations, (ii) the generation of matching
rules, as well as (iii) fine-tuning LLMs using the same pool of training data.
Our experiments show that the best LLMs require no or only a few training
examples to perform comparably to PLMs that were fine-tuned using thousands of
examples. LLM-based matchers further exhibit higher robustness to unseen
entities. We show that GPT4 can generate structured explanations for matching
decisions and can automatically identify potential causes of matching errors by
analyzing explanations of wrong decisions. We demonstrate that the model can
generate meaningful textual descriptions of the identified error classes, which
can help data engineers to improve entity matching pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Proceedings of the 28th International Conference on
  Extending Database Technology (EDBT), 25th March-28th March, 2025, ISBN
  978-3-89318-098-1 on OpenProceedings.org</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Survey of Multi-Agent Deep Reinforcement Learning with Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08975v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08975v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changxi Zhu, Mehdi Dastani, Shihan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Communication is an effective mechanism for coordinating the behaviors of
multiple agents, broadening their views of the environment, and to support
their collaborations. In the field of multi-agent deep reinforcement learning
(MADRL), agents can improve the overall learning performance and achieve their
objectives by communication. Agents can communicate various types of messages,
either to all agents or to specific agent groups, or conditioned on specific
constraints. With the growing body of research work in MADRL with communication
(Comm-MADRL), there is a lack of a systematic and structural approach to
distinguish and classify existing Comm-MADRL approaches. In this paper, we
survey recent works in the Comm-MADRL field and consider various aspects of
communication that can play a role in designing and developing multi-agent
reinforcement learning systems. With these aspects in mind, we propose 9
dimensions along which Comm-MADRL approaches can be analyzed, developed, and
compared. By projecting existing works into the multi-dimensional space, we
discover interesting trends. We also propose some novel directions for
designing future Comm-MADRL systems through exploring possible combinations of
the dimensions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 5 figures, 13 tables; published on Autonomous Agents and
  Multi-Agent Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Likelihood Over-optimisation in Direct Alignment
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyan Shi, Sander Land, Acyr Locatelli, Matthieu Geist, Max Bartolo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Alignment Algorithms (DAAs), such as Direct Preference Optimisation
(DPO) and Identity Preference Optimisation (IPO), have emerged as alternatives
to online Reinforcement Learning from Human Feedback (RLHF) algorithms such as
Proximal Policy Optimisation (PPO) for aligning language models to human
preferences, without the need for explicit reward modelling. These methods
generally aim to increase the likelihood of generating better (preferred)
completions while discouraging worse (non-preferred) ones, while staying close
to the original model's behaviour. In this work, we explore the relationship
between completion likelihood and model performance in state-of-the-art DAAs,
and identify a critical issue of likelihood over-optimisation. Contrary to
expectations, we find that higher likelihood of better completions and larger
margins between better and worse completion likelihoods do not necessarily lead
to better performance, and may even degrade it. Our analysis reveals that while
higher likelihood correlates with better memorisation of factual knowledge
patterns, a slightly lower completion likelihood tends to improve output
diversity, thus leading to better generalisation to unseen scenarios. Moreover,
we identify two key indicators that signal when over-optimised output diversity
begins to harm performance: Decreasing Entropy over Top-k Tokens and
Diminishing Top-k Probability Mass. Our experimental results validate that
these indicators are reliable signs of declining performance under different
regularisations, helping prevent over-optimisation and improve alignment with
human preferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GLANCE: Global Actions in a Nutshell for Counterfactual Explainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18921v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18921v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Loukas Kavouras, Eleni Psaroudaki, Konstantinos Tsopelas, Dimitrios Rontogiannis, Nikolaos Theologitis, Dimitris Sacharidis, Giorgos Giannopoulos, Dimitrios Tomaras, Kleopatra Markou, Dimitrios Gunopulos, Dimitris Fotakis, Ioannis Emiris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread deployment of machine learning systems in critical real-world
decision-making applications has highlighted the urgent need for counterfactual
explainability methods that operate effectively. Global counterfactual
explanations, expressed as actions to offer recourse, aim to provide succinct
explanations and insights applicable to large population subgroups.
Effectiveness is measured by the fraction of the population that is provided
recourse, ensuring that the actions benefit as many individuals as possible.
Keeping the cost of actions low ensures the proposed recourse actions remain
practical and actionable. Limiting the number of actions that provide global
counterfactuals is essential to maximize interpretability. The primary
challenge, therefore, is balancing these trade-offs, i.e., maximizing
effectiveness, minimizing cost, while maintaining a small number of actions. We
introduce GLANCE, a versatile and adaptive framework, comprising two
algorithms, that allows the careful balancing of the trade-offs among the three
key objectives, with the size objective functioning as a tunable parameter to
keep the actions few and easy to interpret. C-GLANCE employs a clustering
approach that considers both the feature space and the space of counterfactual
actions, thereby accounting for the distribution of points in a way that aligns
with the structure of the model. T-GLANCE provides additional features to
enhance flexibility. It employs a tree-based approach, that allows users to
specify split features, to build a decision tree with a single counterfactual
action at each node that can be used as a subgroup policy. Our extensive
experimental evaluation demonstrates that our method consistently shows greater
robustness and performance compared to existing methods across various datasets
and models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dating ancient manuscripts using radiocarbon and AI-based writing style
  analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12013v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12013v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mladen Popović, Maruf A. Dhali, Lambert Schomaker, Johannes van der Plicht, Kaare Lund Rasmussen, Jacopo La Nasa, Ilaria Degano, Maria Perla Colombini, Eibert Tigchelaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining the chronology of ancient handwritten manuscripts is essential
for reconstructing the evolution of ideas. For the Dead Sea Scrolls, this is
particularly important. However, there is an almost complete lack of
date-bearing manuscripts evenly distributed across the timeline and written in
similar scripts available for palaeographic comparison. Here, we present Enoch,
a state-of-the-art AI-based date-prediction model, trained on the basis of new
radiocarbon-dated samples of the scrolls. Enoch uses established
handwriting-style descriptors and applies Bayesian ridge regression. The
challenge of this study is that the number of radiocarbon-dated manuscripts is
small, while current machine learning requires an abundance of training data.
We show that by using combined angular and allographic writing style feature
vectors and applying Bayesian ridge regression, Enoch could predict the
radiocarbon-based dates from style, supported by leave-one-out validation, with
varied MAEs of 27.9 to 30.7 years relative to the radiocarbon dating. Enoch was
then used to estimate the dates of 135 unseen manuscripts, revealing that 79
per cent of the samples were considered 'realistic' upon palaeographic post-hoc
evaluation. We present a new chronology of the scrolls. The radiocarbon ranges
and Enoch's style-based predictions are often older than the traditionally
assumed palaeographic estimates. In the range of 300-50 BCE, Enoch's date
prediction provides an improved granularity. The study is in line with current
developments in multimodal machine-learning techniques, and the methods can be
used for date prediction in other partially-dated manuscript collections. This
research shows how Enoch's quantitative, probability-based approach can be a
tool for palaeographers and historians, re-dating ancient Jewish key texts and
contributing to current debates on Jewish and Christian origins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages of main article, 103 pages of supplementary materials; the
  first version of this article is originally prepared in July 2023 after the
  completion of all the experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13754v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13754v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjie Ni, Yifan Song, Deepanway Ghosal, Bo Li, David Junhao Zhang, Xiang Yue, Fuzhao Xue, Zian Zheng, Kaichen Zhang, Mahir Shah, Kabir Jain, Yang You, Michael Shieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceiving and generating diverse modalities are crucial for AI models to
effectively learn from and engage with real-world signals, necessitating
reliable evaluations for their development. We identify two major issues in
current evaluations: (1) inconsistent standards, shaped by different
communities with varying protocols and maturity levels; and (2) significant
query, grading, and generalization biases. To address these, we introduce
MixEval-X, the first any-to-any, real-world benchmark designed to optimize and
standardize evaluations across diverse input and output modalities. We propose
multi-modal benchmark mixture and adaptation-rectification pipelines to
reconstruct real-world task distributions, ensuring evaluations generalize
effectively to real-world use cases. Extensive meta-evaluations show our
approach effectively aligns benchmark samples with real-world task
distributions. Meanwhile, MixEval-X's model rankings correlate strongly with
that of crowd-sourced real-world evaluations (up to 0.98) while being much more
efficient. We provide comprehensive leaderboards to rerank existing models and
organizations and offer insights to enhance understanding of multi-modal
evaluations and inform future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLEdge: Benchmarking Federated Machine Learning Applications in Edge
  Computing Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05172v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05172v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Herbert Woisetschläger, Alexander Isenko, Ruben Mayer, Shiqiang Wang, Hans-Arno Jacobsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has become a viable technique for realizing
privacy-enhancing distributed deep learning on the network edge. Heterogeneous
hardware, unreliable client devices, and energy constraints often characterize
edge computing systems. In this paper, we propose FLEdge, which complements
existing FL benchmarks by enabling a systematic evaluation of client
capabilities. We focus on computational and communication bottlenecks, client
behavior, and data security implications. Our experiments with models varying
from 14K to 80M trainable parameters are carried out on dedicated hardware with
emulated network characteristics and client behavior. We find that
state-of-the-art embedded hardware has significant memory bottlenecks, leading
to 4x longer processing times than on modern data center GPUs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted for publication at the ACM/IFIP Middleware Conference
  2024. Please cite the published version via
  https://doi.org/10.1145/3652892.3700751</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context-Enhanced Multi-View Trajectory Representation Learning: Bridging
  the Gap through Self-Supervised Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13196v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13196v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tangwen Qian, Junhe Li, Yile Chen, Gao Cong, Tao Sun, Fei Wang, Yongjun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling trajectory data with generic-purpose dense representations has
become a prevalent paradigm for various downstream applications, such as
trajectory classification, travel time estimation and similarity computation.
However, existing methods typically rely on trajectories from a single spatial
view, limiting their ability to capture the rich contextual information that is
crucial for gaining deeper insights into movement patterns across different
geospatial contexts. To this end, we propose MVTraj, a novel multi-view
modeling method for trajectory representation learning. MVTraj integrates
diverse contextual knowledge, from GPS to road network and points-of-interest
to provide a more comprehensive understanding of trajectory data. To align the
learning process across multiple views, we utilize GPS trajectories as a bridge
and employ self-supervised pretext tasks to capture and distinguish movement
patterns across different spatial views. Following this, we treat trajectories
from different views as distinct modalities and apply a hierarchical
cross-modal interaction module to fuse the representations, thereby enriching
the knowledge derived from multiple sources. Extensive experiments on
real-world datasets demonstrate that MVTraj significantly outperforms existing
baselines in tasks associated with various spatial views, validating its
effectiveness and practical utility in spatio-temporal modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Model Openness Framework: Promoting Completeness and Openness for
  Reproducibility, Transparency, and Usability in Artificial Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13784v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13784v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matt White, Ibrahim Haddad, Cailean Osborne, Xiao-Yang Yanglet Liu, Ahmed Abdelmonsef, Sachin Varghese, Arnaud Le Hors
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative artificial intelligence (AI) offers numerous opportunities for
research and innovation, but its commercialization has raised concerns about
the transparency and safety of frontier AI models. Most models lack the
necessary components for full understanding, auditing, and reproducibility, and
some model producers use restrictive licenses whilst claiming that their models
are "open source". To address these concerns, we introduce the Model Openness
Framework (MOF), a three-tiered ranked classification system that rates machine
learning models based on their completeness and openness, following open
science principles. For each MOF class, we specify code, data, and
documentation components of the model development lifecycle that must be
released and under which open licenses. In addition, the Model Openness Tool
(MOT) provides a user-friendly reference implementation to evaluate the
openness and completeness of models against the MOF classification system.
Together, the MOF and MOT provide timely practical guidance for (i) model
producers to enhance the openness and completeness of their publicly-released
models, and (ii) model consumers to identify open models and their constituent
components that can be permissively used, studied, modified, and redistributed.
Through the MOF, we seek to establish completeness and openness as core tenets
of responsible AI research and development, and to promote best practices in
the burgeoning open AI ecosystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TotalVibeSegmentator: Full Body MRI Segmentation for the NAKO and UK
  Biobank 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00125v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00125v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Graf, Paul-Sören Platzek, Evamaria Olga Riedel, Constanze Ramschütz, Sophie Starck, Hendrik Kristian Möller, Matan Atad, Henry Völzke, Robin Bülow, Carsten Oliver Schmidt, Julia Rüdebusch, Matthias Jung, Marco Reisert, Jakob Weiss, Maximilian Löffler, Fabian Bamberg, Bene Wiestler, Johannes C. Paetzold, Daniel Rueckert, Jan Stefan Kirschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objectives: To present a publicly available torso segmentation network for
large epidemiology datasets on volumetric interpolated breath-hold examination
(VIBE) images. Materials & Methods: We extracted preliminary segmentations from
TotalSegmentator, spine, and body composition networks for VIBE images, then
improved them iteratively and retrained a nnUNet network. Using subsets of NAKO
(85 subjects) and UK Biobank (16 subjects), we evaluated with Dice-score on a
holdout set (12 subjects) and existing organ segmentation approach (1000
subjects), generating 71 semantic segmentation types for VIBE images. We
provide an additional network for the vertebra segments 22 individual vertebra
types. Results: We achieved an average Dice score of 0.89 +- 0.07 overall 71
segmentation labels. We scored > 0.90 Dice-score on the abdominal organs except
for the pancreas with a Dice of 0.70. Conclusion: Our work offers a detailed
and refined publicly available full torso segmentation on VIBE images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/robert-graf/TotalVibeSegmentator</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Graph Pooling with Persistent Homology <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16346v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16346v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaolong Ying, Xinjian Zhao, Tianshu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been an emerging trend to integrate persistent homology
(PH) into graph neural networks (GNNs) to enrich expressive power. However,
naively plugging PH features into GNN layers always results in marginal
improvement with low interpretability. In this paper, we investigate a novel
mechanism for injecting global topological invariance into pooling layers using
PH, motivated by the observation that filtration operation in PH naturally
aligns graph pooling in a cut-off manner. In this fashion, message passing in
the coarsened graph acts along persistent pooled topology, leading to improved
performance. Experimentally, we apply our mechanism to a collection of graph
pooling methods and observe consistent and substantial performance gain over
several popular datasets, demonstrating its wide applicability and flexibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Communication-Efficient Distributed Deep Learning via Federated Dynamic
  Averaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20988v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20988v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michail Theologitis, Georgios Frangias, Georgios Anestis, Vasilis Samoladas, Antonios Deligiannakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driven by the ever-growing volume and decentralized nature of data, coupled
with the need to harness this data and generate knowledge from it, has led to
the extensive use of distributed deep learning (DDL) techniques for training.
These techniques rely on local training that is performed at the distributed
nodes based on locally collected data, followed by a periodic synchronization
process that combines these models to create a global model. However, frequent
synchronization of DL models, encompassing millions to many billions of
parameters, creates a communication bottleneck, severely hindering scalability.
Worse yet, DDL algorithms typically waste valuable bandwidth, and make
themselves less practical in bandwidth-constrained federated settings, by
relying on overly simplistic, periodic, and rigid synchronization schedules.
These drawbacks also have a direct impact on the time required for the training
process, necessitating excessive time for data communication. To address these
shortcomings, we propose Federated Dynamic Averaging (FDA), a
communication-efficient DDL strategy that dynamically triggers synchronization
based on the value of the model variance. In essence, the costly
synchronization step is triggered only if the local models, which are
initialized from a common global model after each synchronization, have
significantly diverged. This decision is facilitated by the communication of a
small local state from each distributed node/worker. Through extensive
experiments across a wide range of learning tasks we demonstrate that FDA
reduces communication cost by orders of magnitude, compared to both traditional
and cutting-edge communication-efficient algorithms. Additionally, we show that
FDA maintains robust performance across diverse data heterogeneity settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as research paper at EDBT 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedECA: A Federated External Control Arm Method for Causal Inference
  with Time-To-Event Data in Distributed Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16984v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16984v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Ogier du Terrail, Quentin Klopfenstein, Honghao Li, Imke Mayer, Nicolas Loiseau, Mohammad Hallal, Michael Debouver, Thibault Camalon, Thibault Fouqueray, Jorge Arellano Castro, Zahia Yanes, Laetitia Dahan, Julien Taïeb, Pierre Laurent-Puig, Jean-Baptiste Bachet, Shulin Zhao, Remy Nicolle, Jérome Cros, Daniel Gonzalez, Robert Carreras-Torres, Adelaida Garcia Velasco, Kawther Abdilleh, Sudheer Doss, Félix Balazard, Mathieu Andreux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  External control arms (ECA) can inform the early clinical development of
experimental drugs and provide efficacy evidence for regulatory approval.
However, the main challenge in implementing ECA lies in accessing real-world or
historical clinical trials data. Indeed, regulations protecting patients'
rights by strictly controlling data processing make pooling data from multiple
sources in a central server often difficult. To address these limitations, we
develop a new method, 'FedECA' that leverages federated learning (FL) to enable
inverse probability of treatment weighting (IPTW) for time-to-event outcomes on
separate cohorts without needing to pool data. To showcase the potential of
FedECA, we apply it in different settings of increasing complexity culminating
with a real-world use-case in which FedECA provides evidence for a differential
effect between two drugs that would have otherwise gone unnoticed. By sharing
our code, we hope FedECA will foster the creation of federated research
networks and thus accelerate drug development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>code available at: https://github.com/owkin/fedeca, bug in SMD
  computation present in v1 and v2 has been fixed, many experiments on real
  data have been added + fix in YODA experiments using imputed data instead of
  raw data as well as typos and affiliations fix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple Opinion Dynamics for No-Regret Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08670v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08670v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Lazarsfeld, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a cooperative multi-agent bandit setting in the distributed GOSSIP
model: in every round, each of $n$ agents chooses an action from a common set,
observes the action's corresponding reward, and subsequently exchanges
information with a single randomly chosen neighbor, which may inform its choice
in the next round. We introduce and analyze families of memoryless and
time-independent protocols for this setting, inspired by opinion dynamics that
are well-studied for other algorithmic tasks in the GOSSIP model. For
stationary reward settings, we prove for the first time that these simple
protocols exhibit best-of-both-worlds behavior, simultaneously obtaining
constant cumulative regret scaling like $R(T)/T = \widetilde O(1/T)$, and also
reaching consensus on the highest-mean action within $\widetilde O(\sqrt{n})$
rounds. We obtain these results by showing a new connection between the global
evolution of these decentralized protocols and a class of zero-sum
multiplicative weights update} processes. Using this connection, we establish a
general framework for analyzing the population-level regret and other
properties of our protocols. Finally, we show our protocols are also
surprisingly robust to adversarial rewards, and in this regime we obtain
sublinear regret scaling like $R(T)/T = \widetilde O(1/\sqrt{T})$ as long as
the number of rounds does not grow too fast as a function of $n$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrating spoken instructions into flight trajectory prediction to
  optimize automation in air traffic control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.01661v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.01661v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyue Guo, Zheng Zhang, Bo Yang, Jianwei Zhang, Hongyu Yang, Yi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The booming air transportation industry inevitably burdens air traffic
controllers' workload, causing unexpected human factor-related incidents.
Current air traffic control systems fail to consider spoken instructions for
traffic prediction, bringing significant challenges in detecting human errors
during real-time traffic operations. Here, we present an automation paradigm
integrating controlling intent into the information processing loop through the
spoken instruction-aware flight trajectory prediction framework. A 3-stage
progressive multi-modal learning paradigm is proposed to address the modality
gap between the trajectory and spoken instructions, as well as minimize the
data requirements. Experiments on a real-world dataset show the proposed
framework achieves flight trajectory prediction with high predictability and
timeliness, obtaining over 20% relative reduction in mean deviation error.
Moreover, the generalizability of the proposed framework is also confirmed by
various model architectures. The proposed framework can formulate
full-automated information processing in real-world air traffic applications,
supporting human error detection and enhancing aviation safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted in principle by Nature Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model Based Generative Error Correction: A Challenge and
  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09785v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09785v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao-Han Huck Yang, Taejin Park, Yuan Gong, Yuanchao Li, Zhehuai Chen, Yen-Ting Lin, Chen Chen, Yuchen Hu, Kunal Dhawan, Piotr Żelasko, Chao Zhang, Yun-Nung Chen, Yu Tsao, Jagadeesh Balam, Boris Ginsburg, Sabato Marco Siniscalchi, Eng Siong Chng, Peter Bell, Catherine Lai, Shinji Watanabe, Andreas Stolcke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given recent advances in generative AI technology, a key question is how
large language models (LLMs) can enhance acoustic modeling tasks using text
decoding results from a frozen, pretrained automatic speech recognition (ASR)
model. To explore new capabilities in language modeling for speech processing,
we introduce the generative speech transcription error correction (GenSEC)
challenge. This challenge comprises three post-ASR language modeling tasks: (i)
post-ASR transcription correction, (ii) speaker tagging, and (iii) emotion
recognition. These tasks aim to emulate future LLM-based agents handling
voice-based interfaces while remaining accessible to a broad audience by
utilizing open pretrained language models or agent-based APIs. We also discuss
insights from baseline evaluations, as well as lessons learned for designing
future evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE SLT 2024. The initial draft version has been done in December
  2023. Post-ASR Text Processing and Understanding Community and LlaMA-7B
  pre-training correction model:
  https://huggingface.co/GenSEC-LLM/SLT-Task1-Llama2-7b-HyPo-baseline</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WaterMax: breaking the LLM watermark detectability-robustness-quality
  trade-off 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04808v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04808v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eva Giboulot, Teddy Furon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking is a technical means to dissuade malfeasant usage of Large
Language Models. This paper proposes a novel watermarking scheme, so-called
WaterMax, that enjoys high detectability while sustaining the quality of the
generated text of the original LLM. Its new design leaves the LLM untouched (no
modification of the weights, logits, temperature, or sampling technique).
WaterMax balances robustness and complexity contrary to the watermarking
techniques of the literature inherently provoking a trade-off between quality
and robustness. Its performance is both theoretically proven and experimentally
validated. It outperforms all the SotA techniques under the most complete
benchmark suite. Code available at https://github.com/eva-giboulot/WaterMax.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Satellite Non-IID Imagery: A Spectral Clustering-Assisted
  Federated Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13602v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13602v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luyao Zou, Yu Min Park, Chu Myaet Thwal, Yan Kyaw Tun, Zhu Han, Choong Seon Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low Earth orbit (LEO) satellites are capable of gathering abundant Earth
observation data (EOD) to enable different Internet of Things (IoT)
applications. However, to accomplish an effective EOD processing mechanism, it
is imperative to investigate: 1) the challenge of processing the observed data
without transmitting those large-size data to the ground because the connection
between the satellites and the ground stations is intermittent, and 2) the
challenge of processing the non-independent and identically distributed
(non-IID) satellite data. In this paper, to cope with those challenges, we
propose an orbit-based spectral clustering-assisted clustered federated
self-knowledge distillation (OSC-FSKD) approach for each orbit of an LEO
satellite constellation, which retains the advantage of FL that the observed
data does not need to be sent to the ground. Specifically, we introduce
normalized Laplacian-based spectral clustering (NLSC) into federated learning
(FL) to create clustered FL in each round to address the challenge resulting
from non-IID data. Particularly, NLSC is adopted to dynamically group clients
into several clusters based on cosine similarities calculated by model updates.
In addition, self-knowledge distillation is utilized to construct each local
client, where the most recent updated local model is used to guide current
local model training. Experiments demonstrate that the observation accuracy
obtained by the proposed method is separately 1.01x, 2.15x, 1.10x, and 1.03x
higher than that of pFedSD, FedProx, FedAU, and FedALA approaches using the
SAT4 dataset. The proposed method also shows superiority when using other
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Node Identifiers: Compact, Discrete Representations for Efficient Graph
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16435v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16435v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuankai Luo, Hongkang Li, Qijiong Liu, Lei Shi, Xiao-Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel end-to-end framework that generates highly compact
(typically 6-15 dimensions), discrete (int4 type), and interpretable node
representations, termed node identifiers (node IDs), to tackle inference
challenges on large-scale graphs. By employing vector quantization, we compress
continuous node embeddings from multiple layers of a Graph Neural Network (GNN)
into discrete codes, applicable under both self-supervised and supervised
learning paradigms. These node IDs capture high-level abstractions of graph
data and offer interpretability that traditional GNN embeddings lack. Extensive
experiments on 34 datasets, encompassing node classification, graph
classification, link prediction, and attributed graph clustering tasks,
demonstrate that the generated node IDs significantly enhance speed and memory
efficiency while achieving competitive performance compared to current
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ScoreFusion: fusing score-based generative models via Kullback-Leibler
  barycenters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19619v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19619v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liu, Junze Tony Ye, Jose Blanchet, Nian Si
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ScoreFusion, a theoretically grounded method for fusing multiple
pre-trained diffusion models that are assumed to generate from auxiliary
populations. ScoreFusion is particularly useful for enhancing the generative
modeling of a target population with limited observed data. Our starting point
considers the family of KL barycenters of the auxiliary populations, which is
proven to be an optimal parametric class in the KL sense, but difficult to
learn. Nevertheless, by recasting the learning problem as score matching in
denoising diffusion, we obtain a tractable way of computing the optimal KL
barycenter weights. We prove a dimension-free sample complexity bound in total
variation distance, provided that the auxiliary models are well fitted for
their own task and the auxiliary tasks combined capture the target well. We
also explain a connection of the practice of checkpoint merging in AI art
creation to an approximation of our KL-barycenter-based fusion approach.
However, our fusion method differs in key aspects, allowing generation of new
populations, as we illustrate in experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>53 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedSN: A Federated Learning Framework over Heterogeneous LEO Satellite
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01483v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01483v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Lin, Zhe Chen, Zihan Fang, Xianhao Chen, Xiong Wang, Yue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a large number of Low Earth Orbit (LEO) satellites have been
launched and deployed successfully in space by commercial companies, such as
SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve
not only for communication but also for various machine learning applications,
such as space modulation recognition, remote sensing image classification, etc.
However, the ground station (GS) may be incapable of downloading such a large
volume of raw sensing data for centralized model training due to the limited
contact time with LEO satellites (e.g. 5 minutes). Therefore, federated
learning (FL) has emerged as the promising solution to address this problem via
on-device training. Unfortunately, to enable FL on LEO satellites, we still
face three critical challenges that are i) heterogeneous computing and memory
capabilities, ii) limited uplink rate, and iii) model staleness. To this end,
we propose FedSN as a general FL framework to tackle the above challenges, and
fully explore data diversity on LEO satellites. Specifically, we first present
a novel sub-structure scheme to enable heterogeneous local model training
considering different computing, memory, and communication constraints on LEO
satellites. Additionally, we propose a pseudo-synchronous model aggregation
strategy to dynamically schedule model aggregation for compensating model
staleness. To further demonstrate the effectiveness of the FedSN, we evaluate
it using space modulation recognition and remote sensing image classification
tasks by leveraging the data from real-world satellite networks. Extensive
experimental results demonstrate that FedSN framework achieves higher accuracy,
lower computing, and communication overhead than the state-of-the-art
benchmarks and the effectiveness of each components in FedSN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Social Dynamics of Consumer Response: A Unified Framework Integrating
  Statistical Physics and Marketing Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Marin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding how consumers react to advertising inputs is essential for
marketers aiming to optimize advertising strategies and improve campaign
effectiveness. This study examines the complex nature of consumer behaviour by
applying theoretical frameworks derived from physics and social psychology. We
present an innovative equation that captures the relation between spending on
advertising and consumer response, using concepts such as symmetries, scaling
laws, and phase transitions. By validating our equation against well-known
models such as the Michaelis-Menten and Hill equations, we prove its
effectiveness in accurately representing the complexity of consumer response
dynamics. The analysis emphasizes the importance of key model parameters, such
as marketing effectiveness, response sensitivity, and behavioural sensitivity,
in influencing consumer behaviour. The work explores the practical implications
for advertisers and marketers, as well as discussing the limitations and future
research directions. In summary, this study provides a thorough framework for
comprehending and forecasting consumer reactions to advertising, which has
implications for optimizing advertising strategies and allocating resources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Theories of synaptic memory consolidation and intelligent plasticity for
  continual learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16922v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16922v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Friedemann Zenke, Axel Laborieux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans and animals learn throughout life. Such continual learning is crucial
for intelligence. In this chapter, we examine the pivotal role plasticity
mechanisms with complex internal synaptic dynamics could play in enabling this
ability in neural networks. By surveying theoretical research, we highlight two
fundamental enablers for continual learning. First, synaptic plasticity
mechanisms must maintain and evolve an internal state over several behaviorally
relevant timescales. Second, plasticity algorithms must leverage the internal
state to intelligently regulate plasticity at individual synapses to facilitate
the seamless integration of new memories while avoiding detrimental
interference with existing ones. Our chapter covers successful applications of
these principles to deep neural networks and underscores the significance of
synaptic metaplasticity in sustaining continual learning capabilities. Finally,
we outline avenues for further research to understand the brain's superb
continual learning abilities and harness similar mechanisms for artificial
intelligence systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An introductory-level book chapter. 35 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallel Backpropagation for Inverse of a Convolution with Application
  to Normalizing Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandeep Nagar, Girish Varma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse of an invertible convolution is an important operation that comes up
in Normalizing Flows, Image Deblurring, etc. The naive algorithm for
backpropagation of this operation using Gaussian elimination has running time
$O(n^3)$ where $n$ is the number of pixels in the image. We give a fast
parallel backpropagation algorithm with running time $O(\sqrt{n})$ for a square
image and provide a GPU implementation of the same. Inverse Convolutions are
usually used in Normalizing Flows in the sampling pass, making them slow. We
propose to use Inverse Convolutions in the forward (image to latent vector)
pass of the Normalizing flow. Since the sampling pass is the inverse of the
forward pass, it will use convolutions only, resulting in efficient sampling
times. We use our parallel backpropagation algorithm for optimizing the inverse
convolution layer resulting in fast training times also. We implement this
approach in various Normalizing Flow backbones, resulting in our Inverse-Flow
models. We benchmark Inverse-Flow on standard datasets and show significantly
improved sampling times with similar bits per dimension compared to previous
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RA-BLIP: <span class="highlight-title">Multimodal</span> Adaptive Retrieval-Augmented Bootstrapping
  Language-Image <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhe Ding, Yang Ma, Pengda Qin, Jianlong Wu, Yuhong Li, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have recently received substantial
interest, which shows their emerging potential as general-purpose models for
various vision-language tasks. MLLMs involve significant external knowledge
within their parameters; however, it is challenging to continually update these
models with the latest knowledge, which involves huge computational costs and
poor interpretability. Retrieval augmentation techniques have proven to be
effective plugins for both LLMs and MLLMs. In this study, we propose multimodal
adaptive Retrieval-Augmented Bootstrapping Language-Image Pre-training
(RA-BLIP), a novel retrieval-augmented framework for various MLLMs. Considering
the redundant information within vision modality, we first leverage the
question to instruct the extraction of visual information through interactions
with one set of learnable queries, minimizing irrelevant interference during
retrieval and generation. Besides, we introduce a pre-trained multimodal
adaptive fusion module to achieve question text-to-multimodal retrieval and
integration of multimodal knowledge by projecting visual and language
modalities into a unified semantic space. Furthermore, we present an Adaptive
Selection Knowledge Generation (ASKG) strategy to train the generator to
autonomously discern the relevance of retrieved knowledge, which realizes
excellent denoising performance. Extensive experiments on open multimodal
question-answering datasets demonstrate that RA-BLIP achieves significant
performance and surpasses the state-of-the-art retrieval-augmented models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Movie101v2: Improved Movie Narration Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Yue, Yepeng Zhang, Ziheng Wang, Qin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic movie narration aims to generate video-aligned plot descriptions to
assist visually impaired audiences. Unlike standard video captioning, it
involves not only describing key visual details but also inferring plots that
unfold across multiple movie shots, presenting distinct and complex challenges.
To advance this field, we introduce Movie101v2, a large-scale, bilingual
dataset with enhanced data quality specifically designed for movie narration.
Revisiting the task, we propose breaking down the ultimate goal of automatic
movie narration into three progressive stages, offering a clear roadmap with
corresponding evaluation metrics. Based on our new benchmark, we baseline a
range of large vision-language models, including GPT-4V, and conduct an
in-depth analysis of the challenges in narration generation. Our findings
highlight that achieving applicable movie narration generation is a fascinating
goal that requires significant research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perceptual Quality Assessment of Octree-RAHT Encoded 3D Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongshuai Duan, Honglei Su, Qi Liu, Hui Yuan, Wei Gao, Jiarun Song, Zhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  No-reference bitstream-layer point cloud quality assessment (PCQA) can be
deployed without full decoding at any network node to achieve real-time quality
monitoring. In this work, we focus on the PCQA problem dedicated to Octree-RAHT
encoding mode. First, to address the issue that existing PCQA databases have a
small scale and limited distortion levels, we establish the WPC5.0 database
which is the first one dedicated to Octree-RAHT encoding mode with a scale of
400 distorted point clouds (PCs) including 4 geometric multiplied by 5 attitude
distortion levels. Then, we propose the first PCQA model dedicated to
Octree-RAHT encoding mode by parsing PC bitstreams without full decoding. The
model introduces texture bitrate (TBPP) to predict texture complexity (TC) and
further derives the texture distortion factor. In addition, the Geometric
Quantization Parameter (PQS) is used to estimate the geometric distortion
factor, which is then integrated into the model along with the texture
distortion factor to obtain the proposed PCQA model named streamPCQ-OR. The
proposed model has been compared with other advanced PCQA methods on the
WPC5.0, BASICS and M-PCCD databases, and experimental results show that our
model has excellent performance while having very low computational complexity,
providing a reliable choice for time-critical applications. To facilitate
subsequent research, the database and source code will be publicly released at
https://github.com/qdushl/Waterloo-Point-Cloud-Database-5.0.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangru Zhu, Penglei Sun, Yaoxian Song, Yanghua Xiao, Zhixu Li, Chengyu Wang, Jun Huang, Bei Yang, Xiaoxiao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate interpretation and visualization of human instructions are crucial
for text-to-image (T2I) synthesis. However, current models struggle to capture
semantic variations from word order changes, and existing evaluations, relying
on indirect metrics like text-image similarity, fail to reliably assess these
challenges. This often obscures poor performance on complex or uncommon
linguistic patterns by the focus on frequent word combinations. To address
these deficiencies, we propose a novel metric called SemVarEffect and a
benchmark named SemVarBench, designed to evaluate the causality between
semantic variations in inputs and outputs in T2I synthesis. Semantic variations
are achieved through two types of linguistic permutations, while avoiding
easily predictable literal variations. Experiments reveal that the
CogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.
Semantic variations in object relations are less understood than attributes,
scoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in
UNet or Transformers plays a crucial role in handling semantic variations, a
factor previously overlooked by a focus on textual encoders. Our work
establishes an effective evaluation framework that advances the T2I synthesis
community's exploration of human instruction understanding. Our benchmark and
code are available at https://github.com/zhuxiangru/SemVarBench .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The only change in the current version update is the replacement of
  the template with a more precise one</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13754v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13754v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjie Ni, Yifan Song, Deepanway Ghosal, Bo Li, David Junhao Zhang, Xiang Yue, Fuzhao Xue, Zian Zheng, Kaichen Zhang, Mahir Shah, Kabir Jain, Yang You, Michael Shieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceiving and generating diverse modalities are crucial for AI models to
effectively learn from and engage with real-world signals, necessitating
reliable evaluations for their development. We identify two major issues in
current evaluations: (1) inconsistent standards, shaped by different
communities with varying protocols and maturity levels; and (2) significant
query, grading, and generalization biases. To address these, we introduce
MixEval-X, the first any-to-any, real-world benchmark designed to optimize and
standardize evaluations across diverse input and output modalities. We propose
multi-modal benchmark mixture and adaptation-rectification pipelines to
reconstruct real-world task distributions, ensuring evaluations generalize
effectively to real-world use cases. Extensive meta-evaluations show our
approach effectively aligns benchmark samples with real-world task
distributions. Meanwhile, MixEval-X's model rankings correlate strongly with
that of crowd-sourced real-world evaluations (up to 0.98) while being much more
efficient. We provide comprehensive leaderboards to rerank existing models and
organizations and offer insights to enhance understanding of multi-modal
evaluations and inform future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthesizing Sentiment-Controlled Feedback For <span class="highlight-title">Multimodal</span> Text and Image
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07640v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07640v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puneet Kumar, Sarthak Malik, Balasubramanian Raman, Xiaobai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to generate sentiment-controlled feedback in response to
multimodal inputs comprising text and images addresses a critical gap in
human-computer interaction. This capability allows systems to provide
empathetic, accurate, and engaging responses, with useful applications in
education, healthcare, marketing, and customer service. To this end, we have
constructed a large-scale Controllable Multimodal Feedback Synthesis (CMFeed)
dataset and propose a controllable feedback synthesis system. The system
features an encoder, decoder, and controllability block for textual and visual
inputs. It extracts features using a transformer and Faster R-CNN networks,
combining them to generate feedback. The CMFeed dataset includes images, texts,
reactions to the posts, human comments with relevance scores, and reactions to
these comments. These reactions train the model to produce feedback with
specified sentiments, achieving a sentiment classification accuracy of 77.23\%,
which is 18.82\% higher than the accuracy without controllability. The system
also incorporates a similarity module for assessing feedback relevance through
rank-based metrics and an interpretability technique to analyze the
contributions of textual and visual features during feedback generation. Access
to the CMFeed dataset and the system's code is available at
https://github.com/MIntelligence-Group/CMFeed.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-17T00:00:00Z">2024-10-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Numerical Precision Affects Mathematical Reasoning Capabilities of
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guhao Feng, Kai Yang, Yuntian Gu, Xinyue Ai, Shengjie Luo, Jiacheng Sun, Di He, Zhenguo Li, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable success of Transformer-based Large Language Models
(LLMs) across various domains, understanding and enhancing their mathematical
capabilities remains a significant challenge. In this paper, we conduct a
rigorous theoretical analysis of LLMs' mathematical abilities, with a specific
focus on their arithmetic performances. We identify numerical precision as a
key factor that influences their effectiveness in mathematical tasks. Our
results show that Transformers operating with low numerical precision fail to
address arithmetic tasks, such as iterated addition and integer multiplication,
unless the model size grows super-polynomially with respect to the input
length. In contrast, Transformers with standard numerical precision can
efficiently handle these tasks with significantly smaller model sizes. We
further support our theoretical findings through empirical experiments that
explore the impact of varying numerical precision on arithmetic tasks,
providing valuable insights for improving the mathematical reasoning
capabilities of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can MLLMs Understand the Deep Implication Behind Chinese Images? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhao Zhang, Xi Feng, Yuelin Bai, Xinrun Du, Jinchang Hou, Kaixin Deng, Guangzeng Han, Qinrui Li, Bingli Wang, Jiaheng Liu, Xingwei Qu, Yifei Zhang, Qixuan Zhao, Yiming Liang, Ziqiang Liu, Feiteng Fang, Min Yang, Wenhao Huang, Chenghua Lin, Ge Zhang, Shiwen Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the capabilities of Multimodal Large Language Models (MLLMs) continue to
improve, the need for higher-order capability evaluation of MLLMs is
increasing. However, there is a lack of work evaluating MLLM for higher-order
perception and understanding of Chinese visual content. To fill the gap, we
introduce the **C**hinese **I**mage **I**mplication understanding
**Bench**mark, **CII-Bench**, which aims to assess the higher-order perception
and understanding capabilities of MLLMs for Chinese images. CII-Bench stands
out in several ways compared to existing benchmarks. Firstly, to ensure the
authenticity of the Chinese context, images in CII-Bench are sourced from the
Chinese Internet and manually reviewed, with corresponding answers also
manually crafted. Additionally, CII-Bench incorporates images that represent
Chinese traditional culture, such as famous Chinese traditional paintings,
which can deeply reflect the model's understanding of Chinese traditional
culture. Through extensive experiments on CII-Bench across multiple MLLMs, we
have made significant findings. Initially, a substantial gap is observed
between the performance of MLLMs and humans on CII-Bench. The highest accuracy
of MLLMs attains 64.4%, where as human accuracy averages 78.2%, peaking at an
impressive 81.0%. Subsequently, MLLMs perform worse on Chinese traditional
culture images, suggesting limitations in their ability to understand
high-level semantics and lack a deep knowledge base of Chinese traditional
culture. Finally, it is observed that most models exhibit enhanced accuracy
when image emotion hints are incorporated into the prompts. We believe that
CII-Bench will enable MLLMs to gain a better understanding of Chinese semantics
and Chinese-specific images, advancing the journey towards expert artificial
general intelligence (AGI). Our project is publicly available at
https://cii-bench.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages,18 figures. Project Page: https://cii-bench.github.io/ Code:
  https://github.com/MING_X/CII-Bench Dataset:
  https://huggingface.co/datasets/m-a-p/CII-Bench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrospective Learning from Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zizhao Chen, Mustafa Omer Gul, Yiwei Chen, Gloria Geng, Anne Wu, Yoav Artzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-turn interactions between large language models (LLMs) and users
naturally include implicit feedback signals. If an LLM responds in an
unexpected way to an instruction, the user is likely to signal it by rephrasing
the request, expressing frustration, or pivoting to an alternative task. Such
signals are task-independent and occupy a relatively constrained subspace of
language, allowing the LLM to identify them even if it fails on the actual
task. This creates an avenue for continually learning from interactions without
additional annotations. We introduce ReSpect, a method to learn from such
signals in past interactions via retrospection. We deploy ReSpect in a new
multimodal interaction scenario, where humans instruct an LLM to solve an
abstract reasoning task with a combinatorial solution space. Through thousands
of interactions with humans, we show how ReSpect gradually improves task
completion rate from 31% to 82%, all without any external annotation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Janus: Decoupling Visual Encoding for Unified <span class="highlight-title">Multimodal</span> Understanding
  and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Janus, an autoregressive framework that unifies
multimodal understanding and generation. Prior research often relies on a
single visual encoder for both tasks, such as Chameleon. However, due to the
differing levels of information granularity required by multimodal
understanding and generation, this approach can lead to suboptimal performance,
particularly in multimodal understanding. To address this issue, we decouple
visual encoding into separate pathways, while still leveraging a single,
unified transformer architecture for processing. The decoupling not only
alleviates the conflict between the visual encoder's roles in understanding and
generation, but also enhances the framework's flexibility. For instance, both
the multimodal understanding and generation components can independently select
their most suitable encoding methods. Experiments show that Janus surpasses
previous unified model and matches or exceeds the performance of task-specific
models. The simplicity, high flexibility, and effectiveness of Janus make it a
strong candidate for next-generation unified multimodal models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Zhang, Cunxiao Du, Chao Du, Tianyu Pang, Wei Gao, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have extended their
capabilities to handle long contexts. However, increasing the number of model
layers and the length of input sequences significantly escalates the memory
required to store key-value (KV) cache, posing challenges for efficient
inference. To mitigate this issue, we present SimLayerKV, a simple yet
effective method that reduces inter-layer KV cache redundancies by selectively
dropping cache in identified lazy layers. Our approach is based on the
observation that certain layers in long-context LLMs exhibit "lazy" behavior,
contributing less to modeling long-range dependencies compared to non-lazy
layers. By analyzing attention weight patterns, we find that the behavior of
these lazy layers is consistent across tokens during generation for a given
input. This insight motivates our SimLayerKV, which identifies lazy layers and
reduces their KV cache accordingly. SimLayerKV is training-free, generalizable,
and can be implemented with only seven lines of code. We conduct extensive
experiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and
Mistral-7B across 16 tasks from the LongBench benchmark. The results
demonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\times$
with only a 1.2% performance drop when combined with 4-bit quantization. Our
code is available at https://github.com/sail-sg/SimLayerKV.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified View of Delta Parameter Editing in Post-Trained Large-Scale
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaoyu Tang, Le Yu, Bowen Yu, Hongyu Lin, Keming Lu, Yaojie Lu, Xianpei Han, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training has emerged as a crucial paradigm for adapting large-scale
pre-trained models to various tasks, whose effects are fully reflected by delta
parameters (i.e., the disparity between post-trained and pre-trained
parameters). While numerous studies have explored delta parameter properties
via operations like pruning, quantization, low-rank approximation, and
extrapolation, a unified framework for systematically examining these
characteristics has been lacking. In this paper, we propose a novel perspective
based on Riemann sum approximation of the loss function to elucidate delta
parameter editing operations. Our analysis categorizes existing methods into
three classes based on their post-editing performance: competitive, decreased,
and improved, explaining how they are expressed by the Riemann sum
approximation term and how they alter the model performance. Extensive
experiments on both visual and language models, including ViT, LLaMA 3, Qwen 2,
and Mistral, corroborate our theoretical findings. Furthermore, we introduce
extensions to existing techniques like DARE and BitDelta, highlighting their
limitations in leveraging the properties of delta parameters and reorganizing
them into general expressions to enhance the applicability and effectiveness of
delta parameter editing in post-trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Common Pitfall of Margin-based Language Model Alignment: Gradient
  Entanglement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Yuan, Yifan Zeng, Yue Wu, Huazheng Wang, Mengdi Wang, Liu Leqi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback (RLHF) has become the predominant
approach for language model (LM) alignment. At its core, RLHF uses a
margin-based loss for preference optimization, specifying ideal LM behavior
only by the difference between preferred and dispreferred responses. In this
paper, we identify a common pitfall of margin-based methods -- the
under-specification of ideal LM behavior on preferred and dispreferred
responses individually, which leads to two unintended consequences as the
margin increases: (1) The probability of dispreferred (e.g., unsafe) responses
may increase, resulting in potential safety alignment failures. (2) The
probability of preferred responses may decrease, even when those responses are
ideal. We demystify the reasons behind these problematic behaviors:
margin-based losses couple the change in the preferred probability to the
gradient of the dispreferred one, and vice versa, often preventing the
preferred probability from increasing while the dispreferred one decreases, and
thus causing a synchronized increase or decrease in both probabilities. We term
this effect, inherent in margin-based objectives, gradient entanglement.
Formally, we derive conditions for general margin-based alignment objectives
under which gradient entanglement becomes concerning: the inner product of the
gradients of preferred and dispreferred log-probabilities is large relative to
the individual gradient norms. We theoretically investigate why such inner
products can be large when aligning language models and empirically validate
our findings. Empirical implications of our framework extend to explaining
important differences in the training dynamics of various preference
optimization algorithms, and suggesting potential algorithm designs to mitigate
the under-specification issue of margin-based methods and thereby improving
language model alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Yang, Yao Liu, Sapana Chaudhary, Rasool Fakoor, Pratik Chaudhari, George Karypis, Huzefa Rangwala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomy via agents using large language models (LLMs) for personalized,
standardized tasks boosts human efficiency. Automating web tasks (like booking
hotels within a budget) is increasingly sought after. Fulfilling practical
needs, the web agent also serves as an important proof-of-concept example for
various agent grounding scenarios, with its success promising advancements in
many future applications. Prior research often handcrafts web agent strategies
(e.g., prompting templates, multi-agent systems, search methods, etc.) and the
corresponding in-context examples, which may not generalize well across all
real-world scenarios. On the other hand, there has been limited study on the
misalignment between a web agent's observation/action representation and the
pre-training data of the LLM it's based on. This discrepancy is especially
notable when LLMs are primarily trained for language completion rather than
tasks involving embodied navigation actions and symbolic web elements. Our
study enhances an LLM-based web agent by simply refining its observation and
action space to better align with the LLM's capabilities. This approach enables
our base agent to significantly outperform previous methods on a wide variety
of web tasks. Specifically, on WebArena, a benchmark featuring general-purpose
web interaction tasks, our agent AgentOccam surpasses the previous
state-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute
points respectively, and boosts the success rate by 26.6 points (+161%) over
similar plain web agents with its observation and action space alignment. We
achieve this without using in-context examples, new agent roles, online
feedback or search strategies. AgentOccam's simple design highlights LLMs'
impressive zero-shot performance on web tasks, and underlines the critical role
of carefully tuning observation and action spaces for LLM-based agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing Webpage UIs for Text-Rich Visual Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, Xiang Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-rich visual understanding-the ability to process environments where
dense textual content is integrated with visuals-is crucial for multimodal
large language models (MLLMs) to interact effectively with structured
environments. To enhance this capability, we propose synthesizing general
multimodal instructions from webpage UIs using text-based large language models
(LLMs). Despite lacking direct visual input, text-based LLMs are able to
process structured text representations from webpage accessibility trees. These
instructions are then paired with UI screenshots to train multimodal models. We
introduce MultiUI, a dataset containing 7.3 million samples from 1 million
websites, covering diverse multimodal tasks and UI layouts. Models trained on
MultiUI not only excel in web UI tasks-achieving up to a 48\% improvement on
VisualWebBench and a 19.1\% boost in action accuracy on a web agent dataset
Mind2Web-but also generalize surprisingly well to non-web UI tasks and even to
non-UI domains, such as document understanding, OCR, and chart interpretation.
These results highlight the broad applicability of web UI data for advancing
text-rich visual understanding across various scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ De-mark: Watermark Removal in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruibo Chen, Yihan Wu, Junfeng Guo, Heng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking techniques offer a promising way to identify machine-generated
content via embedding covert information into the contents generated from
language models (LMs). However, the robustness of the watermarking schemes has
not been well explored. In this paper, we present De-mark, an advanced
framework designed to remove n-gram-based watermarks effectively. Our method
utilizes a novel querying strategy, termed random selection probing, which aids
in assessing the strength of the watermark and identifying the red-green list
within the n-gram watermark. Experiments on popular LMs, such as Llama3 and
ChatGPT, demonstrate the efficiency and effectiveness of De-mark in watermark
removal and exploitation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Watermark for Order-Agnostic Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruibo Chen, Yihan Wu, Yanshuo Chen, Chenxi Liu, Junfeng Guo, Heng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Statistical watermarking techniques are well-established for sequentially
decoded language models (LMs). However, these techniques cannot be directly
applied to order-agnostic LMs, as the tokens in order-agnostic LMs are not
generated sequentially. In this work, we introduce Pattern-mark, a
pattern-based watermarking framework specifically designed for order-agnostic
LMs. We develop a Markov-chain-based watermark generator that produces
watermark key sequences with high-frequency key patterns. Correspondingly, we
propose a statistical pattern-based detection algorithm that recovers the key
sequence during detection and conducts statistical tests based on the count of
high-frequency patterns. Our extensive evaluations on order-agnostic LMs, such
as ProteinMPNN and CMLM, demonstrate Pattern-mark's enhanced detection
efficiency, generation quality, and robustness, positioning it as a superior
watermarking technique for order-agnostic LMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BenTo: Benchmark Task Reduction with In-Context Transferability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Zhao, Ming Li, Lichao Sun, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating large language models (LLMs) is costly: it requires the generation
and examination of LLM outputs on a large-scale benchmark of various tasks.
This paper investigates how to efficiently reduce the tasks used to benchmark
LLMs without affecting the evaluation quality. Our study reveals that task
transferability and relevance provide critical information to identify the most
representative subset of tasks via optimizing a facility location function. We
propose a practically efficient metric for estimating the transferability
between two tasks via in-context learning (ICL). By analyzing the pairwise
transferability, we can reduce tasks in a modern LLM benchmark (e.g., MMLU or
FLAN) to 5% while inducing only a <4% difference to the evaluation on the
original benchmark. Compared to prior works, our method is training-free,
gradient-free, and highly efficient requiring ICL only.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying
  Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael J. Q. Zhang, W. Bradley Knox, Eunsol Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) must often respond to highly ambiguous user
requests. In such cases, the LLM's best response may be to ask a clarifying
question to elicit more information. We observe existing LLMs often respond by
presupposing a single interpretation of such ambiguous requests, frustrating
users who intended a different interpretation. We speculate this is caused by
current preference data labeling practice, where LLM responses are evaluated
only on their prior contexts. To address this, we propose to assign preference
labels by simulating their expected outcomes in the future turns. This allows
LLMs to learn to ask clarifying questions when it can generate responses that
are tailored to each user interpretation in future turns. In experiments on
open-domain QA, we compare systems that trained using our proposed preference
labeling methods against standard methods, which assign preferences based on
only prior context. We evaluate systems based on their ability to ask
clarifying questions that can recover each user's interpretation and expected
answer, and find that our training with our proposed method trains LLMs to ask
clarifying questions with a 5% improvement in F1 measured against the answer
set from different interpretations of each query
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Looking Inward: Language Models Can Learn About Themselves by
  Introspection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix J Binder, James Chua, Tomek Korbak, Henry Sleight, John Hughes, Robert Long, Ethan Perez, Miles Turpin, Owain Evans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans acquire knowledge by observing the external world, but also by
introspection. Introspection gives a person privileged access to their current
state of mind (e.g., thoughts and feelings) that is not accessible to external
observers. Can LLMs introspect? We define introspection as acquiring knowledge
that is not contained in or derived from training data but instead originates
from internal states. Such a capability could enhance model interpretability.
Instead of painstakingly analyzing a model's internal workings, we could simply
ask the model about its beliefs, world models, and goals. More speculatively,
an introspective model might self-report on whether it possesses certain
internal states such as subjective feelings or desires and this could inform us
about the moral status of these states. Such self-reports would not be entirely
dictated by the model's training data.
  We study introspection by finetuning LLMs to predict properties of their own
behavior in hypothetical scenarios. For example, "Given the input P, would your
output favor the short- or long-term option?" If a model M1 can introspect, it
should outperform a different model M2 in predicting M1's behavior even if M2
is trained on M1's ground-truth behavior. The idea is that M1 has privileged
access to its own behavioral tendencies, and this enables it to predict itself
better than M2 (even if M2 is generally stronger).
  In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to
predict itself), we find that the model M1 outperforms M2 in predicting itself,
providing evidence for introspection. Notably, M1 continues to predict its
behavior accurately even after we intentionally modify its ground-truth
behavior. However, while we successfully elicit introspection on simple tasks,
we are unsuccessful on more complex tasks or those requiring
out-of-distribution generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PopAlign: Diversifying Contrasting Patterns for a More Comprehensive
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Moore Wang, Shawn Wang, Kang Zhu, Jiaheng Liu, Ke Xu, Jie Fu, Wangchunshu Zhou, Wenhao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alignment of large language models (LLMs) involves training models on
preference-contrastive output pairs to adjust their responses according to
human preferences. To obtain such contrastive pairs, traditional methods like
RLHF and RLAIF rely on limited contrasting patterns, such as varying model
variants or decoding temperatures. This singularity leads to two issues: (1)
alignment is not comprehensive; and thereby (2) models are susceptible to
jailbreaking attacks. To address these issues, we investigate how to construct
more comprehensive and diversified contrasting patterns to enhance preference
data (RQ1) and verify the impact of the diversification of contrasting patterns
on model alignment (RQ2). For RQ1, we propose PopAlign, a framework that
integrates diversified contrasting patterns across the prompt, model, and
pipeline levels, introducing six contrasting strategies that do not require
additional feedback labeling procedures. Regarding RQ2, we conduct thorough
experiments demonstrating that PopAlign significantly outperforms existing
methods, leading to more comprehensive alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantity vs. Quality of Monolingual Source Data in Automatic Text
  Translation: Can It Be Too Little If It Is Too Good? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idris Abdulmumin, Bashir Shehu Galadanci, Garba Aliyu, Shamsuddeen Hassan Muhammad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monolingual data, being readily available in large quantities, has been used
to upscale the scarcely available parallel data to train better models for
automatic translation. Self-learning, where a model is made to learn from its
output, is one approach to exploit such data. However, it has been shown that
too much of this data can be detrimental to the performance of the model if the
available parallel data is comparatively extremely low. In this study, we
investigate whether the monolingual data can also be too little and if this
reduction, based on quality, has any effect on the performance of the
translation model. Experiments have shown that on English-German low-resource
NMT, it is often better to select only the most useful additional data, based
on quality or closeness to the domain of the test data, than utilizing all of
the available data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Quantization for Matrix Multiplication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Or Ordentlich, Yury Polyanskiy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work in machine learning community proposed multiple methods for
performing lossy compression (quantization) of large matrices. This
quantization is important for accelerating matrix multiplication (main
component of large language models), which is often bottlenecked by the speed
of loading these matrices from memory. Unlike classical vector quantization and
rate-distortion theory, the goal of these new compression algorithms is to be
able to approximate not the matrices themselves, but their matrix product.
Specifically, given a pair of real matrices $A,B$ an encoder (compressor) is
applied to each of them independently producing descriptions with $R$ bits per
entry. These representations subsequently are used by the decoder to estimate
matrix product $A^\top B$. In this work, we provide a non-asymptotic lower
bound on the mean squared error of this approximation (as a function of rate
$R$) for the case of matrices $A,B$ with iid Gaussian entries. Algorithmically,
we construct a universal quantizer based on nested lattices with an explicit
guarantee of approximation error for any (non-random) pair of matrices $A$, $B$
in terms of only Frobenius norms $\|A\|_F, \|B\|_F$ and $\|A^\top B\|_F$. For
iid Gaussian matrices our quantizer achieves the lower bound and is, thus,
asymptotically optimal. A practical low-complexity version of our quantizer
achieves performance quite close to optimal. In information-theoretic terms we
derive rate-distortion function for matrix multiplication of iid Gaussian
matrices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Mystery of the Pathological Path-star Task for Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arvid Frydenlund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recently introduced path-star task is a minimal task designed to
exemplify limitations to the abilities of language models (Bachmann and
Nagarajan, 2024). It involves a path-star graph where multiple arms radiate
from a single starting node and each node is unique. Given the start node and a
specified target node that ends an arm, the task is to generate the arm
containing that target node. This is straightforward for a human but
surprisingly difficult for language models, which did not outperform the random
baseline. The authors hypothesized this is due to a deficiency in
teacher-forcing and the next-token prediction paradigm.
  We demonstrate the task is learnable using teacher-forcing in alternative
settings and that the issue is partially due to representation. We introduce a
regularization method using structured samples of the same graph but with
differing target nodes, improving results across a variety of model types. We
provide RASP proofs showing the task is theoretically solvable. Finally, we
find settings where an encoder-only model can consistently solve the task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aggregation Artifacts in Subjective Tasks Collapse Large Language
  Models' Posteriors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Chochlakis, Alexandros Potamianos, Kristina Lerman, Shrikanth Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context Learning (ICL) has become the primary method for performing
natural language tasks with Large Language Models (LLMs). The knowledge
acquired during pre-training is crucial for this few-shot capability, providing
the model with task priors. However, recent studies have shown that ICL
predominantly relies on retrieving task priors rather than "learning" to
perform tasks. This limitation is particularly evident in complex subjective
domains such as emotion and morality, where priors significantly influence
posterior predictions. In this work, we examine whether this is the result of
the aggregation used in corresponding datasets, where trying to combine
low-agreement, disparate annotations might lead to annotation artifacts that
create detrimental noise in the prompt. Moreover, we evaluate the posterior
bias towards certain annotators by grounding our study in appropriate,
quantitative measures of LLM priors. Our results indicate that aggregation is a
confounding factor in the modeling of subjective tasks, and advocate focusing
on modeling individuals instead. However, aggregation does not explain the
entire gap between ICL and the state of the art, meaning other factors in such
tasks also account for the observed phenomena. Finally, by rigorously studying
annotator-level labels, we find that it is possible for minority annotators to
both better align with LLMs and have their perspectives further amplified.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-Aware Query Expansion with Large Language Models for Textual
  and Relational Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Xia, Junda Wu, Sungchul Kim, Tong Yu, Ryan A. Rossi, Haoliang Wang, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been used to generate query expansions
augmenting original queries for improving information search. Recent studies
also explore providing LLMs with initial retrieval results to generate query
expansions more grounded to document corpus. However, these methods mostly
focus on enhancing textual similarities between search queries and target
documents, overlooking document relations. For queries like "Find me a highly
rated camera for wildlife photography compatible with my Nikon F-Mount lenses",
existing methods may generate expansions that are semantically similar but
structurally unrelated to user intents. To handle such semi-structured queries
with both textual and relational requirements, in this paper we propose a
knowledge-aware query expansion framework, augmenting LLMs with structured
document relations from knowledge graph (KG). To further address the limitation
of entity-based scoring in existing KG-based methods, we leverage document
texts as rich KG node representations and use document-based relation filtering
for our Knowledge-Aware Retrieval (KAR). Extensive experiments on three
datasets of diverse domains show the advantages of our method compared against
state-of-the-art baselines on textual and relational semi-structured retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MobA: A Two-Level Agent System for Efficient Mobile Task Automation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Zhu, Hao Tang, Yansi Li, Kunyao Lan, Yixuan Jiang, Hao Zhou, Yixiao Wang, Situo Zhang, Liangtai Sun, Lu Chen, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current mobile assistants are limited by dependence on system APIs or
struggle with complex user instructions and diverse interfaces due to
restricted comprehension and decision-making abilities. To address these
challenges, we propose MobA, a novel Mobile phone Agent powered by multimodal
large language models that enhances comprehension and planning capabilities
through a sophisticated two-level agent architecture. The high-level Global
Agent (GA) is responsible for understanding user commands, tracking history
memories, and planning tasks. The low-level Local Agent (LA) predicts detailed
actions in the form of function calls, guided by sub-tasks and memory from the
GA. Integrating a Reflection Module allows for efficient task completion and
enables the system to handle previously unseen complex tasks. MobA demonstrates
significant improvements in task execution efficiency and completion rate in
real-life evaluations, underscoring the potential of MLLM-empowered mobile
assistants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 6 figures, and 5 tables. We will release our source code in
  a few days</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-Human Pipeline for Cultural Context Grounding of Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajkumar Pujari, Dan Goldwasser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversations often adhere to well-understood social norms that vary across
cultures. For example, while "addressing parents by name" is commonplace in the
West, it is rare in most Asian cultures. Adherence or violation of such norms
often dictates the tenor of conversations. Humans are able to navigate social
situations requiring cultural awareness quite adeptly. However, it is a hard
task for NLP models.
  In this paper, we tackle this problem by introducing a "Cultural Context
Schema" for conversations. It comprises (1) conversational information such as
emotions, dialogue acts, etc., and (2) cultural information such as social
norms, violations, etc. We generate ~110k social norm and violation
descriptions for ~23k conversations from Chinese culture using LLMs. We refine
them using automated verification strategies which are evaluated against
culturally aware human judgements. We organize these descriptions into
meaningful structures we call "Norm Concepts", using an interactive
human-in-loop framework. We ground the norm concepts and the descriptions in
conversations using symbolic annotation. Finally, we use the obtained dataset
for downstream tasks such as emotion, sentiment, and dialogue act detection. We
show that it significantly improves the empirical performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 9 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIRAGE-Bench: Automatic Multilingual Benchmark Arena for
  Retrieval-Augmented Generation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nandan Thakur, Suleman Kazi, Ge Luo, Jimmy Lin, Amin Ahmad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Retrieval-Augmented Generation (RAG) benchmarks rely on different
heuristic-based metrics for evaluation, but these require human preferences as
ground truth for reference. In contrast, arena-based benchmarks, where two
models compete each other, require an expensive Large Language Model (LLM) as a
judge for a reliable evaluation. We present an easy and efficient technique to
get the best of both worlds. The idea is to train a learning to rank model as a
"surrogate" judge using RAG-based evaluation heuristics as input, to produce a
synthetic arena-based leaderboard. Using this idea, We develop MIRAGE-Bench, a
standardized arena-based multilingual RAG benchmark for 18 diverse languages on
Wikipedia. The benchmark is constructed using MIRACL, a retrieval dataset, and
extended for multilingual generation evaluation. MIRAGE-Bench evaluates RAG
extensively coupling both heuristic features and LLM as a judge evaluator. In
our work, we benchmark 19 diverse multilingual-focused LLMs, and achieve a high
correlation (Kendall Tau ($\tau$) = 0.909) using our surrogate judge learned
using heuristic features with pairwise evaluations and between GPT-4o as a
teacher on the MIRAGE-Bench leaderboard using the Bradley-Terry framework. We
observe proprietary and large open-source LLMs currently dominate in
multilingual RAG. MIRAGE-Bench is available at:
https://github.com/vectara/mirage-bench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Role of Attention Heads in Large Language Model Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhong Zhou, Haiyang Yu, Xinghua Zhang, Rongwu Xu, Fei Huang, Kun Wang, Yang Liu, Junfeng Fang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) achieve state-of-the-art performance on multiple
language tasks, yet their safety guardrails can be circumvented, leading to
harmful generations. In light of this, recent research on safety mechanisms has
emerged, revealing that when safety representations or component are
suppressed, the safety capability of LLMs are compromised. However, existing
research tends to overlook the safety impact of multi-head attention
mechanisms, despite their crucial role in various model functionalities. Hence,
in this paper, we aim to explore the connection between standard attention
mechanisms and safety capability to fill this gap in the safety-related
mechanistic interpretability. We propose a novel metric which tailored for
multi-head attention, the Safety Head ImPortant Score (Ships), to assess the
individual heads' contributions to model safety. Based on this, we generalize
Ships to the dataset level and further introduce the Safety Attention Head
AttRibution Algorithm (Sahara) to attribute the critical safety attention heads
inside the model. Our findings show that the special attention head has a
significant impact on safety. Ablating a single safety head allows aligned
model (e.g., Llama-2-7b-chat) to respond to 16 times more harmful queries,
while only modifying 0.006% of the parameters, in contrast to the ~ 5%
modification required in previous studies. More importantly, we demonstrate
that attention heads primarily function as feature extractors for safety and
models fine-tuned from the same base model exhibit overlapping safety heads
through comprehensive experiments. Together, our attribution approach and
findings provide a novel perspective for unpacking the black box of safety
mechanisms within large models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 18 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unconstrained Model Merging for Enhanced LLM Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Zhang, Baoyi He, Shengyu Zhang, Yuhao Fu, Qi Zhou, Zhijie Sang, Zijin Hong, Kejing Yang, Wenjun Wang, Jianbo Yuan, Guangning Han, Linyi Li, Chunlin Ji, Fei Wu, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in building domain-specific large language models (LLMs)
have shown remarkable success, especially in tasks requiring reasoning
abilities like logical inference over complex relationships and multi-step
problem solving. However, creating a powerful all-in-one LLM remains
challenging due to the need for proprietary data and vast computational
resources. As a resource-friendly alternative, we explore the potential of
merging multiple expert models into a single LLM. Existing studies on model
merging mainly focus on generalist LLMs instead of domain experts, or the LLMs
under the same architecture and size. In this work, we propose an unconstrained
model merging framework that accommodates both homogeneous and heterogeneous
model architectures with a focus on reasoning tasks. A fine-grained layer-wise
weight merging strategy is designed for homogeneous models merging, while
heterogeneous model merging is built upon the probabilistic distribution
knowledge derived from instruction-response fine-tuning data. Across 7
benchmarks and 9 reasoning-optimized LLMs, we reveal key findings that
combinatorial reasoning emerges from merging which surpasses simple additive
effects. We propose that unconstrained model merging could serve as a
foundation for decentralized LLMs, marking a notable progression from the
existing centralized LLM framework. This evolution could enhance wider
participation and stimulate additional advancement in the field of artificial
intelligence, effectively addressing the constraints posed by centralized
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Design Space of Visual Context Representation in Video
  MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Du, Yuqi Huo, Kun Zhou, Zijia Zhao, Haoyu Lu, Han Huang, Wayne Xin Zhao, Bingning Wang, Weipeng Chen, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Multimodal Large Language Models (MLLMs) have shown remarkable
capability of understanding the video semantics on various downstream tasks.
Despite the advancements, there is still a lack of systematic research on
visual context representation, which refers to the scheme to select frames from
a video and further select the tokens from a frame. In this paper, we explore
the design space for visual context representation, and aim to improve the
performance of video MLLMs by finding more effective representation schemes.
Firstly, we formulate the task of visual context representation as a
constrained optimization problem, and model the language modeling loss as a
function of the number of frames and the number of embeddings (or tokens) per
frame, given the maximum visual context window size. Then, we explore the
scaling effects in frame selection and token selection respectively, and fit
the corresponding function curve by conducting extensive empirical experiments.
We examine the effectiveness of typical selection strategies and present
empirical findings to determine the two factors. Furthermore, we study the
joint effect of frame selection and token selection, and derive the optimal
formula for determining the two factors. We demonstrate that the derived
optimal settings show alignment with the best-performed results of empirical
experiments. Our code and model are available at:
https://github.com/RUCAIBox/Opt-Visor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Long Video MLLM; work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pose-Based Sign Language Appearance Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Moryossef, Gerard Sant, Zifan Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a method for transferring the signer's appearance in sign
language skeletal poses while preserving the sign content. Using estimated
poses, we transfer the appearance of one signer to another, maintaining natural
movements and transitions. This approach improves pose-based rendering and sign
stitching while obfuscating identity. Our experiments show that while the
method reduces signer identification accuracy, it slightly harms sign
recognition performance, highlighting a tradeoff between privacy and utility.
Our code is available at
\url{https://github.com/sign-language-processing/pose-anonymization}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World
  Multilingual Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varun Gumma, Anandhita Raghunath, Mohit Jain, Sunayana Sitaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing the capabilities and limitations of large language models (LLMs)
has garnered significant interest, yet the evaluation of multiple models in
real-world scenarios remains rare. Multilingual evaluation often relies on
translated benchmarks, which typically do not capture linguistic and cultural
nuances present in the source language. This study provides an extensive
assessment of 24 LLMs on real world data collected from Indian patients
interacting with a medical chatbot in Indian English and 4 other Indic
languages. We employ a uniform Retrieval Augmented Generation framework to
generate responses, which are evaluated using both automated techniques and
human evaluators on four specific metrics relevant to our application. We find
that models vary significantly in their performance and that instruction tuned
Indic models do not always perform well on Indic language queries. Further, we
empirically show that factual correctness is generally lower for responses to
Indic queries compared to English queries. Finally, our qualitative work shows
that code-mixed and culturally relevant queries in our dataset pose challenges
to evaluated models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ signwriting-evaluation: Effective Sign Language Evaluation via
  SignWriting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Moryossef, Rotem Zilberman, Ohad Langer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The lack of automatic evaluation metrics tailored for SignWriting presents a
significant obstacle in developing effective transcription and translation
models for signed languages. This paper introduces a comprehensive suite of
evaluation metrics specifically designed for SignWriting, including adaptations
of standard metrics such as \texttt{BLEU} and \texttt{chrF}, the application of
\texttt{CLIPScore} to SignWriting images, and a novel symbol distance metric
unique to our approach. We address the distinct challenges of evaluating single
signs versus continuous signing and provide qualitative demonstrations of
metric efficacy through score distribution analyses and nearest-neighbor
searches within the SignBank corpus. Our findings reveal the strengths and
limitations of each metric, offering valuable insights for future advancements
using SignWriting. This work contributes essential tools for evaluating
SignWriting models, facilitating progress in the field of sign language
processing. Our code is available at
\url{https://github.com/sign-language-processing/signwriting-evaluation}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ORCHID: A Chinese Debate Corpus for Target-Independent Stance Detection
  and Argumentative Dialogue Summarization <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiutian Zhao, Ke Wang, Wei Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue agents have been receiving increasing attention for years, and this
trend has been further boosted by the recent progress of large language models
(LLMs). Stance detection and dialogue summarization are two core tasks of
dialogue agents in application scenarios that involve argumentative dialogues.
However, research on these tasks is limited by the insufficiency of public
datasets, especially for non-English languages. To address this language
resource gap in Chinese, we present ORCHID (Oral Chinese Debate), the first
Chinese dataset for benchmarking target-independent stance detection and debate
summarization. Our dataset consists of 1,218 real-world debates that were
conducted in Chinese on 476 unique topics, containing 2,436 stance-specific
summaries and 14,133 fully annotated utterances. Besides providing a versatile
testbed for future research, we also conduct an empirical study on the dataset
and propose an integrated task. The results show the challenging nature of the
dataset and suggest a potential of incorporating stance detection in
summarization for argumentative dialogue.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VL-GLUE: A Suite of Fundamental yet Challenging Visuo-Linguistic
  Reasoning Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shailaja Keyur Sampat, Mutsumi Nakamura, Shankar Kailas, Kartik Aggarwal, Mandy Zhou, Yezhou Yang, Chitta Baral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deriving inference from heterogeneous inputs (such as images, text, and
audio) is an important skill for humans to perform day-to-day tasks. A similar
ability is desirable for the development of advanced Artificial Intelligence
(AI) systems. While state-of-the-art models are rapidly closing the gap with
human-level performance on diverse computer vision and NLP tasks separately,
they struggle to solve tasks that require joint reasoning over visual and
textual modalities. Inspired by GLUE (Wang et. al., 2018)- a multitask
benchmark for natural language understanding, we propose VL-GLUE in this paper.
VL-GLUE consists of over 100k samples spanned across seven different tasks,
which at their core require visuo-linguistic reasoning. Moreover, our benchmark
comprises of diverse image types (from synthetically rendered figures, and
day-to-day scenes to charts and complex diagrams) and includes a broad variety
of domain-specific text (from cooking, politics, and sports to high-school
curricula), demonstrating the need for multi-modal understanding in the
real-world. We show that this benchmark is quite challenging for existing
large-scale vision-language models and encourage development of systems that
possess robust visuo-linguistic reasoning capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Red and blue language: Word choices in the Trump & Harris 2024
  presidential debate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Wicke, Marianna M. Bolognesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Political debates are a peculiar type of political discourse, in which
candidates directly confront one another, addressing not only the the
moderator's questions, but also their opponent's statements, as well as the
concerns of voters from both parties and undecided voters. Therefore, language
is adjusted to meet specific expectations and achieve persuasion. We analyse
how the language of Trump and Harris during the debate (September 10th 2024)
differs in relation to the following semantic and pragmatic features, for which
we formulated targeted hypotheses: framing values and ideology, appealing to
emotion, using words with different degrees of concreteness and specificity,
addressing others through singular or plural pronouns. Our findings include:
differences in the use of figurative frames (Harris often framing issues around
recovery and empowerment, Trump often focused on crisis and decline); similar
use of emotional language, with Trump showing a slight higher tendency toward
negativity and toward less subjective language compared to Harris; no
significant difference in the specificity of candidates' responses; similar use
of abstract language, with Trump showing more variability than Harris,
depending on the subject discussed; differences in addressing the opponent,
with Trump not mentioning Harris by name, while Harris referring to Trump
frequently; different uses of pronouns, with Harris using both singular and
plural pronouns equally, while Trump using more singular pronouns. The results
are discussed in relation to previous literature on Red and Blue language,
which refers to distinct linguistic patterns associated with conservative (Red)
and liberal (Blue) political ideologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to PLOS ONE, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A new approach for <span class="highlight-title">fine-tuning</span> sentence transformers for intent
  classification and out-of-scope detection tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Zhang, Atta Norouzian, Aanchan Mohan, Frederick Ducatelle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In virtual assistant (VA) systems it is important to reject or redirect user
queries that fall outside the scope of the system. One of the most accurate
approaches for out-of-scope (OOS) rejection is to combine it with the task of
intent classification on in-scope queries, and to use methods based on the
similarity of embeddings produced by transformer-based sentence encoders.
Typically, such encoders are fine-tuned for the intent-classification task,
using cross-entropy loss. Recent work has shown that while this produces
suitable embeddings for the intent-classification task, it also tends to
disperse in-scope embeddings over the full sentence embedding space. This
causes the in-scope embeddings to potentially overlap with OOS embeddings,
thereby making OOS rejection difficult. This is compounded when OOS data is
unknown. To mitigate this issue our work proposes to regularize the
cross-entropy loss with an in-scope embedding reconstruction loss learned using
an auto-encoder. Our method achieves a 1-4% improvement in the area under the
precision-recall curve for rejecting out-of-sample (OOS) instances, without
compromising intent classification performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appearing at Empirical Methods in Natural Language Processing 2025 -
  Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimpleToM: Exposing the Gap between Explicit ToM Inference and Implicit
  ToM Application in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuling Gu, Oyvind Tafjord, Hyunwoo Kim, Jared Moore, Ronan Le Bras, Peter Clark, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While prior work has explored whether large language models (LLMs) possess a
"theory of mind" (ToM) - the ability to attribute mental states to oneself and
others - there has been little work testing whether LLMs can implicitly apply
such knowledge to predict behavior, or to judge whether an observed behavior is
rational. Such skills are critical for appropriate interaction in social
environments. We create a new dataset, SimpleTom, containing concise, diverse
stories (e.g., "The can of Pringles has moldy chips in it. Mary picks up the
can in the supermarket and walks to the cashier."), each with three questions
that test different degrees of ToM reasoning, asking models to predict (a)
mental state ("Is Mary aware of the mold?"), (b) behavior ("Will Mary pay for
the chips or report the mold?"), and (c) judgment ("Mary paid for the chips.
Was that reasonable?"). To our knowledge, SimpleToM is the first dataset to
systematically explore downstream reasoning requiring knowledge of mental
states in realistic scenarios. Our experimental results are intriguing: While
most models can reliably predict mental state on our dataset (a), they often
fail to correctly predict the behavior (b), and fare even worse at judging
whether given behaviors are reasonable (c), despite being correctly aware of
the protagonist's mental state should make such secondary predictions obvious.
We further show that we can help models do better at (b) and (c) via
interventions such as reminding the model of its earlier mental state answer
and mental-state-specific chain-of-thought prompting, raising the action
prediction accuracies (e.g., from 49.5% to 93.5% for GPT-4o) and judgment
accuracies (e.g., from 15.3% to 94.7% in GPT-4o). While this shows that models
can be coaxed to perform well, it requires task-specific interventions, and the
natural model performances remain low, a cautionary tale for LLM deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Active Learning Framework for Inclusive Generation by Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabit Hassan, Anthony Sicilia, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring that Large Language Models (LLMs) generate text representative of
diverse sub-populations is essential, particularly when key concepts related to
under-represented groups are scarce in the training data. We address this
challenge with a novel clustering-based active learning framework, enhanced
with knowledge distillation. The proposed framework transforms the intermediate
outputs of the learner model, enabling effective active learning for generative
tasks for the first time. Integration of clustering and knowledge distillation
yields more representative models without prior knowledge of underlying data
distribution and overbearing human efforts. We validate our approach in
practice through case studies in counter-narration and style transfer. We
construct two new datasets in tandem with model training, showing a performance
improvement of 2%-10% over baseline models. Our results also show more
consistent performance across various data subgroups and increased lexical
diversity, underscoring our model's resilience to skewness in available data.
Further, our results show that the data acquired via our approach improves the
performance of secondary models not involved in the learning loop, showcasing
practical utility of the framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Wang, Pei Zhang, Baosong Yang, Derek F. Wong, Rui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM self-evaluation relies on the LLM's own ability to estimate response
correctness, which can greatly improve its deployment reliability. In this
research track, we propose the Chain-of-Embedding (CoE) in the latent space to
enable LLMs to perform output-free self-evaluation. CoE consists of all
progressive hidden states produced during the inference time, which can be
treated as the latent thinking path of LLMs. We find that when LLMs respond
correctly and incorrectly, their CoE features differ, these discrepancies
assist us in estimating LLM response correctness. Experiments in four diverse
domains and seven LLMs fully demonstrate the effectiveness of our method.
Meanwhile, its label-free design intent without any training and
millisecond-level computational cost ensure real-time feedback in large-scale
scenarios. More importantly, we provide interesting insights into LLM response
correctness from the perspective of hidden state changes inside LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 18 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Study on Reasoning Patterns of OpenAI's o1 Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siwei Wu, Zhongyuan Peng, Xinrun Du, Tuney Zheng, Minghao Liu, Jialong Wu, Jiachen Ma, Yizhi Li, Jian Yang, Wangchunshu Zhou, Qunshu Lin, Junbo Zhao, Zhaoxiang Zhang, Wenhao Huang, Ge Zhang, Chenghua Lin, J. H. Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enabling Large Language Models (LLMs) to handle a wider range of complex
tasks (e.g., coding, math) has drawn great attention from many researchers. As
LLMs continue to evolve, merely increasing the number of model parameters
yields diminishing performance improvements and heavy computational costs.
Recently, OpenAI's o1 model has shown that inference strategies (i.e.,
Test-time Compute methods) can also significantly enhance the reasoning
capabilities of LLMs. However, the mechanisms behind these methods are still
unexplored. In our work, to investigate the reasoning patterns of o1, we
compare o1 with existing Test-time Compute methods (BoN, Step-wise BoN, Agent
Workflow, and Self-Refine) by using OpenAI's GPT-4o as a backbone on general
reasoning benchmarks in three domains (i.e., math, coding, commonsense
reasoning). Specifically, first, our experiments show that the o1 model has
achieved the best performance on most datasets. Second, as for the methods of
searching diverse responses (e.g., BoN), we find the reward models' capability
and the search space both limit the upper boundary of these methods. Third, as
for the methods that break the problem into many sub-problems, the Agent
Workflow has achieved better performance than Step-wise BoN due to the
domain-specific system prompt for planning better reasoning processes. Fourth,
it is worth mentioning that we have summarized six reasoning patterns of o1,
and provided a detailed analysis on several reasoning benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ H2OVL-Mississippi <span class="highlight-title">Vision Language</span> Models Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaikat Galib, Shanshan Wang, Guanshuo Xu, Pascal Pfeiffer, Ryan Chesler, Mark Landry, Sri Satish Ambati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Smaller vision-language models (VLMs) are becoming increasingly important for
privacy-focused, on-device applications due to their ability to run efficiently
on consumer hardware for processing enterprise commercial documents and images.
These models require strong language understanding and visual capabilities to
enhance human-machine interaction. To address this need, we present
H2OVL-Mississippi, a pair of small VLMs trained on 37 million image-text pairs
using 240 hours of compute on 8 x H100 GPUs. H2OVL-Mississippi-0.8B is a tiny
model with 0.8 billion parameters that specializes in text recognition,
achieving state of the art performance on the Text Recognition portion of
OCRBench and surpassing much larger models in this area. Additionally, we are
releasing H2OVL-Mississippi-2B, a 2 billion parameter model for general use
cases, exhibiting highly competitive metrics across various academic
benchmarks. Both models build upon our prior work with H2O-Danube language
models, extending their capabilities into the visual domain. We release them
under the Apache 2.0 license, making VLMs accessible to everyone, democratizing
document AI and visual LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool
  Calling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yakun Zhu, Shaohang Wei, Xu Wang, Kui Xue, Xiaofan Zhang, Shaoting Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating tools into Large Language Models (LLMs) has facilitated the
widespread application. Despite this, in specialized downstream task contexts,
reliance solely on tools is insufficient to fully address the complexities of
the real world. This particularly restricts the effective deployment of LLMs in
fields such as medicine. In this paper, we focus on the downstream tasks of
medical calculators, which use standardized tests to assess an individual's
health status. We introduce MeNTi, a universal agent architecture for LLMs.
MeNTi integrates a specialized medical toolkit and employs meta-tool and nested
calling mechanisms to enhance LLM tool utilization. Specifically, it achieves
flexible tool selection and nested tool calling to address practical issues
faced in intricate medical scenarios, including calculator selection, slot
filling, and unit conversion. To assess the capabilities of LLMs for
quantitative assessment throughout the clinical process of calculator
scenarios, we introduce CalcQA. This benchmark requires LLMs to use medical
calculators to perform calculations and assess patient health status. CalcQA is
constructed by professional physicians and includes 100 case-calculator pairs,
complemented by a toolkit of 281 medical tools. The experimental results
demonstrate significant performance improvements with our framework. This
research paves new directions for applying LLMs in demanding scenarios of
medicine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models as Narrative-Driven Recommenders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Eberhard, Thorsten Ruprechter, Denis Helic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Narrative-driven recommenders aim to provide personalized suggestions for
user requests expressed in free-form text such as "I want to watch a thriller
with a mind-bending story, like Shutter Island." Although large language models
(LLMs) have been shown to excel in processing general natural language queries,
their effectiveness for handling such recommendation requests remains
relatively unexplored. To close this gap, we compare the performance of 38
open- and closed-source LLMs of various sizes, such as LLama 3.2 and GPT-4o, in
a movie recommendation setting. For this, we utilize a gold-standard,
crowdworker-annotated dataset of posts from reddit's movie suggestion community
and employ various prompting strategies, including zero-shot, identity, and
few-shot prompting. Our findings demonstrate the ability of LLMs to generate
contextually relevant movie recommendations, significantly outperforming other
state-of-the-art approaches, such as doc2vec. While we find that closed-source
and large-parameterized models generally perform best, medium-sized open-source
models remain competitive, being only slightly outperformed by their more
computationally expensive counterparts. Furthermore, we observe no significant
differences across prompting strategies for most models, underscoring the
effectiveness of simple approaches such as zero-shot prompting for
narrative-driven recommendations. Overall, this work offers valuable insights
for recommender system researchers as well as practitioners aiming to integrate
LLMs into real-world recommendation tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review; 19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Fact Retrieval in PLMs through Truthfulness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Youssef, Jörg Schlötterer, Christin Seifert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained Language Models (PLMs) encode various facts about the world at
their pre-training phase as they are trained to predict the next or missing
word in a sentence. There has a been an interest in quantifying and improving
the amount of facts that can be extracted from PLMs, as they have been
envisioned to act as soft knowledge bases, which can be queried in natural
language. Different approaches exist to enhance fact retrieval from PLM. Recent
work shows that the hidden states of PLMs can be leveraged to determine the
truthfulness of the PLMs' inputs. Leveraging this finding to improve factual
knowledge retrieval remains unexplored. In this work, we investigate the use of
a helper model to improve fact retrieval. The helper model assesses the
truthfulness of an input based on the corresponding hidden states
representations from the PLMs. We evaluate this approach on several masked PLMs
and show that it enhances fact retrieval by up to 33\%. Our findings highlight
the potential of hidden states representations from PLMs in improving their
factual knowledge retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Temporal Representations for Dynamic Memory Retrieval and
  Management in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuki Hou, Haruki Tamoto, Homei Miyashita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional dialogue agents often struggle with effective memory recall,
leading to redundant retrieval and inadequate management of unique user
associations. To address this, we propose SynapticRAG, a novel approach
integrating synaptic dynamics into Retrieval-Augmented Generation (RAG).
SynapticRAG integrates temporal representations into memory vectors, mimicking
biological synapses by differentiating events based on occurrence times and
dynamically updating memory significance. This model employs temporal scoring
for memory connections and a synaptic-inspired propagation control mechanism.
Experiments across English, Japanese, and Chinese datasets demonstrate
SynapticRAG's superiority over existing methods, including traditional RAG,
with up to 14.66\% improvement in memory retrieval accuracy. Our approach
advances context-aware dialogue AI systems by enhancing long-term context
maintenance and specific information extraction from conversations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bias in the Mirror : Are LLMs opinions robust to their own adversarial
  attacks ? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Virgile Rennard, Christos Xypolopoulos, Michalis Vazirgiannis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) inherit biases from their training data and
alignment processes, influencing their responses in subtle ways. While many
studies have examined these biases, little work has explored their robustness
during interactions. In this paper, we introduce a novel approach where two
instances of an LLM engage in self-debate, arguing opposing viewpoints to
persuade a neutral version of the model. Through this, we evaluate how firmly
biases hold and whether models are susceptible to reinforcing misinformation or
shifting to harmful viewpoints. Our experiments span multiple LLMs of varying
sizes, origins, and languages, providing deeper insights into bias persistence
and flexibility across linguistic and cultural contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeoCoder: Solving Geometry Problems by Generating Modular Code through
  <span class="highlight-title">Vision-Language</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Sharma, Aman Dalmia, Mehran Kazemi, Amal Zouaq, Christopher J. Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometry problem-solving demands advanced reasoning abilities to process
multimodal inputs and employ mathematical knowledge effectively.
Vision-language models (VLMs) have made significant progress in various
multimodal tasks. Yet, they still struggle with geometry problems and are
significantly limited by their inability to perform mathematical operations not
seen during pre-training, such as calculating the cosine of an arbitrary angle,
and by difficulties in correctly applying relevant geometry formulas. To
overcome these challenges, we present GeoCoder, which leverages modular
code-finetuning to generate and execute code using a predefined geometry
function library. By executing the code, we achieve accurate and deterministic
calculations, contrasting the stochastic nature of autoregressive token
prediction, while the function library minimizes errors in formula usage. We
also propose a multimodal retrieval-augmented variant of GeoCoder, named
RAG-GeoCoder, which incorporates a non-parametric memory module for retrieving
functions from the geometry library, thereby reducing reliance on parametric
memory. Our modular code-finetuning approach enhances the geometric reasoning
capabilities of VLMs, yielding an average improvement of over 16% across
various question complexities on the GeomVerse dataset compared to other
finetuning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable
  Data Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinze Li, Sen Mei, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Hao Chen, Ge Yu, Zhiyuan Liu, Maosong Sun, Chenyan Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has proven its effectiveness in
mitigating hallucinations in Large Language Models (LLMs) by retrieving
knowledge from external resources. To adapt LLMs for RAG pipelines, current
approaches use instruction tuning to optimize LLMs, improving their ability to
utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses
on equipping LLMs to handle diverse RAG tasks using different instructions.
However, it trains RAG modules to overfit training signals and overlooks the
varying data preferences among agents within the RAG system. In this paper, we
propose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG
systems by aligning data preferences between different RAG modules. DDR works
by collecting the rewards to optimize each agent with a rollout method. This
method prompts agents to sample some potential responses as perturbations,
evaluates the impact of these perturbations on the whole RAG system, and
subsequently optimizes the agent to produce outputs that improve the
performance of the RAG system. Our experiments on various knowledge-intensive
tasks demonstrate that DDR significantly outperforms the SFT method,
particularly for LLMs with smaller-scale parameters that depend more on the
retrieved knowledge. Additionally, DDR exhibits a stronger capability to align
the data preference between RAG modules. The DDR method makes generation module
more effective in extracting key information from documents and mitigating
conflicts between parametric memory and external knowledge. All codes are
available at https://github.com/OpenMatch/RAG-DDR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily
  Complex Proofs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Opedal, Haruki Shirakami, Bernhard Schölkopf, Abulhair Saparov, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can solve arithmetic word problems with high
accuracy, but little is known about how well they generalize to problems that
are more complex than the ones on which they have been trained. Empirical
investigations of such questions are impeded by two major flaws of current
evaluations: (i) much of the evaluation data is contaminated, in the sense that
it has already been seen during training, and (ii) benchmark datasets do not
capture how problem proofs may be arbitrarily complex in various ways. As a
step towards addressing these issues, we present a framework for evaluating
LLMs on problems that have arbitrarily complex arithmetic proofs, called
MathGAP. MathGAP generates problems that follow fixed proof specifications --
along with chain-of-thought reasoning annotations -- enabling systematic
studies on generalization with respect to arithmetic proof complexity. We apply
MathGAP to analyze how in-context learning interacts with generalization to
problems that have more complex proofs. We find that among the models tested,
most show a significant decrease in performance as proofs get deeper and wider.
This effect is more pronounced in complex, nonlinear proof structures, which
are challenging even for GPT-4o. Surprisingly, providing in-context examples
from the same distribution as the test set is not always beneficial for
performance. In particular, zero-shot prompting as well as demonstrating a
diverse range of examples that are less complex than the test data sometimes
yield similar or higher accuracies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Text Generation in Joint NLG/NLU Learning Through Curriculum
  Learning, Semi-Supervised Training, and Advanced Optimization Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahimanuddin Shaik, Katikela Sreeharsha Kishore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text generation is the automated process of producing written or spoken
language using computational methods. It involves generating coherent and
contextually relevant text based on predefined rules or learned patterns.
However, challenges in text generation arise from maintaining coherence,
ensuring diversity and creativity, and avoiding biases or inappropriate
content. This research paper developed a novel approach to improve text
generation in the context of joint Natural Language Generation (NLG) and
Natural Language Understanding (NLU) learning. The data is prepared by
gathering and preprocessing annotated datasets, including cleaning,
tokenization, stemming, and stop-word removal. Feature extraction techniques
such as POS tagging, Bag of words, and Term Frequency-Inverse Document
Frequency (TF-IDF) are applied. Transformer-based encoders and decoders,
capturing long range dependencies and improving source-target sequence
modelling. Pre-trained language models like Optimized BERT are incorporated,
along with a Hybrid Redfox Artificial Hummingbird Algorithm (HRAHA).
Reinforcement learning with policy gradient techniques, semi-supervised
training, improved attention mechanisms, and differentiable approximations like
straight-through Gumbel SoftMax estimator are employed to fine-tune the models
and handle complex linguistic tasks effectively. The proposed model is
implemented using Python.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Repetition Neurons: How Do Language Models Produce Repetitions? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tatsuya Hiraoka, Kentaro Inui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces repetition neurons, regarded as skill neurons
responsible for the repetition problem in text generation tasks. These neurons
are progressively activated more strongly as repetition continues, indicating
that they perceive repetition as a task to copy the previous context
repeatedly, similar to in-context learning. We identify these repetition
neurons by comparing activation values before and after the onset of repetition
in texts generated by recent pre-trained language models. We analyze the
repetition neurons in three English and one Japanese pre-trained language
models and observe similar patterns across them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing Through VisualBERT: A Causal Adventure on Memetic Landscapes <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dibyanayan Bandyopadhyay, Mohammed Hasanuzzaman, Asif Ekbal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting offensive memes is crucial, yet standard deep neural network
systems often remain opaque. Various input attribution-based methods attempt to
interpret their behavior, but they face challenges with implicitly offensive
memes and non-causal attributions. To address these issues, we propose a
framework based on a Structural Causal Model (SCM). In this framework,
VisualBERT is trained to predict the class of an input meme based on both meme
input and causal concepts, allowing for transparent interpretation. Our
qualitative evaluation demonstrates the framework's effectiveness in
understanding model behavior, particularly in determining whether the model was
right due to the right reason, and in identifying reasons behind
misclassification. Additionally, quantitative analysis assesses the
significance of proposed modelling choices, such as de-confounding, adversarial
learning, and dynamic routing, and compares them with input attribution
methods. Surprisingly, we find that input attribution methods do not guarantee
causality within our framework, raising questions about their reliability in
safety-critical applications. The project page is at:
https://newcodevelop.github.io/causality_adventure/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IterSelectTune: An Iterative Training Framework for Efficient
  Instruction-Tuning Data Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jielin Song, Siyu Liu, Bin Zhu, Yanghui Rao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) continue to advance, instruction tuning has
become critical for improving their ability to generate accurate and
contextually appropriate responses. Although numerous instruction-tuning
datasets have been developed to enhance LLM performance, selecting high-quality
instruction data from large source datasets typically demands significant human
effort. In this work, we introduce $\textbf{IterSelectTune}$, an efficient,
cost-effective iterative training policy for selecting high-quality instruction
data with no human involvement and limited reliance on GPT-4. By fine-tuning on
approximately 20\% of the source data, our method consistently outperforms
models fine-tuned on the full dataset across multiple benchmarks and public
test datasets. These results highlight the effectiveness of our approach in
enhancing LLM performance while reducing the computational resources required
for instruction tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Mixed-Precision Decoding for Efficient LLM Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Mark Chen, Fuwen Tan, Alexandros Kouris, Royson Lee, Hongxiang Fan, Stylianos I. Venieris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In spite of the great potential of large language models (LLMs) across
various tasks, their deployment on resource-constrained devices remains
challenging due to their excessive computational and memory demands.
Quantization has emerged as an effective solution by storing weights in reduced
precision. However, utilizing low precisions (i.e.~2/3-bit) to substantially
alleviate the memory-boundedness of LLM decoding, still suffers from
prohibitive performance drop. In this work, we argue that existing approaches
fail to explore the diversity in computational patterns, redundancy, and
sensitivity to approximations of the different phases of LLM inference,
resorting to a uniform quantization policy throughout. Instead, we propose a
novel phase-aware method that selectively allocates precision during different
phases of LLM inference, achieving both strong context extraction during
prefill and efficient memory bandwidth utilization during decoding. To further
address the memory-boundedness of the decoding phase, we introduce Progressive
Mixed-Precision Decoding (PMPD), a technique that enables the gradual lowering
of precision deeper in the generated sequence, together with a spectrum of
precision-switching schedulers that dynamically drive the precision-lowering
decisions in either task-adaptive or prompt-adaptive manner. Extensive
evaluation across diverse language tasks shows that when targeting Nvidia GPUs,
PMPD achieves 1.4$-$12.2$\times$ speedup in matrix-vector multiplications over
fp16 models, while when targeting an LLM-optimized NPU, our approach delivers a
throughput gain of 3.8$-$8.0$\times$ over fp16 models and up to 1.54$\times$
over uniform quantization approaches while preserving the output quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking the Manual Annotation Bottleneck: Creating a Comprehensive
  Legal Case Criticality Dataset through Semi-Automated Labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronja Stern, Ken Kawamura, Matthias Stürmer, Ilias Chalkidis, Joel Niklaus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting case criticality helps legal professionals in the court system
manage large volumes of case law. This paper introduces the Criticality
Prediction dataset, a new resource for evaluating the potential influence of
Swiss Federal Supreme Court decisions on future jurisprudence. Unlike existing
approaches that rely on resource-intensive manual annotations, we
semi-automatically derive labels leading to a much larger dataset than
otherwise possible. Our dataset features a two-tier labeling system: (1) the
LD-Label, which identifies cases published as Leading Decisions (LD), and (2)
the Citation-Label, which ranks cases by their citation frequency and recency.
This allows for a more nuanced evaluation of case importance. We evaluate
several multilingual models, including fine-tuned variants and large language
models, and find that fine-tuned models consistently outperform zero-shot
baselines, demonstrating the need for task-specific adaptation. Our
contributions include the introduction of this task and the release of a
multilingual dataset to the research community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedINST: Meta Dataset of Biomedical Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhan Han, Meng Fang, Zihan Zhang, Yu Yin, Zirui Song, Ling Chen, Mykola Pechenizkiy, Qingyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of large language model (LLM) techniques in the field of
medical analysis has brought about significant advancements, yet the scarcity
of large, diverse, and well-annotated datasets remains a major challenge.
Medical data and tasks, which vary in format, size, and other parameters,
require extensive preprocessing and standardization for effective use in
training LLMs. To address these challenges, we introduce MedINST, the Meta
Dataset of Biomedical Instructions, a novel multi-domain, multi-task
instructional meta-dataset. MedINST comprises 133 biomedical NLP tasks and over
7 million training samples, making it the most comprehensive biomedical
instruction dataset to date. Using MedINST as the meta dataset, we curate
MedINST32, a challenging benchmark with different task difficulties aiming to
evaluate LLMs' generalization ability. We fine-tune several LLMs on MedINST and
evaluate on MedINST32, showcasing enhanced cross-task generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking Legal Knowledge: A Multilingual Dataset for Judicial
  Summarization in Switzerland 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Rolshoven, Vishvaksenan Rasiah, Srinanda Brügger Bose, Matthias Stürmer, Joel Niklaus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal research is a time-consuming task that most lawyers face on a daily
basis. A large part of legal research entails looking up relevant caselaw and
bringing it in relation to the case at hand. Lawyers heavily rely on summaries
(also called headnotes) to find the right cases quickly. However, not all
decisions are annotated with headnotes and writing them is time-consuming.
Automated headnote creation has the potential to make hundreds of thousands of
decisions more accessible for legal research in Switzerland alone. To kickstart
this, we introduce the Swiss Leading Decision Summarization ( SLDS) dataset, a
novel cross-lingual resource featuring 18K court rulings from the Swiss Federal
Supreme Court (SFSC), in German, French, and Italian, along with German
headnotes. We fine-tune and evaluate three mT5 variants, along with proprietary
models. Our analysis highlights that while proprietary models perform well in
zero-shot and one-shot settings, fine-tuned smaller models still provide a
strong competitive edge. We publicly release the dataset to facilitate further
research in multilingual legal summarization and the development of assistive
technologies for legal professionals
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter-efficient Adaptation of Multilingual <span class="highlight-title">Multimodal</span> Models for
  Low-resource ASR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Gupta, Amruta Parulekar, Sameep Chattopadhyay, Preethi Jyothi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic speech recognition (ASR) for low-resource languages remains a
challenge due to the scarcity of labeled training data. Parameter-efficient
fine-tuning and text-only adaptation are two popular methods that have been
used to address such low-resource settings. In this work, we investigate how
these techniques can be effectively combined using a multilingual multimodal
model like SeamlessM4T. Multimodal models are able to leverage unlabeled text
via text-only adaptation with further parameter-efficient ASR fine-tuning, thus
boosting ASR performance. We also show cross-lingual transfer from a
high-resource language, achieving up to a relative 17% WER reduction over a
baseline in a zero-shot setting without any labeled speech.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NLIP_Lab-IITH Multilingual MT System for WAT24 MT Shared Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maharaj Brahma, Pramit Sahoo, Maunendra Sankar Desarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes NLIP Lab's multilingual machine translation system for
the WAT24 shared task on multilingual Indic MT task for 22 scheduled languages
belonging to 4 language families. We explore pre-training for Indic languages
using alignment agreement objectives. We utilize bi-lingual dictionaries to
substitute words from source sentences. Furthermore, we fine-tuned language
direction-specific multilingual translation models using small and high-quality
seed data. Our primary submission is a 243M parameters multilingual translation
model covering 22 Indic languages. In the IN22-Gen benchmark, we achieved an
average chrF++ score of 46.80 and 18.19 BLEU score for the En-Indic direction.
In the Indic-En direction, we achieved an average chrF++ score of 56.34 and
30.82 BLEU score. In the In22-Conv benchmark, we achieved an average chrF++
score of 43.43 and BLEU score of 16.58 in the En-Indic direction, and in the
Indic-En direction, we achieved an average of 52.44 and 29.77 for chrF++ and
BLEU respectively. Our model\footnote{Our code and models are available at
\url{https://github.com/maharajbrahma/WAT2024-MultiIndicMT}} is competitive
with IndicTransv1 (474M parameter model).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WMT 24 WAT Shared Task IndicMultiMT (Best System)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Similarity-Dissimilarity Loss with Supervised Contrastive Learning for
  Multi-label Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangming Huang, Yunfei Long, Cunjin Luo, Sheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised contrastive learning has been explored in making use of label
information for multi-label classification, but determining positive samples in
multi-label scenario remains challenging. Previous studies have examined
strategies for identifying positive samples, considering label overlap
proportion between anchors and samples. However, they ignore various relations
between given anchors and samples, as well as how to dynamically adjust the
weights in contrastive loss functions based on different relations, leading to
great ambiguity. In this paper, we introduce five distinct relations between
multi-label samples and propose a Similarity-Dissimilarity Loss with
contrastive learning for multi-label classification. Our loss function
re-weights the loss by computing the similarity and dissimilarity between
positive samples and a given anchor based on the introduced relations. We
mainly conduct experiments for multi-label text classification on MIMIC
datasets, then further extend the evaluation on MS-COCO. The Experimental
results show that our proposed loss effectively improves the performance on all
encoders under supervised contrastive learning paradigm, demonstrating its
effectiveness and robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Think Thrice Before You Act: Progressive Thought Refinement in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyu Du, Jinyi Han, Yizhou Ying, Aili Chen, Qianyu He, Haokun Zhao, Sirui Xia, Haoran Guo, Jiaqing Liang, Zulong Chen, Liangyue Li, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have demonstrated that
progressive refinement, rather than providing a single answer, results in more
accurate and thoughtful outputs. However, existing methods often rely heavily
on supervision signals to evaluate previous responses, making it difficult to
assess output quality in more open-ended scenarios effectively. Additionally,
these methods are typically designed for specific tasks, which limits their
generalization to new domains. To address these limitations, we propose
Progressive Thought Refinement (PTR), a framework that enables LLMs to refine
their responses progressively. PTR operates in two phases: (1) Thought data
construction stage: We propose a weak and strong model collaborative selection
strategy to build a high-quality progressive refinement dataset to ensure
logical consistency from thought to answers, and the answers are gradually
refined in each round. (2) Thought-Mask Fine-Tuning Phase: We design a training
structure to mask the "thought" and adjust loss weights to encourage LLMs to
refine prior thought, teaching them to implicitly understand "how to improve"
rather than "what is correct." Experimental results show that PTR significantly
enhances LLM performance across ten diverse tasks (avg. from 49.6% to 53.5%)
without task-specific fine-tuning. Notably, in more open-ended tasks, LLMs also
demonstrate substantial improvements in the quality of responses beyond mere
accuracy, suggesting that PTR truly teaches LLMs to self-improve over time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attr-Int: A Simple and Effective Entity Alignment Framework for
  Heterogeneous Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linyan Yang, Jingwei Cheng, Chuanhao Xu, Xihao Wang, Jiayi Li, Fu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity alignment (EA) refers to the task of linking entities in different
knowledge graphs (KGs). Existing EA methods rely heavily on structural
isomorphism. However, in real-world KGs, aligned entities usually have
non-isomorphic neighborhood structures, which paralyses the application of
these structure-dependent methods. In this paper, we investigate and tackle the
problem of entity alignment between heterogeneous KGs. First, we propose two
new benchmarks to closely simulate real-world EA scenarios of heterogeneity.
Then we conduct extensive experiments to evaluate the performance of
representative EA methods on the new benchmarks. Finally, we propose a simple
and effective entity alignment framework called Attr-Int, in which innovative
attribute information interaction methods can be seamlessly integrated with any
embedding encoder for entity alignment, improving the performance of existing
entity alignment techniques. Experiments demonstrate that our framework
outperforms the state-of-the-art approaches on two new benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoR: Mixture of Ranks for Low-Rank Adaptation Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyu Tang, Yilong Chen, Zhenyu Zhang, Junyuan Shang, Wenyuan Zhang, Yong Huang, Tingwen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) drives research to align its performance with full
fine-tuning. However, significant challenges remain: (1) Simply increasing the
rank size of LoRA does not effectively capture high-rank information, which
leads to a performance bottleneck.(2) MoE-style LoRA methods substantially
increase parameters and inference latency, contradicting the goals of efficient
fine-tuning and ease of application. To address these challenges, we introduce
Mixture of Ranks (MoR), which learns rank-specific information for different
tasks based on input and efficiently integrates multi-rank information. We
firstly propose a new framework that equates the integration of multiple LoRAs
to expanding the rank of LoRA. Moreover, we hypothesize that low-rank LoRA
already captures sufficient intrinsic information, and MoR can derive high-rank
information through mathematical transformations of the low-rank components.
Thus, MoR can reduces the learning difficulty of LoRA and enhances its
multi-task capabilities. MoR achieves impressive results, with MoR delivering a
1.31\% performance improvement while using only 93.93\% of the parameters
compared to baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Hybrid Intelligence in Journalism: Findings and Lessons Learnt
  from a Collaborative Analysis of Greek Political Rhetoric by ChatGPT and
  Humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanasis Troboukis, Kelly Kiki, Antonis Galanopoulos, Pavlos Sermpezis, Stelios Karamanidis, Ilias Dimitriadis, Athena Vakali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This chapter introduces a research project titled "Analyzing the Political
Discourse: A Collaboration Between Humans and Artificial Intelligence", which
was initiated in preparation for Greece's 2023 general elections. The project
focused on the analysis of political leaders' campaign speeches, employing
Artificial Intelligence (AI), in conjunction with an interdisciplinary team
comprising journalists, a political scientist, and data scientists. The chapter
delves into various aspects of political discourse analysis, including
sentiment analysis, polarization, populism, topic detection, and Named Entities
Recognition (NER). This experimental study investigates the capabilities of
large language model (LLMs), and in particular OpenAI's ChatGPT, for analyzing
political speech, evaluates its strengths and weaknesses, and highlights the
essential role of human oversight in using AI in journalism projects and
potentially other societal sectors. The project stands as an innovative example
of human-AI collaboration (known also as "hybrid intelligence") within the
realm of digital humanities, offering valuable insights for future initiatives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linguistically Grounded Analysis of Language Models using Shapley Head
  Values 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcell Fekete, Johannes Bjerva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding how linguistic knowledge is encoded in language models is
crucial for improving their generalisation capabilities. In this paper, we
investigate the processing of morphosyntactic phenomena, by leveraging a
recently proposed method for probing language models via Shapley Head Values
(SHVs). Using the English language BLiMP dataset, we test our approach on two
widely used models, BERT and RoBERTa, and compare how linguistic constructions
such as anaphor agreement and filler-gap dependencies are handled. Through
quantitative pruning and qualitative clustering analysis, we demonstrate that
attention heads responsible for processing related linguistic phenomena cluster
together. Our results show that SHV-based attributions reveal distinct patterns
across both models, providing insights into how language models organize and
process linguistic information. These findings support the hypothesis that
language models learn subnetworks corresponding to linguistic theory, with
potential implications for cross-linguistic model analysis and interpretability
in Natural Language Processing (NLP).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumanth Doddapaneni, Mohammed Safi Ur Rahman Khan, Dilip Venkatesh, Raj Dabre, Anoop Kunchukuttan, Mitesh M. Khapra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating machine-generated text remains a significant challenge in NLP,
especially for non-English languages. Current methodologies, including
automated metrics, human assessments, and LLM-based evaluations, predominantly
focus on English, revealing a significant gap in multilingual evaluation
frameworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an
extensible framework that includes evaluator LLMs (Hercule) and a novel test
set (Recon) specifically designed for multilingual evaluation. Our test set
features 500 human-annotated instructions spanning various task capabilities
along with human judgment scores across six languages. This would enable
benchmarking of general-purpose multilingual LLMs and facilitate
meta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a
cross-lingual evaluation model that addresses the scarcity of reference answers
in the target language by learning to assign scores to responses based on
easily available reference answers in English. Our experiments demonstrate that
Hercule aligns more closely with human judgments compared to proprietary
models, demonstrating the effectiveness of such cross-lingual evaluation in low
resource scenarios. Further, it is also effective in zero-shot evaluation on
unseen languages. This study is the first comprehensive examination of
cross-lingual evaluation using LLMs, presenting a scalable and effective
approach for multilingual assessment. All code, datasets, and models will be
publicly available to enable further research in this important area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metacognitive Monitoring: A Human Ability Beyond Generative Artificial
  Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Huff, Elanur Ulakçı
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown impressive alignment with human
cognitive processes, raising questions about the extent of their similarity to
human cognition. This study investigates whether LLMs, specifically ChatGPT,
possess metacognitive monitoring abilities akin to humans-particularly in
predicting memory performance on an item-by-item basis. We employed a
cross-agent prediction model to compare the metacognitive performance of humans
and ChatGPT in a language-based memory task involving garden-path sentences
preceded by either fitting or unfitting context sentences. Both humans and
ChatGPT rated the memorability of these sentences; humans then completed a
surprise recognition memory test. Our findings reveal a significant positive
relationship between humans' memorability ratings and their actual recognition
performance, indicating reliable metacognitive monitoring. In contrast, ChatGPT
did not exhibit a similar predictive capability. Bootstrapping analyses
demonstrated that none of the GPT models tested (GPT-3.5-turbo, GPT-4-turbo,
GPT-4o) could accurately predict human memory performance on a per-item basis.
This suggests that, despite their advanced language processing abilities and
alignment with human cognition at the object level, current LLMs lack the
metacognitive mechanisms that enable humans to anticipate their memory
performance. These results highlight a fundamental difference between human and
AI cognition at the metacognitive level. Addressing this gap is crucial for
developing AI systems capable of effective self-monitoring and adaptation to
human needs, thereby enhancing human-AI interactions across domains such as
education and personalized learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:2403.05152</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Use of Audio to Improve Dialogue Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Roncel, Federico Costa, Javier Hernando
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the significant progress of speech technologies, spoken goal-oriented
dialogue systems are becoming increasingly popular. One of the main modules of
a dialogue system is typically the dialogue policy, which is responsible for
determining system actions. This component usually relies only on audio
transcriptions, being strongly dependent on their quality and ignoring very
important extralinguistic information embedded in the user's speech. In this
paper, we propose new architectures to add audio information by combining
speech and text embeddings using a Double Multi-Head Attention component. Our
experiments show that audio embedding-aware dialogue policies outperform
text-based ones, particularly in noisy transcription scenarios, and that how
text and audio embeddings are combined is crucial to improve performance. We
obtained a 9.8% relative improvement in the User Request Score compared to an
only-text-based dialogue system on the DSTC2 dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IberSpeech 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Remember, Retrieve and Generate: Understanding Infinite Visual Concepts
  as Your Personalized Assistant 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Hao, Jiaming Han, Changsheng Li, Yu-Feng Li, Xiangyu Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of large language models (LLMs) has significantly enhanced
the capabilities of multimodal LLMs (MLLMs) as general assistants. However,
lack of user-specific knowledge still restricts their application in human's
daily life. In this paper, we introduce the Retrieval Augmented Personalization
(RAP) framework for MLLMs' personalization. Starting from a general MLLM, we
turn it into a personalized assistant in three steps. (a) Remember: We design a
key-value database to store user-related information, e.g., user's name, avatar
and other attributes. (b) Retrieve: When the user initiates a conversation, RAP
will retrieve relevant information from the database using a multimodal
retriever. (c) Generate: The input query and retrieved concepts' information
are fed into MLLMs to generate personalized, knowledge-augmented responses.
Unlike previous methods, RAP allows real-time concept editing via updating the
external database. To further improve generation quality and alignment with
user-specific information, we design a pipeline for data collection and create
a specialized dataset for personalized training of MLLMs. Based on the dataset,
we train a series of MLLMs as personalized multimodal assistants. By
pretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual
concepts without additional finetuning. Our models demonstrate outstanding
flexibility and generation quality across a variety of tasks, such as
personalized image captioning, question answering and visual recognition. The
code, data and models are available at https://github.com/Hoar012/RAP-MLLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LAR-ECHR: A New Legal Argument Reasoning Task and Dataset for Cases of
  the European Court of Human Rights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Odysseas S. Chlapanis, Dimitrios Galanis, Ion Androutsopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Legal Argument Reasoning (LAR), a novel task designed to evaluate
the legal reasoning capabilities of Large Language Models (LLMs). The task
requires selecting the correct next statement (from multiple choice options) in
a chain of legal arguments from court proceedings, given the facts of the case.
We constructed a dataset (LAR-ECHR) for this task using cases from the European
Court of Human Rights (ECHR). We evaluated seven general-purpose LLMs on
LAR-ECHR and found that (a) the ranking of the models is aligned with that of
LegalBench, an established US-based legal reasoning benchmark, even though
LAR-ECHR is based on EU law, (b) LAR-ECHR distinguishes top models more
clearly, compared to LegalBench, (c) even the best model (GPT-4o) obtains 75.8%
accuracy on LAR-ECHR, indicating significant potential for further model
improvement. The process followed to construct LAR-ECHR can be replicated with
cases from other legal systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Natural Legal Language Processing (NLLP) 2024 workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representation Learning of Structured Data for Medical Foundation Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijay Prakash Dwivedi, Viktor Schlegel, Andy T. Liu, Thanh-Tung Nguyen, Abhinav Ramesh Kashyap, Jeng Wei, Wei-Hsian Yin, Stefan Winkler, Robby T. Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable performance across
various domains, including healthcare. However, their ability to effectively
represent structured non-textual data, such as the alphanumeric medical codes
used in records like ICD-10 or SNOMED-CT, is limited and has been particularly
exposed in recent research. This paper examines the challenges LLMs face in
processing medical codes due to the shortcomings of current tokenization
methods. As a result, we introduce the UniStruct architecture to design a
multimodal medical foundation model of unstructured text and structured data,
which addresses these challenges by adapting subword tokenization techniques
specifically for the structured medical codes. Our approach is validated
through model pre-training on both an extensive internal medical database and a
public repository of structured medical records. Trained on over 1 billion
tokens on the internal medical database, the proposed model achieves up to a
23% improvement in evaluation metrics, with around 2% gain attributed to our
proposed tokenization. Additionally, when evaluated on the EHRSHOT public
benchmark with a 1/1000 fraction of the pre-training data, the UniStruct model
improves performance on over 42% of the downstream tasks. Our approach not only
enhances the representation and generalization capabilities of patient-centric
models but also bridges a critical gap in representation learning models'
ability to handle complex structured medical data, alongside unstructured text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Workshop on Unifying Representations in Neural Models
  (UniReps 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cerberus: Efficient Inference with Adaptive Parallel Decoding and
  Sequential Knowledge Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Liu, Wenyuan Li, Laizhong Cui, Hailiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often face a bottleneck in inference speed due
to their reliance on auto-regressive decoding. Recently, parallel decoding has
shown significant promise in enhancing inference efficiency. However, we have
identified two key issues with existing parallel decoding frameworks: (1)
decoding heads fail to balance prediction accuracy and the parallelism of
execution, and (2) parallel decoding is not a universal solution, as it can
bring unnecessary overheads at some challenging decoding steps. To address
these issues, we propose Cerberus, an adaptive parallel decoding framework
introduces the gating mechanism to enable the LLMs to adaptively choose
appropriate decoding approaches at each decoding step, along with introducing a
new paradigm of decoding heads that introduce the sequential knowledge while
maintaining execution parallelism. The experiment results demonstrate that the
Cerberus can achieve up to 2.12x speed up compared to auto-regressive decoding,
and outperforms one of the leading parallel decoding frameworks, Medusa, with a
10% - 30% increase in acceleration and superior generation quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges
  in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Yuan, Lili Zhao, Kai Zhang, Guangting Zheng, Qi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable capabilities in various
natural language processing tasks. However, LLMs may rely on dataset biases as
shortcuts for prediction, which can significantly impair their robustness and
generalization capabilities. This paper presents Shortcut Suite, a
comprehensive test suite designed to evaluate the impact of shortcuts on LLMs'
performance, incorporating six shortcut types, five evaluation metrics, and
four prompting strategies. Our extensive experiments yield several key
findings: 1) LLMs demonstrate varying reliance on shortcuts for downstream
tasks, significantly impairing their performance. 2) Larger LLMs are more
likely to utilize shortcuts under zero-shot and few-shot in-context learning
prompts. 3) Chain-of-thought prompting notably reduces shortcut reliance and
outperforms other prompting strategies, while few-shot prompts generally
underperform compared to zero-shot prompts. 4) LLMs often exhibit
overconfidence in their predictions, especially when dealing with datasets that
contain shortcuts. 5) LLMs generally have a lower explanation quality in
shortcut-laden datasets, with errors falling into three types: distraction,
disguised comprehension, and logical fallacy. Our findings offer new insights
for evaluating robustness and generalization in LLMs and suggest potential
directions for mitigating the reliance on shortcuts. The code is available at
\url {https://github.com/yyhappier/ShortcutSuite.git}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probing-RAG: Self-Probing to Guide Language Models in Selective Document
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ingeol Baek, Hwan Chang, Byeongjeong Kim, Jimin Lee, Hwanhee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) enhances language models by retrieving
and incorporating relevant external knowledge. However, traditional
retrieve-and-generate processes may not be optimized for real-world scenarios,
where queries might require multiple retrieval steps or none at all. In this
paper, we propose a Probing-RAG, which utilizes the hidden state
representations from the intermediate layers of language models to adaptively
determine the necessity of additional retrievals for a given query. By
employing a pre-trained prober, Probing-RAG effectively captures the model's
internal cognition, enabling reliable decision-making about retrieving external
documents. Experimental results across five open-domain QA datasets demonstrate
that Probing-RAG outperforms previous methods while reducing the number of
redundant retrieval steps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLMs Have Political Correctness? Analyzing Ethical Biases and
  Jailbreak Vulnerabilities in AI Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isack Lee, Haebin Seong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large language models (LLMs) demonstrate impressive proficiency in
various tasks, they present potential safety risks, such as `jailbreaks', where
malicious inputs can coerce LLMs into generating harmful content. To address
these issues, many LLM developers have implemented various safety measures to
align these models. This alignment involves several techniques, including data
filtering during pre-training, supervised fine-tuning, reinforcement learning
from human feedback, and red-teaming exercises. These methods often introduce
deliberate and intentional biases similar to Political Correctness (PC) to
ensure the ethical behavior of LLMs. In this paper, we delve into the
intentional biases injected into LLMs for safety purposes and examine methods
to circumvent these safety alignment techniques. Notably, these intentional
biases result in a jailbreaking success rate in GPT-4o models that differs by
20% between non-binary and cisgender keywords and by 16% between white and
black keywords, even when the other parts of the prompts are identical. We
introduce the concept of PCJailbreak, highlighting the inherent risks posed by
these safety-induced biases. Additionally, we propose an efficient defense
method PCDefense, which prevents jailbreak attempts by injecting defense
prompts prior to generation. PCDefense stands as an appealing alternative to
Guard Models, such as Llama-Guard, that require additional inference cost after
text generation. Our findings emphasize the urgent need for LLM developers to
adopt a more responsible approach when designing and implementing safety
measures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Fine-Tuning</span> Language Models on Multiple Datasets for Citation Intention
  Classification <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeren Shui, Petros Karypis, Daniel S. Karls, Mingjian Wen, Saurav Manchanda, Ellad B. Tadmor, George Karypis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Citation intention Classification (CIC) tools classify citations by their
intention (e.g., background, motivation) and assist readers in evaluating the
contribution of scientific literature. Prior research has shown that pretrained
language models (PLMs) such as SciBERT can achieve state-of-the-art performance
on CIC benchmarks. PLMs are trained via self-supervision tasks on a large
corpus of general text and can quickly adapt to CIC tasks via moderate
fine-tuning on the corresponding dataset. Despite their advantages, PLMs can
easily overfit small datasets during fine-tuning. In this paper, we propose a
multi-task learning (MTL) framework that jointly fine-tunes PLMs on a dataset
of primary interest together with multiple auxiliary CIC datasets to take
advantage of additional supervision signals. We develop a data-driven task
relation learning (TRL) method that controls the contribution of auxiliary
datasets to avoid negative transfer and expensive hyper-parameter tuning. We
conduct experiments on three CIC datasets and show that fine-tuning with
additional datasets can improve the PLMs' generalization performance on the
primary dataset. PLMs fine-tuned with our proposed framework outperform the
current state-of-the-art models by 7% to 11% on small datasets while aligning
with the best-performing model on a large dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be appear as a Findings paper at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucinations in Large <span class="highlight-title">Vision-Language</span> Models via
  Summary-Guided Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyungmin Min, Minbeom Kim, Kang-il Lee, Dongryeol Lee, Kyomin Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) demonstrate impressive capabilities in
generating detailed and coherent responses from visual inputs. However, they
are prone to generate hallucinations due to an over-reliance on language
priors. To address this issue, we investigate the language priors in LVLMs and
make two key observations: (1) Even when predicting the tokens associated with
image-related part-of-speech (POS), models increasingly rely on linguistic
priors as the token sequences grow, thereby amplifying hallucinations. (2)
Methods that directly calibrate LVLM's output distribution to mitigate language
priors can lead to a degradation in text quality or even exacerbate
hallucinations. Based on these findings, we propose a novel method,
Summary-Guided Decoding (SGD). This method naturally encourages the model to
focus more on image information by reducing the text context through summaries,
while controlling only the image-related POS tokens to maintain text quality.
Through experiments, we demonstrate that SGD achieves state-of-the-art
performance on object hallucination benchmarks. Furthermore, in terms of the
trade-off between precision and recall, SGD achieves Pareto optimality among
the existing methods. Lastly, we observe that although existing methods
struggle to balance the reduction of object hallucinations with maintaining
text quality, SGD demonstrates robustness in handling this challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computational Approaches to Arabic-English Code-Switching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caroline Sabty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) is a vital computational method for
addressing language processing, analysis, and generation. NLP tasks form the
core of many daily applications, from automatic text correction to speech
recognition. While significant research has focused on NLP tasks for the
English language, less attention has been given to Modern Standard Arabic and
Dialectal Arabic. Globalization has also contributed to the rise of
Code-Switching (CS), where speakers mix languages within conversations and even
within individual words (intra-word CS). This is especially common in Arab
countries, where people often switch between dialects or between dialects and a
foreign language they master. CS between Arabic and English is frequent in
Egypt, especially on social media. Consequently, a significant amount of
code-switched content can be found online. Such code-switched data needs to be
investigated and analyzed for several NLP tasks to tackle the challenges of
this multilingual phenomenon and Arabic language challenges. No work has been
done before for several integral NLP tasks on Arabic-English CS data. In this
work, we focus on the Named Entity Recognition (NER) task and other tasks that
help propose a solution for the NER task on CS data, e.g., Language
Identification. This work addresses this gap by proposing and applying
state-of-the-art techniques for Modern Standard Arabic and Arabic-English NER.
We have created the first annotated CS Arabic-English corpus for the NER task.
Also, we apply two enhancement techniques to improve the NER tagger on CS data
using CS contextual embeddings and data augmentation techniques. All methods
showed improvements in the performance of the NER taggers on CS data. Finally,
we propose several intra-word language identification approaches to determine
the language type of a mixed text and identify whether it is a named entity or
not.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD thesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Biases to Embrace Diversity: A Comprehensive Annotation
  Benchmark for Toxic Language <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinmeng Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces a prescriptive annotation benchmark grounded in
humanities research to ensure consistent, unbiased labeling of offensive
language, particularly for casual and non-mainstream language uses. We
contribute two newly annotated datasets that achieve higher inter-annotator
agreement between human and language model (LLM) annotations compared to
original datasets based on descriptive instructions. Our experiments show that
LLMs can serve as effective alternatives when professional annotators are
unavailable. Moreover, smaller models fine-tuned on multi-source LLM-annotated
data outperform models trained on larger, single-source human-annotated
datasets. These findings highlight the value of structured guidelines in
reducing subjective variability, maintaining performance with limited data, and
embracing language diversity.
  Content Warning: This article only analyzes offensive language for academic
purposes. Discretion is advised.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures, EMNLP-NLP4DH 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reference-Based Post-OCR Processing with LLM for Diacritic Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thao Do
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting fine-grained OCR text from aged documents in diacritic languages
remains challenging due to unexpected artifacts, time-induced degradation, and
lack of datasets. While standalone spell correction approaches have been
proposed, they show limited performance for historical documents due to
numerous possible OCR error combinations and differences between modern and
classical corpus distributions. We propose a method utilizing available
content-focused ebooks as a reference base to correct imperfect OCR-generated
text, supported by large language models. This technique generates
high-precision pseudo-page-to-page labels for diacritic languages, where small
strokes pose significant challenges in historical conditions. The pipeline
eliminates various types of noise from aged documents and addresses issues such
as missing characters, words, and disordered sequences. Our post-processing
method, which generated a large OCR dataset of classical Vietnamese books,
achieved a mean grading score of 8.72 on a 10-point scale. This outperformed
the state-of-the-art transformer-based Vietnamese spell correction model, which
scored 7.03 when evaluated on a sampled subset of the dataset. We also trained
a baseline OCR model to assess and compare it with well-known engines.
Experimental results demonstrate the strength of our baseline model compared to
widely used open-source solutions. The resulting dataset will be released
publicly to support future studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Large Language Model Attribution through Self-Improving <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Huang, Xiaocheng Feng, Weitao Ma, Liang Zhao, Yuchun Fan, Weihong Zhong, Dongliang Xu, Qing Yang, Hongtao Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Teaching large language models (LLMs) to generate text with citations to
evidence sources can mitigate hallucinations and enhance verifiability in
information-seeking systems. However, improving this capability requires
high-quality attribution data, which is costly and labor-intensive. Inspired by
recent advances in self-improvement that enhance LLMs without manual
annotation, we present START, a Self-Taught AttRibuTion framework for
iteratively improving the attribution capability of LLMs. First, to prevent
models from stagnating due to initially insufficient supervision signals, START
leverages the model to self-construct synthetic training data for warming up.
To further self-improve the model's attribution ability, START iteratively
utilizes fine-grained preference supervision signals constructed from its
sampled responses to encourage robust, comprehensive, and attributable
generation. Experiments on three open-domain question-answering datasets,
covering long-form QA and multi-step reasoning, demonstrate significant
performance gains of 25.13% on average without relying on human annotations and
more advanced models. Further analysis reveals that START excels in aggregating
information across multiple sources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Route with Confidence Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Neng Chuang, Helen Zhou, Prathusha Kameswara Sarma, Parikshit Gopalan, John Boccio, Sara Bolouki, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive performance on
several tasks and are increasingly deployed in real-world applications.
However, especially in high-stakes settings, it becomes vital to know when the
output of an LLM may be unreliable. Depending on whether an answer is
trustworthy, a system can then choose to route the question to another expert,
or otherwise fall back on a safe default behavior. In this work, we study the
extent to which LLMs can reliably indicate confidence in their answers, and how
this notion of confidence can translate into downstream accuracy gains. We
propose Self-REF, a lightweight training strategy to teach LLMs to express
confidence in whether their answers are correct in a reliable manner. Self-REF
introduces confidence tokens into the LLM, from which a confidence score can be
extracted. Compared to conventional approaches such as verbalizing confidence
and examining token probabilities, we demonstrate empirically that confidence
tokens show significant improvements in downstream routing and rejection
learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BANTH: A Multi-label Hate Speech Detection Dataset for Transliterated
  Bangla 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabiha Haider, Fariha Tanjim Shifat, Md Farhan Ishmam, Deeparghya Dutta Barua, Md Sakib Ul Rahman Sourove, Md Fahim, Md Farhad Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of transliterated texts in digital spaces has emphasized
the need for detecting and classifying hate speech in languages beyond English,
particularly in low-resource languages. As online discourse can perpetuate
discrimination based on target groups, e.g. gender, religion, and origin,
multi-label classification of hateful content can help in comprehending hate
motivation and enhance content moderation. While previous efforts have focused
on monolingual or binary hate classification tasks, no work has yet addressed
the challenge of multi-label hate speech classification in transliterated
Bangla. We introduce BanTH, the first multi-label transliterated Bangla hate
speech dataset comprising 37.3k samples. The samples are sourced from YouTube
comments, where each instance is labeled with one or more target groups,
reflecting the regional demographic. We establish novel transformer
encoder-based baselines by further pre-training on transliterated Bangla
corpus. We also propose a novel translation-based LLM prompting strategy for
transliterated text. Experiments reveal that our further pre-trained encoders
are achieving state-of-the-art performance on the BanTH dataset, while our
translation-based prompting outperforms other strategies in the zero-shot
setting. The introduction of BanTH not only fills a critical gap in hate speech
research for Bangla but also sets the stage for future exploration into
code-mixed and multi-label classification challenges in underrepresented
languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Hayden Kwok-Hay So, Ting Cao, Fan Yang, Mao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention is the cornerstone of modern Large Language Models (LLMs). Yet its
quadratic complexity limits the efficiency and scalability of LLMs, especially
for those with a long-context window. A promising approach addressing this
limitation is to leverage the sparsity in attention. However, existing
sparsity-based solutions predominantly rely on predefined patterns or
heuristics to approximate sparsity. This practice falls short to fully capture
the dynamic nature of attention sparsity in language-based tasks. This paper
argues that attention sparsity should be learned rather than predefined. To
this end, we design SeerAttention, a new Attention mechanism that augments the
conventional attention with a learnable gate that adaptively selects
significant blocks in an attention map and deems the rest blocks sparse. Such
block-level sparsity effectively balances accuracy and speedup. To enable
efficient learning of the gating network, we develop a customized
FlashAttention implementation that extracts the block-level ground truth of
attention map with minimum overhead. SeerAttention not only applies to
post-training, but also excels in long-context fine-tuning. Our results show
that at post-training stages, SeerAttention significantly outperforms
state-of-the-art static or heuristic-based sparse attention methods, while also
being more versatile and flexible to adapt to varying context lengths and
sparsity ratios. When applied to long-context fine-tuning with YaRN,
SeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context
length with minimal perplexity loss, offering a 5.67x speedup over
FlashAttention-2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minseok Choi, ChaeHun Park, Dohyun Lee, Jaegul Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) serve as giant information stores, often
including personal or copyrighted data, and retraining them from scratch is not
a viable option. This has led to the development of various fast, approximate
unlearning techniques to selectively remove knowledge from LLMs. Prior research
has largely focused on minimizing the probabilities of specific token sequences
by reversing the language modeling objective. However, these methods still
leave LLMs vulnerable to adversarial attacks that exploit indirect references.
In this work, we examine the limitations of current unlearning techniques in
effectively erasing a particular type of indirect prompt: multi-hop queries.
Our findings reveal that existing methods fail to completely remove multi-hop
knowledge when one of the intermediate hops is unlearned. To address this
issue, we propose MUNCH, a simple uncertainty-based approach that breaks down
multi-hop queries into subquestions and leverages the uncertainty of the
unlearned model in final decision-making. Empirical results demonstrate the
effectiveness of our framework, and MUNCH can be easily integrated with
existing unlearning techniques, making it a flexible and useful solution for
enhancing unlearning processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Roadmap towards Superhuman Speech Understanding using Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Bu, Yuhao Zhang, Xidong Wang, Benyou Wang, Qun Liu, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of large language models (LLMs) has prompted efforts to integrate
speech and audio data, aiming to create general foundation models capable of
processing both textual and non-textual inputs. Recent advances, such as
GPT-4o, highlight the potential for end-to-end speech LLMs, which preserves
non-semantic information and world knowledge for deeper speech understanding.
To guide the development of speech LLMs, we propose a five-level roadmap,
ranging from basic automatic speech recognition (ASR) to advanced superhuman
models capable of integrating non-semantic information with abstract acoustic
knowledge for complex tasks. Moreover, we design a benchmark, SAGI Bechmark,
that standardizes critical aspects across various tasks in these five levels,
uncovering challenges in using abstract acoustic knowledge and completeness of
capability. Our findings reveal gaps in handling paralinguistic cues and
abstract acoustic knowledge, and we offer future directions. This paper
outlines a roadmap for advancing speech LLMs, introduces a benchmark for
evaluation, and provides key insights into their current limitations and
potential.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLaMP 2: <span class="highlight-title">Multimodal</span> Music Information Retrieval Across 101 Languages
  Using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangda Wu, Yashan Wang, Ruibin Yuan, Zhancheng Guo, Xu Tan, Ge Zhang, Monan Zhou, Jing Chen, Xuefeng Mu, Yuejie Gao, Yuanliang Dong, Jiafeng Liu, Xiaobing Li, Feng Yu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Challenges in managing linguistic diversity and integrating various musical
modalities are faced by current music information retrieval systems. These
limitations reduce their effectiveness in a global, multimodal music
environment. To address these issues, we introduce CLaMP 2, a system compatible
with 101 languages that supports both ABC notation (a text-based musical
notation format) and MIDI (Musical Instrument Digital Interface) for music
information retrieval. CLaMP 2, pre-trained on 1.5 million ABC-MIDI-text
triplets, includes a multilingual text encoder and a multimodal music encoder
aligned via contrastive learning. By leveraging large language models, we
obtain refined and consistent multilingual descriptions at scale, significantly
reducing textual noise and balancing language distribution. Our experiments
show that CLaMP 2 achieves state-of-the-art results in both multilingual
semantic search and music classification across modalities, thus establishing a
new standard for inclusive and global music information retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Babbling to Fluency: Evaluating the Evolution of Language Models in
  Terms of Human Language Acquisition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyuan Yang, Pengda Wang, Luke D. Plonsky, Frederick L. Oswald, Hanjie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine the language capabilities of language models (LMs) from the
critical perspective of human language acquisition. Building on classical
language development theories, we propose a three-stage framework to assess the
abilities of LMs, ranging from preliminary word understanding to complex
grammar and complex logical reasoning. Using this framework, we evaluate the
generative capacities of LMs using methods from linguistic research. Results
indicate that although recent LMs outperform earlier models in overall
performance, their developmental trajectory does not strictly follow the path
of human language acquisition. Notably, in generation tasks, LMs are more
similar to human performance in areas where information is easier to extract
from the corpus, such as average word length, clauses, and auxiliary verbs.
Newer LMs did not exhibit significant progress in terms of specific dimensions,
such as clauses and auxiliary verbs, where the variation across corpora is
relatively limited. Register theory offers a plausible explanation for these
observations, suggesting that the linguistic features of the training data have
a substantial impact on the models' abilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Systematic Investigation of Knowledge Retrieval and Selection for
  Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangci Li, Jessica Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has emerged as a powerful method for
enhancing natural language generation by integrating external knowledge into a
model's output. While prior work has demonstrated the importance of improving
knowledge retrieval for boosting generation quality, the role of knowledge
selection remains less clear. In this paper, we perform a comprehensive
analysis of how knowledge retrieval and selection influence downstream
generation performance in RAG systems. By simulating different retrieval and
selection conditions through a controlled mixture of gold and distractor
knowledge, we assess the impact of these factors on generation outcomes. Our
findings indicate that the downstream generator model's capability, as well as
the complexity of the task and dataset, significantly influence the impact of
knowledge retrieval and selection on the overall RAG system performance. In
typical scenarios, improving the knowledge recall score is key to enhancing
generation outcomes, with the knowledge selector providing a limited additional
benefit when a strong generator model is used on clear, well-defined tasks. For
weaker generator models or more ambiguous tasks and datasets, the knowledge F1
score becomes a critical factor, and the knowledge selector plays a more
prominent role in improving overall performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Translation Alignment Pipeline for Multilingual Digital
  Editions of Literary Works 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Levchenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the application of translation alignment algorithms
in the creation of a Multilingual Digital Edition (MDE) of Alessandro Manzoni's
Italian novel "I promessi sposi" ("The Betrothed"), with translations in eight
languages (English, Spanish, French, German, Dutch, Polish, Russian and
Chinese) from the 19th and 20th centuries. We identify key requirements for the
MDE to improve both the reader experience and support for translation studies.
Our research highlights the limitations of current state-of-the-art algorithms
when applied to the translation of literary texts and outlines an automated
pipeline for MDE creation. This pipeline transforms raw texts into web-based,
side-by-side representations of original and translated texts with different
rendering options. In addition, we propose new metrics for evaluating the
alignment of literary translations and suggest visualization techniques for
future analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, Computational Humanities Research Conference, December 4-6,
  2024, Aarhus, Denmark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling Likes and Dislikes in Personalized Generative Explainable
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryotaro Shimizu, Takashi Wada, Yu Wang, Johannes Kruse, Sean O'Brien, Sai HtaungKham, Linxin Song, Yuya Yoshikawa, Yuki Saito, Fugee Tsung, Masayuki Goto, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research on explainable recommendation generally frames the task as a
standard text generation problem, and evaluates models simply based on the
textual similarity between the predicted and ground-truth explanations.
However, this approach fails to consider one crucial aspect of the systems:
whether their outputs accurately reflect the users' (post-purchase) sentiments,
i.e., whether and why they would like and/or dislike the recommended items. To
shed light on this issue, we introduce new datasets and evaluation methods that
focus on the users' sentiments. Specifically, we construct the datasets by
explicitly extracting users' positive and negative opinions from their
post-purchase reviews using an LLM, and propose to evaluate systems based on
whether the generated explanations 1) align well with the users' sentiments,
and 2) accurately identify both positive and negative opinions of users on the
target items. We benchmark several recent models on our datasets and
demonstrate that achieving strong performance on existing metrics does not
ensure that the generated explanations align well with the users' sentiments.
Lastly, we find that existing models can provide more sentiment-aware
explanations when the users' (predicted) ratings for the target items are
directly fed into the models as input. We will release our code and datasets
upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Atomic Calibration of LLMs in Long-Form Generations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caiqi Zhang, Ruihan Yang, Zhisong Zhang, Xinting Huang, Sen Yang, Dong Yu, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often suffer from hallucinations, posing
significant challenges for real-world applications. Confidence calibration,
which estimates the underlying uncertainty of model predictions, is essential
to enhance the LLMs' trustworthiness. Existing research on LLM calibration has
primarily focused on short-form tasks, providing a single confidence score at
the response level (macro calibration). However, this approach is insufficient
for long-form generations, where responses often contain more complex
statements and may include both accurate and inaccurate information. Therefore,
we introduce atomic calibration, a novel approach that evaluates factuality
calibration at a fine-grained level by breaking down long responses into atomic
claims. We classify confidence elicitation methods into discriminative and
generative types and demonstrate that their combination can enhance
calibration. Our extensive experiments on various LLMs and datasets show that
atomic calibration is well-suited for long-form generation and can also improve
macro calibration results. Additionally, atomic calibration reveals insightful
patterns in LLM confidence throughout the generation process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models are Easily Confused: A Quantitative Metric,
  Security Implications and Typological Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyi Chen, Qiongxiu Li, Russa Biswas, Johannes Bjerva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Confusion is a phenomenon where Large Language Models (LLMs)
generate text that is neither in the desired language, nor in a contextually
appropriate language. This phenomenon presents a critical challenge in text
generation by LLMs, often appearing as erratic and unpredictable behavior. We
hypothesize that there are linguistic regularities to this inherent
vulnerability in LLMs and shed light on patterns of language confusion across
LLMs. We introduce a novel metric, Language Confusion Entropy, designed to
directly measure and quantify this confusion, based on language distributions
informed by linguistic typology and lexical variation. Comprehensive
comparisons with the Language Confusion Benchmark (Marchisio et al., 2024)
confirm the effectiveness of our metric, revealing patterns of language
confusion across LLMs. We further link language confusion to LLM security, and
find patterns in the case of multilingual embedding inversion attacks. Our
analysis demonstrates that linguistic typology offers theoretically grounded
interpretation, and valuable insights into leveraging language similarities as
a prior for LLM alignment and security.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPIN: Self-Supervised <span class="highlight-title">Prompt</span> INjection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Zhou, Junfeng Yang, Chengzhi Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly used in a variety of important
applications, yet their safety and reliability remain as major concerns.
Various adversarial and jailbreak attacks have been proposed to bypass the
safety alignment and cause the model to produce harmful responses. We introduce
Self-supervised Prompt INjection (SPIN) which can detect and reverse these
various attacks on LLMs. As our self-supervised prompt defense is done at
inference-time, it is also compatible with existing alignment and adds an
additional layer of safety for defense. Our benchmarks demonstrate that our
system can reduce the attack success rate by up to 87.9%, while maintaining the
performance on benign user requests. In addition, we discuss the situation of
an adaptive attacker and show that our method is still resilient against
attackers who are aware of our defense.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Web Agents with World Models: Learning and Leveraging Environment
  Dynamics in Web Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyungjoo Chae, Namyoung Kim, Kai Tzu-iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan Kim, Dongha Lee, Jinyoung Yeo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have recently gained much attention in building
autonomous agents. However, the performance of current LLM-based web agents in
long-horizon tasks is far from optimal, often yielding errors such as
repeatedly buying a non-refundable flight ticket. By contrast, humans can avoid
such an irreversible mistake, as we have an awareness of the potential outcomes
(e.g., losing money) of our actions, also known as the "world model". Motivated
by this, our study first starts with preliminary analyses, confirming the
absence of world models in current LLMs (e.g., GPT-4o, Claude-3.5-Sonnet,
etc.). Then, we present a World-model-augmented (WMA) web agent, which
simulates the outcomes of its actions for better decision-making. To overcome
the challenges in training LLMs as world models predicting next observations,
such as repeated elements across observations and long HTML inputs, we propose
a transition-focused observation abstraction, where the prediction objectives
are free-form natural language descriptions exclusively highlighting important
state differences between time steps. Experiments on WebArena and Mind2Web show
that our world models improve agents' policy selection without training and
demonstrate our agents' cost- and time-efficiency compared to recent
tree-search-based agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proof Flow: Preliminary Study on Generative Flow Network Language Model
  Tuning for Formal Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Ho, Vincent Zhu, Xiaoyin Chen, Moksh Jain, Nikolay Malkin, Edwin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning is a fundamental substrate for solving novel and complex problems.
Deliberate efforts in learning and developing frameworks around System 2
reasoning have made great strides, yet problems of sufficient complexity remain
largely out of reach for open models. To address this gap, we examine the
potential of Generative Flow Networks as a fine-tuning method for LLMs to
unlock advanced reasoning capabilities. In this paper, we present a proof of
concept in the domain of formal reasoning, specifically in the Neural Theorem
Proving (NTP) setting, where proofs specified in a formal language such as Lean
can be deterministically and objectively verified. Unlike classical
reward-maximization reinforcement learning, which frequently over-exploits
high-reward actions and fails to effectively explore the state space, GFlowNets
have emerged as a promising approach for sampling compositional objects,
improving generalization, and enabling models to maintain diverse hypotheses.
Our early results demonstrate GFlowNet fine-tuning's potential for enhancing
model performance in a search setting, which is especially relevant given the
paradigm shift towards inference time compute scaling and "thinking slowly."
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CBT-Bench: Evaluating Large Language Models on Assisting Cognitive
  Behavior Therapy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mian Zhang, Xianjun Yang, Xinlu Zhang, Travis Labrum, Jamie C. Chiu, Shaun M. Eack, Fei Fang, William Yang Wang, Zhiyu Zoey Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a significant gap between patient needs and available mental health
support today. In this paper, we aim to thoroughly examine the potential of
using Large Language Models (LLMs) to assist professional psychotherapy. To
this end, we propose a new benchmark, CBT-BENCH, for the systematic evaluation
of cognitive behavioral therapy (CBT) assistance. We include three levels of
tasks in CBT-BENCH: I: Basic CBT knowledge acquisition, with the task of
multiple-choice questions; II: Cognitive model understanding, with the tasks of
cognitive distortion classification, primary core belief classification, and
fine-grained core belief classification; III: Therapeutic response generation,
with the task of generating responses to patient speech in CBT therapy
sessions. These tasks encompass key aspects of CBT that could potentially be
enhanced through AI assistance, while also outlining a hierarchy of capability
requirements, ranging from basic knowledge recitation to engaging in real
therapeutic conversations. We evaluated representative LLMs on our benchmark.
Experimental results indicate that while LLMs perform well in reciting CBT
knowledge, they fall short in complex real-world scenarios requiring deep
analysis of patients' cognitive structures and generating effective responses,
suggesting potential future work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anchored Alignment for Self-Explanations Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Felipe Villa-Arenas, Ata Nizamoglu, Qianli Wang, Sebastian Möller, Vera Schmitt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce a methodology for alignment designed to enhance
the ability of large language models (LLMs) to articulate their reasoning
(self-explanation) even in the absence of annotated rationale explanations. Our
alignment methodology comprises three key components: explanation quality
assessment, self-instruction dataset generation, and model alignment.
Additionally, we present a novel technique called Alignment with Anchor
Preference Pairs, which improves the selection of preference pairs by
categorizing model outputs into three groups: consistently correct,
consistently incorrect, and variable. By applying tailored strategies to each
category, we enhance the effectiveness of Direct Preference Optimization (DPO).
Our experimental results demonstrate that this approach significantly improves
explanation quality while maintaining accuracy compared to other fine-tuning
strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaithBench: A Diverse Hallucination Benchmark for Summarization by
  Modern LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Forrest Sheng Bao, Miaoran Li, Renyi Qu, Ge Luo, Erana Wan, Yujia Tang, Weisi Fan, Manveer Singh Tamber, Suleman Kazi, Vivek Sourabh, Mike Qi, Ruixuan Tu, Chenyu Xu, Matthew Gonzales, Ofer Mendelevitch, Amin Ahmad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Summarization is one of the most common tasks performed by large language
models (LLMs), especially in applications like Retrieval-Augmented Generation
(RAG). However, existing evaluations of hallucinations in LLM-generated
summaries, and evaluations of hallucination detection models both suffer from a
lack of diversity and recency in the LLM and LLM families considered. This
paper introduces FaithBench, a summarization hallucination benchmark comprising
challenging hallucinations made by 10 modern LLMs from 8 different families,
with ground truth annotations by human experts. ``Challenging'' here means
summaries on which popular, state-of-the-art hallucination detection models,
including GPT-4o-as-a-judge, disagreed on. Our results show GPT-4o and
GPT-3.5-Turbo produce the least hallucinations. However, even the best
hallucination detection models have near 50\% accuracies on FaithBench,
indicating lots of room for future improvement. The repo is
https://github.com/vectara/FaithBench
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BQA: Body Language Question Answering Dataset for Video Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shintaro Ozaki, Kazuki Hayashi, Miyu Oba, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A large part of human communication relies on nonverbal cues such as facial
expressions, eye contact, and body language. Unlike language or sign language,
such nonverbal communication lacks formal rules, requiring complex reasoning
based on commonsense understanding. Enabling current Video Large Language
Models (VideoLLMs) to accurately interpret body language is a crucial
challenge, as human unconscious actions can easily cause the model to
misinterpret their intent. To address this, we propose a dataset, BQA, a body
language question answering dataset, to validate whether the model can
correctly interpret emotions from short clips of body language comprising 26
emotion labels of videos of body language. We evaluated various VideoLLMs on
BQA and revealed that understanding body language is challenging, and our
analyses of the wrong answers by VideoLLMs show that certain VideoLLMs made
significantly biased answers depending on the age group and ethnicity of the
individuals in the video. The dataset is available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring Free-Form Decision-Making Inconsistency of Language Models in
  Military Crisis Simulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryan Shrivastava, Jessica Hullman, Max Lamparth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is an increasing interest in using language models (LMs) for automated
decision-making, with multiple countries actively testing LMs to aid in
military crisis decision-making. To scrutinize relying on LM decision-making in
high-stakes settings, we examine the inconsistency of responses in a crisis
simulation ("wargame"), similar to reported tests conducted by the US military.
Prior work illustrated escalatory tendencies and varying levels of aggression
among LMs but were constrained to simulations with pre-defined actions. This
was due to the challenges associated with quantitatively measuring semantic
differences and evaluating natural language decision-making without relying on
pre-defined actions. In this work, we query LMs for free form responses and use
a metric based on BERTScore to measure response inconsistency quantitatively.
Leveraging the benefits of BERTScore, we show that the inconsistency metric is
robust to linguistic variations that preserve semantic meaning in a
question-answering setting across text lengths. We show that all five tested
LMs exhibit levels of inconsistency that indicate semantic differences, even
when adjusting the wargame setting, anonymizing involved conflict countries, or
adjusting the sampling temperature parameter $T$. Further qualitative
evaluation shows that models recommend courses of action that share few to no
similarities. We also study the impact of different prompt sensitivity
variations on inconsistency at temperature $T = 0$. We find that inconsistency
due to semantically equivalent prompt variations can exceed response
inconsistency from temperature sampling for most studied models across
different levels of ablations. Given the high-stakes nature of military
deployment, we recommend further consideration be taken before using LMs to
inform military decisions or other cases of high-stakes decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Multilingual LLM Evaluation for European Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Klaudia Thellmann, Bernhard Stadler, Michael Fromm, Jasper Schulze Buschhoff, Alex Jude, Fabio Barth, Johannes Leveling, Nicolas Flores-Herr, Joachim Köhler, René Jäkel, Mehdi Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of Large Language Models (LLMs) has revolutionized natural language
processing across numerous languages and tasks. However, evaluating LLM
performance in a consistent and meaningful way across multiple European
languages remains challenging, especially due to the scarcity of
language-parallel multilingual benchmarks. We introduce a multilingual
evaluation approach tailored for European languages. We employ translated
versions of five widely-used benchmarks to assess the capabilities of 40 LLMs
across 21 European languages. Our contributions include examining the
effectiveness of translated benchmarks, assessing the impact of different
translation services, and offering a multilingual evaluation framework for LLMs
that includes newly created datasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC,
EU20-TruthfulQA, and EU20-GSM8K. The benchmarks and results are made publicly
available to encourage further research in multilingual LLM evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive
  Study and Hybrid Approach <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16833v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16833v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) has been a powerful tool for Large
Language Models (LLMs) to efficiently process overly lengthy contexts. However,
recent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to
understand long contexts directly. We conduct a comprehensive comparison
between RAG and long-context (LC) LLMs, aiming to leverage the strengths of
both. We benchmark RAG and LC across various public datasets using three latest
LLMs. Results reveal that when resourced sufficiently, LC consistently
outperforms RAG in terms of average performance. However, RAG's significantly
lower cost remains a distinct advantage. Based on this observation, we propose
Self-Route, a simple yet effective method that routes queries to RAG or LC
based on model self-reflection. Self-Route significantly reduces the
computation cost while maintaining a comparable performance to LC. Our findings
provide a guideline for long-context applications of LLMs using RAG and LC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 industry track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Many-Shot In-Context Learning <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11018v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11018v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet, Luis Rosias, Stephanie Chan, Biao Zhang, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu, Feryal Behbahani, Aleksandra Faust, Hugo Larochelle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) excel at few-shot in-context learning (ICL) --
learning from a few examples provided in context at inference, without any
weight updates. Newly expanded context windows allow us to investigate ICL with
hundreds or thousands of examples -- the many-shot regime. Going from few-shot
to many-shot, we observe significant performance gains across a wide variety of
generative and discriminative tasks. While promising, many-shot ICL can be
bottlenecked by the available amount of human-generated examples. To mitigate
this limitation, we explore two new settings: Reinforced and Unsupervised ICL.
Reinforced ICL uses model-generated chain-of-thought rationales in place of
human examples. Unsupervised ICL removes rationales from the prompt altogether,
and prompts the model only with domain-specific questions. We find that both
Reinforced and Unsupervised ICL can be quite effective in the many-shot regime,
particularly on complex reasoning tasks. Finally, we demonstrate that, unlike
few-shot learning, many-shot learning is effective at overriding pretraining
biases, can learn high-dimensional functions with numerical inputs, and
performs comparably to fine-tuning. We also find that inference cost increases
linearly in the many-shot regime, and frontier LLMs benefit from many-shot ICL
to varying degrees. Our analysis also reveals the limitations of next-token
prediction loss as an indicator of downstream ICL performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Topic Language Model on Heterogeneous Children's Mental Health
  Clinical Notes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanwen Ye, Tatiana Moreno, Adrianne Alpern, Louis Ehwerhemuepha, Annie Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mental health diseases affect children's lives and well-beings which have
received increased attention since the COVID-19 pandemic. Analyzing psychiatric
clinical notes with topic models is critical to evaluating children's mental
status over time. However, few topic models are built for longitudinal
settings, and most existing approaches fail to capture temporal trajectories
for each document. To address these challenges, we develop a dynamic topic
model with consistent topics and individualized temporal dependencies on the
evolving document metadata. Our model preserves the semantic meaning of
discovered topics over time and incorporates heterogeneity among documents. In
particular, when documents can be categorized, we propose a classifier-free
approach to maximize topic heterogeneity across different document groups. We
also present an efficient variational optimization procedure adapted for the
multistage longitudinal setting. In this case study, we apply our method to the
psychiatric clinical notes from a large tertiary pediatric hospital in Southern
California and achieve a 38% increase in the overall coherence of extracted
topics. Our real data analysis reveals that children tend to express more
negative emotions during state shutdowns and more positive when schools reopen.
Furthermore, it suggests that sexual and gender minority (SGM) children display
more pronounced reactions to major COVID-19 events and a greater sensitivity to
vaccine-related news than non-SGM children. This study examines children's
mental health progression during the pandemic and offers clinicians valuable
insights to recognize disparities in children's mental health related to their
sexual and gender identities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Impact of Visual Information in Chinese Characters: Evaluating Large
  Models' Ability to Recognize and Utilize Radicals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09013v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09013v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Wu, Karl Stratos, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The glyphic writing system of Chinese incorporates information-rich visual
features in each character, such as radicals that provide hints about meaning
or pronunciation. However, there has been no investigation into whether
contemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) can
harness these sub-character features in Chinese through prompting. In this
study, we establish a benchmark to evaluate LLMs' and VLMs' understanding of
visual elements in Chinese characters, including radicals, composition
structures, strokes, and stroke counts. Our results reveal that models
surprisingly exhibit some, but still limited, knowledge of the visual
information, regardless of whether images of characters are provided. To incite
models' ability to use radicals, we further experiment with incorporating
radicals into the prompts for Chinese language processing (CLP) tasks. We
observe consistent improvement in Part-Of-Speech tagging when providing
additional information about radicals, suggesting the potential to enhance CLP
by integrating sub-character information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Superlatives in Context: Modeling the Implicit Semantics of Superlatives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentina Pyatkin, Bonnie Webber, Ido Dagan, Reut Tsarfaty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Superlatives are used to single out elements with a maximal/minimal property.
Semantically, superlatives perform a set comparison: something (or some things)
has the min/max property out of a set. As such, superlatives provide an ideal
phenomenon for studying implicit phenomena and discourse restrictions. While
this comparison set is often not explicitly defined, its (implicit)
restrictions can be inferred from the discourse context the expression appears
in. In this work we provide an extensive computational study on the semantics
of superlatives. We propose a unified account of superlative semantics which
allows us to derive a broad-coverage annotation schema. Using this unified
schema we annotated a multi-domain dataset of superlatives and their semantic
interpretations. We specifically focus on interpreting implicit or ambiguous
superlative expressions, by analyzing how the discourse context restricts the
set of interpretations. In a set of experiments we then analyze how well models
perform at variations of predicting superlative semantics, with and without
context. We show that the fine-grained semantics of superlatives in context can
be challenging for contemporary models, including GPT-4.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Larger Language Models Don't Care How You Think: Why Chain-of-Thought
  <span class="highlight-title">Prompt</span>ing Fails in Subjective Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06173v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06173v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Chochlakis, Niyantha Maruthu Pandiyan, Kristina Lerman, Shrikanth Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the
dominant technique for performing natural language tasks, as it does not
require updating the model parameters with gradient-based methods. ICL promises
to "adapt" the LLM to perform the present task at a competitive or
state-of-the-art level at a fraction of the computational cost. ICL can be
augmented by incorporating the reasoning process to arrive at the final label
explicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.
However, recent work has found that ICL relies mostly on the retrieval of task
priors and less so on "learning" to perform tasks, especially for complex
subjective domains like emotion and morality, where priors ossify posterior
predictions. In this work, we examine whether "enabling" reasoning also creates
the same behavior in LLMs, wherein the format of CoT retrieves reasoning priors
that remain relatively unchanged despite the evidence in the prompt. We find
that, surprisingly, CoT indeed suffers from the same posterior collapse as ICL
for larger language models. Code is avalaible at
https://github.com/gchochla/cot-priors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, 1 table. arXiv admin note: text overlap with
  arXiv:2403.17125</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Natural Language Processing Methods for the Study of Protein-Ligand
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Michels, Ramya Bandarupalli, Amin Ahangar Akbari, Thai Le, Hong Xiao, Jing Li, Erik F. Y. Hom
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Natural Language Processing (NLP) have ignited interest in
developing effective methods for predicting protein-ligand interactions (PLIs)
given their relevance to drug discovery and protein engineering efforts and the
ever-growing volume of biochemical sequence and structural data available. The
parallels between human languages and the "languages" used to represent
proteins and ligands have enabled the use of NLP machine learning approaches to
advance PLI studies. In this review, we explain where and how such approaches
have been applied in the recent literature and discuss useful mechanisms such
as long short-term memory, transformers, and attention. We conclude with a
discussion of the current limitations of NLP methods for the study of PLIs as
well as key challenges that need to be addressed in future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 Pages and 3 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Models Generate High-quality Patent Claims? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19465v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19465v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lekang Jiang, Caiqi Zhang, Pascal A Scherz, Stephan Goetz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown exceptional performance across
various text generation tasks but remain under-explored in the patent domain,
which offers highly structured and precise language. This paper constructs a
dataset to investigate the performance of current LLMs in patent claim
generation. Our results demonstrate that generating claims based on patent
descriptions outperforms previous research relying on abstracts. Interestingly,
current patent-specific LLMs perform much worse than state-of-the-art general
LLMs, highlighting the necessity for future research on in-domain LLMs. We also
find that LLMs can produce high-quality first independent claims, but their
performances markedly decrease for subsequent dependent claims. Moreover,
fine-tuning can enhance the completeness of inventions' features, conceptual
clarity, and feature linkage. Among the tested LLMs, GPT-4 demonstrates the
best performance in comprehensive human evaluations by patent experts, with
better feature coverage, conceptual clarity, and technical coherence. Despite
these capabilities, comprehensive revision and modification are still necessary
to pass rigorous patent scrutiny and ensure legal robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 2 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling Human Subjectivity in LLMs Using Explicit and Implicit Human
  Factors in Personas <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14462v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14462v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salvatore Giorgi, Tingting Liu, Ankit Aich, Kelsey Isman, Garrick Sherman, Zachary Fried, João Sedoc, Lyle H. Ungar, Brenda Curtis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly being used in human-centered
social scientific tasks, such as data annotation, synthetic data creation, and
engaging in dialog. However, these tasks are highly subjective and dependent on
human factors, such as one's environment, attitudes, beliefs, and lived
experiences. Thus, it may be the case that employing LLMs (which do not have
such human factors) in these tasks results in a lack of variation in data,
failing to reflect the diversity of human experiences. In this paper, we
examine the role of prompting LLMs with human-like personas and asking the
models to answer as if they were a specific human. This is done explicitly,
with exact demographics, political beliefs, and lived experiences, or
implicitly via names prevalent in specific populations. The LLM personas are
then evaluated via (1) subjective annotation task (e.g., detecting toxicity)
and (2) a belief generation task, where both tasks are known to vary across
human factors. We examine the impact of explicit vs. implicit personas and
investigate which human factors LLMs recognize and respond to. Results show
that explicit LLM personas show mixed results when reproducing known human
biases, but generally fail to demonstrate implicit biases. We conclude that
LLMs may capture the statistical patterns of how people speak, but are
generally unable to model the complex interactions and subtleties of human
perceptions, potentially limiting their effectiveness in social science
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ uDistil-Whisper: Label-Free Data Filtering for Knowledge Distillation in
  Low-Data Regimes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01257v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01257v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Waheed, Karima Kadaoui, Bhiksha Raj, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work on distilling Whisper's knowledge into small models using
pseudo-labels shows promising performance while reducing the size by up to
50\%. This results in small, efficient, and dedicated models. However, a
critical step of distillation from pseudo-labels involves filtering
high-quality predictions and using only those during training. This step
requires ground truth labels to compare and filter low-quality examples making
the whole process supervised. In addition to that, the distillation process
requires a large amount of data thereby limiting the ability to distill models
in low-resource settings. To address this challenge, we propose a distillation
framework that does not require any labeled data. Through experimentation, we
show that our best distilled models outperform the teacher model by 5-7 points
in terms of WER compared to those without filtering and are on par with or
perform better than similar supervised data filtering setups. When we scale the
data, our models significantly outperform all zero-shot and supervised models.
We demonstrate that it is possible to distill large Whisper models into
relatively small ones without using any labeled data. Our distilled models are
also 25-50\% more compute- and memory-efficient while maintaining performance
equal to or better than that of the teacher model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ K-Level Reasoning: Establishing Higher Order Beliefs in Large Language
  Models for Strategic Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Yan Xia, Man Lan, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Strategic reasoning is a complex yet essential capability for intelligent
agents. It requires Large Language Model (LLM) agents to adapt their strategies
dynamically in multi-agent environments. Unlike static reasoning tasks, success
in these contexts depends on anticipating other agents' beliefs and actions
while continuously adjusting strategies to achieve individual goals. LLMs and
LLM agents often struggle with strategic reasoning due to the absence of a
reasoning framework that enables them to dynamically infer others' perspectives
and adapt to changing environments. Inspired by the Level-K framework from game
theory and behavioral economics, which extends reasoning from simple reactions
to structured strategic depth, we propose a novel framework: "K-Level Reasoning
with Large Language Models (K-R)." This framework employs recursive mechanisms
to enable LLMs to achieve varying levels of strategic depth, allowing agents to
form higher order beliefs - beliefs about others' beliefs. We validate this
framework through rigorous testing on four testbeds: two classical game theory
problems and two social intelligence tasks. The results demonstrate the
advantages of K-R in strategic reasoning. Our work presents the first recursive
implementation of strategic depth in large language models (LLMs). It
establishes a foundation for future research into theory of mind and strategic
reasoning in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Coarse-Grained Matching in Video-Text Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12407v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12407v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aozhu Chen, Hazel Doughty, Xirong Li, Cees G. M. Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-text retrieval has seen significant advancements, yet the ability of
models to discern subtle differences in captions still requires verification.
In this paper, we introduce a new approach for fine-grained evaluation. Our
approach can be applied to existing datasets by automatically generating hard
negative test captions with subtle single-word variations across nouns, verbs,
adjectives, adverbs, and prepositions. We perform comprehensive experiments
using four state-of-the-art models across two standard benchmarks (MSR-VTT and
VATEX) and two specially curated datasets enriched with detailed descriptions
(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our
analyses show that the current evaluation benchmarks fall short in detecting a
model's ability to perceive subtle single-word differences, 2) our fine-grained
evaluation highlights the difficulty models face in distinguishing such subtle
variations. To enhance fine-grained understanding, we propose a new baseline
that can be easily combined with current methods. Experiments on our
fine-grained evaluations demonstrate that this approach enhances a model's
ability to understand fine-grained differences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding and Mitigating Language Confusion in LLMs <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20052v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20052v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kelly Marchisio, Wei-Yin Ko, Alexandre Bérard, Théo Dehaze, Sebastian Ruder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate a surprising limitation of LLMs: their inability to
consistently generate text in a user's desired language. We create the Language
Confusion Benchmark (LCB) to evaluate such failures, covering 15 typologically
diverse languages with existing and newly-created English and multilingual
prompts. We evaluate a range of LLMs on monolingual and cross-lingual
generation reflecting practical use cases, finding that Llama Instruct and
Mistral models exhibit high degrees of language confusion and even the
strongest models fail to consistently respond in the correct language. We
observe that base and English-centric instruct models are more prone to
language confusion, which is aggravated by complex prompts and high sampling
temperatures. We find that language confusion can be partially mitigated via
few-shot prompting, multilingual SFT and preference tuning. We release our
language confusion benchmark, which serves as a first layer of efficient,
scalable multilingual evaluation at
https://github.com/for-ai/language-confusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main Conference Camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16635v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16635v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Akhauri, Ahmed F AbouElhamayed, Jordan Dotzel, Zhiru Zhang, Alexander M Rush, Safeen Huda, Mohamed S Abdelfattah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high power consumption and latency-sensitive deployments of large
language models (LLMs) have motivated efficiency techniques like quantization
and sparsity. Contextual sparsity, where the sparsity pattern is
input-dependent, is crucial in LLMs because the permanent removal of attention
heads or neurons from LLMs can significantly degrade accuracy. Prior work has
attempted to model contextual sparsity using neural networks trained to predict
activation magnitudes, which can be used to dynamically prune structures with
low predicted activation magnitude. In this paper, we look beyond
magnitude-based pruning criteria to assess attention head and neuron importance
in LLMs. We develop a novel predictor called ShadowLLM, which can shadow the
LLM behavior and enforce better sparsity patterns, resulting in over 15%
improvement in end-to-end accuracy compared to prior methods. In addition,
ShadowLLM achieves up to a 20% speed-up over the state-of-the-art DejaVu
framework. These enhancements are validated on Llama-2 and OPT models with up
to 30 billion parameters. Our code is available at
\href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Main, Long Paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Block-Attention for Efficient RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15355v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15355v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        East Sun, Yan Wang, Lan Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Block-Attention, an attention mechanism designed to address the
increased inference latency and cost in Retrieval-Augmented Generation (RAG)
scenarios. Traditional approaches often encode the entire context. Instead,
Block-Attention divides retrieved documents into discrete blocks, with each
block independently calculating key-value (KV) states except for the final
block. In RAG scenarios, by defining each passage as a block, Block-Attention
enables us to reuse the KV states of passages that have been seen before,
thereby significantly reducing the latency and the computation overhead during
inference. The implementation of Block-Attention involves block segmentation,
position re-encoding, and fine-tuning the LLM to adapt to the Block-Attention
mechanism. Experiments on four RAG benchmarks demonstrate that after block
fine-tuning, the Block-Attention model achieves performance comparable to
self-attention models (68.4\% vs 67.9\% on Llama3) or even superior performance
(62.8\% vs 59.6\% on Mistral). Notably, Block-Attention significantly reduces
the time to first token (TTFT) and floating point operations (FLOPs) to a very
low level. It only takes 45 ms to output the first token for an input sequence
with a total length of 32K. Compared to the self-attention models, the time
consumption and corresponding FLOPs are reduced by 98.7\% and 99.8\%,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>-SAW: Leveraging Relation-Aware Graphs for Textual <span class="highlight-title">Prompt</span>
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Guimin Hu, Weimin Lyu, Lijie Hu, Lu Yu, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown exceptional abilities for multiple
different natural language processing tasks. While prompting is a crucial tool
for LLM inference, we observe that there is a significant cost associated with
exceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead
to substandard results in terms of readability/interpretability of the
compressed prompt, with a detrimental impact on prompt utility. To address
this, we propose PromptSAW: Prompt compresSion via Relation AWare graphs, an
effective strategy for prompt compression over task-agnostic and task-aware
prompts. Prompt-SAW uses the prompt's textual information to build a graph and
later extracts key information elements in the graph to come up with the
compressed prompt. We also propose GSM8K-aug, i.e., an extended version of the
existing GSM8K benchmark for task-agnostic prompts in order to provide a
comprehensive evaluation platform. Experimental evaluation using benchmark
datasets shows that prompts compressed by Prompt-SAW are not only better in
terms of readability, but they also outperform the best-performing baseline
models by up to 10.1 and 77.1, respectively, for task-agnostic and task-aware
settings while compressing the original prompt text by 34.9 and 56.7.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Systematic Analysis of Large Language Models as Soft Reasoners: The
  Case of Syllogistic Inferences <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11341v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11341v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Bertolazzi, Albert Gatt, Raffaella Bernardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reasoning abilities of Large Language Models (LLMs) are becoming a
central focus of study in NLP. In this paper, we consider the case of
syllogistic reasoning, an area of deductive reasoning studied extensively in
logic and cognitive psychology. Previous research has shown that pre-trained
LLMs exhibit reasoning biases, such as $\textit{content effects}$, avoid
answering that $\textit{no conclusion follows}$, display human-like
difficulties, and struggle with multi-step reasoning. We contribute to this
research line by systematically investigating the effects of chain-of-thought
reasoning, in-context learning (ICL), and supervised fine-tuning (SFT) on
syllogistic reasoning, considering syllogisms with conclusions that support or
violate world knowledge, as well as ones with multiple premises. Crucially, we
go beyond the standard focus on accuracy, with an in-depth analysis of the
conclusions generated by the models. Our results suggest that the behavior of
pre-trained LLMs can be explained by heuristics studied in cognitive science
and that both ICL and SFT improve model performance on valid inferences,
although only the latter mitigates most reasoning biases without harming model
consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference
  Attacks in Text-to-SQL Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14545v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14545v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Đorđe Klisura, Anthony Rios
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL systems empower users to interact with databases using natural
language, automatically translating queries into executable SQL code. However,
their reliance on database schema information for SQL generation exposes them
to significant security vulnerabilities, particularly schema inference attacks
that can lead to unauthorized data access or manipulation. In this paper, we
introduce a novel zero-knowledge framework for reconstructing the underlying
database schema of text-to-SQL models without any prior knowledge of the
database. Our approach systematically probes text-to-SQL models with specially
crafted questions and leverages a surrogate GPT-4 model to interpret the
outputs, effectively uncovering hidden schema elements -- including tables,
columns, and data types. We demonstrate that our method achieves high accuracy
in reconstructing table names, with F1 scores of up to .99 for generative
models and .78 for fine-tuned models, underscoring the severity of schema
leakage risks. Furthermore, we propose a simple protection mechanism for
generative models and empirically show its limitations in mitigating these
attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BLT: Can Large Language Models Handle Basic Legal Text? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09693v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09693v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Blair-Stanek, Nils Holzenberger, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We find that the best publicly available LLMs like GPT-4 and Claude currently
perform poorly on basic legal text handling. This motivates the creation of a
benchmark consisting of examples that lawyers and paralegals would expect LLMs
to handle zero-shot, such as looking up the text at a line of a witness
deposition or at a subsection of a contract. LLMs' poor performance on this
benchmark casts into doubt their reliability as-is for legal practice. However,
fine-tuning on our training set brings even a small model to near-perfect
performance. This benchmark will be useful for fine-tuning LLMs for downstream
legal tasks, as well as for tracking LLMs' reliability as-is for basic legal
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Inducing Document-Level Abilities in Standard Multilingual
  Neural Machine Translation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varun Gumma, Pranjal A. Chitale, Kalika Bali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Machine Translation (NMT) models have traditionally used Sinusoidal
Positional Embeddings (PEs), which often struggle to capture long-range
dependencies and are less efficient for handling extended context or
document-level translation tasks. This work addresses the challenge of
transitioning pre-trained NMT models from absolute sinusoidal PEs to relative
PEs, such as Rotary Positional Embeddings (ROPE) and Attention with Linear
Biases (ALIBI), without compromising performance. We demonstrate that
parameter-efficient fine-tuning, using only a small amount of high-quality
data, can successfully facilitate this transition. Experimental results
indicate that switching from sinusoidal to relative PEs results in competitive
translation quality on sentence-level evaluation benchmarks. Additionally,
models trained with ROPE consistently outperform those using ALIBI and
Sinusoidal PEs on document-level benchmarks across both string-based metrics
and qualitative evaluations. Moreover, we find that a small amount of
long-context data in a few languages is sufficient for cross-lingual length
generalization, thereby inducing long-context capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Granular Privacy Control for Geolocation with <span class="highlight-title">Vision Language</span> Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Mendes, Yang Chen, James Hays, Sauvik Das, Wei Xu, Alan Ritter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) are rapidly advancing in their capability to
answer information-seeking questions. As these models are widely deployed in
consumer applications, they could lead to new privacy risks due to emergent
abilities to identify people in photos, geolocate images, etc. As we
demonstrate, somewhat surprisingly, current open-source and proprietary VLMs
are very capable image geolocators, making widespread geolocation with VLMs an
immediate privacy risk, rather than merely a theoretical future concern. As a
first step to address this challenge, we develop a new benchmark, GPTGeoChat,
to test the ability of VLMs to moderate geolocation dialogues with users. We
collect a set of 1,000 image geolocation conversations between in-house
annotators and GPT-4v, which are annotated with the granularity of location
information revealed at each turn. Using this new dataset, we evaluate the
ability of various VLMs to moderate GPT-4v geolocation conversations by
determining when too much location information has been revealed. We find that
custom fine-tuned models perform on par with prompted API-based models when
identifying leaked location information at the country or city level; however,
fine-tuning on supervised data appears to be needed to accurately moderate
finer granularities, such as the name of a restaurant or building.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic
  Analysis of Annotators and Targets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07991v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07991v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tommaso Giorgi, Lorenzo Cima, Tiziano Fagni, Marco Avvenuti, Stefano Cresci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of online platforms exacerbated the spread of hate speech, demanding
scalable and effective detection. However, the accuracy of hate speech
detection systems heavily relies on human-labeled data, which is inherently
susceptible to biases. While previous work has examined the issue, the
interplay between the characteristics of the annotator and those of the target
of the hate are still unexplored. We fill this gap by leveraging an extensive
dataset with rich socio-demographic information of both annotators and targets,
uncovering how human biases manifest in relation to the target's attributes.
Our analysis surfaces the presence of widespread biases, which we
quantitatively describe and characterize based on their intensity and
prevalence, revealing marked differences. Furthermore, we compare human biases
with those exhibited by persona-based LLMs. Our findings indicate that while
persona-based LLMs do exhibit biases, these differ significantly from those of
human annotators. Overall, our work offers new and nuanced results on human
biases in hate speech annotations, as well as fresh insights into the design of
AI-driven hate speech detection systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient In-Domain Question Answering for Resource-Constrained
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17648v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17648v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Chung, Phat Vo, Arman C. Kizilkale, Aaron Reite
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) is a common method for integrating
external knowledge into pretrained Large Language Models (LLMs) to enhance
accuracy and relevancy in question answering (QA) tasks. However, prompt
engineering and resource efficiency remain significant bottlenecks in
developing optimal and robust RAG solutions for real-world QA applications.
Recent studies have shown success in using fine tuning to address these
problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to
smaller 7B models has demonstrated superior performance compared to RAG setups
with much larger models such as GPT-3.5. The combination of RAFT with
parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation
(LoRA), promises an even more efficient solution, yet remains an unexplored
area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage
requirements and gain faster inference times while maintaining comparable RAG
performance. This results in a more compute-efficient RAFT, or CRAFT, which is
particularly useful for knowledge-intensive QA tasks in resource-constrained
environments where internet access may be restricted and hardware resources
limited.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16710v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16710v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed A Aly, Beidi Chen, Carole-Jean Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LayerSkip, an end-to-end solution to speed-up inference of large
language models (LLMs). First, during training we apply layer dropout, with low
dropout rates for earlier layers and higher dropout rates for later layers, and
an early exit loss where all transformer layers share the same exit. Second,
during inference, we show that this training recipe increases the accuracy of
early exit at earlier layers, without adding any auxiliary layers or modules to
the model. Third, we present a novel self-speculative decoding solution where
we exit at early layers and verify and correct with remaining layers of the
model. Our proposed self-speculative decoding approach has less memory
footprint than other speculative decoding approaches and benefits from shared
compute and activations of the draft and verification stages. We run
experiments on different Llama model sizes on different types of training:
pretraining from scratch, continual pretraining, finetuning on specific data
domain, and finetuning on specific task. We implement our inference solution
and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x
on coding, and 2.0x on TOPv2 semantic parsing task. We open source our code and
checkpoints at https://github.com/facebookresearch/LayerSkip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Building Better: Avoiding Pitfalls in Developing Language Resources when
  Data is Scarce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nedjma Ousidhoum, Meriem Beloucif, Saif M. Mohammad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language is a symbolic capital that affects people's lives in many ways
(Bourdieu, 1977, 1991). It is a powerful tool that accounts for identities,
cultures, traditions, and societies in general. Hence, data in a given language
should be viewed as more than a collection of tokens. Good data collection and
labeling practices are key to building more human-centered and socially aware
technologies. While there has been a rising interest in mid- to low-resource
languages within the NLP community, work in this space has to overcome unique
challenges such as data scarcity and access to suitable annotators. In this
paper, we collect feedback from those directly involved in and impacted by NLP
artefacts for mid- to low-resource languages. We conduct a quantitative and
qualitative analysis of the responses and highlight the main issues related to
(1) data quality such as linguistic and cultural data suitability; and (2) the
ethics of common annotation practices such as the misuse of online community
services. Based on these findings, we make several recommendations for the
creation of high-quality language artefacts that reflect the cultural milieu of
its speakers, while simultaneously respecting the dignity and labor of data
workers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-based Cognitive Models of Students with Misconceptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12294v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12294v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Sonkar, Xinghe Chen, Naiming Liu, Richard G. Baraniuk, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately modeling student cognition is crucial for developing effective
AI-driven educational technologies. A key challenge is creating realistic
student models that satisfy two essential properties: (1) accurately
replicating specific misconceptions, and (2) correctly solving problems where
these misconceptions are not applicable. This dual requirement reflects the
complex nature of student understanding, where misconceptions coexist with
correct knowledge. This paper investigates whether Large Language Models (LLMs)
can be instruction-tuned to meet this dual requirement and effectively simulate
student thinking in algebra. We introduce MalAlgoPy, a novel Python library
that generates datasets reflecting authentic student solution patterns through
a graph-based representation of algebraic problem-solving. Utilizing MalAlgoPy,
we define and examine Cognitive Student Models (CSMs) - LLMs instruction tuned
to faithfully emulate realistic student behavior. Our findings reveal that LLMs
trained on misconception examples can efficiently learn to replicate errors.
However, the training diminishes the model's ability to solve problems
correctly, particularly for problem types where the misconceptions are not
applicable, thus failing to satisfy second property of CSMs. We demonstrate
that by carefully calibrating the ratio of correct to misconception examples in
the training data - sometimes as low as 0.25 - it is possible to develop CSMs
that satisfy both properties. Our insights enhance our understanding of
AI-based student models and pave the way for effective adaptive learning
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MuJo: <span class="highlight-title">Multimodal</span> Joint Feature Space Learning for Human Activity
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Gerd Fritsch, Cennet Oguz, Vitor Fortes Rey, Lala Ray, Maximilian Kiefer-Emmanouilidis, Paul Lukowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human Activity Recognition (HAR) is a longstanding problem in AI with
applications in a broad range of areas, including healthcare, sports and
fitness, security, and more. The performance of HAR in real-world settings is
strongly dependent on the type and quality of the input signal that can be
acquired. Given an unobstructed, high-quality camera view of a scene, computer
vision systems, in particular in conjunction with foundation models, can today
fairly reliably distinguish complex activities. On the other hand, recognition
using modalities such as wearable sensors (which are often more broadly
available, e.g., in mobile phones and smartwatches) is a more difficult
problem, as the signals often contain less information and labeled training
data is more difficult to acquire. To alleviate the need for labeled data, we
introduce our comprehensive Fitness Multimodal Activity Dataset (FiMAD) in this
work, which can be used with the proposed pre-training method MuJo (Multimodal
Joint Feature Space Learning) to enhance HAR performance across various
modalities. FiMAD was created using YouTube fitness videos and contains
parallel video, language, pose, and simulated IMU sensor data. MuJo utilizes
this dataset to learn a joint feature space for these modalities. We show that
classifiers pre-trained on FiMAD can increase the performance on real HAR
datasets such as MM-Fit, MyoGym, MotionSense, and MHEALTH. For instance, on
MM-Fit, we achieve an Macro F1-Score of up to 0.855 when fine-tuning on only 2%
of the training data and 0.942 when utilizing the full training set for
classification tasks. We have compared our approach to other self-supervised
ones and showed that, unlike them, ours can consistently improve on the
baseline network performance as well as provide a better data-efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Ask Informative Questions: Enhancing LLMs with Preference
  Optimization and Expected Information Gain <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17453v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17453v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Mazzaccara, Alberto Testoni, Raffaella Bernardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Questions are essential tools for acquiring the necessary information to
complete information-seeking tasks. However, large language models (LLMs),
especially open-source models, often perform poorly in generating informative
questions, as measured by expected information gain (EIG). In this paper, we
propose a method to enhance the informativeness of LLM-generated questions in
20-question game dialogues. We sample multiple questions from the same model
(LLAMA 2-CHAT 7B) for each game and create pairs of low-EIG and high-EIG
questions to apply a Direct Preference Optimization (DPO) algorithm. Our
results show that this method produces more effective questions (in terms of
EIG), even in domains different from those used to train the DPO model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Relay Decoding: Concatenating Large Language Models for Machine
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengpeng Fu, Xiaocheng Feng, Yichong Huang, Wenshuai Huo, Baohang Li, Hui Wang, Bin Qin, Ting Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging large language models for machine translation has demonstrated
promising results. However, it does require the large language models to
possess the capability of handling both the source and target languages in
machine translation. When it is challenging to find large models that support
the desired languages, resorting to continuous learning methods becomes a
costly endeavor. To mitigate these expenses, we propose an innovative approach
called RD (Relay Decoding), which entails concatenating two distinct large
models that individually support the source and target languages. By
incorporating a simple mapping layer to facilitate the connection between these
two models and utilizing a limited amount of parallel data for training, we
successfully achieve superior results in the machine translation task.
Experimental results conducted on the Multi30k and WikiMatrix datasets validate
the effectiveness of our proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Reliability of Large Language Models to Misinformed and
  Demographically-Informed <span class="highlight-title">Prompt</span>s <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10850v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10850v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toluwani Aremu, Oluwakemi Akinwehinmi, Chukwuemeka Nwagu, Syed Ishtiaque Ahmed, Rita Orji, Pedro Arnau Del Amo, Abdulmotaleb El Saddik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate and observe the behaviour and performance of Large Language
Model (LLM)-backed chatbots in addressing misinformed prompts and questions
with demographic information within the domains of Climate Change and Mental
Health. Through a combination of quantitative and qualitative methods, we
assess the chatbots' ability to discern the veracity of statements, their
adherence to facts, and the presence of bias or misinformation in their
responses. Our quantitative analysis using True/False questions reveals that
these chatbots can be relied on to give the right answers to these close-ended
questions. However, the qualitative insights, gathered from domain experts,
shows that there are still concerns regarding privacy, ethical implications,
and the necessity for chatbots to direct users to professional services. We
conclude that while these chatbots hold significant promise, their deployment
in sensitive areas necessitates careful consideration, ethical oversight, and
rigorous refinement to ensure they serve as a beneficial augmentation to human
expertise rather than an autonomous solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Study conducted between August and December 2023. Under review at
  AAAI-AI Magazine. Submitted for archival purposes only</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback
  for Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katherine M. Collins, Najoung Kim, Yonatan Bitton, Verena Rieser, Shayegan Omidshafiei, Yushi Hu, Sherol Chen, Senjuti Dutta, Minsuk Chang, Kimin Lee, Youwei Liang, Georgina Evans, Sahil Singla, Gang Li, Adrian Weller, Junfeng He, Deepak Ramachandran, Krishnamurthy Dj Dvijotham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human feedback plays a critical role in learning and refining reward models
for text-to-image generation, but the optimal form the feedback should take for
learning an accurate reward function has not been conclusively established.
This paper investigates the effectiveness of fine-grained feedback which
captures nuanced distinctions in image quality and prompt-alignment, compared
to traditional coarse-grained feedback (for example, thumbs up/down or ranking
between a set of options). While fine-grained feedback holds promise,
particularly for systems catering to diverse societal preferences, we show that
demonstrating its superiority to coarse-grained feedback is not automatic.
Through experiments on real and synthetic preference data, we surface the
complexities of building effective models due to the interplay of model choice,
feedback type, and the alignment between human judgment and computational
interpretation. We identify key challenges in eliciting and utilizing
fine-grained feedback, prompting a reassessment of its assumed benefits and
practicality. Our findings -- e.g., that fine-grained feedback can lead to
worse models for a fixed budget, in some settings; however, in controlled
settings with known attributes, fine grained rewards can indeed be more helpful
-- call for careful consideration of feedback attributes and potentially beckon
novel modeling approaches to appropriately unlock the potential value of
fine-grained feedback in-the-wild.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InferAct: Inferring Safe Actions for LLM-Based Agents Through Preemptive
  Evaluation and Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11843v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11843v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haishuo Fang, Xiaodan Zhu, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A crucial requirement for deploying LLM-based agents in real-life
applications is the robustness against risky or even irreversible mistakes.
However, the existing research lacks a focus on preemptive evaluation of
reasoning trajectories performed by LLM agents, leading to a gap in ensuring
safe and reliable operations. To explore better solutions, this paper
introduces InferAct, a novel approach that leverages the belief reasoning
ability of LLMs, grounded in Theory-of-Mind, to proactively detect potential
errors before risky actions are executed (e.g., `buy-now' in automatic online
trading or web shopping). InferAct acts as a human proxy, detecting unsafe
actions and alerting users for intervention, which helps prevent irreversible
risks in time and enhances the actor agent's decision-making process.
Experiments on three widely-used tasks demonstrate the effectiveness of
InferAct, presenting a novel solution for safely developing LLM agents in
environments involving critical decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pyramid-Driven Alignment: Pyramid Principle Guided Integration of Large
  Language Models and Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12298v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12298v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Sun, Xinchen Wang, Youdi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) possess impressive reasoning abilities but are
prone to generating incorrect information, often referred to as hallucinations.
While incorporating external Knowledge Graphs (KGs) can partially mitigate this
issue, existing methods primarily treat KGs as static knowledge repositories,
overlooking the critical disparity between KG and LLM knowledge, and failing to
fully exploit the reasoning capabilities inherent in KGs. To address these
limitations, we propose Pyramid-Driven Alignment (PDA), a novel framework for
seamlessly integrating LLMs with KGs. PDA utilizes Pyramid Principle analysis
to construct a hierarchical pyramid structure. This structure is designed to
reflect the input question and generate more validated deductive knowledge,
thereby enhancing the alignment of LLMs and KGs and ensuring more cohesive
integration. Furthermore, PDA employs a recursive mechanism to harness the
underlying reasoning abilities of KGs, resulting in more accurate knowledge
retrieval for question-answering tasks. Our experimental results reveal a
substantial performance advantage of PDA over state-of-the-art baselines, with
improvements reaching 26.70% and 26.78%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autonomous Agents for Collaborative Task under Information Asymmetry <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Liu, Chenxi Wang, Yifei Wang, Zihao Xie, Rennai Qiu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Chen Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model Multi-Agent Systems (LLM-MAS) have achieved great
progress in solving complex tasks. It performs communication among agents
within the system to collaboratively solve tasks, under the premise of shared
information. However, when agents' collaborations are leveraged to perform
multi-person tasks, a new challenge arises due to information asymmetry, since
each agent can only access the information of its human user. Previous MAS
struggle to complete tasks under this condition. To address this, we propose a
new MAS paradigm termed iAgents, which denotes Informative Multi-Agent Systems.
In iAgents, the human social network is mirrored in the agent network, where
agents proactively exchange human information necessary for task resolution,
thereby overcoming information asymmetry. iAgents employs a novel agent
reasoning mechanism, InfoNav, to navigate agents' communication toward
effective information exchange. Together with InfoNav, iAgents organizes human
information in a mixed memory to provide agents with accurate and comprehensive
information for exchange. Additionally, we introduce InformativeBench, the
first benchmark tailored for evaluating LLM agents' task-solving ability under
information asymmetry. Experimental results show that iAgents can collaborate
within a social network of 140 individuals and 588 relationships, autonomously
communicate over 30 turns, and retrieve information from nearly 70,000 messages
to complete tasks within 3 minutes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 12 figures, 6 tables, accepted by NeurIPS 2024, see detail
  at https://thinkwee.top/iagents</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedAide: Towards an Omni Medical Aide via Specialized LLM-based
  Multi-Agent Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12532v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12532v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjie Wei, Dingkang Yang, Yanshu Li, Qingyao Xu, Zhaoyu Chen, Mingcheng Li, Yue Jiang, Xiaolu Hou, Lihua Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM)-driven interactive systems currently show
potential promise in healthcare domains. Despite their remarkable capabilities,
LLMs typically lack personalized recommendations and diagnosis analysis in
sophisticated medical applications, causing hallucinations and performance
bottlenecks. To address these challenges, this paper proposes MedAide, an
LLM-based omni medical multi-agent collaboration framework for specialized
healthcare services. Specifically, MedAide first performs query rewriting
through retrieval-augmented generation to accomplish accurate medical intent
understanding. Immediately, we devise a contextual encoder to obtain intent
prototype embeddings, which are used to recognize fine-grained intents by
similarity matching. According to the intent relevance, the activated agents
collaborate effectively to provide integrated decision analysis. Extensive
experiments are conducted on four medical benchmarks with composite intents.
Experimental results from automated metrics and expert doctor evaluations show
that MedAide outperforms current LLMs and improves their medical proficiency
and strategic reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LLM-based Multi-Agent Collaboration for Medical Applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Skeleton: A New Framework for Accelerating Language Models via Task
  Neuron Localized <span class="highlight-title">Prompt</span> Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nakyeong Yang, Jiwon Moon, Junseok Kim, Yunah Jang, Kyomin Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt tuning methods have shown comparable performance to general training
methods as parameter-efficient fine-tuning (PEFT) methods in various natural
language understanding tasks. However, existing prompt tuning methods still
utilize the entire model architecture even when solving a specific task, which
prevents them from accelerating inference speed during the application
procedure. In this paper, we propose a novel prompt tuning framework called
Skeleton to efficiently utilize a language model in terms of memory and time
complexity for solving various tasks, retaining only task-relevant neurons by
using an explainability method. From our framework, we can efficiently solve
various tasks by using only task-relevant neurons and prepending adequate
task-specific prompt tokens with only a single language model. Experiments
reveal that our method significantly enhances inference efficiency (at most x
1.73 speed up) for various widely used benchmarks, showing comparable
performance to the prompt tuning method. Moreover, our method is applicable
across various transformer-based architectures, confirming its practicality and
scalability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLoCO: Learning Long Contexts Offline <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07979v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07979v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sijun Tan, Xiuyu Li, Shishir Patil, Ziyang Wu, Tianjun Zhang, Kurt Keutzer, Joseph E. Gonzalez, Raluca Ada Popa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Processing long contexts remains a challenge for large language models (LLMs)
due to the quadratic computational and memory overhead of the self-attention
mechanism and the substantial KV cache sizes during generation. We propose
LLoCO, a novel approach to address this problem by learning contexts offline
through context compression and in-domain parameter-efficient finetuning with
LoRA. Our method enables an LLM to create a concise representation of the
original context and efficiently retrieve relevant information to answer
questions accurately. Our approach extends the effective context window of a 4k
token LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on
several long-context question-answering datasets, demonstrating that LLoCO
significantly outperforms in-context learning while using $30\times$ fewer
tokens during inference. LLoCO achieves up to $7.62\times$ speed-up during
inference and $11.52\times$ higher throughput during finetuning, substantially
reduces the cost of long document question answering. This makes it a promising
solution for efficient long context processing. Our code is publicly available
on https://github.com/jeffreysijuntan/lloco.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024. The first two authors contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are Large Language Models Good Classifiers? A Study on Edit Intent
  Classification in Scientific Document Revisions <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Ruan, Ilia Kuznetsov, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification is a core NLP task architecture with many potential
applications. While large language models (LLMs) have brought substantial
advancements in text generation, their potential for enhancing classification
tasks remains underexplored. To address this gap, we propose a framework for
thoroughly investigating fine-tuning LLMs for classification, including both
generation- and encoding-based approaches. We instantiate this framework in
edit intent classification (EIC), a challenging and underexplored
classification task. Our extensive experiments and systematic comparisons with
various training approaches and a representative selection of LLMs yield new
insights into their application for EIC. We investigate the generalizability of
these findings on five further classification tasks. To demonstrate the
proposed methods and address the data shortage for empirical edit analysis, we
use our best-performing EIC model to create Re3-Sci2.0, a new large-scale
dataset of 1,780 scientific document revisions with over 94k labeled edits. The
quality of the dataset is assessed through human evaluation. The new dataset
enables an in-depth empirical study of human editing behavior in academic
writing. We make our experimental framework, models and data publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Measurement Instruments to Data: Leveraging Theory-Driven Synthetic
  Training Data for Classifying Social Constructs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Birkenmaier, Matthias Roth, Indira Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational text classification is a challenging task, especially for
multi-dimensional social constructs. Recently, there has been increasing
discussion that synthetic training data could enhance classification by
offering examples of how these constructs are represented in texts. In this
paper, we systematically examine the potential of theory-driven synthetic
training data for improving the measurement of social constructs. In
particular, we explore how researchers can transfer established knowledge from
measurement instruments in the social sciences, such as survey scales or
annotation codebooks, into theory-driven generation of synthetic data. Using
two studies on measuring sexism and political topics, we assess the added value
of synthetic training data for fine-tuning text classification models. Although
the results of the sexism study were less promising, our findings demonstrate
that synthetic data can be highly effective in reducing the need for labeled
data in political topic classification. With only a minimal drop in
performance, synthetic data allows for substituting large amounts of labeled
data. Furthermore, theory-driven synthetic data performed markedly better than
data generated without conceptual information in mind.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pragmatic Competence Evaluation of Large Language Models for the Korean
  Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dojun Park, Jiwoo Lee, Hyeyun Jeong, Seohyun Park, Sungeun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benchmarks play a significant role in the current evaluation of Large
Language Models (LLMs), yet they often overlook the models' abilities to
capture the nuances of human language, primarily focusing on evaluating
embedded knowledge and technical skills. To address this gap, our study
evaluates how well LLMs understand context-dependent expressions from a
pragmatic standpoint, specifically in Korean. We use both Multiple-Choice
Questions (MCQs) for automatic evaluation and Open-Ended Questions (OEQs)
assessed by human experts. Our results show that GPT-4 leads with scores of
81.11 in MCQs and 85.69 in OEQs, closely followed by HyperCLOVA X.
Additionally, while few-shot learning generally improves performance,
Chain-of-Thought (CoT) prompting tends to encourage literal interpretations,
which may limit effective pragmatic inference. Our findings highlight the need
for LLMs to better understand and generate language that reflects human
communicative norms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Pacific Asia Conference on Language, Information and Computation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LightPAL: Lightweight Passage Retrieval for Open Domain Multi-Document
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12494v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12494v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masafumi Enomoto, Kunihiro Takeoka, Kosuke Akimoto, Kiril Gashteovski, Masafumi Oyamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-Domain Multi-Document Summarization (ODMDS) is the task of generating
summaries from large document collections in response to user queries. This
task is crucial for efficiently addressing diverse information needs from
users. Traditional retrieve-then-summarize approaches fall short for open-ended
queries in ODMDS tasks. These queries often require broader context than
initially retrieved passages provide, making it challenging to retrieve all
relevant information in a single search. While iterative retrieval methods has
been explored for multi-hop question answering (MQA), it's impractical for
ODMDS due to high latency from repeated LLM inference. Accordingly, we propose
LightPAL, a lightweight passage retrieval method for ODMDS. LightPAL leverages
an LLM to pre-construct a graph representing passage relationships, then
employs random walk during retrieval, avoiding iterative LLM inference.
Experiments demonstrate that LightPAL outperforms naive sparse and pre-trained
dense retrievers in both retrieval and summarization metrics, while achieving
higher efficiency compared to iterative MQA approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SafeGen: Mitigating Sexually Explicit Content Generation in
  Text-to-Image Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06666v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06666v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinfeng Li, Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu Ji, Wenyuan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) models, such as Stable Diffusion, have exhibited
remarkable performance in generating high-quality images from text descriptions
in recent years. However, text-to-image models may be tricked into generating
not-safe-for-work (NSFW) content, particularly in sexually explicit scenarios.
Existing countermeasures mostly focus on filtering inappropriate inputs and
outputs, or suppressing improper text embeddings, which can block sexually
explicit content (e.g., naked) but may still be vulnerable to adversarial
prompts -- inputs that appear innocent but are ill-intended. In this paper, we
present SafeGen, a framework to mitigate sexual content generation by
text-to-image models in a text-agnostic manner. The key idea is to eliminate
explicit visual representations from the model regardless of the text input. In
this way, the text-to-image model is resistant to adversarial prompts since
such unsafe visual representations are obstructed from within. Extensive
experiments conducted on four datasets and large-scale user studies demonstrate
SafeGen's effectiveness in mitigating sexually explicit content generation
while preserving the high-fidelity of benign images. SafeGen outperforms eight
state-of-the-art baseline methods and achieves 99.4% sexual content removal
performance. Furthermore, our constructed benchmark of adversarial prompts
provides a basis for future development and evaluation of anti-NSFW-generation
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM CCS 2024. Please cite this paper as "Xinfeng Li,
  Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu Ji, Wenyuan Xu.
  SafeGen: Mitigating Sexually Explicit Content Generation in Text-to-Image
  Models. In Proceedings of ACM Conference on Computer and Communications
  Security (CCS), 2024."</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpreadsheetBench: Towards Challenging Real World Spreadsheet
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14991v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14991v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyao Ma, Bohan Zhang, Jing Zhang, Jifan Yu, Xiaokang Zhang, Xiaohan Zhang, Sijia Luo, Xi Wang, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SpreadsheetBench, a challenging spreadsheet manipulation
benchmark exclusively derived from real-world scenarios, designed to immerse
current large language models (LLMs) in the actual workflow of spreadsheet
users. Unlike existing benchmarks that rely on synthesized queries and
simplified spreadsheet files, SpreadsheetBench is built from 912 real questions
gathered from online Excel forums, which reflect the intricate needs of users.
The associated spreadsheets from the forums contain a variety of tabular data
such as multiple tables, non-standard relational tables, and abundant
non-textual elements. Furthermore, we propose a more reliable evaluation metric
akin to online judge platforms, where multiple spreadsheet files are created as
test cases for each instruction, ensuring the evaluation of robust solutions
capable of handling spreadsheets with varying values. Our comprehensive
evaluation of various LLMs under both single-round and multi-round inference
settings reveals a substantial gap between the state-of-the-art (SOTA) models
and human performance, highlighting the benchmark's difficulty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Neurips 2024 (Spotlight); Homepage:
  https://spreadsheetbench.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Instruction Following: Evaluating Inferential Rule Following of
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08440v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08440v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangtao Sun, Chenxiang Zhang, XueYou Zhang, Xuanqing Yu, Ziyang Huang, Pei Chen, Haotian Xu, Shizhu He, Jun Zhao, Kang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Large Language Models (LLMs) have demonstrated strong ability, they
are further supposed to be controlled and guided by in real-world scenarios to
be safe, accurate, and intelligent. This demands the possession of capability
of LLMs. However, no prior work has made a clear evaluation of the inferential
rule-following capability of LLMs. Previous studies that try to evaluate the
inferential rule-following capability of LLMs fail to distinguish the
inferential rule-following scenarios from the instruction-following scenarios.
Therefore, this paper first clarifies the concept of inferential rule-following
and proposes a comprehensive benchmark, RuleBench, to evaluate a diversified
range of inferential rule-following abilities. Our experimental results on a
variety of LLMs show that they are still limited in following rules. Our
analysis based on the evaluation results provides insights into the
improvements for LLMs toward a better inferential rule-following intelligent
agent. We further propose Inferential Rule-Following Tuning (IRFT). The
experimental results show that through IRFT, LLMs can learn abstract
rule-following abilities from purely synthetic data and then generalize to
RuleBench. The data and code can be found at:
https://anonymous.4open.science/r/llm-rule-following-B3E3/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporally Consistent Factuality Probing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14065v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14065v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashutosh Bajpai, Aaryan Goyal, Atif Anwer, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prolific use of Large Language Models (LLMs) as an alternate knowledge
base requires them to be factually consistent, necessitating both correctness
and consistency traits for paraphrased queries. Recently, significant attempts
have been made to benchmark datasets and metrics to evaluate LLMs for these
traits. However, structural simplicity (subject-relation-object) and
contemporary association in their query formulation limit the broader
definition of factuality and consistency. In this study, we introduce TeCFaP, a
novel Temporally Consistent Factuality Probe task to expand the consistent
factuality probe in the temporal dimension. To this end, we propose TEMP-COFAC,
a high-quality dataset of prefix-style English query paraphrases. Subsequently,
we extend the definitions of existing metrics to represent consistent
factuality across temporal dimension. We experiment with a diverse set of LLMs
and find most of them performing poorly on TeCFaP. Next, we propose a novel
solution CoTSeLF (Consistent-Time-Sensitive Learning Framework) combining
multi-task instruction tuning (MT-IT) with consistent-time-sensitive
reinforcement learning (CTSRL) to improve temporally consistent factuality in
LLMs. Our experiments demonstrate the efficacy of CoTSeLF over several
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating Chain-of-thought with ChatGPT for Stance Detection on
  Social Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zhang, Xianghua Fu, Daijun Ding, Hu Huang, Genan Dai, Nan Yin, Yangyang Li, Liwen Jing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stance detection predicts attitudes towards targets in texts and has gained
attention with the rise of social media. Traditional approaches include
conventional machine learning, early deep neural networks, and pre-trained
fine-tuning models. However, with the evolution of very large pre-trained
language models (VLPLMs) like ChatGPT (GPT-3.5), traditional methods face
deployment challenges. The parameter-free Chain-of-Thought (CoT) approach, not
requiring backpropagation training, has emerged as a promising alternative.
This paper examines CoT's effectiveness in stance detection tasks,
demonstrating its superior accuracy and discussing associated challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2212.14548</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedCare: Advancing Medical LLMs through Decoupling Clinical Alignment
  and Knowledge Aggregation <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17484v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17484v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusheng Liao, Shuyang Jiang, Zhe Chen, Yanfeng Wang, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown substantial progress in natural
language understanding and generation, proving valuable especially in the
medical field. Despite advancements, challenges persist due to the complexity
and diversity inherent in medical tasks, which can be categorized as
knowledge-intensive tasks and alignment-required tasks. Previous approaches
either ignore the latter task or focus on a minority of tasks and hence lose
generalization. To address these drawbacks, we propose a progressive
fine-tuning pipeline. This pipeline employs a Knowledge Aggregator and a Noise
aggregator to encode diverse knowledge in the first stage and filter out
detrimental information. In the second stage, we drop the Noise Aggregator to
avoid the interference of suboptimal representation and leverage an additional
alignment module optimized towards an orthogonal direction to the knowledge
space to mitigate knowledge forgetting. Based on this two-stage paradigm, we
proposed a Medical LLM through decoupling Clinical Alignment and Knowledge
Aggregation (MedCare), which is designed to achieve state-of-the-art (SOTA)
performance on over 20 medical tasks, as well as SOTA results on specific
medical alignment tasks. Various model sizes of MedCare (1.8B, 7B, 14B) all
demonstrate significant improvements over existing models with similar model
sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Belief Revision: The Adaptability of Large Language Models Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bryan Wilie, Samuel Cahyawijaya, Etsuko Ishii, Junxian He, Pascale Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capability to reason from text is crucial for real-world NLP
applications. Real-world scenarios often involve incomplete or evolving data.
In response, individuals update their beliefs and understandings accordingly.
However, most existing evaluations assume that language models (LMs) operate
with consistent information. We introduce Belief-R, a new dataset designed to
test LMs' belief revision ability when presented with new evidence. Inspired by
how humans suppress prior inferences, this task assesses LMs within the newly
proposed delta reasoning ($\Delta R$) framework. Belief-R features sequences of
premises designed to simulate scenarios where additional information could
necessitate prior conclusions drawn by LMs. We evaluate $\sim$30 LMs across
diverse prompting strategies and found that LMs generally struggle to
appropriately revise their beliefs in response to new information. Further,
models adept at updating often underperformed in scenarios without necessary
updates, highlighting a critical trade-off. These insights underscore the
importance of improving LMs' adaptiveness to changing information, a step
toward more reliable AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enabling Natural Zero-Shot <span class="highlight-title">Prompt</span>ing on Encoder Models via
  Statement-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12897v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12897v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Elshabrawy, Yongxin Huang, Iryna Gurevych, Alham Fikri Aji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) exhibit remarkable capabilities in
zero-shot and few-shot scenarios, they often require computationally
prohibitive sizes. Conversely, smaller Masked Language Models (MLMs) like BERT
and RoBERTa achieve state-of-the-art results through fine-tuning but struggle
with extending to few-shot and zero-shot settings due to their architectural
constraints. Hence, we propose Statement-Tuning, a technique that models
discriminative tasks as a set of finite statements and trains an encoder model
to discriminate between the potential statements to determine the label. We do
Statement-Tuning on multiple tasks to enable cross-task generalization.
Experimental results demonstrate that Statement-Tuning achieves competitive
performance compared to state-of-the-art LLMs with significantly fewer
parameters. Moreover, the study investigates the impact of several design
choices on few-shot and zero-shot generalization, revealing that
Statement-Tuning can achieve strong performance with modest training data and
benefits from task and statement diversity for unseen task generalizability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in
  Action <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00138v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00138v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijia Shao, Tianshi Li, Weiyan Shi, Yanchen Liu, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As language models (LMs) are widely utilized in personalized communication
scenarios (e.g., sending emails, writing social media posts) and endowed with a
certain level of agency, ensuring they act in accordance with the contextual
privacy norms becomes increasingly critical. However, quantifying the privacy
norm awareness of LMs and the emerging privacy risk in LM-mediated
communication is challenging due to (1) the contextual and long-tailed nature
of privacy-sensitive cases, and (2) the lack of evaluation approaches that
capture realistic application scenarios. To address these challenges, we
propose PrivacyLens, a novel framework designed to extend privacy-sensitive
seeds into expressive vignettes and further into agent trajectories, enabling
multi-level evaluation of privacy leakage in LM agents' actions. We instantiate
PrivacyLens with a collection of privacy norms grounded in privacy literature
and crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM
performance in answering probing questions and their actual behavior when
executing user instructions in an agent setup. State-of-the-art LMs, like GPT-4
and Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even
when prompted with privacy-enhancing instructions. We also demonstrate the
dynamic nature of PrivacyLens by extending each seed into multiple trajectories
to red-team LM privacy leakage risk. Dataset and code are available at
https://github.com/SALT-NLP/PrivacyLens.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span> Compression for Large Language Models: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12388v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12388v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongqian Li, Yinhong Liu, Yixuan Su, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging large language models (LLMs) for complex natural language tasks
typically requires long-form prompts to convey detailed requirements and
information, which results in increased memory usage and inference costs. To
mitigate these challenges, multiple efficient methods have been proposed, with
prompt compression gaining significant research interest. This survey provides
an overview of prompt compression techniques, categorized into hard prompt
methods and soft prompt methods. First, the technical approaches of these
methods are compared, followed by an exploration of various ways to understand
their mechanisms, including the perspectives of attention optimization,
Parameter-Efficient Fine-Tuning (PEFT), modality integration, and new synthetic
language. We also examine the downstream adaptations of various prompt
compression techniques. Finally, the limitations of current prompt compression
methods are analyzed, and several future directions are outlined, such as
optimizing the compression encoder, combining hard and soft prompts methods,
and leveraging insights from multimodality.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">172</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Fluid: Scaling Autoregressive Text-to-image Generative Models with
  Continuous Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, <span class="highlight-author">Kaiming</span> He, Yonglong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling up autoregressive models in vision has not proven as beneficial as in
large language models. In this work, we investigate this scaling problem in the
context of text-to-image generation, focusing on two critical factors: whether
models use discrete or continuous tokens, and whether tokens are generated in a
random or fixed raster order using BERT- or GPT-like transformer architectures.
Our empirical results show that, while all models scale effectively in terms of
validation loss, their evaluation performance -- measured by FID, GenEval
score, and visual quality -- follows different trends. Models based on
continuous tokens achieve significantly better visual quality than those using
discrete tokens. Furthermore, the generation order and attention mechanisms
significantly affect the GenEval score: random-order models achieve notably
better GenEval scores compared to raster-order models. Inspired by these
findings, we train Fluid, a random-order autoregressive model on continuous
tokens. Fluid 10.5B model achieves a new state-of-the-art zero-shot FID of 6.16
on MS-COCO 30K, and 0.69 overall score on the GenEval benchmark. We hope our
findings and results will encourage future efforts to further bridge the
scaling gap between vision and language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniDrive: Towards Universal Driving Perception Across Camera
  Configurations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Li, Wenzhao Zheng, Xiaonan Huang, Kurt Keutzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-centric autonomous driving has demonstrated excellent performance with
economical sensors. As the fundamental step, 3D perception aims to infer 3D
information from 2D images based on 3D-2D projection. This makes driving
perception models susceptible to sensor configuration (e.g., camera intrinsics
and extrinsics) variations. However, generalizing across camera configurations
is important for deploying autonomous driving models on different car models.
In this paper, we present UniDrive, a novel framework for vision-centric
autonomous driving to achieve universal perception across camera
configurations. We deploy a set of unified virtual cameras and propose a
ground-aware projection method to effectively transform the original images
into these unified virtual views. We further propose a virtual configuration
optimization method by minimizing the expected projection error between
original cameras and virtual cameras. The proposed virtual camera projection
can be applied to existing 3D perception methods as a plug-and-play module to
mitigate the challenges posed by camera parameter variability, resulting in
more adaptable and reliable driving perception models. To evaluate the
effectiveness of our framework, we collect a dataset on Carla by driving the
same routes while only modifying the camera configurations. Experimental
results demonstrate that our method trained on one specific camera
configuration can generalize to varying configurations with minor performance
degradation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint; 14 pages, 5 figures, 2 tables; Code at
  https://github.com/ywyeli/UniDrive</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DepthSplat: Connecting Gaussian Splatting and Depth 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haofei Xu, Songyou Peng, Fangjinhua Wang, Hermann Blum, Daniel Barath, Andreas Geiger, Marc Pollefeys
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian splatting and single/multi-view depth estimation are typically
studied in isolation. In this paper, we present DepthSplat to connect Gaussian
splatting and depth estimation and study their interactions. More specifically,
we first contribute a robust multi-view depth model by leveraging pre-trained
monocular depth features, leading to high-quality feed-forward 3D Gaussian
splatting reconstructions. We also show that Gaussian splatting can serve as an
unsupervised pre-training objective for learning powerful depth models from
large-scale unlabelled datasets. We validate the synergy between Gaussian
splatting and depth estimation through extensive ablation and cross-task
transfer experiments. Our DepthSplat achieves state-of-the-art performance on
ScanNet, RealEstate10K and DL3DV datasets in terms of both depth estimation and
novel view synthesis, demonstrating the mutual benefits of connecting both
tasks. Our code, models, and video results are available at
https://haofeixu.github.io/depthsplat/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://haofeixu.github.io/depthsplat/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PUMA: Empowering Unified MLLM with Multi-granular Visual Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongyao Fang, Chengqi Duan, Kun Wang, Hao Li, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Hongsheng Li, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in multimodal foundation models have yielded significant
progress in vision-language understanding. Initial attempts have also explored
the potential of multimodal large language models (MLLMs) for visual content
generation. However, existing works have insufficiently addressed the varying
granularity demands of different image generation tasks within a unified MLLM
paradigm - from the diversity required in text-to-image generation to the
precise controllability needed in image manipulation. In this work, we propose
PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA
unifies multi-granular visual features as both inputs and outputs of MLLMs,
elegantly addressing the different granularity requirements of various image
generation tasks within a unified MLLM framework. Following multimodal
pretraining and task-specific instruction tuning, PUMA demonstrates proficiency
in a wide range of multimodal tasks. This work represents a significant step
towards a truly unified MLLM capable of adapting to the granularity demands of
various visual tasks. The code and model will be released in
https://github.com/rongyaofang/PUMA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://rongyaofang.github.io/puma/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runsen Xu, Zhiwei Huang, Tai Wang, Yilun Chen, Jiangmiao Pang, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D visual grounding is crucial for robots, requiring integration of natural
language and 3D scene understanding. Traditional methods depending on
supervised learning with 3D point clouds are limited by scarce datasets.
Recently zero-shot methods leveraging LLMs have been proposed to address the
data issue. While effective, these methods only use object-centric information,
limiting their ability to handle complex queries. In this work, we present
VLM-Grounder, a novel framework using vision-language models (VLMs) for
zero-shot 3D visual grounding based solely on 2D images. VLM-Grounder
dynamically stitches image sequences, employs a grounding and feedback scheme
to find the target object, and uses a multi-view ensemble projection to
accurately estimate 3D bounding boxes. Experiments on ScanRefer and Nr3D
datasets show VLM-Grounder outperforms previous zero-shot methods, achieving
51.6% Acc@0.25 on ScanRefer and 48.0% Acc on Nr3D, without relying on 3D
geometry or object priors. Codes are available at
https://github.com/OpenRobotLab/VLM-Grounder .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CoRL 2024 Camera Ready. 25 pages. A novel zero-shot 3D visual
  grounding framework based solely on 2D images</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $γ-$MoD: Exploring Mixture-of-Depth Adaptation for <span class="highlight-title">Multimodal</span> Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaxin Luo, Gen Luo, Jiayi Ji, Yiyi Zhou, Xiaoshuai Sun, Zhiqiang Shen, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the significant progress in multimodal large language models (MLLMs),
their high computational cost remains a barrier to real-world deployment.
Inspired by the mixture of depths (MoDs) in natural language processing, we aim
to address this limitation from the perspective of ``activated tokens''. Our
key insight is that if most tokens are redundant for the layer computation,
then can be skipped directly via the MoD layer. However, directly converting
the dense layers of MLLMs to MoD layers leads to substantial performance
degradation. To address this issue, we propose an innovative MoD adaptation
strategy for existing MLLMs called $\gamma$-MoD. In $\gamma$-MoD, a novel
metric is proposed to guide the deployment of MoDs in the MLLM, namely rank of
attention maps (ARank). Through ARank, we can effectively identify which layer
is redundant and should be replaced with the MoD layer. Based on ARank, we
further propose two novel designs to maximize the computational sparsity of
MLLM while maintaining its performance, namely shared vision-language router
and masked routing learning. With these designs, more than 90% dense layers of
the MLLM can be effectively converted to the MoD ones. To validate our method,
we apply it to three popular MLLMs, and conduct extensive experiments on 9
benchmark datasets. Experimental results not only validate the significant
efficiency benefit of $\gamma$-MoD to existing MLLMs but also confirm its
generalization ability on various MLLMs. For example, with a minor performance
drop, i.e., -1.5%, $\gamma$-MoD can reduce the training and inference time of
LLaVA-HR by 31.0% and 53.2%, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can MLLMs Understand the Deep Implication Behind Chinese Images? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhao Zhang, Xi Feng, Yuelin Bai, Xinrun Du, Jinchang Hou, Kaixin Deng, Guangzeng Han, Qinrui Li, Bingli Wang, Jiaheng Liu, Xingwei Qu, Yifei Zhang, Qixuan Zhao, Yiming Liang, Ziqiang Liu, Feiteng Fang, Min Yang, Wenhao Huang, Chenghua Lin, Ge Zhang, Shiwen Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the capabilities of Multimodal Large Language Models (MLLMs) continue to
improve, the need for higher-order capability evaluation of MLLMs is
increasing. However, there is a lack of work evaluating MLLM for higher-order
perception and understanding of Chinese visual content. To fill the gap, we
introduce the **C**hinese **I**mage **I**mplication understanding
**Bench**mark, **CII-Bench**, which aims to assess the higher-order perception
and understanding capabilities of MLLMs for Chinese images. CII-Bench stands
out in several ways compared to existing benchmarks. Firstly, to ensure the
authenticity of the Chinese context, images in CII-Bench are sourced from the
Chinese Internet and manually reviewed, with corresponding answers also
manually crafted. Additionally, CII-Bench incorporates images that represent
Chinese traditional culture, such as famous Chinese traditional paintings,
which can deeply reflect the model's understanding of Chinese traditional
culture. Through extensive experiments on CII-Bench across multiple MLLMs, we
have made significant findings. Initially, a substantial gap is observed
between the performance of MLLMs and humans on CII-Bench. The highest accuracy
of MLLMs attains 64.4%, where as human accuracy averages 78.2%, peaking at an
impressive 81.0%. Subsequently, MLLMs perform worse on Chinese traditional
culture images, suggesting limitations in their ability to understand
high-level semantics and lack a deep knowledge base of Chinese traditional
culture. Finally, it is observed that most models exhibit enhanced accuracy
when image emotion hints are incorporated into the prompts. We believe that
CII-Bench will enable MLLMs to gain a better understanding of Chinese semantics
and Chinese-specific images, advancing the journey towards expert artificial
general intelligence (AGI). Our project is publicly available at
https://cii-bench.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages,18 figures. Project Page: https://cii-bench.github.io/ Code:
  https://github.com/MING_X/CII-Bench Dataset:
  https://huggingface.co/datasets/m-a-p/CII-Bench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrospective Learning from Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zizhao Chen, Mustafa Omer Gul, Yiwei Chen, Gloria Geng, Anne Wu, Yoav Artzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-turn interactions between large language models (LLMs) and users
naturally include implicit feedback signals. If an LLM responds in an
unexpected way to an instruction, the user is likely to signal it by rephrasing
the request, expressing frustration, or pivoting to an alternative task. Such
signals are task-independent and occupy a relatively constrained subspace of
language, allowing the LLM to identify them even if it fails on the actual
task. This creates an avenue for continually learning from interactions without
additional annotations. We introduce ReSpect, a method to learn from such
signals in past interactions via retrospection. We deploy ReSpect in a new
multimodal interaction scenario, where humans instruct an LLM to solve an
abstract reasoning task with a combinatorial solution space. Through thousands
of interactions with humans, we show how ReSpect gradually improves task
completion rate from 31% to 82%, all without any external annotation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentiable Robot Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoshi Liu, Alper Canberk, Shuran Song, Carl Vondrick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision foundation models trained on massive amounts of visual data have shown
unprecedented reasoning and planning skills in open-world settings. A key
challenge in applying them to robotic tasks is the modality gap between visual
data and action data. We introduce differentiable robot rendering, a method
allowing the visual appearance of a robot body to be directly differentiable
with respect to its control parameters. Our model integrates a kinematics-aware
deformable model and Gaussians Splatting and is compatible with any robot form
factors and degrees of freedom. We demonstrate its capability and usage in
applications including reconstruction of robot poses from images and
controlling robots through vision language models. Quantitative and qualitative
results show that our differentiable rendering model provides effective
gradients for robotic control directly from pixels, setting the foundation for
the future applications of vision foundation models in robotics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://drrobot.cs.columbia.edu/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Janus: Decoupling Visual Encoding for Unified <span class="highlight-title">Multimodal</span> Understanding
  and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Janus, an autoregressive framework that unifies
multimodal understanding and generation. Prior research often relies on a
single visual encoder for both tasks, such as Chameleon. However, due to the
differing levels of information granularity required by multimodal
understanding and generation, this approach can lead to suboptimal performance,
particularly in multimodal understanding. To address this issue, we decouple
visual encoding into separate pathways, while still leveraging a single,
unified transformer architecture for processing. The decoupling not only
alleviates the conflict between the visual encoder's roles in understanding and
generation, but also enhances the framework's flexibility. For instance, both
the multimodal understanding and generation components can independently select
their most suitable encoding methods. Experiments show that Janus surpasses
previous unified model and matches or exceeds the performance of task-specific
models. The simplicity, high flexibility, and effectiveness of Janus make it a
strong candidate for next-generation unified multimodal models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution
  Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yansong Peng, Hebei Li, Peixi Wu, Yueyi Zhang, Xiaoyan Sun, Feng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce D-FINE, a powerful real-time object detector that achieves
outstanding localization precision by redefining the bounding box regression
task in DETR models. D-FINE comprises two key components: Fine-grained
Distribution Refinement (FDR) and Global Optimal Localization Self-Distillation
(GO-LSD). FDR transforms the regression process from predicting fixed
coordinates to iteratively refining probability distributions, providing a
fine-grained intermediate representation that significantly enhances
localization accuracy. GO-LSD is a bidirectional optimization strategy that
transfers localization knowledge from refined distributions to shallower layers
through self-distillation, while also simplifying the residual prediction tasks
for deeper layers. Additionally, D-FINE incorporates lightweight optimizations
in computationally intensive modules and operations, achieving a better balance
between speed and accuracy. Specifically, D-FINE-L / X achieves 54.0% / 55.8%
AP on the COCO dataset at 124 / 78 FPS on an NVIDIA T4 GPU. When pretrained on
Objects365, D-FINE-L / X attains 57.1% / 59.3% AP, surpassing all existing
real-time detectors. Furthermore, our method significantly enhances the
performance of a wide range of DETR models by up to 5.3% AP with negligible
extra parameters and training costs. Our code and pretrained models:
https://github.com/Peterande/D-FINE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VidPanos: Generative Panoramic Videos from Casual Panning Videos <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Ma, Erika Lu, Roni Paiss, Shiran Zada, Aleksander Holynski, Tali Dekel, Brian Curless, Michael Rubinstein, Forrester Cole
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Panoramic image stitching provides a unified, wide-angle view of a scene that
extends beyond the camera's field of view. Stitching frames of a panning video
into a panoramic photograph is a well-understood problem for stationary scenes,
but when objects are moving, a still panorama cannot capture the scene. We
present a method for synthesizing a panoramic video from a casually-captured
panning video, as if the original video were captured with a wide-angle camera.
We pose panorama synthesis as a space-time outpainting problem, where we aim to
create a full panoramic video of the same length as the input video. Consistent
completion of the space-time volume requires a powerful, realistic prior over
video content and motion, for which we adapt generative video models. Existing
generative models do not, however, immediately extend to panorama completion,
as we show. We instead apply video generation as a component of our panorama
synthesis system, and demonstrate how to exploit the strengths of the models
while minimizing their limitations. Our system can create video panoramas for a
range of in-the-wild scenes including people, vehicles, and flowing water, as
well as stationary background features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://vidpanos.github.io/. To appear at SIGGRAPH
  Asia 2024 (conference track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise
  Motion Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu, Zhizhong Huang, Jiaxin Ye, Yingya Zhang, Hongming Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in customized video generation have enabled users to create
videos tailored to both specific subjects and motion trajectories. However,
existing methods often require complicated test-time fine-tuning and struggle
with balancing subject learning and motion control, limiting their real-world
applications. In this paper, we present DreamVideo-2, a zero-shot video
customization framework capable of generating videos with a specific subject
and motion trajectory, guided by a single image and a bounding box sequence,
respectively, and without the need for test-time fine-tuning. Specifically, we
introduce reference attention, which leverages the model's inherent
capabilities for subject learning, and devise a mask-guided motion module to
achieve precise motion control by fully utilizing the robust motion signal of
box masks derived from bounding boxes. While these two components achieve their
intended functions, we empirically observe that motion control tends to
dominate over subject learning. To address this, we propose two key designs: 1)
the masked reference attention, which integrates a blended latent mask modeling
scheme into reference attention to enhance subject representations at the
desired positions, and 2) a reweighted diffusion loss, which differentiates the
contributions of regions inside and outside the bounding boxes to ensure a
balance between subject and motion control. Extensive experimental results on a
newly curated dataset demonstrate that DreamVideo-2 outperforms
state-of-the-art methods in both subject customization and motion control. The
dataset, code, and models will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://dreamvideo2.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unearthing Skill-Level Insights for Understanding Trade-Offs of
  Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mazda Moayeri, Vidhisha Balachandran, Varun Chandrasekaran, Safoora Yousefi, Thomas Fel, Soheil Feizi, Besmira Nushi, Neel Joshi, Vibhav Vineet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With models getting stronger, evaluations have grown more complex, testing
multiple skills in one benchmark and even in the same instance at once.
However, skill-wise performance is obscured when inspecting aggregate accuracy,
under-utilizing the rich signal modern benchmarks contain. We propose an
automatic approach to recover the underlying skills relevant for any evaluation
instance, by way of inspecting model-generated rationales. After validating the
relevance of rationale-parsed skills and inferring skills for $46$k instances
over $12$ benchmarks, we observe many skills to be common across benchmarks,
resulting in the curation of hundreds of skill-slices (i.e. sets of instances
testing a common skill). Inspecting accuracy over these slices yields novel
insights on model trade-offs: e.g., compared to GPT-4o and Claude 3.5 Sonnet,
on average, Gemini 1.5 Pro is $18\%$ more accurate in "computing molar mass",
but $19\%$ less accurate in "applying constitutional law", despite the overall
accuracies of the three models differing by a mere $0.4\%$. Furthermore, we
demonstrate the practical utility of our approach by showing that insights
derived from skill slice analysis can generalize to held-out instances: when
routing each instance to the model strongest on the relevant skills, we see a
$3\%$ accuracy improvement over our $12$ dataset corpus. Our skill-slices and
framework open a new avenue in model evaluation, leveraging skill-specific
analyses to unlock a more granular and actionable understanding of model
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code at: github.com/microsoft/skill-slice-insights</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing Webpage UIs for Text-Rich Visual Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, Xiang Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-rich visual understanding-the ability to process environments where
dense textual content is integrated with visuals-is crucial for multimodal
large language models (MLLMs) to interact effectively with structured
environments. To enhance this capability, we propose synthesizing general
multimodal instructions from webpage UIs using text-based large language models
(LLMs). Despite lacking direct visual input, text-based LLMs are able to
process structured text representations from webpage accessibility trees. These
instructions are then paired with UI screenshots to train multimodal models. We
introduce MultiUI, a dataset containing 7.3 million samples from 1 million
websites, covering diverse multimodal tasks and UI layouts. Models trained on
MultiUI not only excel in web UI tasks-achieving up to a 48\% improvement on
VisualWebBench and a 19.1\% boost in action accuracy on a web agent dataset
Mind2Web-but also generalize surprisingly well to non-web UI tasks and even to
non-UI domains, such as document understanding, OCR, and chart interpretation.
These results highlight the broad applicability of web UI data for advancing
text-rich visual understanding across various scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Generative Models Unveil Patterns in Medical Images Through
  <span class="highlight-title">Vision-Language</span> Conditioning <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaodan Xing, Junzhi Ning, Yang Nan, Guang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models have significantly advanced medical imaging analysis
by enhancing dataset size and quality. Beyond mere data augmentation, our
research in this paper highlights an additional, significant capacity of deep
generative models: their ability to reveal and demonstrate patterns in medical
images. We employ a generative structure with hybrid conditions, combining
clinical data and segmentation masks to guide the image synthesis process.
Furthermore, we innovatively transformed the tabular clinical data into textual
descriptions. This approach simplifies the handling of missing values and also
enables us to leverage large pre-trained vision-language models that
investigate the relations between independent clinical entries and comprehend
general terms, such as gender and smoking status. Our approach differs from and
presents a more challenging task than traditional medical report-guided
synthesis due to the less visual correlation of our clinical information with
the images. To overcome this, we introduce a text-visual embedding mechanism
that strengthens the conditions, ensuring the network effectively utilizes the
provided information. Our pipeline is generalizable to both GAN-based and
diffusion models. Experiments on chest CT, particularly focusing on the smoking
status, demonstrated a consistent intensity shift in the lungs which is in
agreement with clinical observations, indicating the effectiveness of our
method in capturing and visualizing the impact of specific attributes on
medical image patterns. Our methods offer a new avenue for the early detection
and precise visualization of complex clinical conditions with deep generative
models. All codes are https://github.com/junzhin/DGM-VLC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AIM-FM Workshop of NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-style conversion for semantic segmentation of lesions in fundus
  images by adversarial attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clément Playout, Renaud Duval, Marie Carole Boucher, Farida Cheriet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The diagnosis of diabetic retinopathy, which relies on fundus images, faces
challenges in achieving transparency and interpretability when using a global
classification approach. However, segmentation-based databases are
significantly more expensive to acquire and combining them is often
problematic. This paper introduces a novel method, termed adversarial style
conversion, to address the lack of standardization in annotation styles across
diverse databases. By training a single architecture on combined databases, the
model spontaneously modifies its segmentation style depending on the input,
demonstrating the ability to convert among different labeling styles. The
proposed methodology adds a linear probe to detect dataset origin based on
encoder features and employs adversarial attacks to condition the model's
segmentation style. Results indicate significant qualitative and quantitative
through dataset combination, offering avenues for improved model
generalization, uncertainty estimation and continuous interpolation between
annotation styles. Our approach enables training a segmentation model with
diverse databases while controlling and leveraging annotation styles for
improved retinopathy diagnosis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConsisSR: Delving Deep into Consistency in Diffusion-based Image
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Gu, Peng-Tao Jiang, Hao Zhang, Mi Zhou, Jinwei Chen, Wenming Yang, Bo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world image super-resolution (Real-ISR) aims at restoring high-quality
(HQ) images from low-quality (LQ) inputs corrupted by unknown and complex
degradations. In particular, pretrained text-to-image (T2I) diffusion models
provide strong generative priors to reconstruct credible and intricate details.
However, T2I generation focuses on semantic consistency while Real-ISR
emphasizes pixel-level reconstruction, which hinders existing methods from
fully exploiting diffusion priors. To address this challenge, we introduce
ConsisSR to handle both semantic and pixel-level consistency. Specifically,
compared to coarse-grained text prompts, we exploit the more powerful CLIP
image embedding and effectively leverage both modalities through our Hybrid
Prompt Adapter (HPA) for semantic guidance. Secondly, we introduce Time-aware
Latent Augmentation (TALA) to mitigate the inherent gap between T2I generation
and Real-ISR consistency requirements. By randomly mixing LQ and HQ latent
inputs, our model not only handle timestep-specific diffusion noise but also
refine the accumulated latent representations. Last but not least, our
GAN-Embedding strategy employs the pretrained Real-ESRGAN model to refine the
diffusion start point. This accelerates the inference process to 10 steps while
preserving sampling quality, in a training-free manner.Our method demonstrates
state-of-the-art performance among both full-scale and accelerated models. The
code will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MotionBank: A Large-scale Video Motion Benchmark with Disentangled
  Rule-based Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Xu, Shaoyang Hua, Zili Lin, Yifan Liu, Feipeng Ma, Yichao Yan, Xin Jin, Xiaokang Yang, Wenjun Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we tackle the problem of how to build and benchmark a large
motion model (LMM). The ultimate goal of LMM is to serve as a foundation model
for versatile motion-related tasks, e.g., human motion generation, with
interpretability and generalizability. Though advanced, recent LMM-related
works are still limited by small-scale motion data and costly text
descriptions. Besides, previous motion benchmarks primarily focus on pure body
movements, neglecting the ubiquitous motions in context, i.e., humans
interacting with humans, objects, and scenes. To address these limitations, we
consolidate large-scale video action datasets as knowledge banks to build
MotionBank, which comprises 13 video action datasets, 1.24M motion sequences,
and 132.9M frames of natural and diverse human motions. Different from
laboratory-captured motions, in-the-wild human-centric videos contain abundant
motions in context. To facilitate better motion text alignment, we also
meticulously devise a motion caption generation algorithm to automatically
produce rule-based, unbiased, and disentangled text descriptions via the
kinematic characteristics for each motion. Extensive experiments show that our
MotionBank is beneficial for general motion-related tasks of human motion
generation, motion in-context generation, and motion understanding. Video
motions together with the rule-based text annotations could serve as an
efficient alternative for larger LMMs. Our dataset, codes, and benchmark will
be publicly available at https://github.com/liangxuy/MotionBank.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emphasizing Semantic Consistency of Salient Posture for Speech-Driven
  Gesture Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengqi Liu, Hexiang Wang, Jingyu Gong, Ran Yi, Qianyu Zhou, Xuequan Lu, Jiangbo Lu, Lizhuang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech-driven gesture generation aims at synthesizing a gesture sequence
synchronized with the input speech signal. Previous methods leverage neural
networks to directly map a compact audio representation to the gesture
sequence, ignoring the semantic association of different modalities and failing
to deal with salient gestures. In this paper, we propose a novel speech-driven
gesture generation method by emphasizing the semantic consistency of salient
posture. Specifically, we first learn a joint manifold space for the individual
representation of audio and body pose to exploit the inherent semantic
association between two modalities, and propose to enforce semantic consistency
via a consistency loss. Furthermore, we emphasize the semantic consistency of
salient postures by introducing a weakly-supervised detector to identify
salient postures, and reweighting the consistency loss to focus more on
learning the correspondence between salient postures and the high-level
semantics of speech content. In addition, we propose to extract audio features
dedicated to facial expression and body gesture separately, and design separate
branches for face and body gesture synthesis. Extensive experimental results
demonstrate the superiority of our method over the state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representing Model Weights with Language using Tree Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eliahu Horwitz, Bar Cavia, Jonathan Kahana, Yedid Hoshen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing availability of public models begs the question: can we train
neural networks that use other networks as input? This paper learns to
represent models within a joint space that embeds both model weights and
language. However, machine learning on model weights is challenging as model
weights often exhibit significant variation unrelated to the models' semantic
properties (nuisance variation). We identify a key property of real-world
models: most public models belong to a small set of Model Trees, where all
models within a tree are fine-tuned from a common ancestor (e.g., a foundation
model). Importantly, we find that within each tree there is less nuisance
variation between models. For example, while classifying models according to
their training dataset generally requires complex architectures, in our case,
even a linear classifier trained on a single layer is often effective. While
effective, linear layers are computationally expensive as model weights are
very high dimensional. To address this, we introduce Probing Experts (ProbeX),
a theoretically motivated, lightweight probing method. Notably, ProbeX is the
first probing method designed to learn from the weights of just a single model
layer. We also construct and release a dataset that simulates the structure of
public model repositories. Our results show that ProbeX can effectively map the
weights of large models into a shared weight-language embedding space.
Furthermore, we demonstrate the impressive generalization of our method,
achieving zero-shot model classification and retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Eyelid Fold Consistency in Facial Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lohit Petikam, Charlie Hewitt, Fatemeh Saleh, Tadas Baltrušaitis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Eyelid shape is integral to identity and likeness in human facial modeling.
Human eyelids are diverse in appearance with varied skin fold and epicanthal
fold morphology between individuals. Existing parametric face models express
eyelid shape variation to an extent, but do not preserve sufficient likeness
across a diverse range of individuals. We propose a new definition of eyelid
fold consistency and implement geometric processing techniques to model diverse
eyelid shapes in a unified topology. Using this method we reprocess data used
to train a parametric face model and demonstrate significant improvements in
face-related machine learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving <span class="highlight-title">Multi-modal</span> Large Language Model through Boosting Vision
  Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanpeng Sun, Huaxin Zhang, Qiang Chen, Xinyu Zhang, Nong Sang, Gang Zhang, Jingdong Wang, Zechao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We focus on improving the visual understanding capability for boosting the
vision-language models. We propose \textbf{Arcana}, a multiModal language
model, which introduces two crucial techniques. First, we present Multimodal
LoRA (MM-LoRA), a module designed to enhance the decoder. Unlike traditional
language-driven decoders, MM-LoRA consists of two parallel LoRAs -- one for
vision and one for language -- each with its own parameters. This disentangled
parameters design allows for more specialized learning in each modality and
better integration of multimodal information. Second, we introduce the Query
Ladder adapter (QLadder) to improve the visual encoder. QLadder employs a
learnable ``\textit{ladder}'' structure to deeply aggregates the intermediate
representations from the frozen pretrained visual encoder (e.g., CLIP image
encoder). This enables the model to learn new and informative visual features,
as well as remaining the powerful capabilities of the pretrained visual
encoder. These techniques collectively enhance Arcana's visual perception
power, enabling it to leverage improved visual information for more accurate
and contextually relevant outputs across various multimodal scenarios.
Extensive experiments and ablation studies demonstrate the effectiveness and
generalization capability of our Arcana. The code and re-annotated data are
available at \url{https://arcana-project-page.github.io}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework
  for Talking Head Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanbo Cheng, Limin Lin, Chenyu Liu, Pengcheng Xia, Pengfei Hu, Jiefeng Ma, Jun Du, Jia Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Talking head generation intends to produce vivid and realistic talking head
videos from a single portrait and speech audio clip. Although significant
progress has been made in diffusion-based talking head generation, almost all
methods rely on autoregressive strategies, which suffer from limited context
utilization beyond the current generation step, error accumulation, and slower
generation speed. To address these challenges, we present DAWN (Dynamic frame
Avatar With Non-autoregressive diffusion), a framework that enables all-at-once
generation of dynamic-length video sequences. Specifically, it consists of two
main components: (1) audio-driven holistic facial dynamics generation in the
latent motion space, and (2) audio-driven head pose and blink generation.
Extensive experiments demonstrate that our method generates authentic and vivid
videos with precise lip motions, and natural pose/blink movements.
Additionally, with a high generation speed, DAWN possesses strong extrapolation
capabilities, ensuring the stable production of high-quality long videos. These
results highlight the considerable promise and potential impact of DAWN in the
field of talking head video generation. Furthermore, we hope that DAWN sparks
further exploration of non-autoregressive approaches in diffusion models. Our
code will be publicly at https://github.com/Hanbo-Cheng/DAWN-pytorch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Movie Gen: A Cast of Media Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, Yuming Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Movie Gen, a cast of foundation models that generates
high-quality, 1080p HD videos with different aspect ratios and synchronized
audio. We also show additional capabilities such as precise instruction-based
video editing and generation of personalized videos based on a user's image.
Our models set a new state-of-the-art on multiple tasks: text-to-video
synthesis, video personalization, video editing, video-to-audio generation, and
text-to-audio generation. Our largest video generation model is a 30B parameter
transformer trained with a maximum context length of 73K video tokens,
corresponding to a generated video of 16 seconds at 16 frames-per-second. We
show multiple technical innovations and simplifications on the architecture,
latent spaces, training objectives and recipes, data curation, evaluation
protocols, parallelization techniques, and inference optimizations that allow
us to reap the benefits of scaling pre-training data, model size, and training
compute for training large scale media generation models. We hope this paper
helps the research community to accelerate progress and innovation in media
generation models. All videos from this paper are available at
https://go.fb.me/MovieGenResearchVideos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Design Space of Visual Context Representation in Video
  MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Du, Yuqi Huo, Kun Zhou, Zijia Zhao, Haoyu Lu, Han Huang, Wayne Xin Zhao, Bingning Wang, Weipeng Chen, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Multimodal Large Language Models (MLLMs) have shown remarkable
capability of understanding the video semantics on various downstream tasks.
Despite the advancements, there is still a lack of systematic research on
visual context representation, which refers to the scheme to select frames from
a video and further select the tokens from a frame. In this paper, we explore
the design space for visual context representation, and aim to improve the
performance of video MLLMs by finding more effective representation schemes.
Firstly, we formulate the task of visual context representation as a
constrained optimization problem, and model the language modeling loss as a
function of the number of frames and the number of embeddings (or tokens) per
frame, given the maximum visual context window size. Then, we explore the
scaling effects in frame selection and token selection respectively, and fit
the corresponding function curve by conducting extensive empirical experiments.
We examine the effectiveness of typical selection strategies and present
empirical findings to determine the two factors. Furthermore, we study the
joint effect of frame selection and token selection, and derive the optimal
formula for determining the two factors. We demonstrate that the derived
optimal settings show alignment with the best-performed results of empirical
experiments. Our code and model are available at:
https://github.com/RUCAIBox/Opt-Visor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Long Video MLLM; work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Label-free prediction of fluorescence markers in bovine satellite cells
  using deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sania Sinha, Aarham Wasit, Won Seob Kim, Jongkyoo Kim, Jiyoon Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing the quality of bovine satellite cells (BSCs) is essential for the
cultivated meat industry, which aims to address global food sustainability
challenges. This study aims to develop a label-free method for predicting
fluorescence markers in isolated BSCs using deep learning. We employed a
U-Net-based CNN model to predict multiple fluorescence signals from a single
bright-field microscopy image of cell culture. Two key biomarkers, DAPI and
Pax7, were used to determine the abundance and quality of BSCs. The image
pre-processing pipeline included fluorescence denoising to improve prediction
performance and consistency. A total of 48 biological replicates were used,
with statistical performance metrics such as Pearson correlation coefficient
and SSIM employed for model evaluation. The model exhibited better performance
with DAPI predictions due to uniform staining. Pax7 predictions were more
variable, reflecting biological heterogeneity. Enhanced visualization
techniques, including color mapping and image overlay, improved the
interpretability of the predictions by providing better contextual and
perceptual information. The findings highlight the importance of data
pre-processing and demonstrate the potential of deep learning to advance
non-invasive, label-free assessment techniques in the cultivated meat industry,
paving the way for reliable and actionable AI-driven evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pose-Based Sign Language Appearance Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Moryossef, Gerard Sant, Zifan Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a method for transferring the signer's appearance in sign
language skeletal poses while preserving the sign content. Using estimated
poses, we transfer the appearance of one signer to another, maintaining natural
movements and transitions. This approach improves pose-based rendering and sign
stitching while obfuscating identity. Our experiments show that while the
method reduces signer identification accuracy, it slightly harms sign
recognition performance, highlighting a tradeoff between privacy and utility.
Our code is available at
\url{https://github.com/sign-language-processing/pose-anonymization}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning
  via Image-Guided Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijun Liang, Shweta Bhardwaj, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-quality or scarce data has posed significant challenges for training deep
neural networks in practice. While classical data augmentation cannot
contribute very different new data, diffusion models opens up a new door to
build self-evolving AI by generating high-quality and diverse synthetic data
through text-guided prompts. However, text-only guidance cannot control
synthetic images' proximity to the original images, resulting in
out-of-distribution data detrimental to the model performance. To overcome the
limitation, we study image guidance to achieve a spectrum of interpolations
between synthetic and real images. With stronger image guidance, the generated
images are similar to the training data but hard to learn. While with weaker
image guidance, the synthetic images will be easier for model but contribute to
a larger distribution gap with the original data. The generated full spectrum
of data enables us to build a novel "Diffusion Curriculum (DisCL)". DisCL
adjusts the image guidance level of image synthesis for each training stage: It
identifies and focuses on hard samples for the model and assesses the most
effective guidance level of synthetic images to improve hard data learning. We
apply DisCL to two challenging tasks: long-tail (LT) classification and
learning from low-quality data. It focuses on lower-guidance images of
high-quality to learn prototypical features as a warm-up of learning
higher-guidance images that might be weak on diversity or quality. Extensive
experiments showcase a gain of 2.7% and 2.1% in OOD and ID macro-accuracy when
applying DisCL to iWildCam dataset. On ImageNet-LT, DisCL improves the base
model's tail-class accuracy from 4.4% to 23.64% and leads to a 4.02%
improvement in all-class accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VL-GLUE: A Suite of Fundamental yet Challenging Visuo-Linguistic
  Reasoning Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shailaja Keyur Sampat, Mutsumi Nakamura, Shankar Kailas, Kartik Aggarwal, Mandy Zhou, Yezhou Yang, Chitta Baral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deriving inference from heterogeneous inputs (such as images, text, and
audio) is an important skill for humans to perform day-to-day tasks. A similar
ability is desirable for the development of advanced Artificial Intelligence
(AI) systems. While state-of-the-art models are rapidly closing the gap with
human-level performance on diverse computer vision and NLP tasks separately,
they struggle to solve tasks that require joint reasoning over visual and
textual modalities. Inspired by GLUE (Wang et. al., 2018)- a multitask
benchmark for natural language understanding, we propose VL-GLUE in this paper.
VL-GLUE consists of over 100k samples spanned across seven different tasks,
which at their core require visuo-linguistic reasoning. Moreover, our benchmark
comprises of diverse image types (from synthetically rendered figures, and
day-to-day scenes to charts and complex diagrams) and includes a broad variety
of domain-specific text (from cooking, politics, and sports to high-school
curricula), demonstrating the need for multi-modal understanding in the
real-world. We show that this benchmark is quite challenging for existing
large-scale vision-language models and encourage development of systems that
possess robust visuo-linguistic reasoning capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiRecNetV2: A Transformer-Enhanced Network for Aerial Disaster
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Demetris Shianios, Panayiotis Kolios, Christos Kyrkou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Unmanned Aerial Vehicles (UAVs) with artificial
intelligence (AI) models for aerial imagery processing in disaster assessment,
necessitates models that demonstrate exceptional accuracy, computational
efficiency, and real-time processing capabilities. Traditionally Convolutional
Neural Networks (CNNs), demonstrate efficiency in local feature extraction but
are limited by their potential for global context interpretation. On the other
hand, Vision Transformers (ViTs) show promise for improved global context
interpretation through the use of attention mechanisms, although they still
remain underinvestigated in UAV-based disaster response applications. Bridging
this research gap, we introduce DiRecNetV2, an improved hybrid model that
utilizes convolutional and transformer layers. It merges the inductive biases
of CNNs for robust feature extraction with the global context understanding of
Transformers, maintaining a low computational load ideal for UAV applications.
Additionally, we introduce a new, compact multi-label dataset of disasters, to
set an initial benchmark for future research, exploring how models trained on
single-label data perform in a multi-label test set. The study assesses
lightweight CNNs and ViTs on the AIDERSv2 dataset, based on the frames per
second (FPS) for efficiency and the weighted F1 scores for classification
performance. DiRecNetV2 not only achieves a weighted F1 score of 0.964 on a
single-label test set but also demonstrates adaptability, with a score of 0.614
on a complex multi-label test set, while functioning at 176.13 FPS on the
Nvidia Orin Jetson device.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ActionCOMET: A Zero-shot Approach to Learn Image-specific Commonsense
  Concepts about Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shailaja Keyur Sampat, Yezhou Yang, Chitta Baral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans observe various actions being performed by other humans (physically or
in videos/images) and can draw a wide range of inferences about it beyond what
they can visually perceive. Such inferences include determining the aspects of
the world that make action execution possible (e.g. liquid objects can undergo
pouring), predicting how the world will change as a result of the action (e.g.
potatoes being golden and crispy after frying), high-level goals associated
with the action (e.g. beat the eggs to make an omelet) and reasoning about
actions that possibly precede or follow the current action (e.g. crack eggs
before whisking or draining pasta after boiling). Similar reasoning ability is
highly desirable in autonomous systems that would assist us in performing
everyday tasks. To that end, we propose a multi-modal task to learn
aforementioned concepts about actions being performed in images. We develop a
dataset consisting of 8.5k images and 59.3k inferences about actions grounded
in those images, collected from an annotated cooking-video dataset. We propose
ActionCOMET, a zero-shot framework to discern knowledge present in language
models specific to the provided visual input. We present baseline results of
ActionCOMET over the collected dataset and compare them with the performance of
the best existing VQA approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 3 figures. arXiv admin note: text overlap with
  arXiv:2004.10796 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Help Me Identify: Is an LLM+VQA System All We Need to Identify Visual
  Concepts? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shailaja Keyur Sampat, Maitreya Patel, Yezhou Yang, Chitta Baral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An ability to learn about new objects from a small amount of visual data and
produce convincing linguistic justification about the presence/absence of
certain concepts (that collectively compose the object) in novel scenarios is
an important characteristic of human cognition. This is possible due to
abstraction of attributes/properties that an object is composed of e.g. an
object `bird' can be identified by the presence of a beak, feathers, legs,
wings, etc. Inspired by this aspect of human reasoning, in this work, we
present a zero-shot framework for fine-grained visual concept learning by
leveraging large language model and Visual Question Answering (VQA) system.
Specifically, we prompt GPT-3 to obtain a rich linguistic description of visual
objects in the dataset. We convert the obtained concept descriptions into a set
of binary questions. We pose these questions along with the query image to a
VQA system and aggregate the answers to determine the presence or absence of an
object in the test images. Our experiments demonstrate comparable performance
with existing zero-shot visual classification methods and few-shot concept
learning approaches, without substantial computational overhead, yet being
fully explainable from the reasoning perspective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced <span class="highlight-title">Prompt</span>-leveraged Weakly Supervised Cancer Segmentation based on
  Segment Anything 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joonhyeon Song, Seohwan Yun, Seongho Yoon, Joohyeok Kim, Sangmin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a novel approach beyond supervised learning for effective
pathological image analysis, addressing the challenge of limited robust labeled
data. Pathological diagnosis of diseases like cancer has conventionally relied
on the evaluation of morphological features by physicians and pathologists.
However, recent advancements in compute-aided diagnosis (CAD) systems are
gaining significant attention as diagnostic support tools. Although the
advancement of deep learning has improved CAD significantly, segmentation
models typically require large pixel-level annotated dataset, and such labeling
is expensive. Existing studies not based on supervised approaches still
struggle with limited generalization, and no practical approach has emerged
yet. To address this issue, we present a weakly supervised semantic
segmentation (WSSS) model by combining class activation map and Segment
Anything Model (SAM)-based pseudo-labeling. For effective pretraining, we adopt
the SAM-a foundation model that is pretrained on large datasets and operates in
zero-shot configurations using only coarse prompts. The proposed approach
transfer enhanced Attention Dropout Layer's knowledge to SAM, thereby
generating pseudo-labels. To demonstrate the superiority of the proposed
method, experimental studies are conducted on histopathological breast cancer
datasets. The proposed method outperformed other WSSS methods across three
datasets, demonstrating its efficiency by achieving this with only 12GB of GPU
memory during training. Our code is available at :
https://github.com/QI-NemoSong/EPLC-SAM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoLDU: Low-Rank Adaptation via Lower-Diag-Upper Decomposition for
  Parameter-Efficient <span class="highlight-title">Fine-Tuning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Shi, Jiwei Wei, Yujia Wu, Ran Ran, Chengwei Sun, Shiyuan He, Yang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of model scale has necessitated substantial computational
resources for fine-tuning. Existing approach such as Low-Rank Adaptation (LoRA)
has sought to address the problem of handling the large updated parameters in
full fine-tuning. However, LoRA utilize random initialization and optimization
of low-rank matrices to approximate updated weights, which can result in
suboptimal convergence and an accuracy gap compared to full fine-tuning. To
address these issues, we propose LoLDU, a Parameter-Efficient Fine-Tuning
(PEFT) approach that significantly reduces trainable parameters by 2600 times
compared to regular PEFT methods while maintaining comparable performance.
LoLDU leverages Lower-Diag-Upper Decomposition (LDU) to initialize low-rank
matrices for faster convergence and orthogonality. We focus on optimizing the
diagonal matrix for scaling transformations. To the best of our knowledge,
LoLDU has the fewest parameters among all PEFT approaches. We conducted
extensive experiments across 4 instruction-following datasets, 6 natural
language understanding (NLU) datasets, 8 image classification datasets, and
image generation datasets with multiple model types (LLaMA2, RoBERTa, ViT, and
Stable Diffusion), providing a comprehensive and detailed analysis. Our
open-source code can be accessed at
\href{https://github.com/SKDDJ/LoLDU}{https://github.com/SKDDJ/LoLDU}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatiotemporal Object Detection for Improved Aerial Vehicle Detection in
  Traffic Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kristina Telegraph, Christos Kyrkou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents advancements in multi-class vehicle detection using UAV
cameras through the development of spatiotemporal object detection models. The
study introduces a Spatio-Temporal Vehicle Detection Dataset (STVD) containing
6, 600 annotated sequential frame images captured by UAVs, enabling
comprehensive training and evaluation of algorithms for holistic spatiotemporal
perception. A YOLO-based object detection algorithm is enhanced to incorporate
temporal dynamics, resulting in improved performance over single frame models.
The integration of attention mechanisms into spatiotemporal models is shown to
further enhance performance. Experimental validation demonstrates significant
progress, with the best spatiotemporal model exhibiting a 16.22% improvement
over single frame models, while it is demonstrated that attention mechanisms
hold the potential for additional performance gains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Material Fingerprinting: Identifying and Predicting Perceptual
  Attributes of Material Appearance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiri Filip, Filip Dechterenko, Filipp Schmidt, Jiri Lukavsky, Veronika Vilimovska, Jan Kotera, Roland W. Fleming
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The world is abundant with diverse materials, each possessing unique surface
appearances that play a crucial role in our daily perception and understanding
of their properties. Despite advancements in technology enabling the capture
and realistic reproduction of material appearances for visualization and
quality control, the interoperability of material property information across
various measurement representations and software platforms remains a complex
challenge. A key to overcoming this challenge lies in the automatic
identification of materials' perceptual features, enabling intuitive
differentiation of properties stored in disparate material data
representations. We reasoned that for many practical purposes, a compact
representation of the perceptual appearance is more useful than an exhaustive
physical description.This paper introduces a novel approach to material
identification by encoding perceptual features obtained from dynamic visual
stimuli. We conducted a psychophysical experiment to select and validate 16
particularly significant perceptual attributes obtained from videos of 347
materials. We then gathered attribute ratings from over twenty participants for
each material, creating a 'material fingerprint' that encodes the unique
perceptual properties of each material. Finally, we trained a multi-layer
perceptron model to predict the relationship between statistical and deep
learning image features and their corresponding perceptual properties. We
demonstrate the model's performance in material retrieval and filtering
according to individual attributes. This model represents a significant step
towards simplifying the sharing and understanding of material properties in
diverse digital environments regardless of their digital representation,
enhancing both the accuracy and efficiency of material identification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 12 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEGA: Memory-Efficient 4D Gaussian Splatting for Dynamic Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinjie Zhang, Zhening Liu, Yifan Zhang, Xingtong Ge, Dailan He, Tongda Xu, Yan Wang, Zehong Lin, Shuicheng Yan, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  4D Gaussian Splatting (4DGS) has recently emerged as a promising technique
for capturing complex dynamic 3D scenes with high fidelity. It utilizes a 4D
Gaussian representation and a GPU-friendly rasterizer, enabling rapid rendering
speeds. Despite its advantages, 4DGS faces significant challenges, notably the
requirement of millions of 4D Gaussians, each with extensive associated
attributes, leading to substantial memory and storage cost. This paper
introduces a memory-efficient framework for 4DGS. We streamline the color
attribute by decomposing it into a per-Gaussian direct color component with
only 3 parameters and a shared lightweight alternating current color predictor.
This approach eliminates the need for spherical harmonics coefficients, which
typically involve up to 144 parameters in classic 4DGS, thereby creating a
memory-efficient 4D Gaussian representation. Furthermore, we introduce an
entropy-constrained Gaussian deformation technique that uses a deformation
field to expand the action range of each Gaussian and integrates an
opacity-based entropy loss to limit the number of Gaussians, thus forcing our
model to use as few Gaussians as possible to fit a dynamic scene well. With
simple half-precision storage and zip compression, our framework achieves a
storage reduction by approximately 190$\times$ and 125$\times$ on the
Technicolor and Neural 3D Video datasets, respectively, compared to the
original 4DGS. Meanwhile, it maintains comparable rendering speeds and scene
representation quality, setting a new standard in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ H2OVL-Mississippi <span class="highlight-title">Vision Language</span> Models Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaikat Galib, Shanshan Wang, Guanshuo Xu, Pascal Pfeiffer, Ryan Chesler, Mark Landry, Sri Satish Ambati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Smaller vision-language models (VLMs) are becoming increasingly important for
privacy-focused, on-device applications due to their ability to run efficiently
on consumer hardware for processing enterprise commercial documents and images.
These models require strong language understanding and visual capabilities to
enhance human-machine interaction. To address this need, we present
H2OVL-Mississippi, a pair of small VLMs trained on 37 million image-text pairs
using 240 hours of compute on 8 x H100 GPUs. H2OVL-Mississippi-0.8B is a tiny
model with 0.8 billion parameters that specializes in text recognition,
achieving state of the art performance on the Text Recognition portion of
OCRBench and surpassing much larger models in this area. Additionally, we are
releasing H2OVL-Mississippi-2B, a 2 billion parameter model for general use
cases, exhibiting highly competitive metrics across various academic
benchmarks. Both models build upon our prior work with H2O-Danube language
models, extending their capabilities into the visual domain. We release them
under the Apache 2.0 license, making VLMs accessible to everyone, democratizing
document AI and visual LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DN-4DGS: Denoised Deformable Network with Temporal-Spatial Aggregation
  for Dynamic Scene Rendering <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Lu, Jiacheng Deng, Ruijie Zhu, Yanzhe Liang, Wenfei Yang, Tianzhu Zhang, Xu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic scenes rendering is an intriguing yet challenging problem. Although
current methods based on NeRF have achieved satisfactory performance, they
still can not reach real-time levels. Recently, 3D Gaussian Splatting (3DGS)
has gar?nered researchers attention due to their outstanding rendering quality
and real?time speed. Therefore, a new paradigm has been proposed: defining a
canonical 3D gaussians and deforming it to individual frames in deformable
fields. How?ever, since the coordinates of canonical 3D gaussians are filled
with noise, which can transfer noise into the deformable fields, and there is
currently no method that adequately considers the aggregation of 4D
information. Therefore, we pro?pose Denoised Deformable Network with
Temporal-Spatial Aggregation for Dy?namic Scene Rendering (DN-4DGS).
Specifically, a Noise Suppression Strategy is introduced to change the
distribution of the coordinates of the canonical 3D gaussians and suppress
noise. Additionally, a Decoupled Temporal-Spatial Ag?gregation Module is
designed to aggregate information from adjacent points and frames. Extensive
experiments on various real-world datasets demonstrate that our method achieves
state-of-the-art rendering quality under a real-time level.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Let Me Finish My Sentence: Video Temporal Grounding with Holistic Text
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongbhin Woo, Hyeonggon Ryu, Youngjoon Jang, Jae Won Cho, Joon Son Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Temporal Grounding (VTG) aims to identify visual frames in a video clip
that match text queries. Recent studies in VTG employ cross-attention to
correlate visual frames and text queries as individual token sequences.
However, these approaches overlook a crucial aspect of the problem: a holistic
understanding of the query sentence. A model may capture correlations between
individual word tokens and arbitrary visual frames while possibly missing out
on the global meaning. To address this, we introduce two primary contributions:
(1) a visual frame-level gate mechanism that incorporates holistic textual
information, (2) cross-modal alignment loss to learn the fine-grained
correlation between query and relevant frames. As a result, we regularize the
effect of individual word tokens and suppress irrelevant visual frames. We
demonstrate that our method outperforms state-of-the-art approaches in VTG
benchmarks, indicating that holistic text understanding guides the model to
focus on the semantically important parts within the video.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACMMM 24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep-learning recognition and tracking of individual nanotubes in
  low-contrast microscopy videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimir Pimonov, Said Tahir, Vincent Jourdain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the challenge of analyzing the growth kinetics of carbon
nanotubes using in-situ homodyne polarization microscopy (HPM) by developing an
automated deep learning (DL) approach. A Mask-RCNN architecture, enhanced with
a ResNet-50 backbone, was employed to recognize and track individual nanotubes
in microscopy videos, significantly improving the efficiency and
reproducibility of kinetic data extraction. The method involves a series of
video processing steps to enhance contrast and used differential treatment
techniques to manage low signal and fast kinetics. The DL model demonstrates
consistency with manual measurements and increased throughput, laying the
foundation for statistical studies of nanotube growth. The approach can be
adapted for other types of in-situ microscopy studies, emphasizing the
importance of automation in high-throughput data acquisition for research on
individual nano-objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 Figures, No supporting information included</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pseudo Dataset Generation for Out-of-Domain Multi-Camera View
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuan-Ying Lee, Qian Zhou, Klara Nahrstedt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-camera systems are indispensable in movies, TV shows, and other media.
Selecting the appropriate camera at every timestamp has a decisive impact on
production quality and audience preferences. Learning-based view recommendation
frameworks can assist professionals in decision-making. However, they often
struggle outside of their training domains. The scarcity of labeled
multi-camera view recommendation datasets exacerbates the issue. Based on the
insight that many videos are edited from the original multi-camera videos, we
propose transforming regular videos into pseudo-labeled multi-camera view
recommendation datasets. Promisingly, by training the model on pseudo-labeled
datasets stemming from videos in the target domain, we achieve a 68% relative
improvement in the model's accuracy in the target domain and bridge the
accuracy gap between in-domain and never-before-seen domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to VCIP 2024. Project page:
  https://eric11220.github.io/publication/VCIP24/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Co-Segmentation without any Pixel-level Supervision with Application to
  Large-Scale Sketch Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos-Antonios Ypsilantis, Ondřej Chum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a novel method for object co-segmentation, i.e.
pixel-level localization of a common object in a set of images, that uses no
pixel-level supervision for training. Two pre-trained Vision Transformer (ViT)
models are exploited: ImageNet classification-trained ViT, whose features are
used to estimate rough object localization through intra-class token relevance,
and a self-supervised DINO-ViT for intra-image token relevance. On recent
challenging benchmarks, the method achieves state-of-the-art performance among
methods trained with the same level of supervision (image labels) while being
competitive with methods trained with pixel-level supervision (binary masks).
The benefits of the proposed co-segmentation method are further demonstrated in
the task of large-scale sketch recognition, that is, the classification of
sketches into a wide range of categories. The limited amount of hand-drawn
sketch training data is leveraged by exploiting readily available
image-level-annotated datasets of natural images containing a large number of
classes. To bridge the domain gap, the classifier is trained on a sketch-like
proxy domain derived from edges detected on natural images. We show that sketch
recognition significantly benefits when the classifier is trained on
sketch-like structures extracted from the co-segmented area rather than from
the full image. Code: https://github.com/nikosips/CBNC .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACCV 2024 Main Paper + Supplementary (Appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving
  Scene Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guosheng Zhao, Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Boyuan Wang, Youyi Zhang, Wenjun Mei, Xingang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Closed-loop simulation is essential for advancing end-to-end autonomous
driving systems. Contemporary sensor simulation methods, such as NeRF and 3DGS,
rely predominantly on conditions closely aligned with training data
distributions, which are largely confined to forward-driving scenarios.
Consequently, these methods face limitations when rendering complex maneuvers
(e.g., lane change, acceleration, deceleration). Recent advancements in
autonomous-driving world models have demonstrated the potential to generate
diverse driving videos. However, these approaches remain constrained to 2D
video generation, inherently lacking the spatiotemporal coherence required to
capture intricacies of dynamic driving environments. In this paper, we
introduce \textit{DriveDreamer4D}, which enhances 4D driving scene
representation leveraging world model priors. Specifically, we utilize the
world model as a data machine to synthesize novel trajectory videos based on
real-world driving data. Notably, we explicitly leverage structured conditions
to control the spatial-temporal consistency of foreground and background
elements, thus the generated data adheres closely to traffic constraints. To
our knowledge, \textit{DriveDreamer4D} is the first to utilize video generation
models for improving 4D reconstruction in driving scenarios. Experimental
results reveal that \textit{DriveDreamer4D} significantly enhances generation
quality under novel trajectory views, achieving a relative improvement in FID
by 24.5\%, 39.0\%, and 10.5\% compared to PVG, $\text{S}^3$Gaussian, and
Deformable-GS. Moreover, \textit{DriveDreamer4D} markedly enhances the
spatiotemporal coherence of driving agents, which is verified by a
comprehensive user study and the relative increases of 20.3\%, 42.0\%, and
13.7\% in the NTA-IoU metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://drivedreamer4d.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RGB to Hyperspectral: Spectral Reconstruction for Enhanced Surgical
  Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Czempiel, Alfie Roddan, Maria Leiloglou, Zepeng Hu, Kevin O'Neill, Giulio Anichini, Danail Stoyanov, Daniel Elson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the reconstruction of hyperspectral signatures from
RGB data to enhance surgical imaging, utilizing the publicly available
HeiPorSPECTRAL dataset from porcine surgery and an in-house neurosurgery
dataset. Various architectures based on convolutional neural networks (CNNs)
and transformer models are evaluated using comprehensive metrics. Transformer
models exhibit superior performance in terms of RMSE, SAM, PSNR and SSIM by
effectively integrating spatial information to predict accurate spectral
profiles, encompassing both visible and extended spectral ranges. Qualitative
assessments demonstrate the capability to predict spectral profiles critical
for informed surgical decision-making during procedures. Challenges associated
with capturing both the visible and extended hyperspectral ranges are
highlighted using the MAE, emphasizing the complexities involved. The findings
open up the new research direction of hyperspectral reconstruction for surgical
applications and clinical use cases in real-time surgical environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CCUP: A Controllable Synthetic Data Generation Pipeline for <span class="highlight-title">Pretrain</span>ing
  Cloth-Changing Person Re-Identification Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujian Zhao, Chengru Wu, Yinong Xu, Xuanzheng Du, Ruiyu Li, Guanglin Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloth-changing person re-identification (CC-ReID), also known as Long-Term
Person Re-Identification (LT-ReID) is a critical and challenging research topic
in computer vision that has recently garnered significant attention. However,
due to the high cost of constructing CC-ReID data, the existing data-driven
models are hard to train efficiently on limited data, causing overfitting
issue. To address this challenge, we propose a low-cost and efficient pipeline
for generating controllable and high-quality synthetic data simulating the
surveillance of real scenarios specific to the CC-ReID task. Particularly, we
construct a new self-annotated CC-ReID dataset named Cloth-Changing Unreal
Person (CCUP), containing 6,000 IDs, 1,179,976 images, 100 cameras, and 26.5
outfits per individual. Based on this large-scale dataset, we introduce an
effective and scalable pretrain-finetune framework for enhancing the
generalization capabilities of the traditional CC-ReID models. The extensive
experiments demonstrate that two typical models namely TransReID and FIRe^2,
when integrated into our framework, outperform other state-of-the-art models
after pretraining on CCUP and finetuning on the benchmarks such as PRCC,
VC-Clothes and NKUP. The CCUP is available at:
https://github.com/yjzhao1019/CCUP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 360U-Former: HDR Illumination Estimation with Panoramic Adapted Vision
  Transformers <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Hilliard, Adrian Hilton, Jean-Yves Guillemaut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent illumination estimation methods have focused on enhancing the
resolution and improving the quality and diversity of the generated textures.
However, few have explored tailoring the neural network architecture to the
Equirectangular Panorama (ERP) format utilised in image-based lighting.
Consequently, high dynamic range images (HDRI) results usually exhibit a seam
at the side borders and textures or objects that are warped at the poles. To
address this shortcoming we propose a novel architecture, 360U-Former, based on
a U-Net style Vision-Transformer which leverages the work of PanoSWIN, an
adapted shifted window attention tailored to the ERP format. To the best of our
knowledge, this is the first purely Vision-Transformer model used in the field
of illumination estimation. We train 360U-Former as a GAN to generate HDRI from
a limited field of view low dynamic range image (LDRI). We evaluate our method
using current illumination estimation evaluation protocols and datasets,
demonstrating that our approach outperforms existing and state-of-the-art
methods without the artefacts typically associated with the use of the ERP
format.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AIM Workshop 2024 at ECCV 2024, 18 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Location Modeling for Spatially Aware Object Insertion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jooyeol Yun, Davide Abati, Mohamed Omran, Jaegul Choo, Amirhossein Habibian, Auke Wiggers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models have become a powerful tool for image editing tasks,
including object insertion. However, these methods often lack spatial
awareness, generating objects with unrealistic locations and scales, or
unintentionally altering the scene background. A key challenge lies in
maintaining visual coherence, which requires both a geometrically suitable
object location and a high-quality image edit. In this paper, we focus on the
former, creating a location model dedicated to identifying realistic object
locations. Specifically, we train an autoregressive model that generates
bounding box coordinates, conditioned on the background image and the desired
object class. This formulation allows to effectively handle sparse placement
annotations and to incorporate implausible locations into a preference dataset
by performing direct preference optimization. Our extensive experiments
demonstrate that our generative location model, when paired with an inpainting
method, substantially outperforms state-of-the-art instruction-tuned models and
location modeling baselines in object insertion tasks, delivering accurate and
visually coherent results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RemoteDet-Mamba: A Hybrid Mamba-CNN Network for <span class="highlight-title">Multi-modal</span> Object
  Detection in Remote Sensing Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kejun Ren, Xin Wu, Lianming Xu, Li Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned aerial vehicle (UAV) remote sensing is widely applied in fields such
as emergency response, owing to its advantages of rapid information acquisition
and low cost. However, due to the effects of shooting distance and imaging
mechanisms, the objects in the images present challenges such as small size,
dense distribution, and low inter-class differentiation. To this end, we
propose a multimodal remote sensing detection network that employs a
quad-directional selective scanning fusion strategy called RemoteDet-Mamba.
RemoteDet-Mamba simultaneously facilitates the learning of single-modal local
features and the integration of patch-level global features across modalities,
enhancing the distinguishability for small objects and utilizing local
information to improve discrimination between different classes. Additionally,
the use of Mamba's serial processing significantly increases detection speed.
Experimental results on the DroneVehicle dataset demonstrate the effectiveness
of RemoteDet-Mamba, which achieves superior detection accuracy compared to
state-of-the-art methods while maintaining computational efficiency and
parameter count.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ L3DG: Latent 3D Gaussian Diffusion <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barbara Roessle, Norman Müller, Lorenzo Porzi, Samuel Rota Bulò, Peter Kontschieder, Angela Dai, Matthias Nießner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose L3DG, the first approach for generative 3D modeling of 3D
Gaussians through a latent 3D Gaussian diffusion formulation. This enables
effective generative 3D modeling, scaling to generation of entire room-scale
scenes which can be very efficiently rendered. To enable effective synthesis of
3D Gaussians, we propose a latent diffusion formulation, operating in a
compressed latent space of 3D Gaussians. This compressed latent space is
learned by a vector-quantized variational autoencoder (VQ-VAE), for which we
employ a sparse convolutional architecture to efficiently operate on room-scale
scenes. This way, the complexity of the costly generation process via diffusion
is substantially reduced, allowing higher detail on object-level generation, as
well as scalability to large scenes. By leveraging the 3D Gaussian
representation, the generated scenes can be rendered from arbitrary viewpoints
in real-time. We demonstrate that our approach significantly improves visual
quality over prior work on unconditional object-level radiance field synthesis
and showcase its applicability to room-scale scene generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH Asia 2024, project page:
  https://barbararoessle.github.io/l3dg , video: https://youtu.be/UHEEiXCYeLU</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Adversarial Synthesis of Radar Point Cloud Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Saad Nawaz, Thomas Dallmann, Torsten Schoen, Dirk Heberling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the validation and verification of automotive radars, datasets of
realistic traffic scenarios are required, which, how ever, are laborious to
acquire. In this paper, we introduce radar scene synthesis using GANs as an
alternative to the real dataset acquisition and simulation-based approaches. We
train a PointNet++ based GAN model to generate realistic radar point cloud
scenes and use a binary classifier to evaluate the performance of scenes
generated using this model against a test set of real scenes. We demonstrate
that our GAN model achieves similar performance (~87%) to the real scenes test
set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICMIM 2024; 7th IEEE MTT Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Medical <span class="highlight-title">Vision-Language</span> <span class="highlight-title">Pre-train</span>ing Succeed with Purely Synthetic
  Data? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Che Liu, Zhongwei Wan, Haozhe Wang, Yinda Chen, Talha Qaiser, Chen Jin, Fariba Yousefi, Nikolay Burlutskiy, Rossella Arcucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical Vision-Language Pre-training (MedVLP) has made significant progress
in enabling zero-shot tasks for medical image understanding. However, training
MedVLP models typically requires large-scale datasets with paired, high-quality
image-text data, which are scarce in the medical domain. Recent advancements in
Large Language Models (LLMs) and diffusion models have made it possible to
generate large-scale synthetic image-text pairs. This raises the question: *Can
MedVLP succeed using purely synthetic data?* To address this, we use
off-the-shelf generative models to create synthetic radiology reports and
paired Chest X-ray (CXR) images, and propose an automated pipeline to build a
diverse, high-quality synthetic dataset, enabling a rigorous study that
isolates model and training settings, focusing entirely from the data
perspective. Our results show that MedVLP models trained *exclusively on
synthetic data* outperform those trained on real data by **3.8%** in averaged
AUC on zero-shot classification. Moreover, using a combination of synthetic and
real data leads to a further improvement of **9.07%**. Additionally, MedVLP
models trained on synthetic or mixed data consistently outperform those trained
on real data in zero-shot grounding, as well as in fine-tuned classification
and segmentation tasks. Our analysis suggests MedVLP trained on well-designed
synthetic data can outperform models trained on real datasets, which may be
limited by low-quality samples and long-tailed distributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeoCoder: Solving Geometry Problems by Generating Modular Code through
  <span class="highlight-title">Vision-Language</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Sharma, Aman Dalmia, Mehran Kazemi, Amal Zouaq, Christopher J. Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometry problem-solving demands advanced reasoning abilities to process
multimodal inputs and employ mathematical knowledge effectively.
Vision-language models (VLMs) have made significant progress in various
multimodal tasks. Yet, they still struggle with geometry problems and are
significantly limited by their inability to perform mathematical operations not
seen during pre-training, such as calculating the cosine of an arbitrary angle,
and by difficulties in correctly applying relevant geometry formulas. To
overcome these challenges, we present GeoCoder, which leverages modular
code-finetuning to generate and execute code using a predefined geometry
function library. By executing the code, we achieve accurate and deterministic
calculations, contrasting the stochastic nature of autoregressive token
prediction, while the function library minimizes errors in formula usage. We
also propose a multimodal retrieval-augmented variant of GeoCoder, named
RAG-GeoCoder, which incorporates a non-parametric memory module for retrieving
functions from the geometry library, thereby reducing reliance on parametric
memory. Our modular code-finetuning approach enhances the geometric reasoning
capabilities of VLMs, yielding an average improvement of over 16% across
various question complexities on the GeomVerse dataset compared to other
finetuning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAda-Net: A Self-Supervised Adaptive Stereo Estimation CNN For Remote
  Sensing Image Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Hirner, Friedrich Fraundorfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stereo estimation has made many advancements in recent years with the
introduction of deep-learning. However the traditional supervised approach to
deep-learning requires the creation of accurate and plentiful ground-truth
data, which is expensive to create and not available in many situations. This
is especially true for remote sensing applications, where there is an excess of
available data without proper ground truth. To tackle this problem, we propose
a self-supervised CNN with self-improving adaptive abilities. In the first
iteration, the created disparity map is inaccurate and noisy. Leveraging the
left-right consistency check, we get a sparse but more accurate disparity map
which is used as an initial pseudo ground-truth. This pseudo ground-truth is
then adapted and updated after every epoch in the training step of the network.
We use the sum of inconsistent points in order to track the network
convergence. The code for our method is publicly available at:
https://github.com/thedodo/SAda-Net}{https://github.com/thedodo/SAda-Net
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Will be presented at ICPR2024 in December 2024 in Kolkata, India</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SemSim: Revisiting Weak-to-Strong Consistency from a Semantic Similarity
  Perspective for Semi-supervised Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiao Xie, Hongyi Wang, Ziwei Niu, Hao Sun, Shuyi Ouyang, Yen-Wei Chen, Lanfen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) for medical image segmentation is a
challenging yet highly practical task, which reduces reliance on large-scale
labeled dataset by leveraging unlabeled samples. Among SSL techniques, the
weak-to-strong consistency framework, popularized by FixMatch, has emerged as a
state-of-the-art method in classification tasks. Notably, such a simple
pipeline has also shown competitive performance in medical image segmentation.
However, two key limitations still persist, impeding its efficient adaptation:
(1) the neglect of contextual dependencies results in inconsistent predictions
for similar semantic features, leading to incomplete object segmentation; (2)
the lack of exploitation of semantic similarity between labeled and unlabeled
data induces considerable class-distribution discrepancy. To address these
limitations, we propose a novel semi-supervised framework based on FixMatch,
named SemSim, powered by two appealing designs from semantic similarity
perspective: (1) rectifying pixel-wise prediction by reasoning about the
intra-image pair-wise affinity map, thus integrating contextual dependencies
explicitly into the final prediction; (2) bridging labeled and unlabeled data
via a feature querying mechanism for compact class representation learning,
which fully considers cross-image anatomical similarities. As the reliable
semantic similarity extraction depends on robust features, we further introduce
an effective spatial-aware fusion module (SFM) to explore distinctive
information from multiple scales. Extensive experiments show that SemSim yields
consistent improvements over the state-of-the-art methods across three public
segmentation benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Day-Night Adaptation: An Innovative Source-free Adaptation Framework for
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Chen, Yiwen Ye, Yongsheng Pan, Yong Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distribution shifts widely exist in medical images acquired from different
medical centers, hindering the deployment of semantic segmentation models
trained on data from one center (source domain) to another (target domain).
While unsupervised domain adaptation (UDA) has shown significant promise in
mitigating these shifts, it poses privacy risks due to sharing data between
centers. To facilitate adaptation while preserving data privacy, source-free
domain adaptation (SFDA) and test-time adaptation (TTA) have emerged as
effective paradigms, relying solely on target domain data. However, the
scenarios currently addressed by SFDA and TTA are limited, making them less
suitable for clinical applications. In a more realistic clinical scenario, the
pre-trained model is deployed in a medical centre to assist with clinical tasks
during the day and rest at night. During the daytime process, TTA can be
employed to enhance inference performance. During the nighttime process, after
collecting the test data from the day, the model can be fine-tuned utilizing
SFDA to further adapt to the target domain. With above insights, we propose a
novel adaptation framework called Day-Night Adaptation (DyNA). This framework
adapts the model to the target domain through day-night loops without requiring
access to source data. Specifically, we implement distinct adaptation
strategies for daytime and nighttime to better meet the demands of clinical
settings. During the daytime, model parameters are frozen, and a specific
low-frequency prompt is trained for each test sample. Additionally, we
construct a memory bank for prompt initialization and develop a warm-up
mechanism to enhance prompt training. During nighttime, we integrate a global
student model into the traditional teacher-student self-training paradigm to
fine-tune the model while ensuring training stability...
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SiamSeg: Self-Training with Contrastive Learning for Unsupervised Domain
  Adaptation in Remote Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Wang, Fei Deng, Shuang Wang, Wen Luo, Zhixuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation of remote sensing (RS) images is a challenging task
with significant potential across various applications. Deep learning,
especially supervised learning with large-scale labeled datasets, has greatly
advanced this field. However, acquiring high-quality labeled data is expensive
and time-consuming. Moreover, variations in ground sampling distance (GSD),
imaging equipment, and geographic diversity contribute to domain shifts between
datasets, which pose significant challenges to models trained solely on source
domain data, leading to poor cross-domain performance. Domain shift is
well-known for undermining a model's generalization ability in the target
domain. To address this, unsupervised domain adaptation (UDA) has emerged as a
promising solution, enabling models to learn from unlabeled target domain data
while training on labeled source domain data. Recent advancements, particularly
in self-supervised learning via pseudo-label generation, have shown potential
in mitigating domain discrepancies. Strategies combining source and target
domain images with their true and pseudo labels for self-supervised training
have been effective in addressing domain bias. Despite progress in computer
vision, the application of pseudo-labeling methods to RS image segmentation
remains underexplored.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object Pose Estimation Using Implicit Representation For Transparent
  Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varun Burde, Artem Moroz, Vit Zeman, Pavel Burget
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object pose estimation is a prominent task in computer vision. The object
pose gives the orientation and translation of the object in real-world space,
which allows various applications such as manipulation, augmented reality, etc.
Various objects exhibit different properties with light, such as reflections,
absorption, etc. This makes it challenging to understand the object's structure
in RGB and depth channels. Recent research has been moving toward
learning-based methods, which provide a more flexible and generalizable
approach to object pose estimation utilizing deep learning. One such approach
is the render-and-compare method, which renders the object from multiple views
and compares it against the given 2D image, which often requires an object
representation in the form of a CAD model. We reason that the synthetic texture
of the CAD model may not be ideal for rendering and comparing operations. We
showed that if the object is represented as an implicit (neural) representation
in the form of Neural Radiance Field (NeRF), it exhibits a more realistic
rendering of the actual scene and retains the crucial spatial features, which
makes the comparison more versatile. We evaluated our NeRF implementation of
the render-and-compare method on transparent datasets and found that it
surpassed the current state-of-the-art results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmentation Policy Generation for Image Classification Using Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ant Duru, Alptekin Temizel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated data augmentation methods have significantly improved the
performance and generalization capability of deep learning models in image
classification. Yet, most state-of-the-art methods are optimized on common
benchmark datasets, limiting their applicability to more diverse or
domain-specific data, such as medical datasets. In this paper, we propose a
strategy that uses large language models to automatically generate efficient
augmentation policies, customized to fit the specific characteristics of any
dataset and model architecture. The proposed method iteratively interacts with
an LLM to obtain and refine the augmentation policies on model performance
feedback, creating a dataset-agnostic data augmentation pipeline. The proposed
method was evaluated on medical imaging datasets, showing a clear improvement
over state-of-the-art methods. The proposed approach offers an adaptive and
scalable solution. Although it increases computational cost, it significantly
boosts model robustness, automates the process, and minimizes the need for
human involvement during model development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, 4 tables, submitted for consideration to the
  International Workshop on Computational Intelligence for Multimedia
  Understanding (IWCIM), ISCAS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Similarity-Dissimilarity Loss with Supervised Contrastive Learning for
  Multi-label Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangming Huang, Yunfei Long, Cunjin Luo, Sheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised contrastive learning has been explored in making use of label
information for multi-label classification, but determining positive samples in
multi-label scenario remains challenging. Previous studies have examined
strategies for identifying positive samples, considering label overlap
proportion between anchors and samples. However, they ignore various relations
between given anchors and samples, as well as how to dynamically adjust the
weights in contrastive loss functions based on different relations, leading to
great ambiguity. In this paper, we introduce five distinct relations between
multi-label samples and propose a Similarity-Dissimilarity Loss with
contrastive learning for multi-label classification. Our loss function
re-weights the loss by computing the similarity and dissimilarity between
positive samples and a given anchor based on the introduced relations. We
mainly conduct experiments for multi-label text classification on MIMIC
datasets, then further extend the evaluation on MS-COCO. The Experimental
results show that our proposed loss effectively improves the performance on all
encoders under supervised contrastive learning paradigm, demonstrating its
effectiveness and robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal-Enhanced <span class="highlight-title">Multimodal</span> Transformer for Referring Multi-Object
  Tracking and Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changcheng Xiao, Qiong Cao, Yujie Zhong, Xiang Zhang, Tao Wang, Canqun Yang, Long Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring multi-object tracking (RMOT) is an emerging cross-modal task that
aims to locate an arbitrary number of target objects and maintain their
identities referred by a language expression in a video. This intricate task
involves the reasoning of linguistic and visual modalities, along with the
temporal association of target objects. However, the seminal work employs only
loose feature fusion and overlooks the utilization of long-term information on
tracked objects. In this study, we introduce a compact Transformer-based
method, termed TenRMOT. We conduct feature fusion at both encoding and decoding
stages to fully exploit the advantages of Transformer architecture.
Specifically, we incrementally perform cross-modal fusion layer-by-layer during
the encoding phase. In the decoding phase, we utilize language-guided queries
to probe memory features for accurate prediction of the desired objects.
Moreover, we introduce a query update module that explicitly leverages temporal
prior information of the tracked objects to enhance the consistency of their
trajectories. In addition, we introduce a novel task called Referring
Multi-Object Tracking and Segmentation (RMOTS) and construct a new dataset
named Ref-KITTI Segmentation. Our dataset consists of 18 videos with 818
expressions, and each expression averages 10.7 masks, which poses a greater
challenge compared to the typical single mask in most existing referring video
segmentation datasets. TenRMOT demonstrates superior performance on both the
referring multi-object tracking and the segmentation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Skull Segmentation via Contrastive MR-to-CT Modality
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamil Kwarciak, Mateusz Daniol, Daria Hemmerling, Marek Wodzinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The skull segmentation from CT scans can be seen as an already solved
problem. However, in MR this task has a significantly greater complexity due to
the presence of soft tissues rather than bones. Capturing the bone structures
from MR images of the head, where the main visualization objective is the
brain, is very demanding. The attempts that make use of skull stripping seem to
not be well suited for this task and fail to work in many cases. On the other
hand, supervised approaches require costly and time-consuming skull
annotations. To overcome the difficulties we propose a fully unsupervised
approach, where we do not perform the segmentation directly on MR images, but
we rather perform a synthetic CT data generation via MR-to-CT translation and
perform the segmentation there. We address many issues associated with
unsupervised skull segmentation including the unpaired nature of MR and CT
datasets (contrastive learning), low resolution and poor quality
(super-resolution), and generalization capabilities. The research has a
significant value for downstream tasks requiring skull segmentation from MR
volumes such as craniectomy or surgery planning and can be seen as an important
step towards the utilization of synthetic data in medical imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures, ACCV 2024 - GAISynMeD Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance of Gaussian Mixture Model Classifiers on Embedded Feature
  Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy Chopin, Rozenn Dahyot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data embeddings with CLIP and ImageBind provide powerful features for the
analysis of multimedia and/or multimodal data. We assess their performance here
for classification using a Gaussian Mixture models (GMMs) based layer as an
alternative to the standard Softmax layer. GMMs based classifiers have recently
been shown to have interesting performances as part of deep learning pipelines
trained end-to-end. Our first contribution is to investigate GMM based
classification performance taking advantage of the embedded spaces CLIP and
ImageBind. Our second contribution is in proposing our own GMM based classifier
with a lower parameters count than previously proposed. Our findings are, that
in most cases, on these tested embedded spaces, one gaussian component in the
GMMs is often enough for capturing each class, and we hypothesize that this may
be due to the contrastive loss used for training these embedded spaces that
naturally concentrates features together for each class. We also observed that
ImageBind often provides better performance than CLIP for classification of
image datasets even when these embedded spaces are compressed using PCA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RescueADI: Adaptive Disaster Interpretation in Remote Sensing Images
  with Autonomous Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoran Liu, Danpei Zhao, Bo Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current methods for disaster scene interpretation in remote sensing images
(RSIs) mostly focus on isolated tasks such as segmentation, detection, or
visual question-answering (VQA). However, current interpretation methods often
fail at tasks that require the combination of multiple perception methods and
specialized tools. To fill this gap, this paper introduces Adaptive Disaster
Interpretation (ADI), a novel task designed to solve requests by planning and
executing multiple sequentially correlative interpretation tasks to provide a
comprehensive analysis of disaster scenes. To facilitate research and
application in this area, we present a new dataset named RescueADI, which
contains high-resolution RSIs with annotations for three connected aspects:
planning, perception, and recognition. The dataset includes 4,044 RSIs, 16,949
semantic masks, 14,483 object bounding boxes, and 13,424 interpretation
requests across nine challenging request types. Moreover, we propose a new
disaster interpretation method employing autonomous agents driven by large
language models (LLMs) for task planning and execution, proving its efficacy in
handling complex disaster interpretations. The proposed agent-based method
solves various complex interpretation requests such as counting, area
calculation, and path-finding without human intervention, which traditional
single-task approaches cannot handle effectively. Experimental results on
RescueADI demonstrate the feasibility of the proposed task and show that our
method achieves an accuracy 9% higher than existing VQA methods, highlighting
its advantages over conventional disaster interpretation approaches. The
dataset will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Railway LiDAR semantic segmentation based on intelligent semi-automated
  data annotation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Wulff, Bernd Schaeufele, Julian Pfeifer, Ilja Radusch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated vehicles rely on an accurate and robust perception of the
environment. Similarly to automated cars, highly automated trains require an
environmental perception. Although there is a lot of research based on either
camera or LiDAR sensors in the automotive domain, very few contributions for
this task exist yet for automated trains. Additionally, no public dataset or
described approach for a 3D LiDAR semantic segmentation in the railway
environment exists yet. Thus, we propose an approach for a point-wise 3D
semantic segmentation based on the 2DPass network architecture using scans and
images jointly. In addition, we present a semi-automated intelligent data
annotation approach, which we use to efficiently and accurately label the
required dataset recorded on a railway track in Germany. To improve performance
despite a still small number of labeled scans, we apply an active learning
approach to intelligently select scans for the training dataset. Our
contributions are threefold: We annotate rail data including camera and LiDAR
data from the railway environment, transfer label the raw LiDAR point clouds
using an image segmentation network, and train a state-of-the-art 3D LiDAR
semantic segmentation network efficiently leveraging active learning. The
trained network achieves good segmentation results with a mean IoU of 71.48% of
9 classes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article has been accepted for publication in the IEEE VTC Fall
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accurate Checkerboard Corner Detection under Defoucs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zezhun Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camera calibration is a critical process in 3D vision, im pacting
applications in autonomous driving, robotics, ar chitecture, and so on. This
paper focuses on enhancing feature extraction for chessboard corner detection,
a key step in calibration. We analyze existing methods, high lighting their
limitations and propose a novel sub-pixel refinement approach based on
symmetry, which signifi cantly improves accuracy for visible light cameras. Un
like prior symmetry based method that assume a contin uous physical pattern,
our approach accounts for abrupt changes in visible light camera images and
defocus ef fects. We introduce a simplified objective function that reduces
computation time and mitigates overfitting risks. Furthermore, we derive an
explicit expression for the pixel value of a blurred edge, providing insights
into the relationship between pixel value and center intensity. Our method
demonstrates superior performance, achiev ing substantial accuracy improvements
over existing tech niques, particularly in the context of visible light cam era
calibration. Our code is available from https:
//github.com/spdfghi/Accurate-Checkerboard Corner-Detection-under-Defoucs.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MagicTailor: Component-Controllable Personalization in Text-to-Image
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghao Zhou, Jiancheng Huang, Jinbin Bai, Jiaze Wang, Hao Chen, Guangyong Chen, Xiaowei Hu, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text-to-image (T2I) diffusion models have enabled the
creation of high-quality images from text prompts, but they still struggle to
generate images with precise control over specific visual concepts. Existing
approaches can replicate a given concept by learning from reference images, yet
they lack the flexibility for fine-grained customization of the individual
component within the concept. In this paper, we introduce
component-controllable personalization, a novel task that pushes the boundaries
of T2I models by allowing users to reconfigure specific components when
personalizing visual concepts. This task is particularly challenging due to two
primary obstacles: semantic pollution, where unwanted visual elements corrupt
the personalized concept, and semantic imbalance, which causes disproportionate
learning of the concept and component. To overcome these challenges, we design
MagicTailor, an innovative framework that leverages Dynamic Masked Degradation
(DM-Deg) to dynamically perturb undesired visual semantics and Dual-Stream
Balancing (DS-Bal) to establish a balanced learning paradigm for desired visual
semantics. Extensive comparisons, ablations, and analyses demonstrate that
MagicTailor not only excels in this challenging task but also holds significant
promise for practical applications, paving the way for more nuanced and
creative image generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://correr-zhou.github.io/MagicTailor</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Remember, Retrieve and Generate: Understanding Infinite Visual Concepts
  as Your Personalized Assistant 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Hao, Jiaming Han, Changsheng Li, Yu-Feng Li, Xiangyu Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of large language models (LLMs) has significantly enhanced
the capabilities of multimodal LLMs (MLLMs) as general assistants. However,
lack of user-specific knowledge still restricts their application in human's
daily life. In this paper, we introduce the Retrieval Augmented Personalization
(RAP) framework for MLLMs' personalization. Starting from a general MLLM, we
turn it into a personalized assistant in three steps. (a) Remember: We design a
key-value database to store user-related information, e.g., user's name, avatar
and other attributes. (b) Retrieve: When the user initiates a conversation, RAP
will retrieve relevant information from the database using a multimodal
retriever. (c) Generate: The input query and retrieved concepts' information
are fed into MLLMs to generate personalized, knowledge-augmented responses.
Unlike previous methods, RAP allows real-time concept editing via updating the
external database. To further improve generation quality and alignment with
user-specific information, we design a pipeline for data collection and create
a specialized dataset for personalized training of MLLMs. Based on the dataset,
we train a series of MLLMs as personalized multimodal assistants. By
pretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual
concepts without additional finetuning. Our models demonstrate outstanding
flexibility and generation quality across a variety of tasks, such as
personalized image captioning, question answering and visual recognition. The
code, data and models are available at https://github.com/Hoar012/RAP-MLLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Supervised Scene Flow Estimation with Point-Voxel Fusion and
  Surface Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuezhi Xiang, Xi Wang, Lei Zhang, Denis Ombati, Himaloy Himu, Xiantong Zhen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene flow estimation aims to generate the 3D motion field of points between
two consecutive frames of point clouds, which has wide applications in various
fields. Existing point-based methods ignore the irregularity of point clouds
and have difficulty capturing long-range dependencies due to the inefficiency
of point-level computation. Voxel-based methods suffer from the loss of detail
information. In this paper, we propose a point-voxel fusion method, where we
utilize a voxel branch based on sparse grid attention and the shifted window
strategy to capture long-range dependencies and a point branch to capture
fine-grained features to compensate for the information loss in the voxel
branch. In addition, since xyz coordinates are difficult to describe the
geometric structure of complex 3D objects in the scene, we explicitly encode
the local surface information of the point cloud through the umbrella surface
feature extraction (USFE) module. We verify the effectiveness of our method by
conducting experiments on the Flyingthings3D and KITTI datasets. Our method
outperforms all other self-supervised methods and achieves highly competitive
results compared to fully supervised methods. We achieve improvements in all
metrics, especially EPE, which is reduced by 8.51% and 10.52% on the KITTIo and
KITTIs datasets, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is under consideration at 2025 IEEE International
  Conference on Acoustics, Speech, and Signal Processing (ICASSP 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GlossyGS: Inverse Rendering of Glossy Objects with 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuichang Lai, Letian Huang, Jie Guo, Kai Cheng, Bowen Pan, Xiaoxiao Long, Jiangjing Lyu, Chengfei Lv, Yanwen Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing objects from posed images is a crucial and complex task in
computer graphics and computer vision. While NeRF-based neural reconstruction
methods have exhibited impressive reconstruction ability, they tend to be
time-comsuming. Recent strategies have adopted 3D Gaussian Splatting (3D-GS)
for inverse rendering, which have led to quick and effective outcomes. However,
these techniques generally have difficulty in producing believable geometries
and materials for glossy objects, a challenge that stems from the inherent
ambiguities of inverse rendering. To address this, we introduce GlossyGS, an
innovative 3D-GS-based inverse rendering framework that aims to precisely
reconstruct the geometry and materials of glossy objects by integrating
material priors. The key idea is the use of micro-facet geometry segmentation
prior, which helps to reduce the intrinsic ambiguities and improve the
decomposition of geometries and materials. Additionally, we introduce a normal
map prefiltering strategy to more accurately simulate the normal distribution
of reflective surfaces. These strategies are integrated into a hybrid geometry
and material representation that employs both explicit and implicit methods to
depict glossy objects. We demonstrate through quantitative analysis and
qualitative visualization that the proposed method is effective to reconstruct
high-fidelity geometries and materials of glossy objects, and performs
favorably against state-of-the-arts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucinations in Large <span class="highlight-title">Vision-Language</span> Models via
  Summary-Guided Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyungmin Min, Minbeom Kim, Kang-il Lee, Dongryeol Lee, Kyomin Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) demonstrate impressive capabilities in
generating detailed and coherent responses from visual inputs. However, they
are prone to generate hallucinations due to an over-reliance on language
priors. To address this issue, we investigate the language priors in LVLMs and
make two key observations: (1) Even when predicting the tokens associated with
image-related part-of-speech (POS), models increasingly rely on linguistic
priors as the token sequences grow, thereby amplifying hallucinations. (2)
Methods that directly calibrate LVLM's output distribution to mitigate language
priors can lead to a degradation in text quality or even exacerbate
hallucinations. Based on these findings, we propose a novel method,
Summary-Guided Decoding (SGD). This method naturally encourages the model to
focus more on image information by reducing the text context through summaries,
while controlling only the image-related POS tokens to maintain text quality.
Through experiments, we demonstrate that SGD achieves state-of-the-art
performance on object hallucination benchmarks. Furthermore, in terms of the
trade-off between precision and recall, SGD achieves Pareto optimality among
the existing methods. Lastly, we observe that although existing methods
struggle to balance the reduction of object hallucinations with maintaining
text quality, SGD demonstrates robustness in handling this challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inadequate contrast ratio of road markings as an indicator for ADAS
  failure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Novel Certad, Cristina Olaverri-Monreal, Friedrich Wiesinger, Tomasz E. Burghardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Road markings were reported as critical road safety features, equally needed
for both human drivers and for machine vision technologies utilised by advanced
driver assistance systems (ADAS) and in driving automation. Visibility of road
markings is achieved because of their colour contrasting with the roadway
surface. During recent testing of an open-source camera-based ADAS under
several visibility conditions (day, night, rain, glare), significant failures
in trajectory planning were recorded and quantified. Consistently, better ADAS
reliability under poor visibility conditions was achieved with Type II road
markings (i.e. structured markings, facilitating moisture drainage) as compared
to Type I road marking (i.e. flat lines). To further understand these failures,
analysis of contrast ratio of road markings, which the tested ADAS was
detecting for traffic lane recognition, was performed. The highest contrast
ratio (greater than 0.5, calculated per Michelson equation) was measured at
night in the absence of confounding factors, with statistically significant
difference of 0.1 in favour of Type II road markings over Type I. Under
daylight conditions, contrast ratio was reduced, with slightly higher values
measured with Type I. The presence of rain or wet roads caused the
deterioration of the contrast ratio, with Type II road markings exhibiting
significantly higher contrast ratio than Type I, even though the values were
low (less than 0.1). These findings matched the output of the ADAS related to
traffic lane detection and underlined the importance of road marking
visibility. Inadequate lane recognition by ADAS was associated with very low
contrast ratio of road markings indeed. Importantly, specific minimum contrast
ratio value could not be found, which was due to the complexity of ADAS
algorithms...
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IRF World Congress 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precipitation Nowcasting Using Diffusion Transformer with Causal
  Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        ChaoRong Li, XuDong Ling, YiLan Xue, Wenjie Luo, LiHong Zhu, FengQing Qin, Yaodong Zhou, Yuanyuan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Short-term precipitation forecasting remains challenging due to the
difficulty in capturing long-term spatiotemporal dependencies. Current deep
learning methods fall short in establishing effective dependencies between
conditions and forecast results, while also lacking interpretability. To
address this issue, we propose a Precipitation Nowcasting Using Diffusion
Transformer with Causal Attention model. Our model leverages Transformer and
combines causal attention mechanisms to establish spatiotemporal queries
between conditional information (causes) and forecast results (results). This
design enables the model to effectively capture long-term dependencies,
allowing forecast results to maintain strong causal relationships with input
conditions over a wide range of time and space. We explore four variants of
spatiotemporal information interactions for DTCA, demonstrating that global
spatiotemporal labeling interactions yield the best performance. In addition,
we introduce a Channel-To-Batch shift operation to further enhance the model's
ability to represent complex rainfall dynamics. We conducted experiments on two
datasets. Compared to state-of-the-art U-Net-based methods, our approach
improved the CSI (Critical Success Index) for predicting heavy precipitation by
approximately 15% and 8% respectively, achieving state-of-the-art performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Dataset Distillation via Label Inconsistency Elimination and
  Learning Pattern Refinement <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuhao Zhou, Chenxi Jiang, Yi Xie, Haozhi Cao, Jianfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset Distillation (DD) seeks to create a condensed dataset that, when used
to train a model, enables the model to achieve performance similar to that of a
model trained on the entire original dataset. It relieves the model training
from processing massive data and thus reduces the computation resources,
storage, and time costs. This paper illustrates our solution that ranks 1st in
the ECCV-2024 Data Distillation Challenge (track 1). Our solution, Modified
Difficulty-Aligned Trajectory Matching (M-DATM), introduces two key
modifications to the original state-of-the-art method DATM: (1) the soft labels
learned by DATM do not achieve one-to-one correspondence with the counterparts
generated by the official evaluation script, so we remove the soft labels
technique to alleviate such inconsistency; (2) since the removal of soft labels
makes it harder for the synthetic dataset to learn late trajectory information,
particularly on Tiny ImageNet, we reduce the matching range, allowing the
synthetic data to concentrate more on the easier patterns. In the final
evaluation, our M-DATM achieved accuracies of 0.4061 and 0.1831 on the
CIFAR-100 and Tiny ImageNet datasets, ranking 1st in the Fixed Images Per Class
(IPC) Track.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024 Dataset Distillation Challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reference-Based Post-OCR Processing with LLM for Diacritic Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thao Do
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting fine-grained OCR text from aged documents in diacritic languages
remains challenging due to unexpected artifacts, time-induced degradation, and
lack of datasets. While standalone spell correction approaches have been
proposed, they show limited performance for historical documents due to
numerous possible OCR error combinations and differences between modern and
classical corpus distributions. We propose a method utilizing available
content-focused ebooks as a reference base to correct imperfect OCR-generated
text, supported by large language models. This technique generates
high-precision pseudo-page-to-page labels for diacritic languages, where small
strokes pose significant challenges in historical conditions. The pipeline
eliminates various types of noise from aged documents and addresses issues such
as missing characters, words, and disordered sequences. Our post-processing
method, which generated a large OCR dataset of classical Vietnamese books,
achieved a mean grading score of 8.72 on a 10-point scale. This outperformed
the state-of-the-art transformer-based Vietnamese spell correction model, which
scored 7.03 when evaluated on a sampled subset of the dataset. We also trained
a baseline OCR model to assess and compare it with well-known engines.
Experimental results demonstrate the strength of our baseline model compared to
widely used open-source solutions. The resulting dataset will be released
publicly to support future studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PiLocNet: Physics-informed neural network on 3D localization with
  rotating point spread function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingda Lu, Zitian Ao, Chao Wang, Sudhakar Prasad, Raymond H. Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the 3D localization problem using point spread function (PSF)
engineering, we propose a novel enhancement of our previously introduced
localization neural network, LocNet. The improved network is a physics-informed
neural network (PINN) that we call PiLocNet. Previous works on the localization
problem may be categorized separately into model-based optimization and neural
network approaches. Our PiLocNet combines the unique strengths of both
approaches by incorporating forward-model-based information into the network
via a data-fitting loss term that constrains the neural network to yield
results that are physically sensible. We additionally incorporate certain
regularization terms from the variational method, which further improves the
robustness of the network in the presence of image noise, as we show for the
Poisson and Gaussian noise models. This framework accords interpretability to
the neural network, and the results we obtain show its superiority. Although
the paper focuses on the use of single-lobe rotating PSF to encode the full 3D
source location, we expect the method to be widely applicable to other PSFs and
imaging problems that are constrained by known forward processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LESS: Label-Efficient and Single-Stage Referring 3D Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuexun Liu, Xiaoxu Xu, Jinlong Li, Qiudan Zhang, Xu Wang, Nicu Sebe, Lin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring 3D Segmentation is a visual-language task that segments all points
of the specified object from a 3D point cloud described by a sentence of query.
Previous works perform a two-stage paradigm, first conducting language-agnostic
instance segmentation then matching with given text query. However, the
semantic concepts from text query and visual cues are separately interacted
during the training, and both instance and semantic labels for each object are
required, which is time consuming and human-labor intensive. To mitigate these
issues, we propose a novel Referring 3D Segmentation pipeline, Label-Efficient
and Single-Stage, dubbed LESS, which is only under the supervision of efficient
binary mask. Specifically, we design a Point-Word Cross-Modal Alignment module
for aligning the fine-grained features of points and textual embedding. Query
Mask Predictor module and Query-Sentence Alignment module are introduced for
coarse-grained alignment between masks and query. Furthermore, we propose an
area regularization loss, which coarsely reduces irrelevant background
predictions on a large scale. Besides, a point-to-point contrastive loss is
proposed concentrating on distinguishing points with subtly similar features.
Through extensive experiments, we achieve state-of-the-art performance on
ScanRefer dataset by surpassing the previous methods about 3.7% mIoU using only
binary labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Composing Novel Classes: A Concept-Driven Approach to Generalized
  Category Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuyu Zhang, Peiyan Gu, Xueyang Yu, Xuming He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the generalized category discovery (GCD) problem, which aims to
discover novel classes in unlabeled datasets by leveraging the knowledge of
known classes. Previous works utilize the known class knowledge through shared
representation spaces. Despite their progress, our analysis experiments show
that novel classes can achieve impressive clustering results on the feature
space of a known class pre-trained model, suggesting that existing methods may
not fully utilize known class knowledge. To address it, we introduce a novel
concept learning framework for GCD, named ConceptGCD, that categorizes concepts
into two types: derivable and underivable from known class concepts, and adopts
a stage-wise learning strategy to learn them separately. Specifically, our
framework first extracts known class concepts by a known class pre-trained
model and then produces derivable concepts from them by a generator layer with
a covariance-augmented loss. Subsequently, we expand the generator layer to
learn underivable concepts in a balanced manner ensured by a concept score
normalization strategy and integrate a contrastive loss to preserve previously
learned concepts. Extensive experiments on various benchmark datasets
demonstrate the superiority of our approach over the previous state-of-the-art
methods. Code will be available soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Underreview. The first two authors contribute equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid bundle-adjusting 3D Gaussians for view consistent rendering with
  pose optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanan Guo, Ying Xie, Ying Chang, Benkui Zhang, Bo Jia, Lin Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel view synthesis has made significant progress in the field of 3D
computer vision. However, the rendering of view-consistent novel views from
imperfect camera poses remains challenging. In this paper, we introduce a
hybrid bundle-adjusting 3D Gaussians model that enables view-consistent
rendering with pose optimization. This model jointly extract image-based and
neural 3D representations to simultaneously generate view-consistent images and
camera poses within forward-facing scenes. The effective of our model is
demonstrated through extensive experiments conducted on both real and synthetic
datasets. These experiments clearly illustrate that our model can effectively
optimize neural scene representations while simultaneously resolving
significant camera pose misalignments. The source code is available at
https://github.com/Bistu3DV/hybridBA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Photonics Asia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inductive Gradient Adjustment For Spectral Bias In Implicit Neural
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kexuan Shi, Hai Chen, Leheng Zhang, Shuhang Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit Neural Representations (INRs), as a versatile representation
paradigm, have achieved success in various computer vision tasks. Due to the
spectral bias of the vanilla multi-layer perceptrons (MLPs), existing methods
focus on designing MLPs with sophisticated architectures or repurposing
training techniques for highly accurate INRs. In this paper, we delve into the
linear dynamics model of MLPs and theoretically identify the empirical Neural
Tangent Kernel (eNTK) matrix as a reliable link between spectral bias and
training dynamics. Based on eNTK matrix, we propose a practical inductive
gradient adjustment method, which could purposefully improve the spectral bias
via inductive generalization of eNTK-based gradient transformation matrix. We
evaluate our method on different INRs tasks with various INR architectures and
compare to existing training techniques. The superior representation
performance clearly validates the advantage of our proposed method. Armed with
our gradient adjustment method, better INRs with more enhanced texture details
and sharpened edges can be learned from data by tailored improvements on
spectral bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fundus to Fluorescein Angiography Video Generation as a Retinal
  Generative Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiyi Zhang, Jiancheng Yang, Ruoyu Chen, Siyu Huang, Pusheng Xu, Xiaolan Chen, Shanfu Lu, Hongyu Cao, Mingguang He, Danli Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fundus fluorescein angiography (FFA) is crucial for diagnosing and monitoring
retinal vascular issues but is limited by its invasive nature and restricted
accessibility compared to color fundus (CF) imaging. Existing methods that
convert CF images to FFA are confined to static image generation, missing the
dynamic lesional changes. We introduce Fundus2Video, an autoregressive
generative adversarial network (GAN) model that generates dynamic FFA videos
from single CF images. Fundus2Video excels in video generation, achieving an
FVD of 1497.12 and a PSNR of 11.77. Clinical experts have validated the
fidelity of the generated videos. Additionally, the model's generator
demonstrates remarkable downstream transferability across ten external public
datasets, including blood vessel segmentation, retinal disease diagnosis,
systemic disease prediction, and multimodal retrieval, showcasing impressive
zero-shot and few-shot capabilities. These findings position Fundus2Video as a
powerful, non-invasive alternative to FFA exams and a versatile retinal
generative foundation model that captures both static and temporal retinal
features, enabling the representation of complex inter-modality relationships.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Image and Video Resolution Prediction using Convolutional Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rittwika Kansabanik, Adrian Barbu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a Video Quality Assessment (VQA) problem that has
received little attention in the literature, called the latent resolution
prediction problem. The problem arises when images or videos are upscaled from
their native resolution and are reported as having a higher resolution than
their native resolution. This paper formulates the problem, constructs a
dataset for training and evaluation, and introduces several machine learning
algorithms, including two Convolutional Neural Networks (CNNs), to address this
problem. Experiments indicate that some proposed methods can predict the latent
video resolution with about 95% accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted in ICIP conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniG: Modelling Unitary 3D Gaussians for View-consistent 3D
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiamin Wu, Kenkun Liu, Yukai Shi, Xiaoke Jiang, Yuan Yao, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present UniG, a view-consistent 3D reconstruction and novel
view synthesis model that generates a high-fidelity representation of 3D
Gaussians from sparse images. Existing 3D Gaussians-based methods usually
regress Gaussians per-pixel of each view, create 3D Gaussians per view
separately, and merge them through point concatenation. Such a view-independent
reconstruction approach often results in a view inconsistency issue, where the
predicted positions of the same 3D point from different views may have
discrepancies. To address this problem, we develop a DETR (DEtection
TRansformer)-like framework, which treats 3D Gaussians as decoder queries and
updates their parameters layer by layer by performing multi-view
cross-attention (MVDFA) over multiple input images. In this way, multiple views
naturally contribute to modeling a unitary representation of 3D Gaussians,
thereby making 3D reconstruction more view-consistent. Moreover, as the number
of 3D Gaussians used as decoder queries is irrespective of the number of input
views, allow an arbitrary number of input images without causing memory
explosion. Extensive experiments validate the advantages of our approach,
showcasing superior performance over existing methods quantitatively (improving
PSNR by 4.2 dB when trained on Objaverse and tested on the GSO benchmark) and
qualitatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Golyadkin's Torment: Doppelgängers and Adversarial Vulnerability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George I. Kamberov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many machine learning (ML) classifiers are claimed to outperform humans, but
they still make mistakes that humans do not. The most notorious examples of
such mistakes are adversarial visual metamers. This paper aims to define and
investigate the phenomenon of adversarial Doppelgangers (AD), which includes
adversarial visual metamers, and to compare the performance and robustness of
ML classifiers to human performance.
  We find that AD are inputs that are close to each other with respect to a
perceptual metric defined in this paper. AD are qualitatively different from
the usual adversarial examples. The vast majority of classifiers are vulnerable
to AD and robustness-accuracy trade-offs may not improve them. Some
classification problems may not admit any AD robust classifiers because the
underlying classes are ambiguous. We provide criteria that can be used to
determine whether a classification problem is well defined or not; describe the
structure and attributes of an AD-robust classifier; introduce and explore the
notions of conceptual entropy and regions of conceptual ambiguity for
classifiers that are vulnerable to AD attacks, along with methods to bound the
AD fooling rate of an attack. We define the notion of classifiers that exhibit
hypersensitive behavior, that is, classifiers whose only mistakes are
adversarial Doppelgangers. Improving the AD robustness of hyper-sensitive
classifiers is equivalent to improving accuracy. We identify conditions
guaranteeing that all classifiers with sufficiently high accuracy are
hyper-sensitive.
  Our findings are aimed at significant improvements in the reliability and
security of machine learning systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Drift Monitoring in Medical Imaging AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jameson Merkow, Felix J. Dorfner, Xiyu Yang, Alexander Ersoy, Giridhar Dasegowda, Mannudeep Kalra, Matthew P. Lungren, Christopher P. Bridge, Ivan Tarapov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of artificial intelligence (AI) into medical imaging has
advanced clinical diagnostics but poses challenges in managing model drift and
ensuring long-term reliability. To address these challenges, we develop MMC+,
an enhanced framework for scalable drift monitoring, building upon the
CheXstray framework that introduced real-time drift detection for medical
imaging AI models using multi-modal data concordance. This work extends the
original framework's methodologies, providing a more scalable and adaptable
solution for real-world healthcare settings and offers a reliable and
cost-effective alternative to continuous performance monitoring addressing
limitations of both continuous and periodic monitoring methods. MMC+ introduces
critical improvements to the original framework, including more robust handling
of diverse data streams, improved scalability with the integration of
foundation models like MedImageInsight for high-dimensional image embeddings
without site-specific training, and the introduction of uncertainty bounds to
better capture drift in dynamic clinical environments. Validated with
real-world data from Massachusetts General Hospital during the COVID-19
pandemic, MMC+ effectively detects significant data shifts and correlates them
with model performance changes. While not directly predicting performance
degradation, MMC+ serves as an early warning system, indicating when AI systems
may deviate from acceptable performance bounds and enabling timely
interventions. By emphasizing the importance of monitoring diverse data streams
and evaluating data shifts alongside model performance, this work contributes
to the broader adoption and integration of AI solutions in clinical settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FAMSeC: A Few-shot-sample-based General AI-generated Image Detection
  Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juncong Xu, Yang Yang, Han Fang, Honggu Liu, Weiming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The explosive growth of generative AI has saturated the internet with
AI-generated images, raising security concerns and increasing the need for
reliable detection methods. The primary requirement for such detection is
generalizability, typically achieved by training on numerous fake images from
various models. However, practical limitations, such as closed-source models
and restricted access, often result in limited training samples. Therefore,
training a general detector with few-shot samples is essential for modern
detection mechanisms. To address this challenge, we propose FAMSeC, a general
AI-generated image detection method based on LoRA-based Forgery Awareness
Module and Semantic feature-guided Contrastive learning strategy. To
effectively learn from limited samples and prevent overfitting, we developed a
Forgery Awareness Module (FAM) based on LoRA, maintaining the generalization of
pre-trained features. Additionally, to cooperate with FAM, we designed a
Semantic feature-guided Contrastive learning strategy (SeC), making the FAM
focus more on the differences between real/fake image than on the features of
the samples themselves. Experiments show that FAMSeC outperforms
state-of-the-art method, enhancing classification accuracy by 14.55% with just
0.56% of the training samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilizing Large Language Models in An Iterative Paradigm with Domain
  Feedback for Molecule Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khiem Le, Nitesh V. Chawla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Molecule optimization is a critical task in drug discovery to optimize
desired properties of a given molecule through chemical modification. Despite
Large Language Models (LLMs) holding the potential to efficiently simulate this
task by using natural language to direct the optimization, straightforwardly
utilizing shows limited performance. In this work, we facilitate utilizing LLMs
in an iterative paradigm by proposing a simple yet highly effective domain
feedback provider, namely $\text{Re}^2$DF. In detail, $\text{Re}^2$DF harnesses
an external toolkit, RDKit, to handle the molecule hallucination, if the
modified molecule is chemically invalid. Otherwise, its desired properties are
computed and compared to the original one, establishing reliable domain
feedback with correct direction and distance towards the objective, followed by
a retrieved example, to explicitly guide the LLM to refine the modified
molecule. We conduct experiments across both single- and multi-property
objectives with 2 thresholds, where $\text{Re}^2$DF shows significant
improvements. Particularly, for 20 single-property objectives, $\text{Re}^2$DF
enhances the Hit ratio by 16.95\% and 20.76\% under loose and strict
thresholds, respectively. For 32 multi-property objectives, $\text{Re}^2$DF
enhances the Hit ratio by 6.04\% and 5.25\%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mapping Bias in <span class="highlight-title">Vision Language</span> Models: Signposts, Pitfalls, and the
  Road Ahead <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuleen Sasse, Shan Chen, Jackson Pond, Danielle Bitterman, John Osborne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Vision Language Models (VLMs) gain widespread use, their fairness remains
under-explored. In this paper, we analyze demographic biases across five models
and six datasets. We find that portrait datasets like UTKFace and CelebA are
the best tools for bias detection, finding gaps in performance and fairness
between LLaVa and CLIP models. However, scene based datasets like PATA,
VLStereoSet fail to be useful benchmarks for bias due to their construction. As
for pronoun based datasets like VisoGender, we receive mixed signals as only
some subsets of the data are useful in providing insights. To alleviate this
problem, we introduce a more difficult version of VisoGender to serve as a more
rigorous evaluation. Based on these results, we call for more effective and
carefully designed datasets to ensure VLMs are both fair and reliable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review at NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ See Behind Walls in Real-time Using Aerial Drones and Augmented Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sikai Yang, Kang Yang, Yuning Chen, Fan Zhao, Wan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents ARD2, a framework that enables real-time through-wall
surveillance using two aerial drones and an augmented reality (AR) device. ARD2
consists of two main steps: target direction estimation and contour
reconstruction. In the first stage, ARD2 leverages geometric relationships
between the drones, the user, and the target to project the target's direction
onto the user's AR display. In the second stage, images from the drones are
synthesized to reconstruct the target's contour, allowing the user to visualize
the target behind walls. Experimental results demonstrate the system's accuracy
in both direction estimation and contour reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking the Capabilities of Masked Generative Models for Image
  Synthesis via Self-Guidance <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwan Hur, Dong-Jae Lee, Gyojin Han, Jaehyun Choi, Yunho Jeon, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked generative models (MGMs) have shown impressive generative ability
while providing an order of magnitude efficient sampling steps compared to
continuous diffusion models. However, MGMs still underperform in image
synthesis compared to recent well-developed continuous diffusion models with
similar size in terms of quality and diversity of generated samples. A key
factor in the performance of continuous diffusion models stems from the
guidance methods, which enhance the sample quality at the expense of diversity.
In this paper, we extend these guidance methods to generalized guidance
formulation for MGMs and propose a self-guidance sampling method, which leads
to better generation quality. The proposed approach leverages an auxiliary task
for semantic smoothing in vector-quantized token space, analogous to the
Gaussian blur in continuous pixel space. Equipped with the parameter-efficient
fine-tuning method and high-temperature sampling, MGMs with the proposed
self-guidance achieve a superior quality-diversity trade-off, outperforming
existing sampling methods in MGMs with more efficient training and sampling
costs. Extensive experiments with the various sampling hyperparameters confirm
the effectiveness of the proposed self-guidance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Code is available at:
  https://github.com/JiwanHur/UnlockMGM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Imperceptibility of Stable Diffusion-based Adversarial Examples
  Generation with Momentum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nashrah Haque, Xiang Li, Zhehui Chen, Yanzhao Wu, Lei Yu, Arun Iyengar, Wenqi Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel framework, Stable Diffusion-based Momentum Integrated
Adversarial Examples (SD-MIAE), for generating adversarial examples that can
effectively mislead neural network classifiers while maintaining visual
imperceptibility and preserving the semantic similarity to the original class
label. Our method leverages the text-to-image generation capabilities of the
Stable Diffusion model by manipulating token embeddings corresponding to the
specified class in its latent space. These token embeddings guide the
generation of adversarial images that maintain high visual fidelity. The
SD-MIAE framework consists of two phases: (1) an initial adversarial
optimization phase that modifies token embeddings to produce misclassified yet
natural-looking images and (2) a momentum-based optimization phase that refines
the adversarial perturbations. By introducing momentum, our approach stabilizes
the optimization of perturbations across iterations, enhancing both the
misclassification rate and visual fidelity of the generated adversarial
examples. Experimental results demonstrate that SD-MIAE achieves a high
misclassification rate of 79%, improving by 35% over the state-of-the-art
method while preserving the imperceptibility of adversarial perturbations and
the semantic similarity to the original class label, making it a practical
method for robust adversarial evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 12 figures. To be published in IEEE TPS 2024 Proceedings.
  Code available on GitHub: https://github.com/nashrahhaque/SD-MIAE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trust but Verify: Programmatic VLM Evaluation in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13121v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13121v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viraj Prabhu, Senthil Purushwalkam, An Yan, Caiming Xiong, Ran Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) often generate plausible but incorrect
responses to visual queries. However, reliably quantifying the effect of such
hallucinations in free-form responses to open-ended queries is challenging as
it requires visually verifying each claim within the response. We propose
Programmatic VLM Evaluation (PROVE), a new benchmarking paradigm for evaluating
VLM responses to open-ended queries. To construct PROVE, we provide a large
language model (LLM) with a high-fidelity scene-graph representation
constructed from a hyper-detailed image caption, and prompt it to generate
diverse question-answer (QA) pairs, as well as programs that can be executed
over the scene graph object to verify each QA pair. We thus construct a
benchmark of 10.5k challenging but visually grounded QA pairs. Next, to
evaluate free-form model responses to queries in PROVE, we propose a
programmatic evaluation strategy that measures both the helpfulness and
truthfulness of a response within a unified scene graph-based framework. We
benchmark the helpfulness-truthfulness trade-offs of a range of VLMs on PROVE,
finding that very few are in-fact able to achieve a good balance between the
two. Project page: \url{https://prove-explorer.netlify.app/}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Neural Networks in Medical Imaging Advancements and
  Challenges in Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Houze Liu, Bo Zhang, Yanlin Xiang, Yuxiang Hu, Aoran Shen, Yang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in artificial intelligence (AI) have precipitated a
paradigm shift in medical imaging, particularly revolutionizing the domain of
brain imaging. This paper systematically investigates the integration of deep
learning -- a principal branch of AI -- into the semantic segmentation of brain
images. Semantic segmentation serves as an indispensable technique for the
delineation of discrete anatomical structures and the identification of
pathological markers, essential for the diagnosis of complex neurological
disorders. Historically, the reliance on manual interpretation by radiologists,
while noteworthy for its accuracy, is plagued by inherent subjectivity and
inter-observer variability. This limitation becomes more pronounced with the
exponential increase in imaging data, which traditional methods struggle to
process efficiently and effectively. In response to these challenges, this
study introduces the application of adversarial neural networks, a novel AI
approach that not only automates but also refines the semantic segmentation
process. By leveraging these advanced neural networks, our approach enhances
the precision of diagnostic outputs, reducing human error and increasing the
throughput of imaging data analysis. The paper provides a detailed discussion
on how adversarial neural networks facilitate a more robust, objective, and
scalable solution, thereby significantly improving diagnostic accuracies in
neurological evaluations. This exploration highlights the transformative impact
of AI on medical imaging, setting a new benchmark for future research and
clinical practice in neurology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConsisSR: Delving Deep into Consistency in Diffusion-based Image
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Gu, Peng-Tao Jiang, Hao Zhang, Mi Zhou, Jinwei Chen, Wenming Yang, Bo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world image super-resolution (Real-ISR) aims at restoring high-quality
(HQ) images from low-quality (LQ) inputs corrupted by unknown and complex
degradations. In particular, pretrained text-to-image (T2I) diffusion models
provide strong generative priors to reconstruct credible and intricate details.
However, T2I generation focuses on semantic consistency while Real-ISR
emphasizes pixel-level reconstruction, which hinders existing methods from
fully exploiting diffusion priors. To address this challenge, we introduce
ConsisSR to handle both semantic and pixel-level consistency. Specifically,
compared to coarse-grained text prompts, we exploit the more powerful CLIP
image embedding and effectively leverage both modalities through our Hybrid
Prompt Adapter (HPA) for semantic guidance. Secondly, we introduce Time-aware
Latent Augmentation (TALA) to mitigate the inherent gap between T2I generation
and Real-ISR consistency requirements. By randomly mixing LQ and HQ latent
inputs, our model not only handle timestep-specific diffusion noise but also
refine the accumulated latent representations. Last but not least, our
GAN-Embedding strategy employs the pretrained Real-ESRGAN model to refine the
diffusion start point. This accelerates the inference process to 10 steps while
preserving sampling quality, in a training-free manner. Our method demonstrates
state-of-the-art performance among both full-scale and accelerated models. The
code will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMAD-Purify: A Precision-Optimized Framework for Efficient and Scalable
  <span class="highlight-title">Multi-Modal</span> Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinxin Liu, Zhongliang Guo, Siyuan Huang, Chun Pong Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks have achieved remarkable performance across a wide range of
tasks, yet they remain susceptible to adversarial perturbations, which pose
significant risks in safety-critical applications. With the rise of
multimodality, diffusion models have emerged as powerful tools not only for
generative tasks but also for various applications such as image editing,
inpainting, and super-resolution. However, these models still lack robustness
due to limited research on attacking them to enhance their resilience.
Traditional attack techniques, such as gradient-based adversarial attacks and
diffusion model-based methods, are hindered by computational inefficiencies and
scalability issues due to their iterative nature. To address these challenges,
we introduce an innovative framework that leverages the distilled backbone of
diffusion models and incorporates a precision-optimized noise predictor to
enhance the effectiveness of our attack framework. This approach not only
enhances the attack's potency but also significantly reduces computational
costs. Our framework provides a cutting-edge solution for multi-modal
adversarial attacks, ensuring reduced latency and the generation of
high-fidelity adversarial examples with superior success rates. Furthermore, we
demonstrate that our framework achieves outstanding transferability and
robustness against purification defenses, outperforming existing gradient-based
attack models in both effectiveness and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Your Interest, Your Summaries: Query-Focused Long Video Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nirav Patel, Payal Prajapati, Maitrik Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating a concise and informative video summary from a long video is
important, yet subjective due to varying scene importance. Users' ability to
specify scene importance through text queries enhances the relevance of such
summaries. This paper introduces an approach for query-focused video
summarization, aiming to align video summaries closely with user queries. To
this end, we propose the Fully Convolutional Sequence Network with Attention
(FCSNA-QFVS), a novel approach designed for this task. Leveraging temporal
convolutional and attention mechanisms, our model effectively extracts and
highlights relevant content based on user-specified queries. Experimental
validation on a benchmark dataset for query-focused video summarization
demonstrates the effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at the 18th International Conference on Control,
  Automation, Robotics and Vision (ICARCV), December 2024, Dubai, UAE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self Supervised Deep Learning for Robot Grasping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danyal Saqib, Wajahat Hussain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning Based Robot Grasping currently involves the use of labeled data.
This approach has two major disadvantages. Firstly, labeling data for grasp
points and angles is a strenuous process, so the dataset remains limited.
Secondly, human labeling is prone to bias due to semantics.
  In order to solve these problems we propose a simpler self-supervised robotic
setup, that will train a Convolutional Neural Network (CNN). The robot will
label and collect the data during the training process. The idea is to make a
robot that is less costly, small and easily maintainable in a lab setup. The
robot will be trained on a large data set for several hundred hours and then
the trained Neural Network can be mapped onto a larger grasping robot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAMReg: SAM-enabled Image Registration with ROI-based Correspondence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiqi Huang, Tingfa Xu, Ziyi Shen, Shaheer Ullah Saeed, Wen Yan, Dean Barratt, Yipeng Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes a new spatial correspondence representation based on
paired regions-of-interest (ROIs), for medical image registration. The distinct
properties of the proposed ROI-based correspondence are discussed, in the
context of potential benefits in clinical applications following image
registration, compared with alternative correspondence-representing approaches,
such as those based on sampled displacements and spatial transformation
functions. These benefits include a clear connection between learning-based
image registration and segmentation, which in turn motivates two cases of image
registration approaches using (pre-)trained segmentation networks. Based on the
segment anything model (SAM), a vision foundation model for segmentation, we
develop a new registration algorithm SAMReg, which does not require any
training (or training data), gradient-based fine-tuning or prompt engineering.
The proposed SAMReg models are evaluated across five real-world applications,
including intra-subject registration tasks with cardiac MR and lung CT,
challenging inter-subject registration scenarios with prostate MR and retinal
imaging, and an additional evaluation with a non-clinical example with aerial
image registration. The proposed methods outperform both intensity-based
iterative algorithms and DDF-predicting learning-based networks across tested
metrics including Dice and target registration errors on anatomical structures,
and further demonstrates competitive performance compared to weakly-supervised
registration approaches that rely on fully-segmented training data. Open source
code and examples are available at: https://github.com/sqhuang0103/SAMReg.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient <span class="highlight-title">Vision-Language</span> Models by Summarizing Visual Tokens into
  Compact Registers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Wen, Qingqing Cao, Qichen Fu, Sachin Mehta, Mahyar Najibi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in vision-language models (VLMs) have expanded their
potential for real-world applications, enabling these models to perform complex
reasoning on images. In the widely used fully autoregressive transformer-based
models like LLaVA, projected visual tokens are prepended to textual tokens.
Oftentimes, visual tokens are significantly more than prompt tokens, resulting
in increased computational overhead during both training and inference. In this
paper, we propose Visual Compact Token Registers (Victor), a method that
reduces the number of visual tokens by summarizing them into a smaller set of
register tokens. Victor adds a few learnable register tokens after the visual
tokens and summarizes the visual information into these registers using the
first few layers in the language tower of VLMs. After these few layers, all
visual tokens are discarded, significantly improving computational efficiency
for both training and inference. Notably, our method is easy to implement and
requires a small number of new trainable parameters with minimal impact on
model performance. In our experiment, with merely 8 visual registers--about 1%
of the original tokens--Victor shows less than a 4% accuracy drop while
reducing the total training time by 43% and boosting the inference throughput
by 3.3X.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaceSaliencyAug: Mitigating Geographic, Gender and Stereotypical Biases
  via Saliency-Based Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teerath Kumar, Alessandra Mileo, Malika Bendechache
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geographical, gender and stereotypical biases in computer vision models pose
significant challenges to their performance and fairness. {In this study, we
present an approach named FaceSaliencyAug aimed at addressing the gender bias
in} {Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs).
Leveraging the salient regions} { of faces detected by saliency, the propose
approach mitigates geographical and stereotypical biases } {in the datasets.
FaceSaliencyAug} randomly selects masks from a predefined search space and
applies them to the salient region of face images, subsequently restoring the
original image with masked salient region. {The proposed} augmentation strategy
enhances data diversity, thereby improving model performance and debiasing
effects. We quantify dataset diversity using Image Similarity Score (ISS)
across five datasets, including Flickr Faces HQ (FFHQ), WIKI, IMDB, Labelled
Faces in the Wild (LFW), UTK Faces, and Diverse Dataset. The proposed approach
demonstrates superior diversity metrics, as evaluated by ISS-intra and
ISS-inter algorithms. Furthermore, we evaluate the effectiveness of our
approach in mitigating gender bias on CEO, Engineer, Nurse, and School Teacher
datasets. We use the Image-Image Association Score (IIAS) to measure gender
bias in these occupations. Our experiments reveal a reduction in gender bias
for both CNNs and ViTs, indicating the efficacy of our method in promoting
fairness and inclusivity in computer vision models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Image Signal and Video processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Partial Prototype Collapse in the DINO Family of Self-Supervised
  Methods <span class="chip">BMVC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hariprasath Govindarajan, Per Sidén, Jacob Roll, Fredrik Lindsten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A prominent self-supervised learning paradigm is to model the representations
as clusters, or more generally as a mixture model. Learning to map the data
samples to compact representations and fitting the mixture model simultaneously
leads to the representation collapse problem. Regularizing the distribution of
data points over the clusters is the prevalent strategy to avoid this issue.
While this is sufficient to prevent full representation collapse, we show that
a partial prototype collapse problem still exists in the DINO family of
methods, that leads to significant redundancies in the prototypes. Such
prototype redundancies serve as shortcuts for the method to achieve a marginal
latent class distribution that matches the prescribed prior. We show that by
encouraging the model to use diverse prototypes, the partial prototype collapse
can be mitigated. Effective utilization of the prototypes enables the methods
to learn more fine-grained clusters, encouraging more informative
representations. We demonstrate that this is especially beneficial when
pre-training on a long-tailed fine-grained dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First version of the paper appeared in OpenReview on 22 Sep 2023.
  Accepted to BMVC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning <span class="highlight-title">Multimodal</span> Cues of Children's Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Cheng, Mert İnan, Rahma Mbarki, Grace Grmek, Theresa Choi, Yiming Sun, Kimele Persaud, Jenny Wang, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding uncertainty plays a critical role in achieving common ground
(Clark et al.,1983). This is especially important for multimodal AI systems
that collaborate with users to solve a problem or guide the user through a
challenging concept. In this work, for the first time, we present a dataset
annotated in collaboration with developmental and cognitive psychologists for
the purpose of studying nonverbal cues of uncertainty. We then present an
analysis of the data, studying different roles of uncertainty and its
relationship with task difficulty and performance. Lastly, we present a
multimodal machine learning model that can predict uncertainty given a
real-time video clip of a participant, which we find improves upon a baseline
multimodal transformer model. This work informs research on cognitive
coordination between human-human and human-AI and has broad implications for
gesture understanding and generation. The anonymized version of our data and
code will be publicly available upon the completion of the required consent
forms and data sheets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGDIAL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Action Anticipation: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolin Lai, Sam Toyer, Tushar Nagarajan, Rohit Girdhar, Shengxin Zha, James M. Rehg, Kris Kitani, Kristen Grauman, Ruta Desai, Miao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting future human behavior is an increasingly popular topic in computer
vision, driven by the interest in applications such as autonomous vehicles,
digital assistants and human-robot interactions. The literature on behavior
prediction spans various tasks, including action anticipation, activity
forecasting, intent prediction, goal prediction, and so on. Our survey aims to
tie together this fragmented literature, covering recent technical innovations
as well as the development of new large-scale datasets for model training and
evaluation. We also summarize the widely-used metrics for different tasks and
provide a comprehensive performance comparison of existing approaches on eleven
action anticipation datasets. This survey serves as not only a reference for
contemporary methodologies in action anticipation, but also a guideline for
future research direction of this evolving landscape.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 9 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Segmentation of Pediatric Brain Tumors using a Radiologically informed,
  Deep Learning Cascade 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothy Mulvany, Daniel Griffiths-King, Jan Novak, Heather Rose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring of Diffuse Intrinsic Pontine Glioma (DIPG) and Diffuse Midline
Glioma (DMG) brain tumors in pediatric patients is key for assessment of
treatment response. Response Assessment in Pediatric Neuro-Oncology (RAPNO)
guidelines recommend the volumetric measurement of these tumors using MRI.
Segmentation challenges, such as the Brain Tumor Segmentation (BraTS)
Challenge, promote development of automated approaches which are replicable,
generalizable and accurate, to aid in these tasks. The current study presents a
novel adaptation of existing nnU-Net approaches for pediatric brain tumor
segmentation, submitted to the BraTS-PEDs 2024 challenge. We apply an adapted
nnU-Net with hierarchical cascades to the segmentation task of the BraTS-PEDs
2024 challenge. The residual encoder variant of nnU-Net, used as our baseline
model, already provides high quality segmentations. We incorporate multiple
changes to the implementation of nnU-Net and devise a novel two-stage cascaded
nnU-Net to segment the substructures of brain tumors from coarse to fine. Using
outputs from the nnU-Net Residual Encoder (trained to segment CC, ED, ET and
NET tumor labels from T1w, T1w-CE, T2w and T2-FLAIR MRI), these are passed to
two additional models one classifying ET versus NET and a second classifying CC
vs ED using cascade learning. We use radiological guidelines to steer which
multi parametric MRI (mpMRI) to use in these cascading models. Compared to a
default nnU-Net and an ensembled nnU-net as baseline approaches, our novel
method provides robust segmentations for the BraTS-PEDs 2024 challenge,
achieving mean Dice scores of 0.657, 0.904, 0.703, and 0.967, and HD95 of 76.2,
10.1, 111.0, and 12.3 for the ET, NET, CC and ED, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probabilistic U-Net with Kendall Shape Spaces for Geometry-Aware
  Segmentations of Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyoung Park, Günay Doğan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the fundamental problems in computer vision is image segmentation, the
task of detecting distinct regions or objects in given images. Deep Neural
Networks (DNN) have been shown to be very effective in segmenting challenging
images, producing convincing segmentations. There is further need for
probabilistic DNNs that can reflect the uncertainties from the input images and
the models into the computed segmentations, in other words, new DNNs that can
generate multiple plausible segmentations and their distributions depending on
the input or the model uncertainties. While there are existing probabilistic
segmentation models, many of them do not take into account the geometry or
shape underlying the segmented regions. In this paper, we propose a
probabilistic image segmentation model that can incorporate the geometry of a
segmentation. Our proposed model builds on the Probabilistic U-Net of
\cite{kohl2018probabilistic} to generate probabilistic segmentations, i.e.\!
multiple likely segmentations for an input image. Our model also adopts the
Kendall Shape Variational Auto-Encoder of \cite{vadgama2023kendall} to encode a
Kendall shape space in the latent variable layers of the prior and posterior
networks of the Probabilistic U-Net. Incorporating the shape space in this
manner leads to a more robust segmentation with spatially coherent regions,
respecting the underlying geometry in the input images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reproducibility study of "LICO: Explainable Models with Language-Image
  Consistency" 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luan Fletcher, Robert van der Klis, Martin Sedláček, Stefan Vasilev, Christos Athanasiadis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing reproducibility crisis in machine learning has brought forward a
need for careful examination of research findings. This paper investigates the
claims made by Lei et al. (2023) regarding their proposed method, LICO, for
enhancing post-hoc interpretability techniques and improving image
classification performance. LICO leverages natural language supervision from a
vision-language model to enrich feature representations and guide the learning
process. We conduct a comprehensive reproducibility study, employing (Wide)
ResNets and established interpretability methods like Grad-CAM and RISE. We
were mostly unable to reproduce the authors' results. In particular, we did not
find that LICO consistently led to improved classification performance or
improvements in quantitative and qualitative measures of interpretability.
Thus, our findings highlight the importance of rigorous evaluation and
transparent reporting in interpretability research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 2 figures, Machine Learning Reproducibility Challenge 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debiasing Large <span class="highlight-title">Vision-Language</span> Models by Ablating Protected Attribute
  Representations <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neale Ratzlaff, Matthew Lyle Olson, Musashi Hinck, Shao-Yen Tseng, Vasudev Lal, Phillip Howard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision Language Models (LVLMs) such as LLaVA have demonstrated
impressive capabilities as general-purpose chatbots that can engage in
conversations about a provided input image. However, their responses are
influenced by societal biases present in their training datasets, leading to
undesirable differences in how the model responds when presented with images
depicting people of different demographics. In this work, we propose a novel
debiasing framework for LVLMs by directly ablating biased attributes during
text generation to avoid generating text related to protected attributes, or
even representing them internally. Our method requires no training and a
relatively small amount of representative biased outputs (~1000 samples). Our
experiments show that not only can we can minimize the propensity of LVLMs to
generate text related to protected attributes, but we can even use synthetic
data to inform the ablation while retaining captioning performance on real data
such as COCO. Furthermore, we find the resulting generations from a debiased
LVLM exhibit similar accuracy as a baseline biased model, showing that
debiasing effects can be achieved without sacrificing model performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS workshop on SafeGenAI, 10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Satellite Streaming Video QoE Prediction: A Real-World Subjective
  Database and Network-Level Prediction Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Chen, Zaixi Shang, Jae Won Chung, David Lerner, Werner Robitza, Rakesh Rao Ramachandra Rao, Alexander Raake, Alan C. Bovik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Demand for streaming services, including satellite, continues to exhibit
unprecedented growth. Internet Service Providers find themselves at the
crossroads of technological advancements and rising customer expectations. To
stay relevant and competitive, these ISPs must ensure their networks deliver
optimal video streaming quality, a key determinant of user satisfaction.
Towards this end, it is important to have accurate Quality of Experience
prediction models in place. However, achieving robust performance by these
models requires extensive data sets labeled by subjective opinion scores on
videos impaired by diverse playback disruptions. To bridge this data gap, we
introduce the LIVE-Viasat Real-World Satellite QoE Database. This database
consists of 179 videos recorded from real-world streaming services affected by
various authentic distortion patterns. We also conducted a comprehensive
subjective study involving 54 participants, who contributed both
continuous-time opinion scores and endpoint (retrospective) QoE scores. Our
analysis sheds light on various determinants influencing subjective QoE, such
as stall events, spatial resolutions, bitrate, and certain network parameters.
We demonstrate the usefulness of this unique new resource by evaluating the
efficacy of prevalent QoE-prediction models on it. We also created a new model
that maps the network parameters to predicted human perception scores, which
can be used by ISPs to optimize the video streaming quality of their networks.
Our proposed model, which we call SatQA, is able to accurately predict QoE
using only network parameters, without any access to pixel data or
video-specific metadata, estimated by Spearman's Rank Order Correlation
Coefficient (SROCC), Pearson Linear Correlation Coefficient (PLCC), and Root
Mean Squared Error (RMSE), indicating high accuracy and reliability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Corrective Machine Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14015v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14015v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashwat Goel, Ameya Prabhu, Philip Torr, Ponnurangam Kumaraguru, Amartya Sanyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning models increasingly face data integrity challenges due to
the use of large-scale training datasets drawn from the Internet. We study what
model developers can do if they detect that some data was manipulated or
incorrect. Such manipulated data can cause adverse effects including
vulnerability to backdoored samples, systemic biases, and reduced accuracy on
certain input domains. Realistically, all manipulated training samples cannot
be identified, and only a small, representative subset of the affected data can
be flagged.
  We formalize Corrective Machine Unlearning as the problem of mitigating the
impact of data affected by unknown manipulations on a trained model, only
having identified a subset of the corrupted data. We demonstrate that the
problem of corrective unlearning has significantly different requirements from
traditional privacy-oriented unlearning. We find most existing unlearning
methods, including retraining-from-scratch without the deletion set, require
most of the manipulated data to be identified for effective corrective
unlearning. However, one approach, Selective Synaptic Dampening, achieves
limited success, unlearning adverse effects with just a small portion of the
manipulated samples in our setting, which shows encouraging signs for future
progress. We hope our work spurs research towards developing better methods for
corrective unlearning and offers practitioners a new strategy to handle data
integrity challenges arising from web-scale training. Code is available at
https://github.com/drimpossible/corrective-unlearning-bench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions of Machine Learning Research (TMLR), 17
  pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Order-aware Interactive Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12214v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12214v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Wang, Anwesa Choudhuri, Meng Zheng, Zhongpai Gao, Benjamin Planche, Andong Deng, Qin Liu, Terrence Chen, Ulas Bagci, Ziyan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive segmentation aims to accurately segment target objects with
minimal user interactions. However, current methods often fail to accurately
separate target objects from the background, due to a limited understanding of
order, the relative depth between objects in a scene. To address this issue, we
propose OIS: order-aware interactive segmentation, where we explicitly encode
the relative depth between objects into order maps. We introduce a novel
order-aware attention, where the order maps seamlessly guide the user
interactions (in the form of clicks) to attend to the image features. We
further present an object-aware attention module to incorporate a strong
object-level understanding to better differentiate objects with similar order.
Our approach allows both dense and sparse integration of user clicks, enhancing
both accuracy and efficiency as compared to prior works. Experimental results
demonstrate that OIS achieves state-of-the-art performance, improving mIoU
after one click by 7.61 on the HQSeg44K dataset and 1.32 on the DAVIS dataset
as compared to the previous state-of-the-art SegNext, while also doubling
inference speed compared to current leading methods. The project page is
https://ukaukaaaa.github.io/projects/OIS/index.html
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Interactive demo can be found in project page:
  https://ukaukaaaa.github.io/projects/OIS/index.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EchoApex: A General-Purpose Vision Foundation Model for Echocardiography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11092v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11092v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdoul Aziz Amadou, Yue Zhang, Sebastien Piat, Paul Klein, Ingo Schmuecking, Tiziano Passerini, Puneet Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantitative evaluation of echocardiography is essential for precise
assessment of cardiac condition, monitoring disease progression, and guiding
treatment decisions. The diverse nature of echo images, including variations in
probe types, manufacturers, and pathologies, poses challenges for developing
artificial intelligent models that can generalize across different clinical
practice. We introduce EchoApex, the first general-purpose vision foundation
model echocardiography with applications on a variety of clinical practice.
Leveraging self-supervised learning, EchoApex is pretrained on over 20 million
echo images from 11 clinical centres. By incorporating task-specific decoders
and adapter modules, we demonstrate the effectiveness of EchoApex on 4
different kind of clinical applications with 28 sub-tasks, including view
classification, interactive structure segmentation, left ventricle hypertrophy
detection and automated ejection fraction estimation from view sequences.
Compared to state-of-the-art task-specific models, EchoApex attains improved
performance with a unified image encoding architecture, demonstrating the
benefits of model pretraining at scale with in-domain data. Furthermore,
EchoApex illustrates the potential for developing a general-purpose vision
foundation model tailored specifically for echocardiography, capable of
addressing a diverse range of clinical applications with high efficiency and
efficacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comprehensive Performance Evaluation of YOLO11, YOLOv10, YOLOv9 and
  YOLOv8 on Detecting and Counting Fruitlet in Complex Orchard Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12040v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12040v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ranjan Sapkota, Zhichao Meng, Martin Churuvija, Xiaoqiang Du, Zenghong Ma, Manoj Karkee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study extensively evaluated You Only Look Once (YOLO) object detection
algorithms across all configurations (total 22) of YOLOv8, YOLOv9, YOLOv10, and
YOLO11 for green fruit detection in commercial orchards. The research also
validated in-field fruitlet counting using an iPhone and machine vision sensors
across four apple varieties: Scifresh, Scilate, Honeycrisp and Cosmic Crisp.
Among the 22 configurations evaluated, YOLO11s and YOLOv9 gelan-base
outperformed others with mAP@50 scores of 0.933 and 0.935 respectively. In
terms of recall, YOLOv9 gelan-base achieved the highest value among YOLOv9
configurations at 0.899, while YOLO11m led YOLO11 variants with 0.897. YOLO11n
emerged as the fastest model, achieving fastest inference speed of only 2.4 ms,
significantly outpacing the leading configurations of YOLOv10n, YOLOv9 gelan-s,
and YOLOv8n, with speeds of 5.5, 11.5, and 4.1 ms, respectively. This
comparative evaluation highlights the strengths of YOLO11, YOLOv9, and YOLOv10,
offering researchers essential insights to choose the best-suited model for
fruitlet detection and possible automation in commercial orchards. For
real-time automation related work in relevant datasets, we recommend using
YOLO11n due to its high detection and image processing speed. Keywords: YOLO11,
YOLO11 Object Detection, YOLOv10, YOLOv9, YOLOv8, You Only Look Once, Fruitlet
Detection, Greenfruit Detection, Green Apple Detection, Agricultural
Automation, Artificial Intelligence, Deep Learning, Machine Learning, Zero-shot
Detection
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LieRE: Generalizing Rotary Position Encodings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10322v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10322v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophie Ostmeier, Brian Axelrod, Michael E. Moseley, Akshay Chaudhari, Curtis Langlotz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Rotary Position Embeddings (RoPE) for large language models have become
widely adopted, their application for other modalities has been slower. Here,
we introduce Lie group Relative position Encodings (LieRE) that goes beyond
RoPE in supporting n-dimensional inputs. We evaluate the performance of LieRE
on 2D and 3D image classification tasks and observe that LieRE leads to marked
relative improvements in performance (up to 9.7% for 2D and up to 25.5% for
3D), training efficiency (3.5x reduction), data efficiency (30%) compared to
the baselines of DeiT III, RoPE-Mixed and Vision-Llama.
https://github.com/Stanford-AIMI/LieRE
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Coarse-Grained Matching in Video-Text Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12407v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12407v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aozhu Chen, Hazel Doughty, Xirong Li, Cees G. M. Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-text retrieval has seen significant advancements, yet the ability of
models to discern subtle differences in captions still requires verification.
In this paper, we introduce a new approach for fine-grained evaluation. Our
approach can be applied to existing datasets by automatically generating hard
negative test captions with subtle single-word variations across nouns, verbs,
adjectives, adverbs, and prepositions. We perform comprehensive experiments
using four state-of-the-art models across two standard benchmarks (MSR-VTT and
VATEX) and two specially curated datasets enriched with detailed descriptions
(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our
analyses show that the current evaluation benchmarks fall short in detecting a
model's ability to perceive subtle single-word differences, 2) our fine-grained
evaluation highlights the difficulty models face in distinguishing such subtle
variations. To enhance fine-grained understanding, we propose a new baseline
that can be easily combined with current methods. Experiments on our
fine-grained evaluations demonstrate that this approach enhances a model's
ability to understand fine-grained differences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlashTex: Fast Relightable Mesh Texturing with LightControlNet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13251v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13251v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangle Deng, Timothy Omernick, Alexander Weiss, Deva Ramanan, Jun-Yan Zhu, Tinghui Zhou, Maneesh Agrawala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manually creating textures for 3D meshes is time-consuming, even for expert
visual content creators. We propose a fast approach for automatically texturing
an input 3D mesh based on a user-provided text prompt. Importantly, our
approach disentangles lighting from surface material/reflectance in the
resulting texture so that the mesh can be properly relit and rendered in any
lighting environment. We introduce LightControlNet, a new text-to-image model
based on the ControlNet architecture, which allows the specification of the
desired lighting as a conditioning image to the model. Our text-to-texture
pipeline then constructs the texture in two stages. The first stage produces a
sparse set of visually consistent reference views of the mesh using
LightControlNet. The second stage applies a texture optimization based on Score
Distillation Sampling (SDS) that works with LightControlNet to increase the
texture quality while disentangling surface material from lighting. Our
algorithm is significantly faster than previous text-to-texture methods, while
producing high-quality and relightable textures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://flashtex.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PTQ4DiT: Post-training Quantization for Diffusion Transformers <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16005v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16005v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi Wu, Haoxuan Wang, Yuzhang Shang, Mubarak Shah, Yan Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent introduction of Diffusion Transformers (DiTs) has demonstrated
exceptional capabilities in image generation by using a different backbone
architecture, departing from traditional U-Nets and embracing the scalable
nature of transformers. Despite their advanced capabilities, the wide
deployment of DiTs, particularly for real-time applications, is currently
hampered by considerable computational demands at the inference stage.
Post-training Quantization (PTQ) has emerged as a fast and data-efficient
solution that can significantly reduce computation and memory footprint by
using low-bit weights and activations. However, its applicability to DiTs has
not yet been explored and faces non-trivial difficulties due to the unique
design of DiTs. In this paper, we propose PTQ4DiT, a specifically designed PTQ
method for DiTs. We discover two primary quantization challenges inherent in
DiTs, notably the presence of salient channels with extreme magnitudes and the
temporal variability in distributions of salient activation over multiple
timesteps. To tackle these challenges, we propose Channel-wise Salience
Balancing (CSB) and Spearmen's $\rho$-guided Salience Calibration (SSC). CSB
leverages the complementarity property of channel magnitudes to redistribute
the extremes, alleviating quantization errors for both activations and weights.
SSC extends this approach by dynamically adjusting the balanced salience to
capture the temporal variations in activation. Additionally, to eliminate extra
computational costs caused by PTQ4DiT during inference, we design an offline
re-parameterization strategy for DiTs. Experiments demonstrate that our PTQ4DiT
successfully quantizes DiTs to 8-bit precision (W8A8) while preserving
comparable generation ability and further enables effective quantization to
4-bit weight precision (W4A8) for the first time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Code is available at
  https://github.com/adreamwu/PTQ4DiT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Action and Reasoning-Centric Image Editing from Videos and
  Simulations <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03471v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03471v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benno Krojer, Dheeraj Vattikonda, Luis Lara, Varun Jampani, Eva Portelance, Christopher Pal, Siva Reddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An image editing model should be able to perform diverse edits, ranging from
object replacement, changing attributes or style, to performing actions or
movement, which require many forms of reasoning. Current general
instruction-guided editing models have significant shortcomings with action and
reasoning-centric edits. Object, attribute or stylistic changes can be learned
from visually static datasets. On the other hand, high-quality data for action
and reasoning-centric edits is scarce and has to come from entirely different
sources that cover e.g. physical dynamics, temporality and spatial reasoning.
To this end, we meticulously curate the AURORA Dataset
(Action-Reasoning-Object-Attribute), a collection of high-quality training
data, human-annotated and curated from videos and simulation engines. We focus
on a key aspect of quality training data: triplets (source image, prompt,
target image) contain a single meaningful visual change described by the
prompt, i.e., truly minimal changes between source and target images. To
demonstrate the value of our dataset, we evaluate an AURORA-finetuned model on
a new expert-curated benchmark (AURORA-Bench) covering 8 diverse editing tasks.
Our model significantly outperforms previous editing models as judged by human
raters. For automatic evaluations, we find important flaws in previous metrics
and caution their use for semantically hard editing tasks. Instead, we propose
a new automatic metric that focuses on discriminative understanding. We hope
that our efforts : (1) curating a quality training dataset and an evaluation
benchmark, (2) developing critical evaluations, and (3) releasing a
state-of-the-art model, will fuel further progress on general image editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 (Dataset & Benchmarks)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stratified Domain Adaptation: A Progressive Self-Training Approach for
  Scene Text Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kha Nhat Le, Hoang-Tuan Nguyen, Hung Tien Tran, Thanh Duc Ngo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised domain adaptation (UDA) has become increasingly prevalent in
scene text recognition (STR), especially where training and testing data reside
in different domains. The efficacy of existing UDA approaches tends to degrade
when there is a large gap between the source and target domains. To deal with
this problem, gradually shifting or progressively learning to shift from domain
to domain is the key issue. In this paper, we introduce the Stratified Domain
Adaptation (StrDA) approach, which examines the gradual escalation of the
domain gap for the learning process. The objective is to partition the training
data into subsets so that the progressively self-trained model can adapt to
gradual changes. We stratify the training data by evaluating the proximity of
each data sample to both the source and target domains. We propose a novel
method for employing domain discriminators to estimate the out-of-distribution
and domain discriminative levels of data samples. Extensive experiments on
benchmark scene-text datasets show that our approach significantly improves the
performance of baseline (source-trained) STR models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 12 figures, 5 tables, include supplementary materials</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Granular Privacy Control for Geolocation with <span class="highlight-title">Vision Language</span> Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Mendes, Yang Chen, James Hays, Sauvik Das, Wei Xu, Alan Ritter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) are rapidly advancing in their capability to
answer information-seeking questions. As these models are widely deployed in
consumer applications, they could lead to new privacy risks due to emergent
abilities to identify people in photos, geolocate images, etc. As we
demonstrate, somewhat surprisingly, current open-source and proprietary VLMs
are very capable image geolocators, making widespread geolocation with VLMs an
immediate privacy risk, rather than merely a theoretical future concern. As a
first step to address this challenge, we develop a new benchmark, GPTGeoChat,
to test the ability of VLMs to moderate geolocation dialogues with users. We
collect a set of 1,000 image geolocation conversations between in-house
annotators and GPT-4v, which are annotated with the granularity of location
information revealed at each turn. Using this new dataset, we evaluate the
ability of various VLMs to moderate GPT-4v geolocation conversations by
determining when too much location information has been revealed. We find that
custom fine-tuned models perform on par with prompted API-based models when
identifying leaked location information at the country or city level; however,
fine-tuning on supervised data appears to be needed to accurately moderate
finer granularities, such as the name of a restaurant or building.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Contrastive Feature Representations for Facial Action Unit
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06165v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06165v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiao Shang, Bin Liu, Fengmao Lv, Fei Teng, Tianrui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial action unit (AU) detection has long encountered the challenge of
detecting subtle feature differences when AUs activate. Existing methods often
rely on encoding pixel-level information of AUs, which not only encodes
additional redundant information but also leads to increased model complexity
and limited generalizability. Additionally, the accuracy of AU detection is
negatively impacted by the class imbalance issue of each AU type, and the
presence of noisy and false AU labels. In this paper, we introduce a novel
contrastive learning framework aimed for AU detection that incorporates both
self-supervised and supervised signals, thereby enhancing the learning of
discriminative features for accurate AU detection. To tackle the class
imbalance issue, we employ a negative sample re-weighting strategy that adjusts
the step size of updating parameters for minority and majority class samples.
Moreover, to address the challenges posed by noisy and false AU labels, we
employ a sampling technique that encompasses three distinct types of positive
sample pairs. This enables us to inject self-supervised signals into the
supervised signal, effectively mitigating the adverse effects of noisy labels.
Our experimental assessments, conducted on four widely-utilized benchmark
datasets (BP4D, DISFA, GFT and Aff-Wild2), underscore the superior performance
of our approach compared to state-of-the-art methods of AU detection. Our code
is available at \url{https://github.com/Ziqiao-Shang/AUNCE}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 18 figures, submitted to Pattern Recognition (PR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context-Aware Full Body Anonymization using Text-to-Image Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Zwick, Kevin Roesch, Marvin Klemp, Oliver Bringmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anonymization plays a key role in protecting sensible information of
individuals in real world datasets. Self-driving cars for example need high
resolution facial features to track people and their viewing direction to
predict future behaviour and react accordingly. In order to protect people's
privacy whilst keeping important features in the dataset, it is important to
replace the full body of a person with a highly detailed anonymized one. In
contrast to doing face anonymization, full body replacement decreases the
ability of recognizing people by their hairstyle or clothes. In this paper, we
propose a workflow for full body person anonymization utilizing Stable
Diffusion as a generative backend. Text-to-image diffusion models, like Stable
Diffusion, OpenAI's DALL-E or Midjourney, have become very popular in recent
time, being able to create photorealistic images from a single text prompt. We
show that our method outperforms state-of-the art anonymization pipelines with
respect to image quality, resolution, Inception Score (IS) and Frechet
Inception Distance (FID). Additionally, our method is invariant with respect to
the image generator and thus able to be used with the latest models available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MuJo: <span class="highlight-title">Multimodal</span> Joint Feature Space Learning for Human Activity
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Gerd Fritsch, Cennet Oguz, Vitor Fortes Rey, Lala Ray, Maximilian Kiefer-Emmanouilidis, Paul Lukowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human Activity Recognition (HAR) is a longstanding problem in AI with
applications in a broad range of areas, including healthcare, sports and
fitness, security, and more. The performance of HAR in real-world settings is
strongly dependent on the type and quality of the input signal that can be
acquired. Given an unobstructed, high-quality camera view of a scene, computer
vision systems, in particular in conjunction with foundation models, can today
fairly reliably distinguish complex activities. On the other hand, recognition
using modalities such as wearable sensors (which are often more broadly
available, e.g., in mobile phones and smartwatches) is a more difficult
problem, as the signals often contain less information and labeled training
data is more difficult to acquire. To alleviate the need for labeled data, we
introduce our comprehensive Fitness Multimodal Activity Dataset (FiMAD) in this
work, which can be used with the proposed pre-training method MuJo (Multimodal
Joint Feature Space Learning) to enhance HAR performance across various
modalities. FiMAD was created using YouTube fitness videos and contains
parallel video, language, pose, and simulated IMU sensor data. MuJo utilizes
this dataset to learn a joint feature space for these modalities. We show that
classifiers pre-trained on FiMAD can increase the performance on real HAR
datasets such as MM-Fit, MyoGym, MotionSense, and MHEALTH. For instance, on
MM-Fit, we achieve an Macro F1-Score of up to 0.855 when fine-tuning on only 2%
of the training data and 0.942 when utilizing the full training set for
classification tasks. We have compared our approach to other self-supervised
ones and showed that, unlike them, ours can consistently improve on the
baseline network performance as well as provide a better data-efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Mapping of Anatomical Landmarks from Free-Text Using Large
  Language Models: Insights from Llama-2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12686v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12686v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamad Abdi, Gerardo Hermosillo Valadez, Halid Ziya Yerebakan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anatomical landmarks are vital in medical imaging for navigation and anomaly
detection. Modern large language models (LLMs), like Llama-2, offer promise for
automating the mapping of these landmarks in free-text radiology reports to
corresponding positions in image data. Recent studies propose LLMs may develop
coherent representations of generative processes. Motivated by these insights,
we investigated whether LLMs accurately represent the spatial positions of
anatomical landmarks. Through experiments with Llama-2 models, we found that
they can linearly represent anatomical landmarks in space with considerable
robustness to different prompts. These results underscore the potential of LLMs
to enhance the efficiency and accuracy of medical imaging workflows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimating Atmospheric Variables from Digital Typhoon Satellite Images
  via Conditional Denoising Diffusion Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07961v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07961v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangyue Ling, Pritthijit Nath, César Quilodrán-Casas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores the application of diffusion models in the field of
typhoons, predicting multiple ERA5 meteorological variables simultaneously from
Digital Typhoon satellite images. The focus of this study is taken to be
Taiwan, an area very vulnerable to typhoons. By comparing the performance of
Conditional Denoising Diffusion Probability Model (CDDPM) with Convolutional
Neural Networks (CNN) and Squeeze-and-Excitation Networks (SENet), results
suggest that the CDDPM performs best in generating accurate and realistic
meteorological data. Specifically, CDDPM achieved a PSNR of 32.807, which is
approximately 7.9% higher than CNN and 5.5% higher than SENet. Furthermore,
CDDPM recorded an RMSE of 0.032, showing a 11.1% improvement over CNN and 8.6%
improvement over SENet. A key application of this research can be for
imputation purposes in missing meteorological datasets and generate additional
high-quality meteorological data using satellite images. It is hoped that the
results of this analysis will enable more robust and detailed forecasting,
reducing the impact of severe weather events on vulnerable regions. Code
accessible at https://github.com/TammyLing/Typhoon-forecasting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for spotlight presentation at the NeurIPS 2024 workshop on
  Tackling Climate Change with Machine Learning. 8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Human Evaluation Protocol for Text-to-Video Models: Enhancing
  Reliability,Reproducibility, and Practicality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08845v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08845v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianle Zhang, Langtian Ma, Yuchen Yan, Yuchen Zhang, Kai Wang, Yue Yang, Ziyao Guo, Wenqi Shao, Yang You, Yu Qiao, Ping Luo, Kaipeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent text-to-video (T2V) technology advancements, as demonstrated by models
such as Gen2, Pika, and Sora, have significantly broadened its applicability
and popularity. Despite these strides, evaluating these models poses
substantial challenges. Primarily, due to the limitations inherent in automatic
metrics, manual evaluation is often considered a superior method for assessing
T2V generation. However, existing manual evaluation protocols face
reproducibility, reliability, and practicality issues. To address these
challenges, this paper introduces the Text-to-Video Human Evaluation (T2VHE)
protocol, a comprehensive and standardized protocol for T2V models. The T2VHE
protocol includes well-defined metrics, thorough annotator training, and an
effective dynamic evaluation module. Experimental results demonstrate that this
protocol not only ensures high-quality annotations but can also reduce
evaluation costs by nearly 50\%. We will open-source the entire setup of the
T2VHE protocol, including the complete protocol workflow, the dynamic
evaluation component details, and the annotation interface code. This will help
communities establish more sophisticated human assessment protocols.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MirrorCheck: Efficient Adversarial Defense for <span class="highlight-title">Vision-Language</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09250v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09250v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samar Fares, Klea Ziu, Toluwani Aremu, Nikita Durasov, Martin Takáč, Pascal Fua, Karthik Nandakumar, Ivan Laptev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) are becoming increasingly vulnerable to
adversarial attacks as various novel attack strategies are being proposed
against these models. While existing defenses excel in unimodal contexts, they
currently fall short in safeguarding VLMs against adversarial threats. To
mitigate this vulnerability, we propose a novel, yet elegantly simple approach
for detecting adversarial samples in VLMs. Our method leverages Text-to-Image
(T2I) models to generate images based on captions produced by target VLMs.
Subsequently, we calculate the similarities of the embeddings of both input and
generated images in the feature space to identify adversarial samples.
Empirical evaluations conducted on different datasets validate the efficacy of
our approach, outperforming baseline methods adapted from image classification
domains. Furthermore, we extend our methodology to classification tasks,
showcasing its adaptability and model-agnostic nature. Theoretical analyses and
empirical findings also show the resilience of our approach against adaptive
attacks, positioning it as an excellent defense mechanism for real-world
deployment against adversarial threats.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback
  for Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katherine M. Collins, Najoung Kim, Yonatan Bitton, Verena Rieser, Shayegan Omidshafiei, Yushi Hu, Sherol Chen, Senjuti Dutta, Minsuk Chang, Kimin Lee, Youwei Liang, Georgina Evans, Sahil Singla, Gang Li, Adrian Weller, Junfeng He, Deepak Ramachandran, Krishnamurthy Dj Dvijotham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human feedback plays a critical role in learning and refining reward models
for text-to-image generation, but the optimal form the feedback should take for
learning an accurate reward function has not been conclusively established.
This paper investigates the effectiveness of fine-grained feedback which
captures nuanced distinctions in image quality and prompt-alignment, compared
to traditional coarse-grained feedback (for example, thumbs up/down or ranking
between a set of options). While fine-grained feedback holds promise,
particularly for systems catering to diverse societal preferences, we show that
demonstrating its superiority to coarse-grained feedback is not automatic.
Through experiments on real and synthetic preference data, we surface the
complexities of building effective models due to the interplay of model choice,
feedback type, and the alignment between human judgment and computational
interpretation. We identify key challenges in eliciting and utilizing
fine-grained feedback, prompting a reassessment of its assumed benefits and
practicality. Our findings -- e.g., that fine-grained feedback can lead to
worse models for a fixed budget, in some settings; however, in controlled
settings with known attributes, fine grained rewards can indeed be more helpful
-- call for careful consideration of feedback attributes and potentially beckon
novel modeling approaches to appropriately unlock the potential value of
fine-grained feedback in-the-wild.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ G2D: From Global to Dense Radiography Representation Learning via
  <span class="highlight-title">Vision-Language</span> <span class="highlight-title">Pre-train</span>ing <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01522v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01522v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Che Liu, Cheng Ouyang, Sibo Cheng, Anand Shah, Wenjia Bai, Rossella Arcucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, medical vision-language pre-training (VLP) has reached substantial
progress to learn global visual representation from medical images and their
paired radiology reports. However, medical imaging tasks in real world usually
require finer granularity in visual features. These tasks include visual
localization tasks (e.g., semantic segmentation, object detection) and visual
grounding task. Yet, current medical VLP methods face challenges in learning
these fine-grained features, as they primarily focus on brute-force alignment
between image patches and individual text tokens for local visual feature
learning, which is suboptimal for downstream dense prediction tasks. In this
work, we propose a new VLP framework, named \textbf{G}lobal to \textbf{D}ense
level representation learning (G2D) that achieves significantly improved
granularity and more accurate grounding for the learned features, compared to
existing medical VLP approaches. In particular, G2D learns dense and
semantically-grounded image representations via a pseudo segmentation task
parallel with the global vision-language alignment. Notably, generating pseudo
segmentation targets does not incur extra trainable parameters: they are
obtained on the fly during VLP with a parameter-free processor. G2D achieves
superior performance across 6 medical imaging tasks and 25 diseases,
particularly in semantic segmentation, which necessitates fine-grained,
semantically-grounded image features. In this task, G2D surpasses peer models
even when fine-tuned with just 1\% of the training data, compared to the 100\%
used by these models. The code will be released upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ t-READi: Transformer-Powered Robust and Efficient <span class="highlight-title">Multimodal</span> Inference
  for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09747v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09747v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Hu, Yuhang Qian, Tianyue Zheng, Ang Li, Zhe Chen, Yue Gao, Xiuzhen Cheng, Jun Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the wide adoption of multimodal sensors (e.g., camera, lidar, radar) by
autonomous vehicles (AVs), deep analytics to fuse their outputs for a robust
perception become imperative. However, existing fusion methods often make two
assumptions rarely holding in practice: i) similar data distributions for all
inputs and ii) constant availability for all sensors. Because, for example,
lidars have various resolutions and failures of radars may occur, such
variability often results in significant performance degradation in fusion. To
this end, we present tREADi, an adaptive inference system that accommodates the
variability of multimodal sensory data and thus enables robust and efficient
perception. t-READi identifies variation-sensitive yet structure-specific model
parameters; it then adapts only these parameters while keeping the rest intact.
t-READi also leverages a cross-modality contrastive learning method to
compensate for the loss from missing modalities. Both functions are implemented
to maintain compatibility with existing multimodal deep fusion methods. The
extensive experiments evidently demonstrate that compared with the status quo
approaches, t-READi not only improves the average inference accuracy by more
than 6% but also reduces the inference latency by almost 15x with the cost of
only 5% extra memory overhead in the worst case under realistic data and modal
variations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-grained Image-to-LiDAR Contrastive Distillation with Visual
  Foundation Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14271v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14271v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zhang, Junhui Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive image-to-LiDAR knowledge transfer, commonly used for learning 3D
representations with synchronized images and point clouds, often faces a
self-conflict dilemma. This issue arises as contrastive losses unintentionally
dissociate features of unmatched points and pixels that share semantic labels,
compromising the integrity of learned representations. To overcome this, we
harness Visual Foundation Models (VFMs), which have revolutionized the
acquisition of pixel-level semantics, to enhance 3D representation learning.
Specifically, we utilize off-the-shelf VFMs to generate semantic labels for
weakly-supervised pixel-to-point contrastive distillation. Additionally, we
employ von Mises-Fisher distributions to structure the feature space, ensuring
semantic embeddings within the same class remain consistent across varying
inputs. Furthermore, we adapt sampling probabilities of points to address
imbalances in spatial distribution and category frequency, promoting
comprehensive and balanced learning. Extensive experiments demonstrate that our
approach mitigates the challenges posed by traditional methods and consistently
surpasses existing image-to-LiDAR contrastive distillation methods in
downstream tasks. The source code is available at
\href{https://github.com/Eaphan/OLIVINE.}{\color{black}https://github.com/Eaphan/OLIVINE}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bias Behind the Wheel: Fairness Testing of Autonomous Driving Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02935v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02935v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyue Li, Zhenpeng Chen, Jie M. Zhang, Federica Sarro, Ying Zhang, Xuanzhe Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper conducts fairness testing of automated pedestrian detection, a
crucial but under-explored issue in autonomous driving systems. We evaluate
eight state-of-the-art deep learning-based pedestrian detectors across
demographic groups on large-scale real-world datasets. To enable thorough
fairness testing, we provide extensive annotations for the datasets, resulting
in 8,311 images with 16,070 gender labels, 20,115 age labels, and 3,513 skin
tone labels. Our findings reveal significant fairness issues, particularly
related to age. The proportion of undetected children is 20.14% higher compared
to adults. Furthermore, we explore how various driving scenarios affect the
fairness of pedestrian detectors. We find that pedestrian detectors demonstrate
significant gender biases during night time, potentially exacerbating the
prevalent societal issue of female safety concerns during nighttime out.
Moreover, we observe that pedestrian detectors can demonstrate both enhanced
fairness and superior performance under specific driving conditions, which
challenges the fairness-performance trade-off theory widely acknowledged in the
fairness literature. We publicly release the code, data, and results to support
future research on fairness in autonomous driving.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Transactions on Software Engineering and Methodology
  (TOSEM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnyDesign: Versatile Area Fashion Editing via Mask-Free Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11553v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11553v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfang Niu, Lingxiang Wu, Dong Yi, Jie Peng, Ning Jiang, Haiying Wu, Jinqiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fashion image editing aims to modify a person's appearance based on a given
instruction. Existing methods require auxiliary tools like segmenters and
keypoint extractors, lacking a flexible and unified framework. Moreover, these
methods are limited in the variety of clothing types they can handle, as most
datasets focus on people in clean backgrounds and only include generic garments
such as tops, pants, and dresses. These limitations restrict their
applicability in real-world scenarios. In this paper, we first extend an
existing dataset for human generation to include a wider range of apparel and
more complex backgrounds. This extended dataset features people wearing diverse
items such as tops, pants, dresses, skirts, headwear, scarves, shoes, socks,
and bags. Additionally, we propose AnyDesign, a diffusion-based method that
enables mask-free editing on versatile areas. Users can simply input a human
image along with a corresponding prompt in either text or image format. Our
approach incorporates Fashion DiT, equipped with a Fashion-Guidance Attention
(FGA) module designed to fuse explicit apparel types and CLIP-encoded apparel
features. Both Qualitative and quantitative experiments demonstrate that our
method delivers high-quality fashion editing and outperforms contemporary
text-guided fashion editing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCMM: Calibrating Cross-modal Representations for Text-Based Person
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02278v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02278v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Liu, Donglai Wei, Yang Liu, Sipeng Zhang, Tong Yang, Victor C. M. Leung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-Based Person Search (TBPS) is a crucial task that enables accurate
retrieval of target individuals from large-scale galleries with only given
textual caption. For cross-modal TBPS tasks, it is critical to obtain
well-distributed representation in the common embedding space to reduce the
inter-modal gap. Furthermore, learning detailed image-text correspondences is
essential to discriminate similar targets and enable fine-grained search. To
address these challenges, we present a simple yet effective method named Sew
Calibration and Masked Modeling (SCMM) that calibrates cross-modal
representations by learning compact and well-aligned embeddings. SCMM is
distinguished by two novel losses to provide fine-grained cross-modal
representations: 1) a Sew calibration loss that takes the quality of textual
captions as guidance and aligns features between image and text modalities, and
2) a Masked Caption Modeling (MCM) loss that leverages a masked caption
prediction task to establish detailed and generic relationships between textual
and visual parts. The dual-pronged strategy refines feature alignment and
enriches cross-modal correspondences, enabling the accurate distinction of
similar individuals. Consequently, its streamlined dual-encoder architecture
avoids complex branches and interactions and facilitates high-speed inference
suitable for real-time requirements. Consequently, high-speed inference is
achieved, which is essential for resource-limited applications often demanding
real-time processing. Extensive experiments on three popular TBPS benchmarks
demonstrate the superiority of SCMM, achieving top results with 73.81%, 74.25%,
and 57.35% Rank-1 accuracy on CUHK-PEDES, ICFG-PEDES, and RSTPReID,
respectively. We hope SCMM's scalable and cost-effective design will serve as a
strong baseline and facilitate future research in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version of manuscript is under IEEE TMM review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lost in Tracking: Uncertainty-guided Cardiac Cine MRI Segmentation at
  Right Ventricle Base 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03320v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03320v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yidong Zhao, Yi Zhang, Orlando Simonetti, Yuchi Han, Qian Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate biventricular segmentation of cardiac magnetic resonance (CMR) cine
images is essential for the clinical evaluation of heart function. However,
compared to left ventricle (LV), right ventricle (RV) segmentation is still
more challenging and less reproducible. Degenerate performance frequently
occurs at the RV base, where the in-plane anatomical structures are complex
(with atria, valve, and aorta) and vary due to the strong interplanar motion.
In this work, we propose to address the currently unsolved issues in CMR
segmentation, specifically at the RV base, with two strategies: first, we
complemented the public resource by reannotating the RV base in the ACDC
dataset, with refined delineation of the right ventricle outflow tract (RVOT),
under the guidance of an expert cardiologist. Second, we proposed a novel dual
encoder U-Net architecture that leverages temporal incoherence to inform the
segmentation when interplanar motions occur. The inter-planar motion is
characterized by loss-of-tracking, via Bayesian uncertainty of a
motion-tracking model. Our experiments showed that our method significantly
improved RV base segmentation taking into account temporal incoherence.
Furthermore, we investigated the reproducibility of deep learning-based
segmentation and showed that the combination of consistent annotation and loss
of tracking could enhance the reproducibility of RV segmentation, potentially
facilitating a large number of clinical studies focusing on RV.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sliding Gaussian ball adaptive growth (SlingBAG): point cloud-based
  iterative algorithm for large-scale 3D photoacoustic imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11781v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11781v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuang Li, Yibing Wang, Jian Gao, Chulhong Kim, Seongwook Choi, Yu Zhang, Qian Chen, Yao Yao, Changhui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale photoacoustic (PA) 3D imaging has become increasingly important
for both clinical and pre-clinical applications. Limited by resource and
application constrains, only sparsely-distributed transducer arrays can be
applied, which necessitates advanced image reconstruction algorithms to
overcome artifacts caused by using back-projection algorithm. However, high
computing memory consumption of traditional iterative algorithms for
large-scale 3D cases is practically unacceptable. Here, we propose a point
cloud-based iterative algorithm that reduces memory consumption by several
orders, wherein a 3D photoacoustic scene is modeled as a series of
Gaussian-distributed spherical sources. During the iterative reconstruction
process, the properties of each Gaussian source, including peak intensities,
standard deviations and means are stored in form of point cloud, then
continuously optimized and adaptively undergoing destroying, splitting, and
duplication along the gradient direction, thus manifesting the sliding ball
adaptive growth effect. This method, named the sliding Gaussian ball adaptive
growth (SlingBAG) algorithm, enables high-quality 3D large-scale PA
reconstruction with fast iteration and extremely less memory usage. We
validated SlingBAG algorithm in both simulation study and in vivo animal
experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added SlingBAG reconstruction of rat kidney and rat liver results;
  updated methods; added references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Steerable Conditional Diffusion for Out-of-Distribution Adaptation in
  Medical Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14409v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14409v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Barbano, Alexander Denker, Hyungjin Chung, Tae Hoon Roh, Simon Arridge, Peter Maass, Bangti Jin, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising diffusion models have emerged as the go-to generative framework for
solving inverse problems in imaging. A critical concern regarding these models
is their performance on out-of-distribution tasks, which remains an
under-explored challenge. Using a diffusion model on an out-of-distribution
dataset, realistic reconstructions can be generated, but with hallucinating
image features that are uniquely present in the training dataset. To address
this discrepancy during train-test time and improve reconstruction accuracy, we
introduce a novel sampling framework called Steerable Conditional Diffusion.
Specifically, this framework adapts the diffusion model, concurrently with
image reconstruction, based solely on the information provided by the available
measurement. Utilising our proposed method, we achieve substantial enhancements
in out-of-distribution performance across diverse imaging modalities, advancing
the robust deployment of denoising diffusion models in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ASTM :Autonomous Smart Traffic Management System Using Artificial
  Intelligence CNN and LSTM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christofel Rio Goenawan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the modern world, the development of Artificial Intelligence (AI) has
contributed to improvements in various areas, including automation, computer
vision, fraud detection, and more. AI can be leveraged to enhance the
efficiency of Autonomous Smart Traffic Management (ASTM) systems and reduce
traffic congestion rates. This paper presents an Autonomous Smart Traffic
Management (STM) system that uses AI to improve traffic flow rates. The system
employs the YOLO V5 Convolutional Neural Network to detect vehicles in traffic
management images. Additionally, it predicts the number of vehicles for the
next 12 hours using a Recurrent Neural Network with Long Short-Term Memory
(RNN-LSTM). The Smart Traffic Management Cycle Length Analysis manages the
traffic cycle length based on these vehicle predictions, aided by AI. From the
results of the RNN-LSTM model for predicting vehicle numbers over the next 12
hours, we observe that the model predicts traffic with a Mean Squared Error
(MSE) of 4.521 vehicles and a Root Mean Squared Error (RMSE) of 2.232 vehicles.
After simulating the STM system in the CARLA simulation environment, we found
that the Traffic Management Congestion Flow Rate with ASTM (21 vehicles per
minute) is 50\% higher than the rate without STM (around 15 vehicles per
minute). Additionally, the Traffic Management Vehicle Pass Delay with STM (5
seconds per vehicle) is 70\% lower than without STM (around 12 seconds per
vehicle). These results demonstrate that the STM system using AI can increase
traffic flow by 50\% and reduce vehicle pass delays by 70\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In process to IEEE Intelligent Vehicle Symposium 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LinFusion: 1 GPU, 1 Minute, 16K Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02097v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02097v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songhua Liu, Weihao Yu, Zhenxiong Tan, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern diffusion models, particularly those utilizing a Transformer-based
UNet for denoising, rely heavily on self-attention operations to manage complex
spatial relationships, thus achieving impressive generation performance.
However, this existing paradigm faces significant challenges in generating
high-resolution visual content due to its quadratic time and memory complexity
with respect to the number of spatial tokens. To address this limitation, we
aim at a novel linear attention mechanism as an alternative in this paper.
Specifically, we begin our exploration from recently introduced models with
linear complexity, e.g., Mamba2, RWKV6, Gated Linear Attention, etc, and
identify two key features--attention normalization and non-causal
inference--that enhance high-resolution visual generation performance. Building
on these insights, we introduce a generalized linear attention paradigm, which
serves as a low-rank approximation of a wide spectrum of popular linear token
mixers. To save the training cost and better leverage pre-trained models, we
initialize our models and distill the knowledge from pre-trained
StableDiffusion (SD). We find that the distilled model, termed LinFusion,
achieves performance on par with or superior to the original SD after only
modest training, while significantly reducing time and memory complexity.
Extensive experiments on SD-v1.5, SD-v2.1, and SD-XL demonstrate that LinFusion
enables satisfactory and efficient zero-shot cross-resolution generation,
accommodating ultra-resolution images like 16K on a single GPU. Moreover, it is
highly compatible with pre-trained SD components and pipelines, such as
ControlNet, IP-Adapter, DemoFusion, DistriFusion, etc, requiring no adaptation
efforts. Codes are available at https://github.com/Huage001/LinFusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress. Codes are available at
  https://github.com/Huage001/LinFusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthetic Augmentation for Anatomical Landmark Localization using DDPMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnela Hadzic, Lea Bogensperger, Simon Johannes Joham, Martin Urschler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning techniques for anatomical landmark localization (ALL) have
shown great success, but their reliance on large annotated datasets remains a
problem due to the tedious and costly nature of medical data acquisition and
annotation. While traditional data augmentation, variational autoencoders
(VAEs), and generative adversarial networks (GANs) have already been used to
synthetically expand medical datasets, diffusion-based generative models have
recently started to gain attention for their ability to generate high-quality
synthetic images. In this study, we explore the use of denoising diffusion
probabilistic models (DDPMs) for generating medical images and their
corresponding heatmaps of landmarks to enhance the training of a supervised
deep learning model for ALL. Our novel approach involves a DDPM with a
2-channel input, incorporating both the original medical image and its heatmap
of annotated landmarks. We also propose a novel way to assess the quality of
the generated images using a Markov Random Field (MRF) model for landmark
matching and a Statistical Shape Model (SSM) to check landmark plausibility,
before we evaluate the DDPM-augmented dataset in the context of an ALL task
involving hand X-Rays.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the SASHIMI workshop of MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PixLore: A Dataset-driven Approach to Rich Image Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diego Bonilla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of vision-language integration, generating detailed image
captions poses a significant challenge due to the lack of curated and rich
datasets. This study introduces PixLore, a novel method that leverages Querying
Transformers through the fine-tuning of the BLIP-2 model using the LoRa method
on a standard commercial GPU. The followed approach, which involves training on
a carefully assembled dataset from state-of-the-art Computer Vision models
combined and augmented by ChatGPT, addresses the question of whether intricate
image understanding can be achieved with an ensemble of smaller-scale models,
referred to as Knowledge Stitching. Comparative evaluations against major
models such as GPT-4 and Google Bard demonstrate that PixLore-2.7B, despite
having considerably fewer parameters, is rated higher than the existing
State-of-the-Art models in over half of the assessments. Precisely, PixLore
outperform Bard and BLIP-2, which score approximately 35.18% and 27.98% lower
than PixLore in the task of image captioning. This research not only presents a
groundbreaking approach but also highlights the importance of well-curated
datasets in enhancing the performance of smaller models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper in preprint pending of publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCA: Highly Efficient Semantic-Consistent Unrestricted Adversarial
  Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02240v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02240v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Pan, Weibin Wu, Yuhang Cao, Zibin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural network based systems deployed in sensitive environments are
vulnerable to adversarial attacks. Unrestricted adversarial attacks typically
manipulate the semantic content of an image (e.g., color or texture) to create
adversarial examples that are both effective and photorealistic. Recent works
have utilized the diffusion inversion process to map images into a latent
space, where high-level semantics are manipulated by introducing perturbations.
However, they often results in substantial semantic distortions in the denoised
output and suffers from low efficiency. In this study, we propose a novel
framework called Semantic-Consistent Unrestricted Adversarial Attacks (SCA),
which employs an inversion method to extract edit-friendly noise maps and
utilizes Multimodal Large Language Model (MLLM) to provide semantic guidance
throughout the process. Under the condition of rich semantic information
provided by MLLM, we perform the DDPM denoising process of each step using a
series of edit-friendly noise maps, and leverage DPM Solver++ to accelerate
this process, enabling efficient sampling with semantic consistency. Compared
to existing methods, our framework enables the efficient generation of
adversarial examples that exhibit minimal discernible semantic changes.
Consequently, we for the first time introduce Semantic-Consistent Adversarial
Examples (SCAE). Extensive experiments and visualizations have demonstrated the
high efficiency of SCA, particularly in being on average 12 times faster than
the state-of-the-art attacks. Our research can further draw attention to the
security of multimedia information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SafeGen: Mitigating Sexually Explicit Content Generation in
  Text-to-Image Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06666v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06666v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinfeng Li, Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu Ji, Wenyuan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) models, such as Stable Diffusion, have exhibited
remarkable performance in generating high-quality images from text descriptions
in recent years. However, text-to-image models may be tricked into generating
not-safe-for-work (NSFW) content, particularly in sexually explicit scenarios.
Existing countermeasures mostly focus on filtering inappropriate inputs and
outputs, or suppressing improper text embeddings, which can block sexually
explicit content (e.g., naked) but may still be vulnerable to adversarial
prompts -- inputs that appear innocent but are ill-intended. In this paper, we
present SafeGen, a framework to mitigate sexual content generation by
text-to-image models in a text-agnostic manner. The key idea is to eliminate
explicit visual representations from the model regardless of the text input. In
this way, the text-to-image model is resistant to adversarial prompts since
such unsafe visual representations are obstructed from within. Extensive
experiments conducted on four datasets and large-scale user studies demonstrate
SafeGen's effectiveness in mitigating sexually explicit content generation
while preserving the high-fidelity of benign images. SafeGen outperforms eight
state-of-the-art baseline methods and achieves 99.4% sexual content removal
performance. Furthermore, our constructed benchmark of adversarial prompts
provides a basis for future development and evaluation of anti-NSFW-generation
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM CCS 2024. Please cite this paper as "Xinfeng Li,
  Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu Ji, Wenyuan Xu.
  SafeGen: Mitigating Sexually Explicit Content Generation in Text-to-Image
  Models. In Proceedings of ACM Conference on Computer and Communications
  Security (CCS), 2024."</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAM-Guided Masked Token Prediction for 3D Scene Understanding <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhimin Chen, Liang Yang, Yingwei Li, Longlong Jing, Bing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models have significantly enhanced 2D task performance, and recent
works like Bridge3D have successfully applied these models to improve 3D scene
understanding through knowledge distillation, marking considerable
advancements. Nonetheless, challenges such as the misalignment between 2D and
3D representations and the persistent long-tail distribution in 3D datasets
still restrict the effectiveness of knowledge distillation from 2D to 3D using
foundation models. To tackle these issues, we introduce a novel SAM-guided
tokenization method that seamlessly aligns 3D transformer structures with
region-level knowledge distillation, replacing the traditional KNN-based
tokenization techniques. Additionally, we implement a group-balanced
re-weighting strategy to effectively address the long-tail problem in knowledge
distillation. Furthermore, inspired by the recent success of masked feature
prediction, our framework incorporates a two-stage masked token prediction
process in which the student model predicts both the global embeddings and the
token-wise local embeddings derived from the teacher models trained in the
first stage. Our methodology has been validated across multiple datasets,
including SUN RGB-D, ScanNet, and S3DIS, for tasks like 3D object detection and
semantic segmentation. The results demonstrate significant improvements over
current State-of-the-art self-supervised methods, establishing new benchmarks
in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GeoCalib: Learning Single-image Calibration with Geometric Optimization <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Veicht, Paul-Edouard Sarlin, Philipp Lindenberger, Marc Pollefeys
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  From a single image, visual cues can help deduce intrinsic and extrinsic
camera parameters like the focal length and the gravity direction. This
single-image calibration can benefit various downstream applications like image
editing and 3D mapping. Current approaches to this problem are based on either
classical geometry with lines and vanishing points or on deep neural networks
trained end-to-end. The learned approaches are more robust but struggle to
generalize to new environments and are less accurate than their classical
counterparts. We hypothesize that they lack the constraints that 3D geometry
provides. In this work, we introduce GeoCalib, a deep neural network that
leverages universal rules of 3D geometry through an optimization process.
GeoCalib is trained end-to-end to estimate camera parameters and learns to find
useful visual cues from the data. Experiments on various benchmarks show that
GeoCalib is more robust and more accurate than existing classical and learned
approaches. Its internal optimization estimates uncertainties, which help flag
failure cases and benefit downstream applications like visual localization. The
code and trained models are publicly available at
https://github.com/cvg/GeoCalib.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D
  Gaussians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17898v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17898v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, Bo Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering
fidelity and efficiency compared to NeRF-based neural scene representations.
While demonstrating the potential for real-time rendering, 3D-GS encounters
rendering bottlenecks in large scenes with complex details due to an excessive
number of Gaussian primitives located within the viewing frustum. This
limitation is particularly noticeable in zoom-out views and can lead to
inconsistent rendering speeds in scenes with varying details. Moreover, it
often struggles to capture the corresponding level of details at different
scales with its heuristic density control operation. Inspired by the
Level-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an
LOD-structured 3D Gaussian approach supporting level-of-detail decomposition
for scene representation that contributes to the final rendering results. Our
model dynamically selects the appropriate level from the set of
multi-resolution anchor points, ensuring consistent rendering performance with
adaptive LOD adjustments while maintaining high-fidelity rendering results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://city-super.github.io/octree-gs/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenDAS: <span class="highlight-title">Open-Vocabulary</span> Domain Adaptation for Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20141v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20141v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gonca Yilmaz, Songyou Peng, Marc Pollefeys, Francis Engelmann, Hermann Blum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Vision-Language Models (VLMs) have advanced segmentation techniques
by shifting from the traditional segmentation of a closed-set of predefined
object classes to open-vocabulary segmentation (OVS), allowing users to segment
novel classes and concepts unseen during training of the segmentation model.
However, this flexibility comes with a trade-off: fully-supervised closed-set
methods still outperform OVS methods on base classes, that is on classes on
which they have been explicitly trained. This is due to the lack of
pixel-aligned training masks for VLMs (which are trained on image-caption
pairs), and the absence of domain-specific knowledge, such as autonomous
driving. Therefore, we propose the task of open-vocabulary domain adaptation to
infuse domain-specific knowledge into VLMs while preserving their
open-vocabulary nature. By doing so, we achieve improved performance in base
and novel classes. Existing VLM adaptation methods improve performance on base
(training) queries, but fail to fully preserve the open-set capabilities of
VLMs on novel queries. To address this shortcoming, we combine
parameter-efficient prompt tuning with a triplet-loss-based training strategy
that uses auxiliary negative queries. Notably, our approach is the only
parameter-efficient method that consistently surpasses the original VLM on
novel classes. Our adapted VLMs can seamlessly be integrated into existing OVS
pipelines, e.g., improving OVSeg by +6.0% mIoU on ADE20K for open-vocabulary 2D
segmentation, and OpenMask3D by +4.1% AP on ScanNet++ Offices for
open-vocabulary 3D instance segmentation without other changes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tri-Cam: Practical Eye Gaze Tracking via Camera Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19554v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19554v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sikai Yang, Wan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As human eyes serve as conduits of rich information, unveiling emotions,
intentions, and even aspects of an individual's health and overall well-being,
gaze tracking also enables various human-computer interaction applications, as
well as insights in psychological and medical research. However, existing gaze
tracking solutions fall short at handling free user movement, and also require
laborious user effort in system calibration. We introduce Tri-Cam, a practical
deep learning-based gaze tracking system using three affordable RGB webcams. It
features a split network structure for efficient training, as well as
designated network designs to handle the separated gaze tracking tasks. Tri-Cam
is also equipped with an implicit calibration module, which makes use of mouse
click opportunities to reduce calibration overhead on the user's end. We
evaluate Tri-Cam against Tobii, the state-of-the-art commercial eye tracker,
achieving comparable accuracy, while supporting a wider free movement area. In
conclusion, Tri-Cam provides a user-friendly, affordable, and robust gaze
tracking solution that could practically enable various applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ See Where You Read with Eye Gaze Tracking and Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19454v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19454v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sikai Yang, Gang Yan, Wan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Losing track of reading progress during line switching can be frustrating.
Eye gaze tracking technology offers a potential solution by highlighting read
paragraphs, aiding users in avoiding wrong line switches. However, the gap
between gaze tracking accuracy (2-3 cm) and text line spacing (3-5 mm) makes
direct application impractical. Existing methods leverage the linear reading
pattern but fail during jump reading. This paper presents a reading tracking
and highlighting system that supports both linear and jump reading. Based on
experimental insights from the gaze nature study of 16 users, two gaze error
models are designed to enable both jump reading detection and relocation. The
system further leverages the large language model's contextual perception
capability in aiding reading tracking. A reading tracking domain-specific
line-gaze alignment opportunity is also exploited to enable dynamic and
frequent calibration of the gaze results. Controlled experiments demonstrate
reliable linear reading tracking, as well as 84% accuracy in tracking jump
reading. Furthermore, real field tests with 18 volunteers demonstrated the
system's effectiveness in tracking and highlighting read paragraphs, improving
reading efficiency, and enhancing user experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tables as Texts or Images: Evaluating the Table Reasoning Ability of
  LLMs and MLLMs <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12424v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12424v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naihao Deng, Zhenjie Sun, Ruiqi He, Aman Sikka, Yulong Chen, Lin Ma, Yue Zhang, Rada Mihalcea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the effectiveness of various LLMs in
interpreting tabular data through different prompting strategies and data
formats. Our analyses extend across six benchmarks for table-related tasks such
as question-answering and fact-checking. We introduce for the first time the
assessment of LLMs' performance on image-based table representations.
Specifically, we compare five text-based and three image-based table
representations, demonstrating the role of representation and prompting on LLM
performance. Our study provides insights into the effective use of LLMs on
table-related tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2024 Findings; Naihao and Zhenjie contributed equally
  to the project; Data available at:
  https://github.com/dnaihao/Tables-as-Texts-or-Images</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Surrogate<span class="highlight-title">Prompt</span>: Bypassing the Safety Filter of Text-to-Image Models via
  Substitution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14122v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14122v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongjie Ba, Jieming Zhong, Jiachen Lei, Peng Cheng, Qinglong Wang, Zhan Qin, Zhibo Wang, Kui Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advanced text-to-image models such as DALL$\cdot$E 2 and Midjourney possess
the capacity to generate highly realistic images, raising significant concerns
regarding the potential proliferation of unsafe content. This includes adult,
violent, or deceptive imagery of political figures. Despite claims of rigorous
safety mechanisms implemented in these models to restrict the generation of
not-safe-for-work (NSFW) content, we successfully devise and exhibit the first
prompt attacks on Midjourney, resulting in the production of abundant
photorealistic NSFW images. We reveal the fundamental principles of such prompt
attacks and suggest strategically substituting high-risk sections within a
suspect prompt to evade closed-source safety measures. Our novel framework,
SurrogatePrompt, systematically generates attack prompts, utilizing large
language models, image-to-text, and image-to-image modules to automate attack
prompt creation at scale. Evaluation results disclose an 88% success rate in
bypassing Midjourney's proprietary safety filter with our attack prompts,
leading to the generation of counterfeit images depicting political figures in
violent scenarios. Both subjective and objective assessments validate that the
images generated from our attack prompts present considerable safety hazards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the the 31st ACM Conference on Computer and
  Communications Security (CCS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GeoReasoner: Geo-localization with Reasoning in Street Views using a
  Large <span class="highlight-title">Vision-Language</span> Model <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18572v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18572v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Li, Yu Ye, Bingchuan Jiang, Wei Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work tackles the problem of geo-localization with a new paradigm using a
large vision-language model (LVLM) augmented with human inference knowledge. A
primary challenge here is the scarcity of data for training the LVLM - existing
street-view datasets often contain numerous low-quality images lacking visual
clues, and lack any reasoning inference. To address the data-quality issue, we
devise a CLIP-based network to quantify the degree of street-view images being
locatable, leading to the creation of a new dataset comprising highly locatable
street views. To enhance reasoning inference, we integrate external knowledge
obtained from real geo-localization games, tapping into valuable human
inference capabilities. The data are utilized to train GeoReasoner, which
undergoes fine-tuning through dedicated reasoning and location-tuning stages.
Qualitative and quantitative evaluations illustrate that GeoReasoner
outperforms counterpart LVLMs by more than 25% at country-level and 38% at
city-level geo-localization tasks, and surpasses StreetCLIP performance while
requiring fewer training resources. The data and code are available at
https://github.com/lingli1996/GeoReasoner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spiking GS: Towards High-Accuracy and Low-Cost Surface Reconstruction
  via Spiking Neuron-based Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07266v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07266v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixing Zhang, Zongrui Li, De Ma, Huajin Tang, Xudong Jiang, Qian Zheng, Gang Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting is capable of reconstructing 3D scenes in minutes.
Despite recent advances in improving surface reconstruction accuracy, the
reconstructed results still exhibit bias and suffer from inefficiency in
storage and training. This paper provides a different observation on the cause
of the inefficiency and the reconstruction bias, which is attributed to the
integration of the low-opacity parts (LOPs) of the generated Gaussians. We show
that LOPs consist of Gaussians with overall low-opacity (LOGs) and the
low-opacity tails (LOTs) of Gaussians. We propose Spiking GS to reduce such two
types of LOPs by integrating spiking neurons into the Gaussian Splatting
pipeline. Specifically, we introduce global and local full-precision
integrate-and-fire spiking neurons to the opacity and representation function
of flattened 3D Gaussians, respectively. Furthermore, we enhance the density
control strategy with spiking neurons' thresholds and a new criterion on the
scale of Gaussians. Our method can represent more accurate reconstructed
surfaces at a lower cost. The supplementary material and code are available at
https://github.com/zju-bmi-lab/SpikingGS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cefdet: Cognitive Effectiveness Network Based on Fuzzy Inference for
  Action Detection <span class="chip">ACM MM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05771v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05771v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Luo, Weina Fu, Shuai Liu, Saeed Anwar, Muhammad Saqib, Sambit Bakshi, Khan Muhammad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Action detection and understanding provide the foundation for the generation
and interaction of multimedia content. However, existing methods mainly focus
on constructing complex relational inference networks, overlooking the judgment
of detection effectiveness. Moreover, these methods frequently generate
detection results with cognitive abnormalities. To solve the above problems,
this study proposes a cognitive effectiveness network based on fuzzy inference
(Cefdet), which introduces the concept of "cognition-based detection" to
simulate human cognition. First, a fuzzy-driven cognitive effectiveness
evaluation module (FCM) is established to introduce fuzzy inference into action
detection. FCM is combined with human action features to simulate the
cognition-based detection process, which clearly locates the position of frames
with cognitive abnormalities. Then, a fuzzy cognitive update strategy (FCS) is
proposed based on the FCM, which utilizes fuzzy logic to re-detect the
cognition-based detection results and effectively update the results with
cognitive abnormalities. Experimental results demonstrate that Cefdet exhibits
superior performance against several mainstream algorithms on the public
datasets, validating its effectiveness and superiority. Code is available at
https://github.com/12sakura/Cefdet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted by ACM MM. If you find this work helpful,
  please consider citing our paper. Zhe Luo, Weina Fu, Shuai Liu, Saeed Anwar,
  Muhammad Saqib, Sambit Bakshi, Khan Muhammad (2024) Cefdet: Cognitive
  Effectiveness Network Based on Fuzzy Inference for Action Detection, 32nd ACM
  International Conference on Multimedia, online first, 10.1145/3664647.3681226</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Supply Chain Poisoning: Backdooring <span class="highlight-title">Pre-train</span>ed Models via
  Embedding Indistinguishability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang, Shangwei Guo, Jialing He, Hangcheng Liu, Tianwei Zhang, Tao Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained models (PTMs) are widely adopted across various downstream tasks
in the machine learning supply chain. Adopting untrustworthy PTMs introduces
significant security risks, where adversaries can poison the model supply chain
by embedding hidden malicious behaviors (backdoors) into PTMs. However,
existing backdoor attacks to PTMs can only achieve partially task-agnostic and
the embedded backdoors are easily erased during the fine-tuning process. This
makes it challenging for the backdoors to persist and propagate through the
supply chain. In this paper, we propose a novel and severer backdoor attack,
TransTroj, which enables the backdoors embedded in PTMs to efficiently transfer
in the model supply chain. In particular, we first formalize this attack as an
indistinguishability problem between poisoned and clean samples in the
embedding space. We decompose embedding indistinguishability into pre- and
post-indistinguishability, representing the similarity of the poisoned and
reference embeddings before and after the attack. Then, we propose a two-stage
optimization that separately optimizes triggers and victim PTMs to achieve
embedding indistinguishability. We evaluate TransTroj on four PTMs and six
downstream tasks. Experimental results show that our method significantly
outperforms SOTA task-agnostic backdoor attacks -- achieving nearly 100\%
attack success rate on most downstream tasks -- and demonstrates robustness
under various system settings. Our findings underscore the urgent need to
secure the model supply chain against such transferable backdoor attacks. The
code is available at https://github.com/haowang-cqu/TransTroj .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeeClear: Semantic Distillation Enhances Pixel Condensation for Video
  Super-Resolution <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05799v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05799v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Tang, Yao Zhao, Meiqin Liu, Chao Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based Video Super-Resolution (VSR) is renowned for generating
perceptually realistic videos, yet it grapples with maintaining detail
consistency across frames due to stochastic fluctuations. The traditional
approach of pixel-level alignment is ineffective for diffusion-processed frames
because of iterative disruptions. To overcome this, we introduce SeeClear--a
novel VSR framework leveraging conditional video generation, orchestrated by
instance-centric and channel-wise semantic controls. This framework integrates
a Semantic Distiller and a Pixel Condenser, which synergize to extract and
upscale semantic details from low-resolution frames. The Instance-Centric
Alignment Module (InCAM) utilizes video-clip-wise tokens to dynamically relate
pixels within and across frames, enhancing coherency. Additionally, the
Channel-wise Texture Aggregation Memory (CaTeGory) infuses extrinsic knowledge,
capitalizing on long-standing semantic textures. Our method also innovates the
blurring diffusion process with the ResShift mechanism, finely balancing
between sharpness and diffusion effects. Comprehensive experiments confirm our
framework's advantage over state-of-the-art diffusion-based VSR techniques. The
code is available: https://github.com/Tang1705/SeeClear-NeurIPS24.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Diffusion-based Xray2MRI Model: Generating Pseudo-MRI Volumes From one
  Single X-ray 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Wang, Rachid Jennane, Aladine Chetouani, Yung Hsin Chen, Fabian Bauer, Mohamed Jarraya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knee osteoarthritis (KOA) is a prevalent musculoskeletal disorder, and X-rays
are commonly used for its diagnosis due to their cost-effectiveness. Magnetic
Resonance Imaging (MRI), on the other hand, offers detailed soft tissue
visualization and has become a valuable supplementary diagnostic tool for KOA.
Unfortunately, the high cost and limited accessibility of MRI hinders its
widespread use, leaving many patients with KOA to rely solely on X-ray imaging.
In this study, we introduce a novel diffusion-based Xray2MRI model capable of
generating pseudo-MRI volumes from a single X-ray image. In addition to using
X-rays as conditional input, our model integrates target depth, KOA probability
distribution, and image intensity distribution modules to guide the synthesis
process, ensuring that the generated corresponding slices accurately correspond
to the anatomical structures. Experimental results demonstrate that by
integrating information from X-rays with additional input data, our proposed
approach is capable of generating pseudo-MRI sequences that approximate real
MRI scans. In addition, by increasing the number of inference steps, the model
achieves effective interpolation, which further improves the continuity and
smoothness of the generated MRI sequences, representing a promising first
attempt at cost-effective medical imaging solutions. This study is available on
https://zwang78.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UAV3D: A Large-scale 3D Perception Benchmark for Unmanned Aerial
  Vehicles <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11125v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11125v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Ye, Rajshekhar Sunderraman, Shihao Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned Aerial Vehicles (UAVs), equipped with cameras, are employed in
numerous applications, including aerial photography, surveillance, and
agriculture. In these applications, robust object detection and tracking are
essential for the effective deployment of UAVs. However, existing benchmarks
for UAV applications are mainly designed for traditional 2D perception tasks,
restricting the development of real-world applications that require a 3D
understanding of the environment. Furthermore, despite recent advancements in
single-UAV perception, limited views of a single UAV platform significantly
constrain its perception capabilities over long distances or in occluded areas.
To address these challenges, we introduce UAV3D, a benchmark designed to
advance research in both 3D and collaborative 3D perception tasks with UAVs.
UAV3D comprises 1,000 scenes, each of which has 20 frames with fully annotated
3D bounding boxes on vehicles. We provide the benchmark for four 3D perception
tasks: single-UAV 3D object detection, single-UAV object tracking,
collaborative-UAV 3D object detection, and collaborative-UAV object tracking.
Our dataset and code are available at
https://huiyegit.github.io/UAV3D_Benchmark/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RULE: Reliable <span class="highlight-title">Multimodal</span> RAG for Factuality in Medical <span class="highlight-title">Vision Language</span>
  Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, Huaxiu Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has
enhanced medical diagnosis. However, current Med-LVLMs frequently encounter
factual issues, often generating responses that do not align with established
medical facts. Retrieval-Augmented Generation (RAG), which utilizes external
knowledge, can improve the factual accuracy of these models but introduces two
major challenges. First, limited retrieved contexts might not cover all
necessary information, while excessive retrieval can introduce irrelevant and
inaccurate references, interfering with the model's generation. Second, in
cases where the model originally responds correctly, applying RAG can lead to
an over-reliance on retrieved contexts, resulting in incorrect answers. To
address these issues, we propose RULE, which consists of two components. First,
we introduce a provably effective strategy for controlling factuality risk
through the calibrated selection of the number of retrieved contexts. Second,
based on samples where over-reliance on retrieved contexts led to errors, we
curate a preference dataset to fine-tune the model, balancing its dependence on
inherent knowledge and retrieved contexts for generation. We demonstrate the
effectiveness of RULE on medical VQA and report generation tasks across three
datasets, achieving an average improvement of 47.4% in factual accuracy. We
publicly release our benchmark and code in
https://github.com/richard-peng-xia/RULE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InstructAny2Pix: Flexible Visual Editing via <span class="highlight-title">Multimodal</span> Instruction
  Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06738v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06738v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shufan Li, Harkanwar Singh, Aditya Grover
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to provide fine-grained control for generating and editing visual
imagery has profound implications for computer vision and its applications.
Previous works have explored extending controllability in two directions:
instruction tuning with text-based prompts and multi-modal conditioning.
However, these works make one or more unnatural assumptions on the number
and/or type of modality inputs used to express controllability. We propose
InstructAny2Pix, a flexible multi-modal instruction-following system that
enables users to edit an input image using instructions involving audio,
images, and text. InstructAny2Pix consists of three building blocks that
facilitate this capability: a multi-modal encoder that encodes different
modalities such as images and audio into a unified latent space, a diffusion
model that learns to decode representations in this latent space into images,
and a multi-modal LLM that can understand instructions involving multiple
images and audio pieces and generate a conditional embedding of the desired
output, which can be used by the diffusion decoder. Additionally, to facilitate
training efficiency and improve generation quality, we include an additional
refinement prior module that enhances the visual quality of LLM outputs. These
designs are critical to the performance of our system. We demonstrate that our
system can perform a series of novel instruction-guided editing tasks. The code
is available at https://github.com/jacklishufan/InstructAny2Pix.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06579v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06579v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Zhang, Yihao Quan, Chen Shen, Xiaosong Yuan, Shaotian Yan, Liang Xie, Wenxiao Wang, Chaochen Gu, Hao Tang, Jieping Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision Language Models (LVLMs) achieve great performance on
visual-language reasoning tasks, however, the black-box nature of LVLMs hinders
in-depth research on the reasoning mechanism. As all images need to be
converted into image tokens to fit the input format of large language models
(LLMs) along with natural language prompts, sequential visual representation is
essential to the performance of LVLMs, and the information flow analysis
approach can be an effective tool for determining interactions between these
representations. In this paper, we propose integrating attention analysis with
LLaVA-CAM, concretely, attention scores highlight relevant regions during
forward propagation, while LLaVA-CAM captures gradient changes through backward
propagation, revealing key image features. By exploring the information flow
from the perspective of visual representation contribution, we observe that it
tends to converge in shallow layers but diversify in deeper layers. To validate
our analysis, we conduct comprehensive experiments with truncation strategies
across various LVLMs for visual question answering and image captioning tasks,
and experimental results not only verify our hypothesis but also reveal a
consistent pattern of information flow convergence in the corresponding layers,
and the information flow cliff layer will be different due to different
contexts. The paper's source code can be accessed from
\url{https://github.com/zhangbaijin/From-Redundancy-to-Relevance}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ D-Net: Dynamic Large Kernel with Dynamic Feature Fusion for Volumetric
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10674v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10674v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Yang, Peijie Qiu, Yichi Zhang, Daniel S. Marcus, Aristeidis Sotiras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical transformers have achieved significant success in medical image
segmentation due to their large receptive field and capabilities of effectively
leveraging global long-range contextual information. Convolutional neural
networks (CNNs) can also deliver a large receptive field by using large
kernels, enabling them to achieve competitive performance with fewer model
parameters. However, CNNs incorporated with large convolutional kernels remain
constrained in adaptively capturing multi-scale features from organs with large
variations in shape and size due to the employment of fixed-sized kernels.
Additionally, they are unable to utilize global contextual information
efficiently. To address these limitations, we propose Dynamic Large Kernel
(DLK) and Dynamic Feature Fusion (DFF) modules. The DLK module employs multiple
large kernels with varying kernel sizes and dilation rates to capture
multi-scale features. Subsequently, a dynamic selection mechanism is utilized
to adaptively highlight the most important spatial features based on global
information. Additionally, the DFF module is proposed to adaptively fuse
multi-scale local feature maps based on their global information. We integrate
DLK and DFF in a hierarchical transformer architecture to develop a novel
architecture, termed D-Net. D-Net is able to effectively utilize a multi-scale
large receptive field and adaptively harness global contextual information.
Extensive experimental results demonstrate that D-Net outperforms other
state-of-the-art models in the two volumetric segmentation tasks, including
abdominal multi-organ segmentation and multi-modality brain tumor segmentation.
Our code is available at https://github.com/sotiraslab/DLK.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 8 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Degraded Polygons Raise Fundamental Questions of Neural Network
  Perception <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04955v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04955v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonard Tang, Dan Ley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is well-known that modern computer vision systems often exhibit behaviors
misaligned with those of humans: from adversarial attacks to image corruptions,
deep learning vision models suffer in a variety of settings that humans capably
handle. In light of these phenomena, here we introduce another, orthogonal
perspective studying the human-machine vision gap. We revisit the task of
recovering images under degradation, first introduced over 30 years ago in the
Recognition-by-Components theory of human vision. Specifically, we study the
performance and behavior of neural networks on the seemingly simple task of
classifying regular polygons at varying orders of degradation along their
perimeters. To this end, we implement the Automated Shape Recoverability Test
for rapidly generating large-scale datasets of perimeter-degraded regular
polygons, modernizing the historically manual creation of image recoverability
experiments. We then investigate the capacity of neural networks to recognize
and recover such degraded shapes when initialized with different priors.
Ultimately, we find that neural networks' behavior on this simple task
conflicts with human behavior, raising a fundamental question of the robustness
and learning capabilities of modern computer vision models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper to NeurIPS 2023 (Datasets & Benchmarks
  Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Depth-supervised NeRF: Fewer Views and Faster Training for Free 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.02791v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.02791v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangle Deng, Andrew Liu, Jun-Yan Zhu, Deva Ramanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting
incorrect geometries when given an insufficient number of input views. One
potential reason is that standard volumetric rendering does not enforce the
constraint that most of a scene's geometry consist of empty space and opaque
surfaces. We formalize the above assumption through DS-NeRF (Depth-supervised
Neural Radiance Fields), a loss for learning radiance fields that takes
advantage of readily-available depth supervision. We leverage the fact that
current NeRF pipelines require images with known camera poses that are
typically estimated by running structure-from-motion (SFM). Crucially, SFM also
produces sparse 3D points that can be used as "free" depth supervision during
training: we add a loss to encourage the distribution of a ray's terminating
depth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can
render better images given fewer training views while training 2-3x faster.
Further, we show that our loss is compatible with other recently proposed NeRF
methods, demonstrating that depth is a cheap and easily digestible supervisory
signal. And finally, we find that DS-NeRF can support other types of depth
supervision such as scanned depth sensors and RGB-D reconstruction outputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: http://www.cs.cmu.edu/~dsnerf/ GitHub:
  https://github.com/dunbar12138/DSNeRF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Exposure Attack on Diabetic Retinopathy Imagery Grading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2009.09231v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2009.09231v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yupeng Cheng, Qing Guo, Felix Juefei-Xu, Huazhu Fu, Shang-Wei Lin, Weisi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diabetic Retinopathy (DR) is a leading cause of vision loss around the world.
To help diagnose it, numerous cutting-edge works have built powerful deep
neural networks (DNNs) to automatically grade DR via retinal fundus images
(RFIs). However, RFIs are commonly affected by camera exposure issues that may
lead to incorrect grades. The mis-graded results can potentially pose high
risks to an aggravation of the condition. In this paper, we study this problem
from the viewpoint of adversarial attacks. We identify and introduce a novel
solution to an entirely new task, termed as adversarial exposure attack, which
is able to produce natural exposure images and mislead the state-of-the-art
DNNs. We validate our proposed method on a real-world public DR dataset with
three DNNs, e.g., ResNet50, MobileNet, and EfficientNet, demonstrating that our
method achieves high image quality and success rate in transferring the
attacks. Our method reveals the potential threats to DNN-based automatic DR
grading and would benefit the development of exposure-robust DR grading methods
in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preserving Cardiac Integrity: A Topology-Infused Approach to Whole Heart
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10551v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10551v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Zhang, Wenxue Guan, Xiaodan Xing, Guang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole heart segmentation (WHS) supports cardiovascular disease (CVD)
diagnosis, disease monitoring, treatment planning, and prognosis. Deep learning
has become the most widely used method for WHS applications in recent years.
However, segmentation of whole-heart structures faces numerous challenges
including heart shape variability during the cardiac cycle, clinical artifacts
like motion and poor contrast-to-noise ratio, domain shifts in multi-center
data, and the distinct modalities of CT and MRI. To address these limitations
and improve segmentation quality, this paper introduces a new
topology-preserving module that is integrated into deep neural networks. The
implementation achieves anatomically plausible segmentation by using learned
topology-preserving fields, which are based entirely on 3D convolution and are
therefore very effective for 3D voxel data. We incorporate natural constraints
between structures into the end-to-end training and enrich the feature
representation of the neural network. The effectiveness of the proposed method
is validated on an open-source medical heart dataset, specifically using the
WHS++ data. The results demonstrate that the architecture performs
exceptionally well, achieving a Dice coefficient of 0.939 during testing. This
indicates full topology preservation for individual structures and
significantly outperforms other baselines in preserving the overall scene
topology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Suitability of KANs for Computer Vision: A preliminary investigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Basim Azam, Naveed Akhtar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kolmogorov-Arnold Networks (KANs) introduce a paradigm of neural modeling
that implements learnable functions on the edges of the networks, diverging
from the traditional node-centric activations in neural networks. This work
assesses the applicability and efficacy of KANs in visual modeling, focusing on
fundamental recognition and segmentation tasks. We mainly analyze the
performance and efficiency of different network architectures built using KAN
concepts along with conventional building blocks of convolutional and linear
layers, enabling a comparative analysis with the conventional models. Our
findings are aimed at contributing to understanding the potential of KANs in
computer vision, highlighting both their strengths and areas for further
research. Our evaluation point toward the fact that while KAN-based
architectures perform in line with the original claims, it may often be
important to employ more complex functions on the network edges to retain the
performance advantage of KANs on more complex visual data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Performance of a GPU- and Time-Efficient Pseudo 3D Network for Magnetic
  Resonance Image Super-Resolution and Motion Artifact Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.14259v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.14259v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Li, Jianan Liu, Marianne Schell, Tao Huang, Arne Lauer, Katharina Schregel, Jessica Jesser, Dominik F Vollherbst, Martin Bendszus, Sabine Heiland, Tim Hilgenfeld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shortening acquisition time and reducing motion artifacts are the most
critical challenges in magnetic resonance imaging (MRI). Deep learning-based
image restoration has emerged as a promising solution capable of generating
high-resolution and motion-artifact-free MRI images from low-resolution images
acquired with shortened acquisition times or from motion-artifact-corrupted
images. To facilitate clinical integration, a time- and GPU-efficient network
with reliable accuracy is essential. In this study, we adopted a unified 2D
deep learning framework for pseudo-3D MRI image super-resolution reconstruction
(SRR) and motion artifact reduction (MAR). The optimal down-sampling factors to
optimize the acquisition time in SRR were identified. Training for MAR was
performed using publicly available in vivo data, employing a novel standardized
method to induce motion artifacts of varying severity in a controlled way. The
accuracy of the network was evaluated through a pixel-wise uncertainty map, and
performance was benchmarked against state-of-the-art methods. The results
demonstrated that the down-sampling factor of 1x1x2 for x2 acceleration and
2x2x2 for x4 acceleration was optimal. For SRR, the proposed TS-RCAN
outperformed the 3D networks of mDCSRN and ReCNN, with an improvement of more
than 0.01 in SSIM and 1.5 dB in PSNR while reducing GPU load by up to and
inference time by up to 90%. For MAR, TS-RCAN exceeded UNet's performance by up
to 0.014 in SSIM and 1.48 dB in PSNR. Additionally, TS-RCAN provided
uncertainty information, which can be used to estimate the quality of the
reconstructed images. TS-RCAN has potential use for SRR and MAR in the clinical
setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Duoduo <span class="highlight-title">CLIP</span>: Efficient 3D Understanding with Multi-View Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11579v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11579v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han-Hung Lee, Yiming Zhang, Angel X. Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Duoduo CLIP, a model for 3D representation learning that learns
shape encodings from multi-view images instead of point-clouds. The choice of
multi-view images allows us to leverage 2D priors from off-the-shelf CLIP
models to facilitate fine-tuning with 3D data. Our approach not only shows
better generalization compared to existing point cloud methods, but also
reduces GPU requirements and training time. In addition, the model is modified
with cross-view attention to leverage information across multiple frames of the
object which further boosts performance. Notably, our model is permutation
invariant to the order of multi-view images while being pose-free. Compared to
the current SOTA point cloud method that requires 480 A100 hours to train 1
billion model parameters we only require 57 A5000 hours and 87 million
parameters. Multi-view images also provide more flexibility including being
able to encode objects with a variable number of images, and performance scales
when more views are used. In contrast, point cloud based methods require an
entire scan or model of the object. We showcase this flexibility with
benchmarks from images of real-world objects. Our model also achieves better
performance in more fine-grained text to shape retrieval, demonstrating better
text-and-shape alignment than point cloud based models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimating Body and Hand Motion in an Ego-sensed World 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brent Yi, Vickie Ye, Maya Zheng, Lea Müller, Georgios Pavlakos, Yi Ma, Jitendra Malik, Angjoo Kanazawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present EgoAllo, a system for human motion estimation from a head-mounted
device. Using only egocentric SLAM poses and images, EgoAllo guides sampling
from a conditional diffusion model to estimate 3D body pose, height, and hand
parameters that capture the wearer's actions in the allocentric coordinate
frame of the scene. To achieve this, our key insight is in representation: we
propose spatial and temporal invariance criteria for improving model
performance, from which we derive a head motion conditioning parameterization
that improves estimation by up to 18%. We also show how the bodies estimated by
our system can improve the hands: the resulting kinematic and temporal
constraints result in over 40% lower hand estimation errors compared to noisy
monocular estimates. Project page: https://egoallo.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: fixed figures for Safari, typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Anatomical Labeling of Pulmonary Tree Structures via Deep
  Point-Graph Representation-based Implicit Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17329v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17329v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangxian Xie, Jiancheng Yang, Donglai Wei, Ziqiao Weng, Pascal Fua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pulmonary diseases rank prominently among the principal causes of death
worldwide. Curing them will require, among other things, a better understanding
of the complex 3D tree-shaped structures within the pulmonary system, such as
airways, arteries, and veins. Traditional approaches using high-resolution
image stacks and standard CNNs on dense voxel grids face challenges in
computational efficiency, limited resolution, local context, and inadequate
preservation of shape topology. Our method addresses these issues by shifting
from dense voxel to sparse point representation, offering better memory
efficiency and global context utilization. However, the inherent sparsity in
point representation can lead to a loss of crucial connectivity in tree-shaped
structures. To mitigate this, we introduce graph learning on skeletonized
structures, incorporating differentiable feature fusion for improved topology
and long-distance context capture. Furthermore, we employ an implicit function
for efficient conversion of sparse representations into dense reconstructions
end-to-end. The proposed method not only delivers state-of-the-art performance
in labeling accuracy, both overall and at key locations, but also enables
efficient inference and the generation of closed surface shapes. Addressing
data scarcity in this field, we have also curated a comprehensive dataset to
validate our approach. Data and code are available at
\url{https://github.com/M3DV/pulmonary-tree-labeling}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Medical Image Analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CYCLO: Cyclic Graph Transformer Approach to Multi-Object Relationship
  Modeling in Aerial Videos <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01029v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01029v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trong-Thuan Nguyen, Pha Nguyen, Xin Li, Jackson Cothren, Alper Yilmaz, Khoa Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video scene graph generation (VidSGG) has emerged as a transformative
approach to capturing and interpreting the intricate relationships among
objects and their temporal dynamics in video sequences. In this paper, we
introduce the new AeroEye dataset that focuses on multi-object relationship
modeling in aerial videos. Our AeroEye dataset features various drone scenes
and includes a visually comprehensive and precise collection of predicates that
capture the intricate relationships and spatial arrangements among objects. To
this end, we propose the novel Cyclic Graph Transformer (CYCLO) approach that
allows the model to capture both direct and long-range temporal dependencies by
continuously updating the history of interactions in a circular manner. The
proposed approach also allows one to handle sequences with inherent cyclical
patterns and process object relationships in the correct sequential order.
Therefore, it can effectively capture periodic and overlapping relationships
while minimizing information loss. The extensive experiments on the AeroEye
dataset demonstrate the effectiveness of the proposed CYCLO model,
demonstrating its potential to perform scene understanding on drone videos.
Finally, the CYCLO method consistently achieves State-of-the-Art (SOTA) results
on two in-the-wild scene graph generation benchmarks, i.e., PVSG and ASPIRe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">37</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-Aware Query Expansion with Large Language Models for Textual
  and Relational Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Xia, Junda Wu, Sungchul Kim, Tong Yu, Ryan A. Rossi, Haoliang Wang, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been used to generate query expansions
augmenting original queries for improving information search. Recent studies
also explore providing LLMs with initial retrieval results to generate query
expansions more grounded to document corpus. However, these methods mostly
focus on enhancing textual similarities between search queries and target
documents, overlooking document relations. For queries like "Find me a highly
rated camera for wildlife photography compatible with my Nikon F-Mount lenses",
existing methods may generate expansions that are semantically similar but
structurally unrelated to user intents. To handle such semi-structured queries
with both textual and relational requirements, in this paper we propose a
knowledge-aware query expansion framework, augmenting LLMs with structured
document relations from knowledge graph (KG). To further address the limitation
of entity-based scoring in existing KG-based methods, we leverage document
texts as rich KG node representations and use document-based relation filtering
for our Knowledge-Aware Retrieval (KAR). Extensive experiments on three
datasets of diverse domains show the advantages of our method compared against
state-of-the-art baselines on textual and relational semi-structured retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disjointness Violations in Wikidata 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ege Atacan Doğan, Peter F. Patel-Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disjointness checks are among the most important constraint checks in a
knowledge base and can be used to help detect and correct incorrect statements
and internal contradictions. Wikidata is a very large, community-managed
knowledge base. Because of both its size and construction, Wikidata contains
many incorrect statements and internal contradictions. We analyze the current
modeling of disjointness on Wikidata, identify patterns that cause these
disjointness violations and categorize them. We use SPARQL queries to identify
each ``culprit'' causing a disjointness violation and lay out formulas to
identify and fix conflicting information. We finally discuss how disjointness
information could be better modeled and expanded in Wikidata in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Sixth International Knowledge Graph and Semantic Web Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pessimistic Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Diaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional evaluation of information access systems has focused primarily on
average utility across a set of information needs (information retrieval) or
users (recommender systems). In this work, we argue that evaluating only with
average metric measurements assumes utilitarian values not aligned with
traditions of information access based on equal access. We advocate for
pessimistic evaluation of information access systems focusing on worst case
utility. These methods are (a) grounded in ethical and pragmatic concepts, (b)
theoretically complementary to existing robustness and fairness methods, and
(c) empirically validated across a set of retrieval and recommendation tasks.
These results suggest that pessimistic evaluation should be included in
existing experimentation processes to better understand the behavior of
systems, especially when concerned with principles of social good.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models as Narrative-Driven Recommenders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Eberhard, Thorsten Ruprechter, Denis Helic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Narrative-driven recommenders aim to provide personalized suggestions for
user requests expressed in free-form text such as "I want to watch a thriller
with a mind-bending story, like Shutter Island." Although large language models
(LLMs) have been shown to excel in processing general natural language queries,
their effectiveness for handling such recommendation requests remains
relatively unexplored. To close this gap, we compare the performance of 38
open- and closed-source LLMs of various sizes, such as LLama 3.2 and GPT-4o, in
a movie recommendation setting. For this, we utilize a gold-standard,
crowdworker-annotated dataset of posts from reddit's movie suggestion community
and employ various prompting strategies, including zero-shot, identity, and
few-shot prompting. Our findings demonstrate the ability of LLMs to generate
contextually relevant movie recommendations, significantly outperforming other
state-of-the-art approaches, such as doc2vec. While we find that closed-source
and large-parameterized models generally perform best, medium-sized open-source
models remain competitive, being only slightly outperformed by their more
computationally expensive counterparts. Furthermore, we observe no significant
differences across prompting strategies for most models, underscoring the
effectiveness of simple approaches such as zero-shot prompting for
narrative-driven recommendations. Overall, this work offers valuable insights
for recommender system researchers as well as practitioners aiming to integrate
LLMs into real-world recommendation tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review; 19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Domain Sequential Recommendation via Neural Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haipeng Li, Jiangxia Cao, Yiwen Gao, Yunhuai Liu, Shuchao Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-Domain Sequential Recommendation (CDSR) is a hot topic in
sequence-based user interest modeling, which aims at utilizing a single model
to predict the next items for different domains. To tackle the CDSR, many
methods are focused on domain overlapped users' behaviors fitting, which
heavily relies on the same user's different-domain item sequences collaborating
signals to capture the synergy of cross-domain item-item correlation. Indeed,
these overlapped users occupy a small fraction of the entire user set only,
which introduces a strong assumption that the small group of domain overlapped
users is enough to represent all domain user behavior characteristics. However,
intuitively, such a suggestion is biased, and the insufficient learning
paradigm in non-overlapped users will inevitably limit model performance.
Further, it is not trivial to model non-overlapped user behaviors in CDSR
because there are no other domain behaviors to collaborate with, which causes
the observed single-domain users' behavior sequences to be hard to contribute
to cross-domain knowledge mining. Considering such a phenomenon, we raise a
challenging and unexplored question: How to unleash the potential of
non-overlapped users' behaviors to empower CDSR?
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generate and Instantiate What You Prefer: Text-Guided Diffusion for
  Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoqing Hu, Zhangyi Yang, Zhibo Cai, An Zhang, Xiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in generative recommendation systems, particularly in the
realm of sequential recommendation tasks, have shown promise in enhancing
generalization to new items. Among these approaches, diffusion-based generative
recommendation has emerged as an effective tool, leveraging its ability to
capture data distributions and generate high-quality samples. Despite
effectiveness, two primary challenges have been identified: 1) the lack of
consistent modeling of data distribution for oracle items; and 2) the
difficulty in scaling to more informative control signals beyond historical
interactions. These issues stem from the uninformative nature of ID embeddings,
which necessitate random initialization and limit the incorporation of
additional control signals. To address these limitations, we propose iDreamRe }
to involve more concrete prior knowledge to establish item embeddings,
particularly through detailed item text descriptions and advanced Text
Embedding Models (TEM). More importantly, by converting item descriptions into
embeddings aligned with TEM, we enable the integration of intention
instructions as control signals to guide the generation of oracle items.
Experimental results on four datasets demonstrate that iDreamRec not only
outperforms existing diffusion-based generative recommenders but also
facilitates the incorporation of intention instructions for more precise and
effective recommendation generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-aware adaptive personalised recommendation: a meta-hybrid 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Tibensky, Michal Kompan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommenders take place on a wide scale of e-commerce systems, reducing the
problem of information overload. The most common approach is to choose a
recommender used by the system to make predictions. However, users vary from
each other; thus, a one-fits-all approach seems to be sub-optimal. In this
paper, we propose a meta-hybrid recommender that uses machine learning to
predict an optimal algorithm. In this way, the best-performing recommender is
used for each specific session and user. This selection depends on contextual
and preferential information collected about the user. We use standard
MovieLens and The Movie DB datasets for offline evaluation. We show that based
on the proposed model, it is possible to predict which recommender will provide
the most precise recommendations to a user. The theoretical performance of our
meta-hybrid outperforms separate approaches by 20-50% in normalized Discounted
Gain and Root Mean Square Error metrics. However, it is hard to obtain the
optimal performance based on widely-used standard information stored about
users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing the Utility, Preference, and Performance of Course Material
  Search Functionality and Retrieval-Augmented Generation Large Language Model
  (RAG-LLM) AI Chatbots in Information-Seeking Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Pasquarelli, Charles Koutcheme, Arto Hellas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Providing sufficient support for students requires substantial resources,
especially considering the growing enrollment numbers. Students need help in a
variety of tasks, ranging from information-seeking to requiring support with
course assignments. To explore the utility of recent large language models
(LLMs) as a support mechanism, we developed an LLM-powered AI chatbot that
augments the answers that are produced with information from the course
materials. To study the effect of the LLM-powered AI chatbot, we conducted a
lab-based user study (N=14), in which the participants worked on tasks from a
web software development course. The participants were divided into two groups,
where one of the groups first had access to the chatbot and then to a more
traditional search functionality, while another group started with the search
functionality and was then given the chatbot. We assessed the participants'
performance and perceptions towards the chatbot and the search functionality
and explored their preferences towards the support functionalities. Our
findings highlight that both support mechanisms are seen as useful and that
support mechanisms work well for specific tasks, while less so for other tasks.
We also observe that students tended to prefer the second support mechanism
more, where students who were first given the chatbot tended to prefer the
search functionality and vice versa.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SBI-RAG: Enhancing Math Word Problem Solving for Students through
  Schema-Based Instruction and Retrieval-Augmented Generation <span class="chip">NeurIPS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prakhar Dixit, Tim Oates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many students struggle with math word problems (MWPs), often finding it
difficult to identify key information and select the appropriate mathematical
operations.Schema-based instruction (SBI) is an evidence-based strategy that
helps students categorize problems based on their structure, improving
problem-solving accuracy. Building on this, we propose a Schema-Based
Instruction Retrieval-Augmented Generation (SBI-RAG) framework that
incorporates a large language model (LLM).Our approach emphasizes step-by-step
reasoning by leveraging schemas to guide solution generation. We evaluate its
performance on the GSM8K dataset, comparing it with GPT-4 and GPT-3.5 Turbo,
and introduce a "reasoning score" metric to assess solution quality. Our
findings suggest that SBI-RAG enhances reasoning clarity and problem-solving
accuracy, potentially providing educational benefits for students
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 4th MATH-AI Workshop at NeurIPS'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling Likes and Dislikes in Personalized Generative Explainable
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryotaro Shimizu, Takashi Wada, Yu Wang, Johannes Kruse, Sean O'Brien, Sai HtaungKham, Linxin Song, Yuya Yoshikawa, Yuki Saito, Fugee Tsung, Masayuki Goto, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research on explainable recommendation generally frames the task as a
standard text generation problem, and evaluates models simply based on the
textual similarity between the predicted and ground-truth explanations.
However, this approach fails to consider one crucial aspect of the systems:
whether their outputs accurately reflect the users' (post-purchase) sentiments,
i.e., whether and why they would like and/or dislike the recommended items. To
shed light on this issue, we introduce new datasets and evaluation methods that
focus on the users' sentiments. Specifically, we construct the datasets by
explicitly extracting users' positive and negative opinions from their
post-purchase reviews using an LLM, and propose to evaluate systems based on
whether the generated explanations 1) align well with the users' sentiments,
and 2) accurately identify both positive and negative opinions of users on the
target items. We benchmark several recent models on our datasets and
demonstrate that achieving strong performance on existing metrics does not
ensure that the generated explanations align well with the users' sentiments.
Lastly, we find that existing models can provide more sentiment-aware
explanations when the users' (predicted) ratings for the target items are
directly fed into the models as input. We will release our code and datasets
upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Starbucks: Improved Training for 2D Matryoshka Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyao Zhuang, Shuai Wang, Bevan Koopman, Guido Zuccon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective approaches that can scale embedding model depth (i.e. layers) and
embedding size allow for the creation of models that are highly scalable across
different computational resources and task requirements. While the recently
proposed 2D Matryoshka training approach can efficiently produce a single
embedding model such that its sub-layers and sub-dimensions can measure text
similarity, its effectiveness is significantly worse than if smaller models
were trained separately. To address this issue, we propose Starbucks, a new
training strategy for Matryoshka-like embedding models, which encompasses both
the fine-tuning and pre-training phases. For the fine-tuning phase, we discover
that, rather than sampling a random sub-layer and sub-dimensions for each
training steps, providing a fixed list of layer-dimension pairs, from small
size to large sizes, and computing the loss across all pairs significantly
improves the effectiveness of 2D Matryoshka embedding models, bringing them on
par with their separately trained counterparts. To further enhance performance,
we introduce a new pre-training strategy, which applies masked autoencoder
language modelling to sub-layers and sub-dimensions during pre-training,
resulting in a stronger backbone for subsequent fine-tuning of the embedding
model. Experimental results on both semantic text similarity and retrieval
benchmarks demonstrate that the proposed pre-training and fine-tuning
strategies significantly improved the effectiveness over 2D Matryoshka models,
enabling Starbucks models to perform more efficiently and effectively than
separately trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Research on Travel Route Planing Problems Based on Greedy Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiquan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The greedy algorithm based route planning problem is a method of finding the
optimal or near optimal route between a given starting and ending point. This
article first uses PCA method to reduce the dimensionality of urban evaluation
indicators, extracts key principal components, and KMO and TOPSIS algorithms to
reduce the dimensionality of the data. Secondly, for datasets that have not
passed the KMO test, a comprehensive evaluation will be conducted using the
entropy weight method and TOPSIS method. Finally, based on the greedy
algorithm, a route planning algorithm was proposed and optimized to provide
personalized route customization according to the different needs of tourists.
We also took into account the local travel efficiency, the time required to
visit tourist attractions, and necessary daily rest time to reduce costs and
avoid falling into the local optimal solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MixEHR-Nest: Identifying Subphenotypes within Electronic Health Records
  through Hierarchical Guided-Topic Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruohan Wang, Zilong Wang, Ziyang Song, David Buckeridge, Yue Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic subphenotyping from electronic health records (EHRs)provides
numerous opportunities to understand diseases with unique subgroups and enhance
personalized medicine for patients. However, existing machine learning
algorithms either focus on specific diseases for better interpretability or
produce coarse-grained phenotype topics without considering nuanced disease
patterns. In this study, we propose a guided topic model, MixEHR-Nest, to infer
sub-phenotype topics from thousands of disease using multi-modal EHR data.
Specifically, MixEHR-Nest detects multiple subtopics from each phenotype topic,
whose prior is guided by the expert-curated phenotype concepts such as
Phenotype Codes (PheCodes) or Clinical Classification Software (CCS) codes. We
evaluated MixEHR-Nest on two EHR datasets: (1) the MIMIC-III dataset consisting
of over 38 thousand patients from intensive care unit (ICU) from Beth Israel
Deaconess Medical Center (BIDMC) in Boston, USA; (2) the healthcare
administrative database PopHR, comprising 1.3 million patients from Montreal,
Canada. Experimental results demonstrate that MixEHR-Nest can identify
subphenotypes with distinct patterns within each phenotype, which are
predictive for disease progression and severity. Consequently, MixEHR-Nest
distinguishes between type 1 and type 2 diabetes by inferring subphenotypes
using CCS codes, which do not differentiate these two subtype concepts.
Additionally, MixEHR-Nest not only improved the prediction accuracy of
short-term mortality of ICU patients and initial insulin treatment in diabetic
patients but also revealed the contributions of subphenotypes. For longitudinal
analysis, MixEHR-Nest identified subphenotypes of distinct age prevalence under
the same phenotypes, such as asthma, leukemia, epilepsy, and depression. The
MixEHR-Nest software is available at GitHub:
https://github.com/li-lab-mcgill/MixEHR-Nest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transformers4NewsRec: A Transformer-based News Recommendation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dairui Liu, Honghui Du, Boming Yang, Neil Hurley, Aonghus Lawlor, Irene Li, Derek Greene, Ruihai Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained transformer models have shown great promise in various natural
language processing tasks, including personalized news recommendations. To
harness the power of these models, we introduce Transformers4NewsRec, a new
Python framework built on the \textbf{Transformers} library. This framework is
designed to unify and compare the performance of various news recommendation
models, including deep neural networks and graph-based models.
Transformers4NewsRec offers flexibility in terms of model selection, data
preprocessing, and evaluation, allowing both quantitative and qualitative
analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-Enhanced Named Entity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enzo Shiraishi, Raphael Y. de Camargo, Henrique L. P. Silva, Ronaldo C. Prati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When combined with In-Context Learning, a technique that enables models to
adapt to new tasks by incorporating task-specific examples or demonstrations
directly within the input prompt, autoregressive language models have achieved
good performance in a wide range of tasks and applications. However, this
combination has not been properly explored in the context of named entity
recognition, where the structure of this task poses unique challenges. We
propose RENER (Retrieval-Enhanced Named Entity Recognition), a technique for
named entity recognition using autoregressive language models based on
In-Context Learning and information retrieval techniques. When presented with
an input text, RENER fetches similar examples from a dataset of training
examples that are used to enhance a language model to recognize named entities
from this input text. RENER is modular and independent of the underlying
language model and information retrieval algorithms. Experimental results show
that in the CrossNER collection we achieve state-of-the-art performance with
the proposed technique and that information retrieval can increase the F-score
by up to 11 percentage points.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preference Diffusion for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Liu, An Zhang, Guoqing Hu, Hong Qian, Tat-seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems predict personalized item rankings based on user
preference distributions derived from historical behavior data. Recently,
diffusion models (DMs) have gained attention in recommendation for their
ability to model complex distributions, yet current DM-based recommenders often
rely on traditional objectives like mean squared error (MSE) or recommendation
objectives, which are not optimized for personalized ranking tasks or fail to
fully leverage DM's generative potential. To address this, we propose
PreferDiff, a tailored optimization objective for DM-based recommenders.
PreferDiff transforms BPR into a log-likelihood ranking objective and
integrates multiple negative samples to better capture user preferences.
Specifically, we employ variational inference to handle the intractability
through minimizing the variational upper bound and replaces MSE with cosine
error to improve alignment with recommendation tasks. Finally, we balance
learning generation and preference to enhance the training stability of DMs.
PreferDiff offers three key benefits: it is the first personalized ranking loss
designed specifically for DM-based recommenders and it improves ranking and
faster convergence by addressing hard negatives. We also prove that it is
theoretically connected to Direct Preference Optimization which indicates that
it has the potential to align user preferences in DM-based recommenders via
generative modeling. Extensive experiments across three benchmarks validate its
superior recommendation performance and commendable general sequential
recommendation capabilities. Our codes are available at
\url{https://github.com/lswhim/PreferDiff}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight Correlation-Aware Table Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihail Stoian, Alexander van Renen, Jan Kobiolka, Ping-Lin Kuo, Josif Grabocka, Andreas Kipf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing adoption of data lakes for managing relational data necessitates
efficient, open storage formats that provide high scan performance and
competitive compression ratios. While existing formats achieve fast scans
through lightweight encoding techniques, they have reached a plateau in terms
of minimizing storage footprint. Recently, correlation-aware compression
schemes have been shown to reduce file sizes further. Yet, current approaches
either incur significant scan overheads or require manual specification of
correlations, limiting their practicability. We present $\texttt{Virtual}$, a
framework that integrates seamlessly with existing open formats to
automatically leverage data correlations, achieving substantial compression
gains while having minimal scan performance overhead. Experiments on
$\texttt{data.gov}$ datasets show that $\texttt{Virtual}$ reduces file sizes by
up to 40% compared to Apache Parquet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Third Table Representation Learning Workshop (TRL 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Best in Tau@LLMJudge: Criteria-Based Relevance Evaluation with Llama3 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naghmeh Farzi, Laura Dietz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional evaluation of information retrieval (IR) systems relies on
human-annotated relevance labels, which can be both biased and costly at scale.
In this context, large language models (LLMs) offer an alternative by allowing
us to directly prompt them to assign relevance labels for passages associated
with each query. In this study, we explore alternative methods to directly
prompt LLMs for assigned relevance labels, by exploring two hypotheses:
  Hypothesis 1 assumes that it is helpful to break down "relevance" into
specific criteria - exactness, coverage, topicality, and contextual fit. We
explore different approaches that prompt large language models (LLMs) to obtain
criteria-level grades for all passages, and we consider various ways to
aggregate criteria-level grades into a relevance label. Hypothesis 2 assumes
that differences in linguistic style between queries and passages may
negatively impact the automatic relevance label prediction. We explore whether
improvements can be achieved by first synthesizing a summary of the passage in
the linguistic style of a query, and then using this summary in place of the
passage to assess its relevance.
  We include an empirical evaluation of our approaches based on data from the
LLMJudge challenge run in Summer 2024, where our "Four Prompts" approach
obtained the highest scores in Kendall's tau.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Retrieval of Temporal Event Sequences from Textual
  Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zefang Liu, Yinzhu Quan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieving temporal event sequences from textual descriptions is essential
for applications such as analyzing e-commerce behavior, monitoring social media
activities, and tracking criminal incidents. In this paper, we introduce
TPP-LLM-Embedding, a unified model for efficiently embedding and retrieving
event sequences based on natural language descriptions. Built on the TPP-LLM
framework, which integrates large language models with temporal point
processes, our model encodes both event types and times, generating a
sequence-level representation through pooling. Textual descriptions are
embedded using the same architecture, ensuring a shared embedding space for
both sequences and descriptions. We optimize a contrastive loss based on
similarity between these embeddings, bringing matching pairs closer and
separating non-matching ones. TPP-LLM-Embedding enables efficient retrieval and
demonstrates superior performance compared to baseline models across diverse
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FinQAPT: Empowering Financial Decisions with End-to-End LLM-driven
  Question Answering Pipeline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuldeep Singh, Simerjot Kaur, Charese Smiley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial decision-making hinges on the analysis of relevant information
embedded in the enormous volume of documents in the financial domain. To
address this challenge, we developed FinQAPT, an end-to-end pipeline that
streamlines the identification of relevant financial reports based on a query,
extracts pertinent context, and leverages Large Language Models (LLMs) to
perform downstream tasks. To evaluate the pipeline, we experimented with
various techniques to optimize the performance of each module using the FinQA
dataset. We introduced a novel clustering-based negative sampling technique to
enhance context extraction and a novel prompting method called Dynamic N-shot
Prompting to boost the numerical question-answering capabilities of LLMs. At
the module level, we achieved state-of-the-art accuracy on FinQA, attaining an
accuracy of 80.6\%. However, at the pipeline level, we observed decreased
performance due to challenges in extracting relevant context from financial
reports. We conducted a detailed error analysis of each module and the
end-to-end pipeline, pinpointing specific challenges that must be addressed to
develop a robust solution for handling complex financial tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICAIF 2024, 8 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying High Consideration E-Commerce Search Queries <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyu Chen, Jason Choi, Besnik Fetahu, Shervin Malmasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In e-commerce, high consideration search missions typically require careful
and elaborate decision making, and involve a substantial research investment
from customers. We consider the task of identifying High Consideration (HC)
queries. Identifying such queries enables e-commerce sites to better serve user
needs using targeted experiences such as curated QA widgets that help users
reach purchase decisions. We explore the task by proposing an Engagement-based
Query Ranking (EQR) approach, focusing on query ranking to indicate potential
engagement levels with query-related shopping knowledge content during product
search. Unlike previous studies on predicting trends, EQR prioritizes
query-level features related to customer behavior, finance, and catalog
information rather than popularity signals. We introduce an accurate and
scalable method for EQR and present experimental results demonstrating its
effectiveness. Offline experiments show strong ranking performance. Human
evaluation shows a precision of 96% for HC queries identified by our model. The
model was commercially deployed, and shown to outperform human-selected queries
in terms of downstream customer impact, as measured through engagement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 (Industry Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixed-Precision Embeddings for Large-Scale Recommendation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20305v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20305v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiwei Li, Zhuoqi Hu, Xing Tang, Haozhao Wang, Shijie Xu, Weihong Luo, Yuhua Li, Xiuqiang He, Ruixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding techniques have become essential components of large databases in
the deep learning era. By encoding discrete entities, such as words, items, or
graph nodes, into continuous vector spaces, embeddings facilitate more
efficient storage, retrieval, and processing in large databases. Especially in
the domain of recommender systems, millions of categorical features are encoded
as unique embedding vectors, which facilitates the modeling of similarities and
interactions among features. However, numerous embedding vectors can result in
significant storage overhead. In this paper, we aim to compress the embedding
table through quantization techniques. Given that features vary in importance
levels, we seek to identify an appropriate precision for each feature to
balance model accuracy and memory usage. To this end, we propose a novel
embedding compression method, termed Mixed-Precision Embeddings (MPE).
Specifically, to reduce the size of the search space, we first group features
by frequency and then search precision for each feature group. MPE further
learns the probability distribution over precision levels for each feature
group, which can be used to identify the most suitable precision with a
specially designed sampling strategy. Extensive experiments on three public
datasets demonstrate that MPE significantly outperforms existing embedding
compression methods. Remarkably, MPE achieves about 200x compression on the
Criteo dataset without comprising the prediction accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under submision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Survey on Intent-aware Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16350v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16350v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dietmar Jannach, Markus Zanker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many modern online services feature personalized recommendations. A central
challenge when providing such recommendations is that the reason why an
individual user accesses the service may change from visit to visit or even
during an ongoing usage session. To be effective, a recommender system should
therefore aim to take the users' probable intent of using the service at a
certain point in time into account. In recent years, researchers have thus
started to address this challenge by incorporating intent-awareness into
recommender systems. Correspondingly, a number of technical approaches were put
forward, including diversification techniques, intent prediction models or
latent intent modeling approaches. In this paper, we survey and categorize
existing approaches to building the next generation of Intent-Aware Recommender
Systems (IARS). Based on an analysis of current evaluation practices, we
outline open gaps and possible future directions in this area, which in
particular include the consideration of additional interaction signals and
contextual information to further improve the effectiveness of such systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chatbot-Based Ontology Interaction Using Large Language Models and
  Domain-Specific Standards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00800v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00800v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Reif, Tom Jeleniewski, Milapji Singh Gill, Felix Gehlhoff, Alexander Fay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The following contribution introduces a concept that employs Large Language
Models (LLMs) and a chatbot interface to enhance SPARQL query generation for
ontologies, thereby facilitating intuitive access to formalized knowledge.
Utilizing natural language inputs, the system converts user inquiries into
accurate SPARQL queries that strictly query the factual content of the
ontology, effectively preventing misinformation or fabrication by the LLM. To
enhance the quality and precision of outcomes, additional textual information
from established domain-specific standards is integrated into the ontology for
precise descriptions of its concepts and relationships. An experimental study
assesses the accuracy of generated SPARQL queries, revealing significant
benefits of using LLMs for querying ontologies and highlighting areas for
future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 2024 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffATR: Diffusion-based Generative Modeling for Audio-Text Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10025v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10025v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Xin, Xuxin Cheng, Zhihong Zhu, Xusheng Yang, Yuexian Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing audio-text retrieval (ATR) methods are essentially discriminative
models that aim to maximize the conditional likelihood, represented as
p(candidates|query). Nevertheless, this methodology fails to consider the
intrinsic data distribution p(query), leading to difficulties in discerning
out-of-distribution data. In this work, we attempt to tackle this constraint
through a generative perspective and model the relationship between audio and
text as their joint probability p(candidates,query). To this end, we present a
diffusion-based ATR framework (DiffATR), which models ATR as an iterative
procedure that progressively generates joint distribution from noise.
Throughout its training phase, DiffATR is optimized from both generative and
discriminative viewpoints: the generator is refined through a generation loss,
while the feature extractor benefits from a contrastive loss, thus combining
the merits of both methodologies. Experiments on the AudioCaps and Clotho
datasets with superior performances, verify the effectiveness of our approach.
Notably, without any alterations, our DiffATR consistently exhibits strong
performance in out-of-domain retrieval settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Interspeech2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probability Distribution Learning: A theoretical framework for Deep
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05666v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05666v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binchuan Qi, Wei Gong, Li Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces probability distribution learning (PD learning), a
novel theoretical learning framework. Departing from the traditional
statistical learning framework, PD learning focuses on learning the underlying
probability distribution, which is modeled as a random variable within the
probability simplex. Within this framework, the learning error is decomposed
into uncertainty, estimation error, and the model's fitting error.
Subsequently, we present the methodology for calculating uncertainty, along
with optimization strategies for both estimation error and fitting error. Given
that minimizing the fitting error typically constitutes a non-convex
optimization problem, we introduce a standard loss function and the gradient
structural control (GSC) algorithm, and demonstrate that by employing this
function, the optima of fitting error minimization can be approached by
reducing the gradient norm and structural error. Furthermore, we apply the PD
learning framework to deep learning, elucidating the mechanisms by which
techniques such as random parameter initialization, over-parameterization,
bias-variance trade-off, and dropout influence deep model training. Finally,
experimental results on various models validate the effectiveness of the
proposed framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2105.04026 by other
  authors. arXiv admin note: text overlap with arXiv:2105.04026 by other
  authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NFT1000: A Cross-Modal Dataset for Non-Fungible Token Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16872v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16872v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuxun Wang, Yunfei Lei, Ziqi Zhang, Wei Liu, Haowei Liu, Li Yang, Wenjuan Li, Bing Li, Weiming Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of "Metaverse" and "Web 3.0", Non-Fungible Token (NFT) has
emerged as a kind of pivotal digital asset, garnering significant attention. By
the end of March 2024, more than 1.7 billion NFTs have been minted across
various blockchain platforms. To effectively locate a desired NFT, conducting
searches within a vast array of NFTs is essential. The challenge in NFT
retrieval is heightened due to the high degree of similarity among different
NFTs, regarding regional and semantic aspects. In this paper, we will introduce
a benchmark dataset named "NFT Top1000 Visual-Text Dataset" (NFT1000),
containing 7.56 million image-text pairs, and being collected from 1000 most
famous PFP1 NFT collections2 by sales volume on the Ethereum blockchain. Based
on this dataset and leveraging the CLIP series of pre-trained models as our
foundation, we propose the dynamic masking fine-tuning scheme. This innovative
approach results in a 7.4\% improvement in the top1 accuracy rate, while
utilizing merely 13\% of the total training data (0.79 million vs. 6.1
million). We also propose a robust metric Comprehensive Variance Index (CVI) to
assess the similarity and retrieval difficulty of visual-text pairs data. The
dataset will be released as an open-source resource. For more details, please
refer to: https://github.com/ShuxunoO/NFT-Net.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages,12figures to be published in ACM Multimedia 2024, see
  https://openreview.net/forum?id=xUtNrKH8iB&noteId=xUtNrKH8iB</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01262v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01262v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunlun Zhu, Yifan Luo, Dingling Xu, Ruobing Wang, Shi Yu, Shuo Wang, Yukun Yan, Zhenghao Liu, Xu Han, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) is a powerful approach that enables
large language models (LLMs) to incorporate external knowledge. However,
evaluating the effectiveness of RAG systems in specialized scenarios remains
challenging due to the high costs of data construction and the lack of suitable
evaluation metrics. This paper introduces RAGEval, a framework designed to
assess RAG systems across diverse scenarios by generating high-quality
documents, questions, answers, and references through a schema-based pipeline.
With a focus on factual accuracy, we propose three novel metrics Completeness,
Hallucination, and Irrelevance to rigorously evaluate LLM-generated responses.
Experimental results show that RAGEval outperforms zero-shot and one-shot
methods in terms of clarity, safety, conformity, and richness of generated
samples. Furthermore, the use of LLMs for scoring the proposed metrics
demonstrates a high level of consistency with human evaluations. RAGEval
establishes a new paradigm for evaluating RAG systems in real-world
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/OpenBMB/RAGEval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Theory for Token-Level Harmonization in Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00944v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00944v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance
large language models (LLMs). Studies show that while RAG provides valuable
external information (benefit), it may also mislead LLMs (detriment) with noisy
or incorrect retrieved texts. Although many existing methods attempt to
preserve benefit and avoid detriment, they lack a theoretical explanation for
RAG. The benefit and detriment in the next token prediction of RAG remain a
black box that cannot be quantified or compared in an explainable manner, so
existing methods are data-driven, need additional utility evaluators or
post-hoc. This paper takes the first step towards providing a theory to explain
and trade off the benefit and detriment in RAG. First, we model RAG as the
fusion between distribution of LLMs knowledge and distribution of retrieved
texts. Then, we formalize the trade-off between the value of external knowledge
(benefit) and its potential risk of misleading LLMs (detriment) in next token
prediction of RAG by distribution difference in this fusion. Finally, we prove
that the actual effect of RAG on the token, which is the comparison between
benefit and detriment, can be predicted without any training or accessing the
utility of retrieval. Based on our theory, we propose a practical novel method,
Tok-RAG, which achieves collaborative generation between the pure LLM and RAG
at token level to preserve benefit and avoid detriment. Experiments in
real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the
effectiveness of our method and support our theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Bilingual Lexicon Induction with Cross-Encoder Reranking <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.16953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.16953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaoyiran Li, Fangyu Liu, Ivan Vulić, Anna Korhonen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilingual lexicon induction (BLI) with limited bilingual supervision is a
crucial yet challenging task in multilingual NLP. Current state-of-the-art BLI
methods rely on the induction of cross-lingual word embeddings (CLWEs) to
capture cross-lingual word similarities; such CLWEs are obtained 1) via
traditional static models (e.g., VecMap), or 2) by extracting type-level CLWEs
from multilingual pretrained language models (mPLMs), or 3) through combining
the former two options. In this work, we propose a novel semi-supervised
post-hoc reranking method termed BLICEr (BLI with Cross-Encoder Reranking),
applicable to any precalculated CLWE space, which improves their BLI
capability. The key idea is to 'extract' cross-lingual lexical knowledge from
mPLMs, and then combine it with the original CLWEs. This crucial step is done
via 1) creating a word similarity dataset, comprising positive word pairs
(i.e., true translations) and hard negative pairs induced from the original
CLWE space, and then 2) fine-tuning an mPLM (e.g., mBERT or XLM-R) in a
cross-encoder manner to predict the similarity scores. At inference, we 3)
combine the similarity score from the original CLWE space with the score from
the BLI-tuned cross-encoder. BLICEr establishes new state-of-the-art results on
two standard BLI benchmarks spanning a wide spectrum of diverse languages: it
substantially outperforms a series of strong baselines across the board. We
also validate the robustness of BLICEr with different CLWEs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Moral Case for Using Language Model Agents for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seth Lazar, Luke Thorburn, Tian Jin, Luca Belli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our information and communication environment has fallen short of the ideals
that networked global communication might have served. Identifying all the
causes of its pathologies is difficult, but existing recommender systems very
likely play a contributing role. In this paper, which draws on the normative
tools of philosophy of computing, informed by empirical and technical insights
from natural language processing and recommender systems, we make the moral
case for an alternative approach. We argue that existing recommenders
incentivise mass surveillance, concentrate power, fall prey to narrow
behaviourism, and compromise user agency. Rather than just trying to avoid
algorithms entirely, or to make incremental improvements to the current
paradigm, researchers and engineers should explore an alternative paradigm: the
use of language model (LM) agents to source and curate content that matches
users' preferences and values, expressed in natural language. The use of LM
agents for recommendation poses its own challenges, including those related to
candidate generation, computational efficiency, preference modelling, and
prompt injection. Nonetheless, if implemented successfully LM agents could:
guide us through the digital public sphere without relying on mass
surveillance; shift power away from platforms towards users; optimise for what
matters instead of just for behavioural proxies; and scaffold our agency
instead of undermining it.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Word Translation via Two-Stage Contrastive Learning <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08307v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08307v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaoyiran Li, Fangyu Liu, Nigel Collier, Anna Korhonen, Ivan Vulić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word translation or bilingual lexicon induction (BLI) is a key cross-lingual
task, aiming to bridge the lexical gap between different languages. In this
work, we propose a robust and effective two-stage contrastive learning
framework for the BLI task. At Stage C1, we propose to refine standard
cross-lingual linear maps between static word embeddings (WEs) via a
contrastive learning objective; we also show how to integrate it into the
self-learning procedure for even more refined cross-lingual maps. In Stage C2,
we conduct BLI-oriented contrastive fine-tuning of mBERT, unlocking its word
translation capability. We also show that static WEs induced from the
`C2-tuned' mBERT complement static WEs from Stage C1. Comprehensive experiments
on standard BLI datasets for diverse languages and different experimental
setups demonstrate substantial gains achieved by our framework. While the BLI
method from Stage C1 already yields substantial gains over all state-of-the-art
BLI methods in our comparison, even stronger improvements are met with the full
two-stage framework: e.g., we report gains for 112/112 BLI setups, spanning 28
language pairs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Into the Unknown Unknowns: Engaged Human Learning through Participation
  in Language Model Agent Conversations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15232v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15232v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Jiang, Yijia Shao, Dekun Ma, Sina J. Semnani, Monica S. Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While language model (LM)-powered chatbots and generative search engines
excel at answering concrete queries, discovering information in the terrain of
unknown unknowns remains challenging for users. To emulate the common
educational scenario where children/students learn by listening to and
participating in conversations of their parents/teachers, we create
Collaborative STORM (Co-STORM). Unlike QA systems that require users to ask all
the questions, Co-STORM lets users observe and occasionally steer the discourse
among several LM agents. The agents ask questions on the user's behalf,
allowing the user to discover unknown unknowns serendipitously. To facilitate
user interaction, Co-STORM assists users in tracking the discourse by
organizing the uncovered information into a dynamic mind map, ultimately
generating a comprehensive report as takeaways. For automatic evaluation, we
construct the WildSeek dataset by collecting real information-seeking records
with user goals. Co-STORM outperforms baseline methods on both discourse trace
and report quality. In a further human evaluation, 70% of participants prefer
Co-STORM over a search engine, and 78% favor it over a RAG chatbot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixed-initiative Query Rewriting in Conversational Passage Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.08803v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.08803v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dayu Yang, Yue Zhang, Hui Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we report our methods and experiments for the TREC
Conversational Assistance Track (CAsT) 2022. In this work, we aim to reproduce
multi-stage retrieval pipelines and explore one of the potential benefits of
involving mixed-initiative interaction in conversational passage retrieval
scenarios: reformulating raw queries. Before the first ranking stage of a
multi-stage retrieval pipeline, we propose a mixed-initiative query rewriting
module, which achieves query rewriting based on the mixed-initiative
interaction between the users and the system, as the replacement for the neural
rewriting method. Specifically, we design an algorithm to generate appropriate
questions related to the ambiguities in raw queries, and another algorithm to
reformulate raw queries by parsing users' feedback and incorporating it into
the raw query. For the first ranking stage of our multi-stage pipelines, we
adopt a sparse ranking function: BM25, and a dense retrieval method:
TCT-ColBERT. For the second-ranking step, we adopt a pointwise reranker:
MonoT5, and a pairwise reranker: DuoT5. Experiments on both TREC CAsT 2021 and
TREC CAsT 2022 datasets show the effectiveness of our mixed-initiative-based
query rewriting (or query reformulation) method on improving retrieval
performance compared with two popular reformulators: a neural reformulator:
CANARD-T5 and a rule-based reformulator: historical query reformulator(HQE).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://trec.nist.gov/pubs/trec31/papers/udel_fang.C.pdf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Behavior Alignment: A New Perspective of Evaluating LLM-based
  Conversational Recommender Systems <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11773v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11773v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dayu Yang, Fumian Chen, Hui Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated great potential in
Conversational Recommender Systems (CRS). However, the application of LLMs to
CRS has exposed a notable discrepancy in behavior between LLM-based CRS and
human recommenders: LLMs often appear inflexible and passive, frequently
rushing to complete the recommendation task without sufficient inquiry.This
behavior discrepancy can lead to decreased accuracy in recommendations and
lower user satisfaction. Despite its importance, existing studies in CRS lack a
study about how to measure such behavior discrepancy. To fill this gap, we
propose Behavior Alignment, a new evaluation metric to measure how well the
recommendation strategies made by a LLM-based CRS are consistent with human
recommenders'. Our experiment results show that the new metric is better
aligned with human preferences and can better differentiate how systems perform
than existing evaluation metrics. As Behavior Alignment requires explicit and
costly human annotations on the recommendation strategies, we also propose a
classification-based method to implicitly measure the Behavior Alignment based
on the responses. The evaluation results confirm the robustness of the method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 47th International ACM SIGIR Conference on Research
  and Development in Information Retrieval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ZeQR: Zero-shot Query Reformulation for Conversational Search <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09384v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09384v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dayu Yang, Yue Zhang, Hui Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the popularity of voice assistants continues to surge, conversational
search has gained increased attention in Information Retrieval. However, data
sparsity issues in conversational search significantly hinder the progress of
supervised conversational search methods. Consequently, researchers are
focusing more on zero-shot conversational search approaches. Nevertheless,
existing zero-shot methods face three primary limitations: they are not
universally applicable to all retrievers, their effectiveness lacks sufficient
explainability, and they struggle to resolve common conversational ambiguities
caused by omission. To address these limitations, we introduce a novel
Zero-shot Query Reformulation (or Query Rewriting) (ZeQR) framework that
reformulates queries based on previous dialogue contexts without requiring
supervision from conversational search data. Specifically, our framework
utilizes language models designed for machine reading comprehension tasks to
explicitly resolve two common ambiguities: coreference and omission, in raw
queries. In comparison to existing zero-shot methods, our approach is
universally applicable to any retriever without additional adaptation or
indexing. It also provides greater explainability and effectively enhances
query intent understanding because ambiguities are explicitly and proactively
resolved. Through extensive experiments on four TREC conversational datasets,
we demonstrate the effectiveness of our method, which consistently outperforms
state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 9th ACM SIGIR International Conference on the Theory
  of Information Retrieval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dataset Condensation for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01038v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01038v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Wu, Wenqi Fan, Jingfan Chen, Shengcai Liu, Qijiong Liu, Rui He, Qing Li, Ke Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training recommendation models on large datasets requires significant time
and resources. It is desired to construct concise yet informative datasets for
efficient training. Recent advances in dataset condensation show promise in
addressing this problem by synthesizing small datasets. However, applying
existing methods of dataset condensation to recommendation has limitations: (1)
they fail to generate discrete user-item interactions, and (2) they could not
preserve users' potential preferences. To address the limitations, we propose a
lightweight condensation framework tailored for recommendation (DConRec),
focusing on condensing user-item historical interaction sets. Specifically, we
model the discrete user-item interactions via a probabilistic approach and
design a pre-augmentation module to incorporate the potential preferences of
users into the condensed datasets. While the substantial size of datasets leads
to costly optimization, we propose a lightweight policy gradient estimation to
accelerate the data synthesis. Experimental results on multiple real-world
datasets have demonstrated the effectiveness and efficiency of our framework.
Besides, we provide a theoretical analysis of the provable convergence of
DConRec. Our implementation is available at:
https://github.com/JiahaoWuGit/DConRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TKDE. Also titled as "Condensing Pre-augmented
  Recommendation Data via Lightweight Policy Gradient Estimation"</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">151</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Fluid: Scaling Autoregressive Text-to-image Generative Models with
  Continuous Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, <span class="highlight-author">Kaiming</span> He, Yonglong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling up autoregressive models in vision has not proven as beneficial as in
large language models. In this work, we investigate this scaling problem in the
context of text-to-image generation, focusing on two critical factors: whether
models use discrete or continuous tokens, and whether tokens are generated in a
random or fixed raster order using BERT- or GPT-like transformer architectures.
Our empirical results show that, while all models scale effectively in terms of
validation loss, their evaluation performance -- measured by FID, GenEval
score, and visual quality -- follows different trends. Models based on
continuous tokens achieve significantly better visual quality than those using
discrete tokens. Furthermore, the generation order and attention mechanisms
significantly affect the GenEval score: random-order models achieve notably
better GenEval scores compared to raster-order models. Inspired by these
findings, we train Fluid, a random-order autoregressive model on continuous
tokens. Fluid 10.5B model achieves a new state-of-the-art zero-shot FID of 6.16
on MS-COCO 30K, and 0.69 overall score on the GenEval benchmark. We hope our
findings and results will encourage future efforts to further bridge the
scaling gap between vision and language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Numerical Precision Affects Mathematical Reasoning Capabilities of
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guhao Feng, Kai Yang, Yuntian Gu, Xinyue Ai, Shengjie Luo, Jiacheng Sun, Di He, Zhenguo Li, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable success of Transformer-based Large Language Models
(LLMs) across various domains, understanding and enhancing their mathematical
capabilities remains a significant challenge. In this paper, we conduct a
rigorous theoretical analysis of LLMs' mathematical abilities, with a specific
focus on their arithmetic performances. We identify numerical precision as a
key factor that influences their effectiveness in mathematical tasks. Our
results show that Transformers operating with low numerical precision fail to
address arithmetic tasks, such as iterated addition and integer multiplication,
unless the model size grows super-polynomially with respect to the input
length. In contrast, Transformers with standard numerical precision can
efficiently handle these tasks with significantly smaller model sizes. We
further support our theoretical findings through empirical experiments that
explore the impact of varying numerical precision on arithmetic tasks,
providing valuable insights for improving the mathematical reasoning
capabilities of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusing States and Matching Scores: A New Framework for Imitation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runzhe Wu, Yiding Chen, Gokul Swamy, Kianté Brantley, Wen Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial Imitation Learning is traditionally framed as a two-player
zero-sum game between a learner and an adversarially chosen cost function, and
can therefore be thought of as the sequential generalization of a Generative
Adversarial Network (GAN). A prominent example of this framework is Generative
Adversarial Imitation Learning (GAIL). However, in recent years, diffusion
models have emerged as a non-adversarial alternative to GANs that merely
require training a score function via regression, yet produce generations of a
higher quality. In response, we investigate how to lift insights from diffusion
modeling to the sequential setting. We propose diffusing states and performing
score-matching along diffused states to measure the discrepancy between the
expert's and learner's states. Thus, our approach only requires training score
functions to predict noises via standard regression, making it significantly
easier and more stable to train than adversarial methods. Theoretically, we
prove first- and second-order instance-dependent bounds with linear scaling in
the horizon, proving that our approach avoids the compounding errors that
stymie offline approaches to imitation learning. Empirically, we show our
approach outperforms GAN-style imitation learning baselines across various
continuous control problems, including complex tasks like controlling humanoids
to walk, sit, and crawl.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoAL: Automated Active Learning with Differentiable Query Strategy
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifeng Wang, Xueying Zhan, Siyu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As deep learning continues to evolve, the need for data efficiency becomes
increasingly important. Considering labeling large datasets is both
time-consuming and expensive, active learning (AL) provides a promising
solution to this challenge by iteratively selecting the most informative
subsets of examples to train deep neural networks, thereby reducing the
labeling cost. However, the effectiveness of different AL algorithms can vary
significantly across data scenarios, and determining which AL algorithm best
fits a given task remains a challenging problem. This work presents the first
differentiable AL strategy search method, named AutoAL, which is designed on
top of existing AL sampling strategies. AutoAL consists of two neural nets,
named SearchNet and FitNet, which are optimized concurrently under a
differentiable bi-level optimization framework. For any given task, SearchNet
and FitNet are iteratively co-optimized using the labeled data, learning how
well a set of candidate AL algorithms perform on that task. With the optimal AL
strategies identified, SearchNet selects a small subset from the unlabeled pool
for querying their annotations, enabling efficient training of the task model.
Experimental results demonstrate that AutoAL consistently achieves superior
accuracy compared to all candidate AL algorithms and other selective AL
approaches, showcasing its potential for adapting and integrating multiple
existing AL methods across diverse tasks and domains. Code will be available
at: https://github.com/haizailache999/AutoAL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrospective Learning from Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zizhao Chen, Mustafa Omer Gul, Yiwei Chen, Gloria Geng, Anne Wu, Yoav Artzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-turn interactions between large language models (LLMs) and users
naturally include implicit feedback signals. If an LLM responds in an
unexpected way to an instruction, the user is likely to signal it by rephrasing
the request, expressing frustration, or pivoting to an alternative task. Such
signals are task-independent and occupy a relatively constrained subspace of
language, allowing the LLM to identify them even if it fails on the actual
task. This creates an avenue for continually learning from interactions without
additional annotations. We introduce ReSpect, a method to learn from such
signals in past interactions via retrospection. We deploy ReSpect in a new
multimodal interaction scenario, where humans instruct an LLM to solve an
abstract reasoning task with a combinatorial solution space. Through thousands
of interactions with humans, we show how ReSpect gradually improves task
completion rate from 31% to 82%, all without any external annotation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Influence Functions for Scalable Data Attribution in Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Mlodozeniec, Runa Eschenhagen, Juhan Bae, Alexander Immer, David Krueger, Richard Turner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have led to significant advancements in generative
modelling. Yet their widespread adoption poses challenges regarding data
attribution and interpretability. In this paper, we aim to help address such
challenges in diffusion models by developing an \textit{influence functions}
framework. Influence function-based data attribution methods approximate how a
model's output would have changed if some training data were removed. In
supervised learning, this is usually used for predicting how the loss on a
particular example would change. For diffusion models, we focus on predicting
the change in the probability of generating a particular example via several
proxy measurements. We show how to formulate influence functions for such
quantities and how previously proposed methods can be interpreted as particular
design choices in our framework. To ensure scalability of the Hessian
computations in influence functions, we systematically develop K-FAC
approximations based on generalised Gauss-Newton matrices specifically tailored
to diffusion models. We recast previously proposed methods as specific design
choices in our framework and show that our recommended method outperforms
previous data attribution approaches on common evaluations, such as the Linear
Data-modelling Score (LDS) or retraining without top influences, without the
need for method-specific hyperparameter tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Gradient <span class="highlight-title">Clip</span>ping to Normalization for Heavy Tailed SGD 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Hübler, Ilyas Fatkhullin, Niao He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent empirical evidence indicates that many machine learning applications
involve heavy-tailed gradient noise, which challenges the standard assumptions
of bounded variance in stochastic optimization. Gradient clipping has emerged
as a popular tool to handle this heavy-tailed noise, as it achieves good
performance in this setting both theoretically and practically. However, our
current theoretical understanding of non-convex gradient clipping has three
main shortcomings. First, the theory hinges on large, increasing clipping
thresholds, which are in stark contrast to the small constant clipping
thresholds employed in practice. Second, clipping thresholds require knowledge
of problem-dependent parameters to guarantee convergence. Lastly, even with
this knowledge, current sampling complexity upper bounds for the method are
sub-optimal in nearly all parameters. To address these issues, we study
convergence of Normalized SGD (NSGD). First, we establish a parameter-free
sample complexity for NSGD of
$\mathcal{O}\left(\varepsilon^{-\frac{2p}{p-1}}\right)$ to find an
$\varepsilon$-stationary point. Furthermore, we prove tightness of this result,
by providing a matching algorithm-specific lower bound. In the setting where
all problem parameters are known, we show this complexity is improved to
$\mathcal{O}\left(\varepsilon^{-\frac{3p-2}{p-1}}\right)$, matching the
previously known lower bound for all first-order methods in all problem
dependent parameters. Finally, we establish high-probability convergence of
NSGD with a mild logarithmic dependence on the failure probability. Our work
complements the studies of gradient clipping under heavy tailed noise improving
the sample complexities of existing algorithms and offering an alternative
mechanism to achieve high probability convergence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Zhang, Cunxiao Du, Chao Du, Tianyu Pang, Wei Gao, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have extended their
capabilities to handle long contexts. However, increasing the number of model
layers and the length of input sequences significantly escalates the memory
required to store key-value (KV) cache, posing challenges for efficient
inference. To mitigate this issue, we present SimLayerKV, a simple yet
effective method that reduces inter-layer KV cache redundancies by selectively
dropping cache in identified lazy layers. Our approach is based on the
observation that certain layers in long-context LLMs exhibit "lazy" behavior,
contributing less to modeling long-range dependencies compared to non-lazy
layers. By analyzing attention weight patterns, we find that the behavior of
these lazy layers is consistent across tokens during generation for a given
input. This insight motivates our SimLayerKV, which identifies lazy layers and
reduces their KV cache accordingly. SimLayerKV is training-free, generalizable,
and can be implemented with only seven lines of code. We conduct extensive
experiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and
Mistral-7B across 16 tasks from the LongBench benchmark. The results
demonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\times$
with only a 1.2% performance drop when combined with 4-bit quantization. Our
code is available at https://github.com/sail-sg/SimLayerKV.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified View of Delta Parameter Editing in Post-Trained Large-Scale
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaoyu Tang, Le Yu, Bowen Yu, Hongyu Lin, Keming Lu, Yaojie Lu, Xianpei Han, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training has emerged as a crucial paradigm for adapting large-scale
pre-trained models to various tasks, whose effects are fully reflected by delta
parameters (i.e., the disparity between post-trained and pre-trained
parameters). While numerous studies have explored delta parameter properties
via operations like pruning, quantization, low-rank approximation, and
extrapolation, a unified framework for systematically examining these
characteristics has been lacking. In this paper, we propose a novel perspective
based on Riemann sum approximation of the loss function to elucidate delta
parameter editing operations. Our analysis categorizes existing methods into
three classes based on their post-editing performance: competitive, decreased,
and improved, explaining how they are expressed by the Riemann sum
approximation term and how they alter the model performance. Extensive
experiments on both visual and language models, including ViT, LLaMA 3, Qwen 2,
and Mistral, corroborate our theoretical findings. Furthermore, we introduce
extensions to existing techniques like DARE and BitDelta, highlighting their
limitations in leveraging the properties of delta parameters and reorganizing
them into general expressions to enhance the applicability and effectiveness of
delta parameter editing in post-trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ORSO: Accelerating Reward Design via Online Reward Selection and Policy
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Bo Calvin Zhang, Zhang-Wei Hong, Aldo Pacchiano, Pulkit Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward shaping is a critical component in reinforcement learning (RL),
particularly for complex tasks where sparse rewards can hinder learning. While
shaping rewards have been introduced to provide additional guidance, selecting
effective shaping functions remains challenging and computationally expensive.
This paper introduces Online Reward Selection and Policy Optimization (ORSO), a
novel approach that frames shaping reward selection as an online model
selection problem. ORSO employs principled exploration strategies to
automatically identify promising shaping reward functions without human
intervention, balancing exploration and exploitation with provable regret
guarantees. We demonstrate ORSO's effectiveness across various continuous
control tasks using the Isaac Gym simulator. Compared to traditional methods
that fully evaluate each shaping reward function, ORSO significantly improves
sample efficiency, reduces computational time, and consistently identifies
high-quality reward functions that produce policies comparable to those
generated by domain experts through hand-engineered rewards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint, 35 pages, 23 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active-Dormant Attention Heads: Mechanistically Demystifying
  Extreme-Token Phenomena in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Guo, Druv Pai, Yu Bai, Jiantao Jiao, Michael I. Jordan, Song Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Practitioners have consistently observed three puzzling phenomena in
transformer-based large language models (LLMs): attention sinks, value-state
drains, and residual-state peaks, collectively referred to as extreme-token
phenomena. These phenomena are characterized by certain so-called "sink tokens"
receiving disproportionately high attention weights, exhibiting significantly
smaller value states, and having much larger residual-state norms than those of
other tokens. These extreme tokens give rise to various challenges in LLM
inference, quantization, and interpretability.
  We elucidate the mechanisms behind extreme-token phenomena. First, we show
that these phenomena arise in very simple architectures -- transformers with
one to three layers -- trained on a toy model, the Bigram-Backcopy (BB) task.
In this setting, we identify an active-dormant mechanism, where attention heads
become sinks for specific input domains while remaining non-sinks for others.
Our theoretical analysis of the training dynamics reveals that these phenomena
are driven by a mutual reinforcement mechanism. Building on these insights, we
propose strategies to mitigate extreme-token phenomena during pretraining,
including replacing softmax with ReLU and Adam with SGD. Next, we extend our
analysis to pretrained LLMs, including Llama and OLMo, showing that many
attention heads exhibit a similar active-dormant mechanism as in the BB task,
and that the mutual reinforcement mechanism also governs the emergence of
extreme-token phenomena during LLM pretraining. Our results reveal that many of
the static and dynamic properties of extreme-token phenomena predicted by the
BB task align with observations in pretrained LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Disparate Benefits of Deep Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kajetan Schweighofer, Adrian Arnaiz-Rodriguez, Sepp Hochreiter, Nuria Oliver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensembles of Deep Neural Networks, Deep Ensembles, are widely used as a
simple way to boost predictive performance. However, their impact on
algorithmic fairness is not well understood yet. Algorithmic fairness
investigates how a model's performance varies across different groups,
typically defined by protected attributes such as age, gender, or race. In this
work, we investigate the interplay between the performance gains from Deep
Ensembles and fairness. Our analysis reveals that they unevenly favor different
groups in what we refer to as a disparate benefits effect. We empirically
investigate this effect with Deep Ensembles applied to popular facial analysis
and medical imaging datasets, where protected group attributes are given and
find that it occurs for multiple established group fairness metrics, including
statistical parity and equal opportunity. Furthermore, we identify the
per-group difference in predictive diversity of ensemble members as the
potential cause of the disparate benefits effect. Finally, we evaluate
different approaches to reduce unfairness due to the disparate benefits effect.
Our findings show that post-processing is an effective method to mitigate this
unfairness while preserving the improved performance of Deep Ensembles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Common Pitfall of Margin-based Language Model Alignment: Gradient
  Entanglement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Yuan, Yifan Zeng, Yue Wu, Huazheng Wang, Mengdi Wang, Liu Leqi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback (RLHF) has become the predominant
approach for language model (LM) alignment. At its core, RLHF uses a
margin-based loss for preference optimization, specifying ideal LM behavior
only by the difference between preferred and dispreferred responses. In this
paper, we identify a common pitfall of margin-based methods -- the
under-specification of ideal LM behavior on preferred and dispreferred
responses individually, which leads to two unintended consequences as the
margin increases: (1) The probability of dispreferred (e.g., unsafe) responses
may increase, resulting in potential safety alignment failures. (2) The
probability of preferred responses may decrease, even when those responses are
ideal. We demystify the reasons behind these problematic behaviors:
margin-based losses couple the change in the preferred probability to the
gradient of the dispreferred one, and vice versa, often preventing the
preferred probability from increasing while the dispreferred one decreases, and
thus causing a synchronized increase or decrease in both probabilities. We term
this effect, inherent in margin-based objectives, gradient entanglement.
Formally, we derive conditions for general margin-based alignment objectives
under which gradient entanglement becomes concerning: the inner product of the
gradients of preferred and dispreferred log-probabilities is large relative to
the individual gradient norms. We theoretically investigate why such inner
products can be large when aligning language models and empirically validate
our findings. Empirical implications of our framework extend to explaining
important differences in the training dynamics of various preference
optimization algorithms, and suggesting potential algorithm designs to mitigate
the under-specification issue of margin-based methods and thereby improving
language model alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unearthing Skill-Level Insights for Understanding Trade-Offs of
  Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mazda Moayeri, Vidhisha Balachandran, Varun Chandrasekaran, Safoora Yousefi, Thomas Fel, Soheil Feizi, Besmira Nushi, Neel Joshi, Vibhav Vineet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With models getting stronger, evaluations have grown more complex, testing
multiple skills in one benchmark and even in the same instance at once.
However, skill-wise performance is obscured when inspecting aggregate accuracy,
under-utilizing the rich signal modern benchmarks contain. We propose an
automatic approach to recover the underlying skills relevant for any evaluation
instance, by way of inspecting model-generated rationales. After validating the
relevance of rationale-parsed skills and inferring skills for $46$k instances
over $12$ benchmarks, we observe many skills to be common across benchmarks,
resulting in the curation of hundreds of skill-slices (i.e. sets of instances
testing a common skill). Inspecting accuracy over these slices yields novel
insights on model trade-offs: e.g., compared to GPT-4o and Claude 3.5 Sonnet,
on average, Gemini 1.5 Pro is $18\%$ more accurate in "computing molar mass",
but $19\%$ less accurate in "applying constitutional law", despite the overall
accuracies of the three models differing by a mere $0.4\%$. Furthermore, we
demonstrate the practical utility of our approach by showing that insights
derived from skill slice analysis can generalize to held-out instances: when
routing each instance to the model strongest on the relevant skills, we see a
$3\%$ accuracy improvement over our $12$ dataset corpus. Our skill-slices and
framework open a new avenue in model evaluation, leveraging skill-specific
analyses to unlock a more granular and actionable understanding of model
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code at: github.com/microsoft/skill-slice-insights</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial Kuramoto Oscillatory Neurons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takeru Miyato, Sindy Löwe, Andreas Geiger, Max Welling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has long been known in both neuroscience and AI that ``binding'' between
neurons leads to a form of competitive learning where representations are
compressed in order to represent more abstract concepts in deeper layers of the
network. More recently, it was also hypothesized that dynamic (spatiotemporal)
representations play an important role in both neuroscience and AI. Building on
these ideas, we introduce Artificial Kuramoto Oscillatory Neurons (AKOrN) as a
dynamical alternative to threshold units, which can be combined with arbitrary
connectivity designs such as fully connected, convolutional, or attentive
mechanisms. Our generalized Kuramoto updates bind neurons together through
their synchronization dynamics. We show that this idea provides performance
improvements across a wide spectrum of tasks such as unsupervised object
discovery, adversarial robustness, calibrated uncertainty quantification, and
reasoning. We believe that these empirical results show the importance of
rethinking our assumptions at the most basic neuronal level of neural
representation, and in particular show the importance of dynamical
representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/autonomousvision/akorn</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steering Your Generalists: Improving Robotic Foundation Models via Value
  Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitsuhiko Nakamoto, Oier Mees, Aviral Kumar, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large, general-purpose robotic policies trained on diverse demonstration
datasets have been shown to be remarkably effective both for controlling a
variety of robots in a range of different scenes, and for acquiring broad
repertoires of manipulation skills. However, the data that such policies are
trained on is generally of mixed quality -- not only are human-collected
demonstrations unlikely to perform the task perfectly, but the larger the
dataset is, the harder it is to curate only the highest quality examples. It
also remains unclear how optimal data from one embodiment is for training on
another embodiment. In this paper, we present a general and broadly applicable
approach that enhances the performance of such generalist robot policies at
deployment time by re-ranking their actions according to a value function
learned via offline RL. This approach, which we call Value-Guided Policy
Steering (V-GPS), is compatible with a wide range of different generalist
policies, without needing to fine-tune or even access the weights of the
policy. We show that the same value function can improve the performance of
five different state-of-the-art policies with different architectures, even
though they were trained on distinct datasets, attaining consistent performance
improvement on multiple robotic platforms across a total of 12 tasks. Code and
videos can be found at: https://nakamotoo.github.io/V-GPS
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Robot Learning (CoRL) 2024. Project Page:
  https://nakamotoo.github.io/V-GPS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Private Counterfactual Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Nomeir, Pasan Dissanayake, Shreya Meel, Sanghamitra Dutta, Sennur Ulukus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transparency and explainability are two extremely important aspects to be
considered when employing black-box machine learning models in high-stake
applications. Providing counterfactual explanations is one way of catering this
requirement. However, this also poses a threat to the privacy of both the
institution that is providing the explanation as well as the user who is
requesting it. In this work, we propose multiple schemes inspired by private
information retrieval (PIR) techniques which ensure the \emph{user's privacy}
when retrieving counterfactual explanations. We present a scheme which
retrieves the \emph{exact} nearest neighbor counterfactual explanation from a
database of accepted points while achieving perfect (information-theoretic)
privacy for the user. While the scheme achieves perfect privacy for the user,
some leakage on the database is inevitable which we quantify using a mutual
information based metric. Furthermore, we propose strategies to reduce this
leakage to achieve an advanced degree of database privacy. We extend these
schemes to incorporate user's preference on transforming their attributes, so
that a more actionable explanation can be received. Since our schemes rely on
finite field arithmetic, we empirically validate our schemes on real datasets
to understand the trade-off between the accuracy and the finite field sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Testing as a Tool for Interpretability: Length-based
  Overfitting of Elementary Functions in Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrik Zavoral, Dušan Variš, Ondřej Bojar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Transformer model has a tendency to overfit various aspects of the
training data, such as the overall sequence length. We study elementary string
edit functions using a defined set of error indicators to interpret the
behaviour of the sequence-to-sequence Transformer. We show that generalization
to shorter sequences is often possible, but confirm that longer sequences are
highly problematic, although partially correct answers are often obtained.
Additionally, we find that other structural characteristics of the sequences,
such as subsegment length, may be equally important. We hypothesize that the
models learn algorithmic aspects of the tasks simultaneously with structural
aspects but adhering to the structural aspects is unfortunately often preferred
by Transformer when they come into conflict.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures, 2 tables; to be published</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine-Learning Analysis of Radiative Decays to Dark Matter at the LHC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ernesto Arganda, Marcela Carena, Martín de los Rios, Andres D. Perez, Duncan Rocha, Rosa M. Sandá Seoane, Carlos E. M. Wagner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The search for weakly interacting matter particles (WIMPs) is one of the main
objectives of the High Luminosity Large Hadron Collider (HL-LHC). In this work
we use Machine Learning (ML) techniques to explore WIMP radiative decays into a
Dark Matter (DM) candidate in a supersymmetric framework. The minimal
supersymmetric WIMP sector includes the lightest neutralino that can provide
the observed DM relic density through its co-annihilation with the second
lightest neutralino and lightest chargino. Moreover, the direct DM detection
cross section rates fulfill current experimental bounds and provide discovery
targets for the same region of model parameters in which the radiative decay of
the second lightest neutralino into a photon and the lightest neutralino is
enhanced. This strongly motivates the search for radiatively decaying
neutralinos which, however, suffers from strong backgrounds. We investigate the
LHC reach in the search for these radiatively decaying particles by means of
cut-based and ML methods and estimate its discovery potential in this
well-motivated, new physics scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 9 figures, 3 tables, 4 appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discrete distributions are learnable from metastable samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhijith Jayakumar, Andrey Y. Lokhov, Sidhant Misra, Marc Vuffray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Markov chain samplers designed to sample from multi-variable distributions
often undesirably get stuck in specific regions of their state space. This
causes such samplers to approximately sample from a metastable distribution
which is usually quite different from the desired, stationary distribution of
the chain. We show that single-variable conditionals of metastable
distributions of reversible Markov chain samplers that satisfy a strong
metastability condition are on average very close to those of the true
distribution. This holds even when the metastable distribution is far away from
the true model in terms of global metrics like Kullback-Leibler divergence or
total variation distance. This property allows us to learn the true model using
a conditional likelihood based estimator, even when the samples come from a
metastable distribution concentrated in a small region of the state space.
Explicit examples of such metastable states can be constructed from regions
that effectively bottleneck the probability flow and cause poor mixing of the
Markov chain. For specific cases of binary pairwise undirected graphical
models, we extend our results to further rigorously show that data coming from
metastable states can be used to learn the parameters of the energy function
and recover the structure of the model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preliminary version, 26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Graph Quantized Tokenizers for Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Limei Wang, Kaveh Hassani, Si Zhang, Dongqi Fu, Baichuan Yuan, Weilin Cong, Zhigang Hua, Hao Wu, Ning Yao, Bo Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers serve as the backbone architectures of Foundational Models,
where a domain-specific tokenizer helps them adapt to various domains. Graph
Transformers (GTs) have recently emerged as a leading model in geometric deep
learning, outperforming Graph Neural Networks (GNNs) in various graph learning
tasks. However, the development of tokenizers for graphs has lagged behind
other modalities, with existing approaches relying on heuristics or GNNs
co-trained with Transformers. To address this, we introduce GQT (\textbf{G}raph
\textbf{Q}uantized \textbf{T}okenizer), which decouples tokenizer training from
Transformer training by leveraging multi-task graph self-supervised learning,
yielding robust and generalizable graph tokens. Furthermore, the GQT utilizes
Residual Vector Quantization (RVQ) to learn hierarchical discrete tokens,
resulting in significantly reduced memory requirements and improved
generalization capabilities. By combining the GQT with token modulation, a
Transformer encoder achieves state-of-the-art performance on 16 out of 18
benchmarks, including large-scale homophilic and heterophilic datasets. The
code is available at: https://github.com/limei0307/graph-tokenizer
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Arbitrarily-Conditioned Multi-Functional Diffusion for Multi-Physics
  Emulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Da Long, Zhitong Xu, Guang Yang, Akil Narayan, Shandian Zhe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern physics simulation often involves multiple functions of interests, and
traditional numerical approaches are known to be complex and computationally
costly. While machine learning-based surrogate models can offer significant
cost reductions, most focus on a single task, such as forward prediction, and
typically lack uncertainty quantification -- an essential component in many
applications. To overcome these limitations, we propose Arbitrarily-Conditioned
Multi-Functional Diffusion (ACMFD), a versatile probabilistic surrogate model
for multi-physics emulation. ACMFD can perform a wide range of tasks within a
single framework, including forward prediction, various inverse problems, and
simulating data for entire systems or subsets of quantities conditioned on
others. Specifically, we extend the standard Denoising Diffusion Probabilistic
Model (DDPM) for multi-functional generation by modeling noise as Gaussian
processes (GP). We then introduce an innovative denoising loss. The training
involves randomly sampling the conditioned part and fitting the corresponding
predicted noise to zero, enabling ACMFD to flexibly generate function values
conditioned on any other functions or quantities. To enable efficient training
and sampling, and to flexibly handle irregularly sampled data, we use GPs to
interpolate function samples onto a grid, inducing a Kronecker product
structure for efficient computation. We demonstrate the advantages of ACMFD
across several fundamental multi-physics systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing Deep Transformer Models for Time Series Forecasting via
  Manifold Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilya Kaufman, Omri Azencot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer models have consistently achieved remarkable results in various
domains such as natural language processing and computer vision. However,
despite ongoing research efforts to better understand these models, the field
still lacks a comprehensive understanding. This is particularly true for deep
time series forecasting methods, where analysis and understanding work is
relatively limited. Time series data, unlike image and text information, can be
more challenging to interpret and analyze. To address this, we approach the
problem from a manifold learning perspective, assuming that the latent
representations of time series forecasting models lie next to a low-dimensional
manifold. In our study, we focus on analyzing the geometric features of these
latent data manifolds, including intrinsic dimension and principal curvatures.
Our findings reveal that deep transformer models exhibit similar geometric
behavior across layers, and these geometric features are correlated with model
performance. Additionally, we observe that untrained models initially have
different structures, but they rapidly converge during training. By leveraging
our geometric analysis and differentiable tools, we can potentially design new
and improved deep forecasting neural networks. This approach complements
existing analysis studies and contributes to a better understanding of
transformer models in the context of time series forecasting. Code is released
at https://github.com/azencot-group/GATLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TMLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DPLM-2: A <span class="highlight-title">Multimodal</span> Diffusion Protein Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, Quanquan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proteins are essential macromolecules defined by their amino acid sequences,
which determine their three-dimensional structures and, consequently, their
functions in all living organisms. Therefore, generative protein modeling
necessitates a multimodal approach to simultaneously model, understand, and
generate both sequences and structures. However, existing methods typically use
separate models for each modality, limiting their ability to capture the
intricate relationships between sequence and structure. This results in
suboptimal performance in tasks that requires joint understanding and
generation of both modalities. In this paper, we introduce DPLM-2, a multimodal
protein foundation model that extends discrete diffusion protein language model
(DPLM) to accommodate both sequences and structures. To enable structural
learning with the language model, 3D coordinates are converted to discrete
tokens using a lookup-free quantization-based tokenizer. By training on both
experimental and high-quality synthetic structures, DPLM-2 learns the joint
distribution of sequence and structure, as well as their marginals and
conditionals. We also implement an efficient warm-up strategy to exploit the
connection between large-scale evolutionary data and structural inductive
biases from pre-trained sequence-based protein language models. Empirical
evaluation shows that DPLM-2 can simultaneously generate highly compatible
amino acid sequences and their corresponding 3D structures eliminating the need
for a two-stage generation approach. Moreover, DPLM-2 demonstrates competitive
performance in various conditional generation tasks, including folding, inverse
folding, and scaffolding with multimodal motif inputs, as well as providing
structure-aware representations for predictive tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Quantization for Matrix Multiplication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Or Ordentlich, Yury Polyanskiy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work in machine learning community proposed multiple methods for
performing lossy compression (quantization) of large matrices. This
quantization is important for accelerating matrix multiplication (main
component of large language models), which is often bottlenecked by the speed
of loading these matrices from memory. Unlike classical vector quantization and
rate-distortion theory, the goal of these new compression algorithms is to be
able to approximate not the matrices themselves, but their matrix product.
Specifically, given a pair of real matrices $A,B$ an encoder (compressor) is
applied to each of them independently producing descriptions with $R$ bits per
entry. These representations subsequently are used by the decoder to estimate
matrix product $A^\top B$. In this work, we provide a non-asymptotic lower
bound on the mean squared error of this approximation (as a function of rate
$R$) for the case of matrices $A,B$ with iid Gaussian entries. Algorithmically,
we construct a universal quantizer based on nested lattices with an explicit
guarantee of approximation error for any (non-random) pair of matrices $A$, $B$
in terms of only Frobenius norms $\|A\|_F, \|B\|_F$ and $\|A^\top B\|_F$. For
iid Gaussian matrices our quantizer achieves the lower bound and is, thus,
asymptotically optimal. A practical low-complexity version of our quantizer
achieves performance quite close to optimal. In information-theoretic terms we
derive rate-distortion function for matrix multiplication of iid Gaussian
matrices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Mystery of the Pathological Path-star Task for Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arvid Frydenlund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recently introduced path-star task is a minimal task designed to
exemplify limitations to the abilities of language models (Bachmann and
Nagarajan, 2024). It involves a path-star graph where multiple arms radiate
from a single starting node and each node is unique. Given the start node and a
specified target node that ends an arm, the task is to generate the arm
containing that target node. This is straightforward for a human but
surprisingly difficult for language models, which did not outperform the random
baseline. The authors hypothesized this is due to a deficiency in
teacher-forcing and the next-token prediction paradigm.
  We demonstrate the task is learnable using teacher-forcing in alternative
settings and that the issue is partially due to representation. We introduce a
regularization method using structured samples of the same graph but with
differing target nodes, improving results across a variety of model types. We
provide RASP proofs showing the task is theoretically solvable. Finally, we
find settings where an encoder-only model can consistently solve the task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Change Detection in Multivariate data streams: Online Analysis with
  Kernel-QuantTree 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michelangelo Olmo Nogara Notarianni, Filippo Leveni, Diego Stucchi, Luca Frittoli, Giacomo Boracchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Kernel-QuantTree Exponentially Weighted Moving Average (KQT-EWMA),
a non-parametric change-detection algorithm that combines the Kernel-QuantTree
(KQT) histogram and the EWMA statistic to monitor multivariate data streams
online. The resulting monitoring scheme is very flexible, since histograms can
be used to model any stationary distribution, and practical, since the
distribution of test statistics does not depend on the distribution of
datastream in stationary conditions (non-parametric monitoring). KQT-EWMA
enables controlling false alarms by operating at a pre-determined Average Run
Length ($ARL_0$), which measures the expected number of stationary samples to
be monitored before triggering a false alarm. The latter peculiarity is in
contrast with most non-parametric change-detection tests, which rarely can
control the $ARL_0$ a priori. Our experiments on synthetic and real-world
datasets demonstrate that KQT-EWMA can control $ARL_0$ while achieving
detection delays comparable to or lower than state-of-the-art methods designed
to work in the same conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AALTD workshop at ECML 2024 (https://ecml-aaltd.github.io/aaltd2024/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representing Model Weights with Language using Tree Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eliahu Horwitz, Bar Cavia, Jonathan Kahana, Yedid Hoshen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing availability of public models begs the question: can we train
neural networks that use other networks as input? This paper learns to
represent models within a joint space that embeds both model weights and
language. However, machine learning on model weights is challenging as model
weights often exhibit significant variation unrelated to the models' semantic
properties (nuisance variation). We identify a key property of real-world
models: most public models belong to a small set of Model Trees, where all
models within a tree are fine-tuned from a common ancestor (e.g., a foundation
model). Importantly, we find that within each tree there is less nuisance
variation between models. For example, while classifying models according to
their training dataset generally requires complex architectures, in our case,
even a linear classifier trained on a single layer is often effective. While
effective, linear layers are computationally expensive as model weights are
very high dimensional. To address this, we introduce Probing Experts (ProbeX),
a theoretically motivated, lightweight probing method. Notably, ProbeX is the
first probing method designed to learn from the weights of just a single model
layer. We also construct and release a dataset that simulates the structure of
public model repositories. Our results show that ProbeX can effectively map the
weights of large models into a shared weight-language embedding space.
Furthermore, we demonstrate the impressive generalization of our method,
achieving zero-shot model classification and retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Retail Sales Forecasting with Optimized Machine Learning
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyam Ganguly, Isha Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In retail sales forecasting, accurately predicting future sales is crucial
for inventory management and strategic planning. Traditional methods like LR
often fall short due to the complexity of sales data, which includes
seasonality and numerous product families. Recent advancements in machine
learning (ML) provide more robust alternatives. This research benefits from the
power of ML, particularly Random Forest (RF), Gradient Boosting (GB), Support
Vector Regression (SVR), and XGBoost, to improve prediction accuracy. Despite
advancements, a significant gap exists in handling complex datasets with high
seasonality and multiple product families. The proposed solution involves
implementing and optimizing a RF model, leveraging hyperparameter tuning
through randomized search cross-validation. This approach addresses the
complexities of the dataset, capturing intricate patterns that traditional
methods miss. The optimized RF model achieved an R-squared value of 0.945,
substantially higher than the initial RF model and traditional LR, which had an
R-squared of 0.531. The model reduced the root mean squared logarithmic error
(RMSLE) to 1.172, demonstrating its superior predictive capability. The
optimized RF model did better than cutting-edge models like Gradient Boosting
(R-squared: 0.942), SVR (R-squared: 0.940), and XGBoost (R-squared: 0.939),
with more minor mean squared error (MSE) and mean absolute error (MAE) numbers.
The results demonstrate that the optimized RF model excels in forecasting
retail sales, handling the datasets complexity with higher accuracy and
reliability. This research highlights the importance of advanced ML techniques
in predictive analytics, offering a significant improvement over traditional
methods and other contemporary models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE 4th ICSES 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Prior-Free Black-Box Non-Stationary Reinforcement Learning Feasible? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Argyrios Gerogiannis, Yu-Han Huang, Venugopal V. Veeravalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of Non-Stationary Reinforcement Learning (NS-RL) without
prior knowledge about the system's non-stationarity. A state-of-the-art,
black-box algorithm, known as MASTER, is considered, with a focus on
identifying the conditions under which it can achieve its stated goals.
Specifically, we prove that MASTER's non-stationarity detection mechanism is
not triggered for practical choices of horizon, leading to performance akin to
a random restarting algorithm. Moreover, we show that the regret bound for
MASTER, while being order optimal, stays above the worst-case linear regret
until unreasonably large values of the horizon. To validate these observations,
MASTER is tested for the special case of piecewise stationary multi-armed
bandits, along with methods that employ random restarting, and others that use
quickest change detection to restart. A simple, order optimal random restarting
algorithm, that has prior knowledge of the non-stationarity is proposed as a
baseline. The behavior of the MASTER algorithm is validated in simulations, and
it is shown that methods employing quickest change detection are more robust
and consistently outperform MASTER and other random restarting approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probing the Latent Hierarchical Structure of Data via Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Sclocchi, Alessandro Favero, Noam Itzhak Levi, Matthieu Wyart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-dimensional data must be highly structured to be learnable. Although the
compositional and hierarchical nature of data is often put forward to explain
learnability, quantitative measurements establishing these properties are
scarce. Likewise, accessing the latent variables underlying such a data
structure remains a challenge. In this work, we show that forward-backward
experiments in diffusion-based models, where data is noised and then denoised
to generate new samples, are a promising tool to probe the latent structure of
data. We predict in simple hierarchical models that, in this process, changes
in data occur by correlated chunks, with a length scale that diverges at a
noise level where a phase transition is known to take place. Remarkably, we
confirm this prediction in both text and image datasets using state-of-the-art
diffusion models. Our results show how latent variable changes manifest in the
data and establish how to measure these effects in real data using diffusion
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Virtual Sensing for Real-Time Degradation Monitoring of Nuclear Systems:
  Leveraging DeepONet for Enhanced Sensing Coverage for Digital Twin-Enabling
  Technology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raisa Bentay Hossain, Farid Ahmed, Kazuma Kobayashi, Seid Koric, Diab Abueidda, Syed Bahauddin Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective real-time monitoring technique is crucial for detecting material
degradation and maintaining the structural integrity of nuclear systems to
ensure both safety and operational efficiency. Traditional physical sensor
systems face limitations such as installation challenges, high costs, and
difficulties in measuring critical parameters in hard-to-reach or harsh
environments, often resulting in incomplete data coverage. Machine
learning-driven virtual sensors offer a promising solution by enhancing
physical sensor capabilities to monitor critical degradation indicators like
pressure, velocity, and turbulence. However, conventional machine learning
models struggle with real-time monitoring due to the high-dimensional nature of
reactor data and the need for frequent retraining. This paper explores the use
of Deep Operator Networks (DeepONet) within a digital twin (DT) framework to
predict key thermal-hydraulic parameters in the hot leg of an AP-1000
Pressurized Water Reactor (PWR). In this study, DeepONet is trained with
different operational conditions, which relaxes the requirement of continuous
retraining, making it suitable for online and real-time prediction components
for DT. Our results show that DeepONet achieves accurate predictions with low
mean squared error and relative L2 error and can make predictions on unknown
data 160,000 times faster than traditional finite element (FE) simulations.
This speed and accuracy make DeepONet a powerful tool for tracking conditions
that contribute to material degradation in real-time, enhancing reactor safety
and longevity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GDeR: Safeguarding Efficiency, Balancing, and Robustness via
  Prototypical Graph Pruning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guibin Zhang, Haonan Dong, Yuchen Zhang, Zhixun Li, Dingshuo Chen, Kai Wang, Tianlong Chen, Yuxuan Liang, Dawei Cheng, Kun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training high-quality deep models necessitates vast amounts of data,
resulting in overwhelming computational and memory demands. Recently, data
pruning, distillation, and coreset selection have been developed to streamline
data volume by retaining, synthesizing, or selecting a small yet informative
subset from the full set. Among these methods, data pruning incurs the least
additional training cost and offers the most practical acceleration benefits.
However, it is the most vulnerable, often suffering significant performance
degradation with imbalanced or biased data schema, thus raising concerns about
its accuracy and reliability in on-device deployment. Therefore, there is a
looming need for a new data pruning paradigm that maintains the efficiency of
previous practices while ensuring balance and robustness. Unlike the fields of
computer vision and natural language processing, where mature solutions have
been developed to address these issues, graph neural networks (GNNs) continue
to struggle with increasingly large-scale, imbalanced, and noisy datasets,
lacking a unified dataset pruning solution. To achieve this, we introduce a
novel dynamic soft-pruning method, GDeR, designed to update the training
``basket'' during the process using trainable prototypes. GDeR first constructs
a well-modeled graph embedding hypersphere and then samples
\textit{representative, balanced, and unbiased subsets} from this embedding
space, which achieves the goal we called Graph Training Debugging. Extensive
experiments on five datasets across three GNN backbones, demonstrate that GDeR
(I) achieves or surpasses the performance of the full dataset with 30%~50%
fewer training samples, (II) attains up to a 2.81x lossless training speedup,
and (III) outperforms state-of-the-art pruning methods in imbalanced training
and noisy training scenarios by 0.3%~4.3% and 3.6%~7.8%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLIMB: Language-Guided Continual Learning for Task Planning with
  Iterative Model Building 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Walker Byrnes, Miroslav Bogdanovic, Avi Balakirsky, Stephen Balakirsky, Animesh Garg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent and reliable task planning is a core capability for generalized
robotics, requiring a descriptive domain representation that sufficiently
models all object and state information for the scene. We present CLIMB, a
continual learning framework for robot task planning that leverages foundation
models and execution feedback to guide domain model construction. CLIMB can
build a model from a natural language description, learn non-obvious predicates
while solving tasks, and store that information for future problems. We
demonstrate the ability of CLIMB to improve performance in common planning
environments compared to baseline methods. We also develop the BlocksWorld++
domain, a simulated environment with an easily usable real counterpart,
together with a curriculum of tasks with progressing difficulty for evaluating
continual learning. Additional details and demonstrations for this system can
be found at https://plan-with-climb.github.io/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjie Ni, Yifan Song, Deepanway Ghosal, Bo Li, David Junhao Zhang, Xiang Yue, Fuzhao Xue, Zian Zheng, Kaichen Zhang, Mahir Shah, Kabir Jain, Yang You, Michael Shieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceiving and generating diverse modalities are crucial for AI models to
effectively learn from and engage with real-world signals, necessitating
reliable evaluations for their development. We identify two major issues in
current evaluations: (1) inconsistent standards, shaped by different
communities with varying protocols and maturity levels; and (2) significant
query, grading, and generalization biases. To address these, we introduce
MixEval-X, the first any-to-any real-world benchmark designed to optimize and
standardize evaluations across input and output modalities. We propose
multi-modal benchmark mixture and adaptation-rectification pipelines to
reconstruct real-world task distributions, ensuring evaluations generalize
effectively to real-world use cases. Extensive meta-evaluations show our
approach effectively aligns benchmark samples with real-world task
distributions and the model rankings correlate strongly with that of
crowd-sourced real-world evaluations (up to 0.98). We provide comprehensive
leaderboards to rerank existing models and organizations and offer insights to
enhance understanding of multi-modal evaluations and inform future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supervised Kernel Thinning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Albert Gong, Kyuseong Choi, Raaz Dwivedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The kernel thinning algorithm of Dwivedi & Mackey (2024) provides a
better-than-i.i.d. compression of a generic set of points. By generating
high-fidelity coresets of size significantly smaller than the input points, KT
is known to speed up unsupervised tasks like Monte Carlo integration,
uncertainty quantification, and non-parametric hypothesis testing, with minimal
loss in statistical accuracy. In this work, we generalize the KT algorithm to
speed up supervised learning problems involving kernel methods. Specifically,
we combine two classical algorithms--Nadaraya-Watson (NW) regression or kernel
smoothing, and kernel ridge regression (KRR)--with KT to provide a quadratic
speed-up in both training and inference times. We show how distribution
compression with KT in each setting reduces to constructing an appropriate
kernel, and introduce the Kernel-Thinned NW and Kernel-Thinned KRR estimators.
We prove that KT-based regression estimators enjoy significantly superior
computational efficiency over the full-data estimators and improved statistical
efficiency over i.i.d. subsampling of the training data. En route, we also
provide a novel multiplicative error guarantee for compressing with KT. We
validate our design choices with both simulations and real data experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theory on Score-Mismatched Diffusion Models and Zero-Shot Conditional
  Samplers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Liang, Peizhong Ju, Yingbin Liang, Ness Shroff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The denoising diffusion model has recently emerged as a powerful generative
technique, capable of transforming noise into meaningful data. While
theoretical convergence guarantees for diffusion models are well established
when the target distribution aligns with the training distribution, practical
scenarios often present mismatches. One common case is in zero-shot conditional
diffusion sampling, where the target conditional distribution is different from
the (unconditional) training distribution. These score-mismatched diffusion
models remain largely unexplored from a theoretical perspective. In this paper,
we present the first performance guarantee with explicit dimensional
dependencies for general score-mismatched diffusion samplers, focusing on
target distributions with finite second moments. We show that score mismatches
result in an asymptotic distributional bias between the target and sampling
distributions, proportional to the accumulated mismatch between the target and
training distributions. This result can be directly applied to zero-shot
conditional samplers for any conditional model, irrespective of measurement
noise. Interestingly, the derived convergence upper bound offers useful
guidance for designing a novel bias-optimal zero-shot sampler in linear
conditional models that minimizes the asymptotic bias. For such bias-optimal
samplers, we further establish convergence guarantees with explicit
dependencies on dimension and conditioning, applied to several interesting
target distributions, including those with bounded support and Gaussian
mixtures. Our findings are supported by numerical studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single-Timescale Multi-Sequence Stochastic Approximation Without Fixed
  Point Smoothness: Theories and Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Huang, Zhaoxian Wu, Shiqian Ma, Qing Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic approximation (SA) that involves multiple coupled sequences, known
as multiple-sequence SA (MSSA), finds diverse applications in the fields of
signal processing and machine learning. However, existing theoretical
understandings {of} MSSA are limited: the multi-timescale analysis implies a
slow convergence rate, whereas the single-timescale analysis relies on a
stringent fixed point smoothness assumption. This paper establishes tighter
single-timescale analysis for MSSA, without assuming smoothness of the fixed
points. Our theoretical findings reveal that, when all involved operators are
strongly monotone, MSSA converges at a rate of $\tilde{\mathcal{O}}(K^{-1})$,
where $K$ denotes the total number of iterations. In addition, when all
involved operators are strongly monotone except for the main one, MSSA
converges at a rate of $\mathcal{O}(K^{-\frac{1}{2}})$. These theoretical
findings align with those established for single-sequence SA. Applying these
theoretical findings to bilevel optimization and communication-efficient
distributed learning offers relaxed assumptions and/or simpler algorithms with
performance guarantees, as validated by numerical experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Convergence Rate for Diffusion Probabilistic Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gen Li, Yuchen Jiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Score-based diffusion models have achieved remarkable empirical performance
in the field of machine learning and artificial intelligence for their ability
to generate high-quality new data instances from complex distributions.
Improving our understanding of diffusion models, including mainly convergence
analysis for such models, has attracted a lot of interests. Despite a lot of
theoretical attempts, there still exists significant gap between theory and
practice. Towards to close this gap, we establish an iteration complexity at
the order of $d^{1/3}\varepsilon^{-2/3}$, which is better than
$d^{5/12}\varepsilon^{-1}$, the best known complexity achieved before our work.
This convergence analysis is based on a randomized midpoint method, which is
first proposed for log-concave sampling (Shen and Lee, 2019), and then extended
to diffusion models by Gupta et al. (2024). Our theory accommodates
$\varepsilon$-accurate score estimates, and does not require log-concavity on
the target distribution. Moreover, the algorithm can also be parallelized to
run in only $O(\log^2(d/\varepsilon))$ parallel rounds in a similar way to
prior works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Probabilistic Conformal Prediction with Vectorized
  Non-Conformity Scores 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minxing Zheng, Shixiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models have shown significant promise in critical domains such as
medical diagnosis, autonomous driving, and climate science, where reliable
decision-making hinges on accurate uncertainty quantification. While
probabilistic conformal prediction (PCP) offers a powerful framework for this
purpose, its coverage efficiency -- the size of the uncertainty set -- is
limited when dealing with complex underlying distributions and a finite number
of generated samples. In this paper, we propose a novel PCP framework that
enhances efficiency by first vectorizing the non-conformity scores with ranked
samples and then optimizing the shape of the prediction set by varying the
quantiles for samples at the same rank. Our method delivers valid coverage
while producing discontinuous and more efficient prediction sets, making it
particularly suited for high-stakes applications. We demonstrate the
effectiveness of our approach through experiments on both synthetic and
real-world datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reducing the Transformer Architecture to a Minimum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernhard Bermeitinger, Tomas Hrycej, Massimo Pavone, Julianus Kath, Siegfried Handschuh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers are a widespread and successful model architecture, particularly
in Natural Language Processing (NLP) and Computer Vision (CV). The essential
innovation of this architecture is the Attention Mechanism, which solves the
problem of extracting relevant context information from long sequences in NLP
and realistic scenes in CV. A classical neural network component, a Multi-Layer
Perceptron (MLP), complements the attention mechanism. Its necessity is
frequently justified by its capability of modeling nonlinear relationships.
However, the attention mechanism itself is nonlinear through its internal use
of similarity measures. A possible hypothesis is that this nonlinearity is
sufficient for modeling typical application problems. As the MLPs usually
contain the most trainable parameters of the whole model, their omission would
substantially reduce the parameter set size. Further components can also be
reorganized to reduce the number of parameters. Under some conditions, query
and key matrices can be collapsed into a single matrix of the same size. The
same is true about value and projection matrices, which can also be omitted
without eliminating the substance of the attention mechanism. Initially, the
similarity measure was defined asymmetrically, with peculiar properties such as
that a token is possibly dissimilar to itself. A possible symmetric definition
requires only half of the parameters. We have laid the groundwork by testing
widespread CV benchmarks: MNIST and CIFAR-10. The tests have shown that
simplified transformer architectures (a) without MLP, (b) with collapsed
matrices, and (c) symmetric similarity matrices exhibit similar performance as
the original architecture, saving up to 90% of parameters without hurting the
classification performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, to appear in KDIR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Movie Gen: A Cast of Media Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, Yuming Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Movie Gen, a cast of foundation models that generates
high-quality, 1080p HD videos with different aspect ratios and synchronized
audio. We also show additional capabilities such as precise instruction-based
video editing and generation of personalized videos based on a user's image.
Our models set a new state-of-the-art on multiple tasks: text-to-video
synthesis, video personalization, video editing, video-to-audio generation, and
text-to-audio generation. Our largest video generation model is a 30B parameter
transformer trained with a maximum context length of 73K video tokens,
corresponding to a generated video of 16 seconds at 16 frames-per-second. We
show multiple technical innovations and simplifications on the architecture,
latent spaces, training objectives and recipes, data curation, evaluation
protocols, parallelization techniques, and inference optimizations that allow
us to reap the benefits of scaling pre-training data, model size, and training
compute for training large scale media generation models. We hope this paper
helps the research community to accelerate progress and innovation in media
generation models. All videos from this paper are available at
https://go.fb.me/MovieGenResearchVideos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generation through the lens of learning theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinod Raman, Ambuj Tewari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study generation through the lens of statistical learning theory. First,
we abstract and formalize the results of Gold [1967], Angluin [1979, 1980], and
Kleinberg and Mullainathan [2024] for language identification/generation in the
limit in terms of a binary hypothesis class defined over an abstract instance
space. Then, we formalize a different paradigm of generation studied by
Kleinberg and Mullainathan [2024], which we call ``uniform generation," and
provide a characterization of which hypothesis classes are uniformly
generatable. As is standard in statistical learning theory, our
characterization is in terms of the finiteness of a new combinatorial dimension
we call the Closure dimension. By doing so, we are able to compare
generatability with predictability (captured via PAC and online learnability)
and show that these two properties of hypothesis classes are
\emph{incompatible} - there are classes that are generatable but not
predictable and vice versa.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrystalX: Ultra-Precision Crystal Structure Resolution and Error
  Correction Using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaipeng Zheng, Weiran Huang, Wanli Ouyang, Han-Sen Zhong, Yuqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Atomic structure analysis of crystalline materials is a paramount endeavor in
both chemical and material sciences. This sophisticated technique necessitates
not only a solid foundation in crystallography but also a profound
comprehension of the intricacies of the accompanying software, posing a
significant challenge in meeting the rigorous daily demands. For the first
time, we confront this challenge head-on by harnessing the power of deep
learning for ultra-precise structural analysis at the full-atom level. To
validate the performance of the model, named CrystalX, we employed a vast
dataset comprising over 50,000 X-ray diffraction measurements derived from
authentic experiments, demonstrating performance that is commensurate with
human experts and adept at deciphering intricate geometric patterns.
Remarkably, CrystalX revealed that even peer-reviewed publications can harbor
errors that are stealthy to human scrutiny, yet CrystalX adeptly rectifies
them. This deep learning model revolutionizes the time frame for crystal
structure analysis, slashing it down to seconds. It has already been
successfully applied in the structure analysis of newly discovered compounds in
the latest research without human intervention. Overall, CrystalX marks the
beginning of a new era in automating routine structural analysis within
self-driving laboratories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On-device Federated Learning in Smartphones for Detecting Depression
  from Reddit Posts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mustofa Ahmed, Abdul Muntakim, Nawrin Tabassum, Mohammad Asifur Rahim, Faisal Muhammad Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depression detection using deep learning models has been widely explored in
previous studies, especially due to the large amounts of data available from
social media posts. These posts provide valuable information about individuals'
mental health conditions and can be leveraged to train models and identify
patterns in the data. However, distributed learning approaches have not been
extensively explored in this domain. In this study, we adopt Federated Learning
(FL) to facilitate decentralized training on smartphones while protecting user
data privacy. We train three neural network architectures--GRU, RNN, and LSTM
on Reddit posts to detect signs of depression and evaluate their performance
under heterogeneous FL settings. To optimize the training process, we leverage
a common tokenizer across all client devices, which reduces the computational
load. Additionally, we analyze resource consumption and communication costs on
smartphones to assess their impact in a real-world FL environment. Our
experimental results demonstrate that the federated models achieve comparable
performance to the centralized models. This study highlights the potential of
FL for decentralized mental health prediction by providing a secure and
efficient model training process on edge devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures, Submitted to IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Role of Attention Heads in Large Language Model Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhong Zhou, Haiyang Yu, Xinghua Zhang, Rongwu Xu, Fei Huang, Kun Wang, Yang Liu, Junfeng Fang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) achieve state-of-the-art performance on multiple
language tasks, yet their safety guardrails can be circumvented, leading to
harmful generations. In light of this, recent research on safety mechanisms has
emerged, revealing that when safety representations or component are
suppressed, the safety capability of LLMs are compromised. However, existing
research tends to overlook the safety impact of multi-head attention
mechanisms, despite their crucial role in various model functionalities. Hence,
in this paper, we aim to explore the connection between standard attention
mechanisms and safety capability to fill this gap in the safety-related
mechanistic interpretability. We propose a novel metric which tailored for
multi-head attention, the Safety Head ImPortant Score (Ships), to assess the
individual heads' contributions to model safety. Based on this, we generalize
Ships to the dataset level and further introduce the Safety Attention Head
AttRibution Algorithm (Sahara) to attribute the critical safety attention heads
inside the model. Our findings show that the special attention head has a
significant impact on safety. Ablating a single safety head allows aligned
model (e.g., Llama-2-7b-chat) to respond to 16 times more harmful queries,
while only modifying 0.006% of the parameters, in contrast to the ~ 5%
modification required in previous studies. More importantly, we demonstrate
that attention heads primarily function as feature extractors for safety and
models fine-tuned from the same base model exhibit overlapping safety heads
through comprehensive experiments. Together, our attribution approach and
findings provide a novel perspective for unpacking the black box of safety
mechanisms within large models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 18 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Function Placement in Virtual Networks: An Online Learning
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Huang, Richard Combes, Hind Castel-Taleb, Badii Jouaber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a model for the virtual function placement problem and several
novel algorithms using ideas based on multi-armed bandits. We prove that these
algorithms learn the optimal placement policy rapidly, and their regret grows
at a rate at most $O( N M \sqrt{T\ln T} )$ while respecting the feasibility
constraints with high probability. We show through numerical experiments that
those algorithms both have good practical performance and modest computational
complexity. Using the proposed acceleration technique, they can be used to
learn in large networks where computational power is limited. Our experiments
are fully reproducible, and the code is publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ab initio nonparametric variable selection for scalable Symbolic
  Regression with large $p$ 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengbin Ye, Meng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symbolic regression (SR) is a powerful technique for discovering symbolic
expressions that characterize nonlinear relationships in data, gaining
increasing attention for its interpretability, compactness, and robustness.
However, existing SR methods do not scale to datasets with a large number of
input variables (referred to as extreme-scale SR), which are common in modern
scientific applications. This ``large $p$'' setting, often accompanied by
measurement error, leads to slow performance of SR methods and overly complex
expressions that are difficult to interpret. To address this scalability
challenge, we propose a method called PAN+SR, which combines a key idea of ab
initio nonparametric variable selection with SR to efficiently pre-screen large
input spaces and reduce search complexity while maintaining accuracy. The use
of nonparametric methods eliminates model misspecification, supporting a
strategy called parametric-assisted nonparametric (PAN). We also extend
SRBench, an open-source benchmarking platform, by incorporating
high-dimensional regression problems with various signal-to-noise ratios. Our
results demonstrate that PAN+SR consistently enhances the performance of 17
contemporary SR methods, enabling several to achieve state-of-the-art
performance on these challenging datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Model Discovery for Tensional Homeostasis: Constitutive
  Machine Learning in Growth and Remodeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hagen Holthusen, Tim Brepols, Kevin Linka, Ellen Kuhl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft biological tissues exhibit a tendency to maintain a preferred state of
tensile stress, known as tensional homeostasis, which is restored even after
external mechanical stimuli. This macroscopic behavior can be described using
the theory of kinematic growth, where the deformation gradient is
multiplicatively decomposed into an elastic part and a part related to growth
and remodeling. Recently, the concept of homeostatic surfaces was introduced to
define the state of homeostasis and the evolution equations for inelastic
deformations.
  However, identifying the optimal model and material parameters to accurately
capture the macroscopic behavior of inelastic materials can only be
accomplished with significant expertise, is often time-consuming, and prone to
error, regardless of the specific inelastic phenomenon. To address this
challenge, built-in physics machine learning algorithms offer significant
potential.
  In this work, we extend our inelastic Constitutive Artificial Neural Networks
(iCANNs) by incorporating kinematic growth and homeostatic surfaces to discover
the scalar model equations, namely the Helmholtz free energy and the pseudo
potential. The latter describes the state of homeostasis in a smeared sense. We
evaluate the ability of the proposed network to learn from experimentally
obtained tissue equivalent data at the material point level, assess its
predictive accuracy beyond the training regime, and discuss its current
limitations when applied at the structural level.
  Our source code, data, examples, and an implementation of the corresponding
material subroutine are made accessible to the public at
https://doi.org/10.5281/zenodo.13946282.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages, 12 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Fine-Tuning</span> Discrete Diffusion Models via Reward Optimization with
  Applications to DNA and Protein Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Wang, Masatoshi Uehara, Yichun He, Amy Wang, Tommaso Biancalani, Avantika Lal, Tommi Jaakkola, Sergey Levine, Hanchen Wang, Aviv Regev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have demonstrated the strong empirical performance of
diffusion models on discrete sequences across domains from natural language to
biological sequence generation. For example, in the protein inverse folding
task, conditional diffusion models have achieved impressive results in
generating natural-like sequences that fold back into the original structure.
However, practical design tasks often require not only modeling a conditional
distribution but also optimizing specific task objectives. For instance, we may
prefer protein sequences with high stability. To address this, we consider the
scenario where we have pre-trained discrete diffusion models that can generate
natural-like sequences, as well as reward models that map sequences to task
objectives. We then formulate the reward maximization problem within discrete
diffusion models, analogous to reinforcement learning (RL), while minimizing
the KL divergence against pretrained diffusion models to preserve naturalness.
To solve this RL problem, we propose a novel algorithm, DRAKES, that enables
direct backpropagation of rewards through entire trajectories generated by
diffusion models, by making the originally non-differentiable trajectories
differentiable using the Gumbel-Softmax trick. Our theoretical analysis
indicates that our approach can generate sequences that are both natural-like
and yield high rewards. While similar tasks have been recently explored in
diffusion models for continuous domains, our work addresses unique algorithmic
and theoretical challenges specific to discrete diffusion models, which arise
from their foundation in continuous-time Markov chains rather than Brownian
motion. Finally, we demonstrate the effectiveness of DRAKES in generating DNA
and protein sequences that optimize enhancer activity and protein stability,
respectively, important tasks for gene therapies and protein-based
therapeutics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Wang, Pei Zhang, Baosong Yang, Derek F. Wong, Rui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM self-evaluation relies on the LLM's own ability to estimate response
correctness, which can greatly improve its deployment reliability. In this
research track, we propose the Chain-of-Embedding (CoE) in the latent space to
enable LLMs to perform output-free self-evaluation. CoE consists of all
progressive hidden states produced during the inference time, which can be
treated as the latent thinking path of LLMs. We find that when LLMs respond
correctly and incorrectly, their CoE features differ, these discrepancies
assist us in estimating LLM response correctness. Experiments in four diverse
domains and seven LLMs fully demonstrate the effectiveness of our method.
Meanwhile, its label-free design intent without any training and
millisecond-level computational cost ensure real-time feedback in large-scale
scenarios. More importantly, we provide interesting insights into LLM response
correctness from the perspective of hidden state changes inside LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 18 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Wearable Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Girish Narayanswamy, Xin Liu, Kumar Ayush, Yuzhe Yang, Xuhai Xu, Shun Liao, Jake Garrison, Shyam Tailor, Jake Sunshine, Yun Liu, Tim Althoff, Shrikanth Narayanan, Pushmeet Kohli, Jiening Zhan, Mark Malhotra, Shwetak Patel, Samy Abdel-Ghaffar, Daniel McDuff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wearable sensors have become ubiquitous thanks to a variety of health
tracking features. The resulting continuous and longitudinal measurements from
everyday life generate large volumes of data; however, making sense of these
observations for scientific and actionable insights is non-trivial. Inspired by
the empirical success of generative modeling, where large neural networks learn
powerful representations from vast amounts of text, image, video, or audio
data, we investigate the scaling properties of sensor foundation models across
compute, data, and model size. Using a dataset of up to 40 million hours of
in-situ heart rate, heart rate variability, electrodermal activity,
accelerometer, skin temperature, and altimeter per-minute data from over
165,000 people, we create LSM, a multimodal foundation model built on the
largest wearable-signals dataset with the most extensive range of sensor
modalities to date. Our results establish the scaling laws of LSM for tasks
such as imputation, interpolation and extrapolation, both across time and
sensor modalities. Moreover, we highlight how LSM enables sample-efficient
downstream learning for tasks like exercise and activity recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Normalizing self-supervised learning for provably reliable Change Point
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandra Bazarova, Evgenia Romanenkova, Alexey Zaytsev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Change point detection (CPD) methods aim to identify abrupt shifts in the
distribution of input data streams. Accurate estimators for this task are
crucial across various real-world scenarios. Yet, traditional unsupervised CPD
techniques face significant limitations, often relying on strong assumptions or
suffering from low expressive power due to inherent model simplicity. In
contrast, representation learning methods overcome these drawbacks by offering
flexibility and the ability to capture the full complexity of the data without
imposing restrictive assumptions. However, these approaches are still emerging
in the CPD field and lack robust theoretical foundations to ensure their
reliability. Our work addresses this gap by integrating the expressive power of
representation learning with the groundedness of traditional CPD techniques. We
adopt spectral normalization (SN) for deep representation learning in CPD tasks
and prove that the embeddings after SN are highly informative for CPD. Our
method significantly outperforms current state-of-the-art methods during the
comprehensive evaluation via three standard CPD datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ H2OVL-Mississippi <span class="highlight-title">Vision Language</span> Models Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaikat Galib, Shanshan Wang, Guanshuo Xu, Pascal Pfeiffer, Ryan Chesler, Mark Landry, Sri Satish Ambati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Smaller vision-language models (VLMs) are becoming increasingly important for
privacy-focused, on-device applications due to their ability to run efficiently
on consumer hardware for processing enterprise commercial documents and images.
These models require strong language understanding and visual capabilities to
enhance human-machine interaction. To address this need, we present
H2OVL-Mississippi, a pair of small VLMs trained on 37 million image-text pairs
using 240 hours of compute on 8 x H100 GPUs. H2OVL-Mississippi-0.8B is a tiny
model with 0.8 billion parameters that specializes in text recognition,
achieving state of the art performance on the Text Recognition portion of
OCRBench and surpassing much larger models in this area. Additionally, we are
releasing H2OVL-Mississippi-2B, a 2 billion parameter model for general use
cases, exhibiting highly competitive metrics across various academic
benchmarks. Both models build upon our prior work with H2O-Danube language
models, extending their capabilities into the visual domain. We release them
under the Apache 2.0 license, making VLMs accessible to everyone, democratizing
document AI and visual LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All models are wrong, some are useful: Model Selection with Limited
  Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrik Okanovic, Andreas Kirsch, Jannes Kasper, Torsten Hoefler, Andreas Krause, Nezihe Merve Gürel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the multitude of pretrained models available thanks to the advancements
in large-scale supervised and self-supervised learning, choosing the right
model is becoming increasingly pivotal in the machine learning lifecycle.
However, much like the training process, choosing the best pretrained
off-the-shelf model for raw, unlabeled data is a labor-intensive task. To
overcome this, we introduce MODEL SELECTOR, a framework for label-efficient
selection of pretrained classifiers. Given a pool of unlabeled target data,
MODEL SELECTOR samples a small subset of highly informative examples for
labeling, in order to efficiently identify the best pretrained model for
deployment on this target dataset. Through extensive experiments, we
demonstrate that MODEL SELECTOR drastically reduces the need for labeled data
while consistently picking the best or near-best performing model. Across 18
model collections on 16 different datasets, comprising over 1,500 pretrained
models, MODEL SELECTOR reduces the labeling cost by up to 94.15% to identify
the best model compared to the cost of the strongest baseline. Our results
further highlight the robustness of MODEL SELECTOR in model selection, as it
reduces the labeling cost by up to 72.41% when selecting a near-best model,
whose accuracy is only within 1% of the best model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transformer-Based Approaches for Sensor-Based Human Activity
  Recognition: Opportunities and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clayton Souza Leite, Henry Mauranen, Aziza Zhanabatyrova, Yu Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have excelled in natural language processing and computer
vision, paving their way to sensor-based Human Activity Recognition (HAR).
Previous studies show that transformers outperform their counterparts
exclusively when they harness abundant data or employ compute-intensive
optimization algorithms. However, neither of these scenarios is viable in
sensor-based HAR due to the scarcity of data in this field and the frequent
need to perform training and inference on resource-constrained devices. Our
extensive investigation into various implementations of transformer-based
versus non-transformer-based HAR using wearable sensors, encompassing more than
500 experiments, corroborates these concerns. We observe that transformer-based
solutions pose higher computational demands, consistently yield inferior
performance, and experience significant performance degradation when quantized
to accommodate resource-constrained devices. Additionally, transformers
demonstrate lower robustness to adversarial attacks, posing a potential threat
to user trust in HAR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Satellite Non-IID Imagery: A Spectral Clustering-Assisted
  Federated Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luyao Zou, Yu Min Park, Chu Myaet Thwal, Yan Kyaw Tun, Zhu Han, Choong Seon Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low Earth orbit (LEO) satellites are capable of gathering abundant Earth
observation data (EOD) to enable different Internet of Things (IoT)
applications. However, to accomplish an effective EOD processing mechanism, it
is imperative to investigate: 1) the challenge of processing the observed data
without transmitting those large-size data to the ground because the connection
between the satellites and the ground stations is intermittent, and 2) the
challenge of processing the non-independent and identically distributed
(non-IID) satellite data. In this paper, to cope with those challenges, we
propose an orbit-based spectral clustering-assisted clustered federated
self-knowledge distillation (OSC-FSKD) approach for each orbit of an LEO
satellite constellation, which retains the advantage of FL that the observed
data does not need to be sent to the ground. Specifically, we introduce
normalized Laplacian-based spectral clustering (NLSC) into federated learning
(FL) to create clustered FL in each round to address the challenge resulting
from non-IID data. Particularly, NLSC is adopted to dynamically group clients
into several clusters based on cosine similarities calculated by model updates.
In addition, self-knowledge distillation is utilized to construct each local
client, where the most recent updated local model is used to guide current
local model training. Experiments demonstrate that the observation accuracy
obtained by the proposed method is separately 1.01x, 2.15x, 1.10x, and 1.03x
higher than that of pFedSD, FedProx, FedAU, and FedALA approaches using the
SAT4 dataset. The proposed method also shows superiority when using other
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-Guided Multi-Property Molecular Optimization with a Diffusion
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Xiong, Kun Li, Weiwei Liu, Jia Wu, Bo Du, Shirui Pan, Wenbin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Molecular optimization (MO) is a crucial stage in drug discovery in which
task-oriented generated molecules are optimized to meet practical industrial
requirements. Existing mainstream MO approaches primarily utilize external
property predictors to guide iterative property optimization. However, learning
all molecular samples in the vast chemical space is unrealistic for predictors.
As a result, errors and noise are inevitably introduced during property
prediction due to the nature of approximation. This leads to discrepancy
accumulation, generalization reduction and suboptimal molecular candidates. In
this paper, we propose a text-guided multi-property molecular optimization
method utilizing transformer-based diffusion language model (TransDLM).
TransDLM leverages standardized chemical nomenclature as semantic
representations of molecules and implicitly embeds property requirements into
textual descriptions, thereby preventing error propagation during diffusion
process. Guided by physically and chemically detailed textual descriptions,
TransDLM samples and optimizes encoded source molecules, retaining core
scaffolds of source molecules and ensuring structural similarities. Moreover,
TransDLM enables simultaneous sampling of multiple molecules, making it ideal
for scalable, efficient large-scale optimization through distributed
computation on web platforms. Furthermore, our approach surpasses
state-of-the-art methods in optimizing molecular structural similarity and
enhancing chemical properties on the benchmark dataset. The code is available
at: https://anonymous.4open.science/r/TransDLM-A901.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Better Performance in Incomplete LDL: Addressing Data Imbalance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Kou, Haoyuan Xuan, Jing Wang, Yuheng Jia, Xin Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Label Distribution Learning (LDL) is a novel machine learning paradigm that
addresses the problem of label ambiguity and has found widespread applications.
Obtaining complete label distributions in real-world scenarios is challenging,
which has led to the emergence of Incomplete Label Distribution Learning
(InLDL). However, the existing InLDL methods overlook a crucial aspect of LDL
data: the inherent imbalance in label distributions. To address this
limitation, we propose \textbf{Incomplete and Imbalance Label Distribution
Learning (I\(^2\)LDL)}, a framework that simultaneously handles incomplete
labels and imbalanced label distributions. Our method decomposes the label
distribution matrix into a low-rank component for frequent labels and a sparse
component for rare labels, effectively capturing the structure of both head and
tail labels. We optimize the model using the Alternating Direction Method of
Multipliers (ADMM) and derive generalization error bounds via Rademacher
complexity, providing strong theoretical guarantees. Extensive experiments on
15 real-world datasets demonstrate the effectiveness and robustness of our
proposed framework compared to existing InLDL methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample Compression Hypernetworks: From Generalization Bounds to
  Meta-Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Leblanc, Mathieu Bazinet, Nathaniel D'Amours, Alexandre Drouin, Pascal Germain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstruction functions are pivotal in sample compression theory, a
framework for deriving tight generalization bounds. From a small sample of the
training set (the compression set) and an optional stream of information (the
message), they recover a predictor previously learned from the whole training
set. While usually fixed, we propose to learn reconstruction functions. To
facilitate the optimization and increase the expressiveness of the message, we
derive a new sample compression generalization bound for real-valued messages.
From this theoretical analysis, we then present a new hypernetwork architecture
that outputs predictors with tight generalization guarantees when trained using
an original meta-learning framework. The results of promising preliminary
experiments are then reported.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the NeurIPS 2024 workshop on Compression in Machine
  Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ornstein-Uhlenbeck Adaptation as a Mechanism for Learning in Brains and
  Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesus Garcia Fernandez, Nasir Ahmad, Marcel van Gerven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning is a fundamental property of intelligent systems, observed across
biological organisms and engineered systems. While modern intelligent systems
typically rely on gradient descent for learning, the need for exact gradients
and complex information flow makes its implementation in biological and
neuromorphic systems challenging. This has motivated the exploration of
alternative learning mechanisms that can operate locally and do not rely on
exact gradients. In this work, we introduce a novel approach that leverages
noise in the parameters of the system and global reinforcement signals. Using
an Ornstein-Uhlenbeck process with adaptive dynamics, our method balances
exploration and exploitation during learning, driven by deviations from error
predictions, akin to reward prediction error. Operating in continuous time,
Orstein-Uhlenbeck adaptation (OUA) is proposed as a general mechanism for
learning dynamic, time-evolving environments. We validate our approach across
diverse tasks, including supervised learning and reinforcement learning in
feedforward and recurrent systems. Additionally, we demonstrate that it can
perform meta-learning, adjusting hyper-parameters autonomously. Our results
indicate that OUA provides a viable alternative to traditional gradient-based
methods, with potential applications in neuromorphic computing. It also hints
at a possible mechanism for noise-driven learning in the brain, where
stochastic neurotransmitter release may guide synaptic adjustments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive and oblivious statistical adversaries are equivalent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Blanc, Gregory Valiant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We resolve a fundamental question about the ability to perform a statistical
task, such as learning, when an adversary corrupts the sample. Such adversaries
are specified by the types of corruption they can make and their level of
knowledge about the sample. The latter distinguishes between sample-adaptive
adversaries which know the contents of the sample when choosing the corruption,
and sample-oblivious adversaries, which do not. We prove that for all types of
corruptions, sample-adaptive and sample-oblivious adversaries are
\emph{equivalent} up to polynomial factors in the sample size. This resolves
the main open question introduced by \cite{BLMT22} and further explored in
\cite{CHLLN23}.
  Specifically, consider any algorithm $A$ that solves a statistical task even
when a sample-oblivious adversary corrupts its input. We show that there is an
algorithm $A'$ that solves the same task when the corresponding sample-adaptive
adversary corrupts its input. The construction of $A'$ is simple and maintains
the computational efficiency of $A$: It requests a polynomially larger sample
than $A$ uses and then runs $A$ on a uniformly random subsample.
  One of our main technical tools is a new structural result relating two
distributions defined on sunflowers which may be of independent interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Adversarial Synthesis of Radar Point Cloud Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Saad Nawaz, Thomas Dallmann, Torsten Schoen, Dirk Heberling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the validation and verification of automotive radars, datasets of
realistic traffic scenarios are required, which, how ever, are laborious to
acquire. In this paper, we introduce radar scene synthesis using GANs as an
alternative to the real dataset acquisition and simulation-based approaches. We
train a PointNet++ based GAN model to generate realistic radar point cloud
scenes and use a binary classifier to evaluate the performance of scenes
generated using this model against a test set of real scenes. We demonstrate
that our GAN model achieves similar performance (~87%) to the real scenes test
set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICMIM 2024; 7th IEEE MTT Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PORTAL: Scalable Tabular Foundation Models via Content-Specific
  Tokenization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Spinaci, Marek Polewczyk, Johannes Hoffart, Markus C. Kohler, Sam Thelin, Tassilo Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning on tabular data seeks to apply advances from natural
language and image domains to the diverse domain of tables. However, current
techniques often struggle with integrating multi-domain data and require data
cleaning or specific structural requirements, limiting the scalability of
pre-training datasets. We introduce PORTAL (Pretraining One-Row-at-a-Time for
All tabLes), a framework that handles various data modalities without the need
for cleaning or preprocessing. This simple yet powerful approach can be
effectively pre-trained on online-collected datasets and fine-tuned to match
state-of-the-art methods on complex classification and regression tasks. This
work offers a practical advancement in self-supervised learning for large-scale
tabular data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Table Representation Learning Workshop at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CERES: Critical-Event Reconstruction via Temporal Scene Graph Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Efimia Panagiotaki, Georgi Pramatarov, Lars Kunze, Daniele De Martini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a method for on-demand scenario generation in simulation,
grounded on real-world data. Evaluating the behaviour of Autonomous Vehicles
(AVs) in both safety-critical and regular scenarios is essential for assessing
their robustness before real-world deployment. By integrating scenarios derived
from real-world datasets into the simulation, we enhance the plausibility and
validity of testing sets. This work introduces a novel approach that employs
temporal scene graphs to capture evolving spatiotemporal relationships among
scene entities from a real-world dataset, enabling the generation of dynamic
scenarios in simulation through Graph Neural Networks (GNNs). User-defined
action and criticality conditioning are used to ensure flexible, tailored
scenario creation. Our model significantly outperforms the benchmarks in
accurately predicting links corresponding to the requested scenarios. We
further evaluate the validity and compatibility of our generated scenarios in
an off-the-shelf simulator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily
  Complex Proofs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Opedal, Haruki Shirakami, Bernhard Schölkopf, Abulhair Saparov, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can solve arithmetic word problems with high
accuracy, but little is known about how well they generalize to problems that
are more complex than the ones on which they have been trained. Empirical
investigations of such questions are impeded by two major flaws of current
evaluations: (i) much of the evaluation data is contaminated, in the sense that
it has already been seen during training, and (ii) benchmark datasets do not
capture how problem proofs may be arbitrarily complex in various ways. As a
step towards addressing these issues, we present a framework for evaluating
LLMs on problems that have arbitrarily complex arithmetic proofs, called
MathGAP. MathGAP generates problems that follow fixed proof specifications --
along with chain-of-thought reasoning annotations -- enabling systematic
studies on generalization with respect to arithmetic proof complexity. We apply
MathGAP to analyze how in-context learning interacts with generalization to
problems that have more complex proofs. We find that among the models tested,
most show a significant decrease in performance as proofs get deeper and wider.
This effect is more pronounced in complex, nonlinear proof structures, which
are challenging even for GPT-4o. Surprisingly, providing in-context examples
from the same distribution as the test set is not always beneficial for
performance. In particular, zero-shot prompting as well as demonstrating a
diverse range of examples that are less complex than the test data sometimes
yield similar or higher accuracies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Large Language Models and Reinforcement Learning for
  Non-Linear Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoav Alon, Cristina David
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) were shown to struggle with long-term planning,
which may be caused by the limited way in which they explore the space of
possible solutions. We propose an architecture where a Reinforcement Learning
(RL) Agent guides an LLM's space exploration: (1) the Agent has access to
domain-specific information, and can therefore make decisions about the quality
of candidate solutions based on specific and relevant metrics, which were not
explicitly considered by the LLM's training objective; (2) the LLM can focus on
generating immediate next steps, without the need for long-term planning. We
allow non-linear reasoning by exploring alternative paths and backtracking. We
evaluate this architecture on the program equivalence task, and compare it
against Chain of Thought (CoT) and Tree of Thoughts (ToT). We assess both the
downstream task, denoting the binary classification, and the intermediate
reasoning steps. Our approach compares positively against CoT and ToT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAda-Net: A Self-Supervised Adaptive Stereo Estimation CNN For Remote
  Sensing Image Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Hirner, Friedrich Fraundorfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stereo estimation has made many advancements in recent years with the
introduction of deep-learning. However the traditional supervised approach to
deep-learning requires the creation of accurate and plentiful ground-truth
data, which is expensive to create and not available in many situations. This
is especially true for remote sensing applications, where there is an excess of
available data without proper ground truth. To tackle this problem, we propose
a self-supervised CNN with self-improving adaptive abilities. In the first
iteration, the created disparity map is inaccurate and noisy. Leveraging the
left-right consistency check, we get a sparse but more accurate disparity map
which is used as an initial pseudo ground-truth. This pseudo ground-truth is
then adapted and updated after every epoch in the training step of the network.
We use the sum of inconsistent points in order to track the network
convergence. The code for our method is publicly available at:
https://github.com/thedodo/SAda-Net}{https://github.com/thedodo/SAda-Net
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Will be presented at ICPR2024 in December 2024 in Kolkata, India</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Text Generation in Joint NLG/NLU Learning Through Curriculum
  Learning, Semi-Supervised Training, and Advanced Optimization Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahimanuddin Shaik, Katikela Sreeharsha Kishore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text generation is the automated process of producing written or spoken
language using computational methods. It involves generating coherent and
contextually relevant text based on predefined rules or learned patterns.
However, challenges in text generation arise from maintaining coherence,
ensuring diversity and creativity, and avoiding biases or inappropriate
content. This research paper developed a novel approach to improve text
generation in the context of joint Natural Language Generation (NLG) and
Natural Language Understanding (NLU) learning. The data is prepared by
gathering and preprocessing annotated datasets, including cleaning,
tokenization, stemming, and stop-word removal. Feature extraction techniques
such as POS tagging, Bag of words, and Term Frequency-Inverse Document
Frequency (TF-IDF) are applied. Transformer-based encoders and decoders,
capturing long range dependencies and improving source-target sequence
modelling. Pre-trained language models like Optimized BERT are incorporated,
along with a Hybrid Redfox Artificial Hummingbird Algorithm (HRAHA).
Reinforcement learning with policy gradient techniques, semi-supervised
training, improved attention mechanisms, and differentiable approximations like
straight-through Gumbel SoftMax estimator are employed to fine-tune the models
and handle complex linguistic tasks effectively. The proposed model is
implemented using Python.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Reinforcement Learning for Online Optimal Execution Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Micheli, Mélodie Monod
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper tackles the challenge of learning non-Markovian optimal execution
strategies in dynamic financial markets. We introduce a novel actor-critic
algorithm based on Deep Deterministic Policy Gradient (DDPG) to address this
issue, with a focus on transient price impact modeled by a general decay
kernel. Through numerical experiments with various decay kernels, we show that
our algorithm successfully approximates the optimal execution strategy.
Additionally, the proposed algorithm demonstrates adaptability to evolving
market conditions, where parameters fluctuate over time. Our findings also show
that modern reinforcement learning algorithms can provide a solution that
reduces the need for frequent and inefficient human intervention in optimal
execution tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Novelty-based Sample Reuse for Continuous Robotics Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Duan, Kai Yang, Houde Liu, Xueqian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reinforcement learning, agents collect state information and rewards
through environmental interactions, essential for policy refinement. This
process is notably time-consuming, especially in complex robotic simulations
and real-world applications. Traditional algorithms usually re-engage with the
environment after processing a single batch of samples, thereby failing to
fully capitalize on historical data. However, frequently observed states, with
reliable value estimates, require minimal updates; in contrast, rare observed
states necessitate more intensive updates for achieving accurate value
estimations. To address uneven sample utilization, we propose Novelty-guided
Sample Reuse (NSR). NSR provides extra updates for infrequent, novel states and
skips additional updates for frequent states, maximizing sample use before
interacting with the environment again. Our experiments show that NSR improves
the convergence rate and success rate of algorithms without significantly
increasing time consumption. Our code is publicly available at
https://github.com/ppksigs/NSR-DDPG-HER.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing Through VisualBERT: A Causal Adventure on Memetic Landscapes <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dibyanayan Bandyopadhyay, Mohammed Hasanuzzaman, Asif Ekbal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting offensive memes is crucial, yet standard deep neural network
systems often remain opaque. Various input attribution-based methods attempt to
interpret their behavior, but they face challenges with implicitly offensive
memes and non-causal attributions. To address these issues, we propose a
framework based on a Structural Causal Model (SCM). In this framework,
VisualBERT is trained to predict the class of an input meme based on both meme
input and causal concepts, allowing for transparent interpretation. Our
qualitative evaluation demonstrates the framework's effectiveness in
understanding model behavior, particularly in determining whether the model was
right due to the right reason, and in identifying reasons behind
misclassification. Additionally, quantitative analysis assesses the
significance of proposed modelling choices, such as de-confounding, adversarial
learning, and dynamic routing, and compares them with input attribution
methods. Surprisingly, we find that input attribution methods do not guarantee
causality within our framework, raising questions about their reliability in
safety-critical applications. The project page is at:
https://newcodevelop.github.io/causality_adventure/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpreting Temporal Graph Neural Networks with Koopman Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Guerra, Simone Scardapane, Filippo Maria Bianchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatiotemporal graph neural networks (STGNNs) have shown promising results in
many domains, from forecasting to epidemiology. However, understanding the
dynamics learned by these models and explaining their behaviour is
significantly more complex than for models dealing with static data. Inspired
by Koopman theory, which allows a simpler description of intricate, nonlinear
dynamical systems, we introduce an explainability approach for temporal graphs.
We present two methods to interpret the STGNN's decision process and identify
the most relevant spatial and temporal patterns in the input for the task at
hand. The first relies on dynamic mode decomposition (DMD), a Koopman-inspired
dimensionality reduction method. The second relies on sparse identification of
nonlinear dynamics (SINDy), a popular method for discovering governing
equations, which we use for the first time as a general tool for
explainability. We show how our methods can correctly identify interpretable
features such as infection times and infected nodes in the context of
dissemination processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Truncating Trajectories in Monte Carlo Policy Evaluation: an Adaptive
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13463v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13463v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Poiani, Nicole Nobili, Alberto Maria Metelli, Marcello Restelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Policy evaluation via Monte Carlo (MC) simulation is at the core of many MC
Reinforcement Learning (RL) algorithms (e.g., policy gradient methods). In this
context, the designer of the learning system specifies an interaction budget
that the agent usually spends by collecting trajectories of fixed length within
a simulator. However, is this data collection strategy the best option? To
answer this question, in this paper, we propose as a quality index a surrogate
of the mean squared error of a return estimator that uses trajectories of
different lengths, i.e., \emph{truncated}. Specifically, this surrogate shows
the sub-optimality of the fixed-length trajectory schedule. Furthermore, it
suggests that adaptive data collection strategies that spend the available
budget sequentially can allocate a larger portion of transitions in timesteps
in which more accurate sampling is required to reduce the error of the final
estimate. Building on these findings, we present an adaptive algorithm called
Robust and Iterative Data collection strategy Optimization (RIDO). The main
intuition behind RIDO is to split the available interaction budget into
mini-batches. At each round, the agent determines the most convenient schedule
of trajectories that minimizes an empirical and robust version of the surrogate
of the estimator's error. After discussing the theoretical properties of our
method, we conclude by assessing its performance across multiple domains. Our
results show that RIDO can adapt its trajectory schedule toward timesteps where
more sampling is required to increase the quality of the final estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Mixed-Precision Decoding for Efficient LLM Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Mark Chen, Fuwen Tan, Alexandros Kouris, Royson Lee, Hongxiang Fan, Stylianos I. Venieris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In spite of the great potential of large language models (LLMs) across
various tasks, their deployment on resource-constrained devices remains
challenging due to their excessive computational and memory demands.
Quantization has emerged as an effective solution by storing weights in reduced
precision. However, utilizing low precisions (i.e.~2/3-bit) to substantially
alleviate the memory-boundedness of LLM decoding, still suffers from
prohibitive performance drop. In this work, we argue that existing approaches
fail to explore the diversity in computational patterns, redundancy, and
sensitivity to approximations of the different phases of LLM inference,
resorting to a uniform quantization policy throughout. Instead, we propose a
novel phase-aware method that selectively allocates precision during different
phases of LLM inference, achieving both strong context extraction during
prefill and efficient memory bandwidth utilization during decoding. To further
address the memory-boundedness of the decoding phase, we introduce Progressive
Mixed-Precision Decoding (PMPD), a technique that enables the gradual lowering
of precision deeper in the generated sequence, together with a spectrum of
precision-switching schedulers that dynamically drive the precision-lowering
decisions in either task-adaptive or prompt-adaptive manner. Extensive
evaluation across diverse language tasks shows that when targeting Nvidia GPUs,
PMPD achieves 1.4$-$12.2$\times$ speedup in matrix-vector multiplications over
fp16 models, while when targeting an LLM-optimized NPU, our approach delivers a
throughput gain of 3.8$-$8.0$\times$ over fp16 models and up to 1.54$\times$
over uniform quantization approaches while preserving the output quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking the Manual Annotation Bottleneck: Creating a Comprehensive
  Legal Case Criticality Dataset through Semi-Automated Labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronja Stern, Ken Kawamura, Matthias Stürmer, Ilias Chalkidis, Joel Niklaus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting case criticality helps legal professionals in the court system
manage large volumes of case law. This paper introduces the Criticality
Prediction dataset, a new resource for evaluating the potential influence of
Swiss Federal Supreme Court decisions on future jurisprudence. Unlike existing
approaches that rely on resource-intensive manual annotations, we
semi-automatically derive labels leading to a much larger dataset than
otherwise possible. Our dataset features a two-tier labeling system: (1) the
LD-Label, which identifies cases published as Leading Decisions (LD), and (2)
the Citation-Label, which ranks cases by their citation frequency and recency.
This allows for a more nuanced evaluation of case importance. We evaluate
several multilingual models, including fine-tuned variants and large language
models, and find that fine-tuned models consistently outperform zero-shot
baselines, demonstrating the need for task-specific adaptation. Our
contributions include the introduction of this task and the release of a
multilingual dataset to the research community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking Legal Knowledge: A Multilingual Dataset for Judicial
  Summarization in Switzerland 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Rolshoven, Vishvaksenan Rasiah, Srinanda Brügger Bose, Matthias Stürmer, Joel Niklaus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal research is a time-consuming task that most lawyers face on a daily
basis. A large part of legal research entails looking up relevant caselaw and
bringing it in relation to the case at hand. Lawyers heavily rely on summaries
(also called headnotes) to find the right cases quickly. However, not all
decisions are annotated with headnotes and writing them is time-consuming.
Automated headnote creation has the potential to make hundreds of thousands of
decisions more accessible for legal research in Switzerland alone. To kickstart
this, we introduce the Swiss Leading Decision Summarization ( SLDS) dataset, a
novel cross-lingual resource featuring 18K court rulings from the Swiss Federal
Supreme Court (SFSC), in German, French, and Italian, along with German
headnotes. We fine-tune and evaluate three mT5 variants, along with proprietary
models. Our analysis highlights that while proprietary models perform well in
zero-shot and one-shot settings, fine-tuned smaller models still provide a
strong competitive edge. We publicly release the dataset to facilitate further
research in multilingual legal summarization and the development of assistive
technologies for legal professionals
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Estimation of Partial Dependence Functions using Trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyang Liu, Tessa Steensgaard, Marvin N. Wright, Niklas Pfister, Munir Hiabu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many existing interpretation methods are based on Partial Dependence (PD)
functions that, for a pre-trained machine learning model, capture how a subset
of the features affects the predictions by averaging over the remaining
features. Notable methods include Shapley additive explanations (SHAP) which
computes feature contributions based on a game theoretical interpretation and
PD plots (i.e., 1-dim PD functions) that capture average marginal main effects.
Recent work has connected these approaches using a functional decomposition and
argues that SHAP values can be misleading since they merge main and interaction
effects into a single local effect. A major advantage of SHAP compared to other
PD-based interpretations, however, has been the availability of fast estimation
techniques, such as \texttt{TreeSHAP}. In this paper, we propose a new
tree-based estimator, \texttt{FastPD}, which efficiently estimates arbitrary PD
functions. We show that \texttt{FastPD} consistently estimates the desired
population quantity -- in contrast to path-dependent \texttt{TreeSHAP} which is
inconsistent when features are correlated. For moderately deep trees,
\texttt{FastPD} improves the complexity of existing methods from quadratic to
linear in the number of observations. By estimating PD functions for arbitrary
feature subsets, \texttt{FastPD} can be used to extract PD-based
interpretations such as SHAP, PD plots and higher order interaction effects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter-efficient Adaptation of Multilingual <span class="highlight-title">Multimodal</span> Models for
  Low-resource ASR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Gupta, Amruta Parulekar, Sameep Chattopadhyay, Preethi Jyothi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic speech recognition (ASR) for low-resource languages remains a
challenge due to the scarcity of labeled training data. Parameter-efficient
fine-tuning and text-only adaptation are two popular methods that have been
used to address such low-resource settings. In this work, we investigate how
these techniques can be effectively combined using a multilingual multimodal
model like SeamlessM4T. Multimodal models are able to leverage unlabeled text
via text-only adaptation with further parameter-efficient ASR fine-tuning, thus
boosting ASR performance. We also show cross-lingual transfer from a
high-resource language, achieving up to a relative 17% WER reduction over a
baseline in a zero-shot setting without any labeled speech.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Similarity-Dissimilarity Loss with Supervised Contrastive Learning for
  Multi-label Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangming Huang, Yunfei Long, Cunjin Luo, Sheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised contrastive learning has been explored in making use of label
information for multi-label classification, but determining positive samples in
multi-label scenario remains challenging. Previous studies have examined
strategies for identifying positive samples, considering label overlap
proportion between anchors and samples. However, they ignore various relations
between given anchors and samples, as well as how to dynamically adjust the
weights in contrastive loss functions based on different relations, leading to
great ambiguity. In this paper, we introduce five distinct relations between
multi-label samples and propose a Similarity-Dissimilarity Loss with
contrastive learning for multi-label classification. Our loss function
re-weights the loss by computing the similarity and dissimilarity between
positive samples and a given anchor based on the introduced relations. We
mainly conduct experiments for multi-label text classification on MIMIC
datasets, then further extend the evaluation on MS-COCO. The Experimental
results show that our proposed loss effectively improves the performance on all
encoders under supervised contrastive learning paradigm, demonstrating its
effectiveness and robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving Prior Distribution Mismatch in Diffusion Models via Optimal
  Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanpeng Wang, Shenghao Li, Chen Wang, Shuting Cao, Na Lei, Zhongxuan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the knowledge surrounding diffusion models(DMs) has grown
significantly, though several theoretical gaps remain. Particularly noteworthy
is prior error, defined as the discrepancy between the termination distribution
of the forward process and the initial distribution of the reverse process. To
address these deficiencies, this paper explores the deeper relationship between
optimal transport(OT) theory and DMs with discrete initial distribution.
Specifically, we demonstrate that the two stages of DMs fundamentally involve
computing time-dependent OT. However, unavoidable prior error result in
deviation during the reverse process under quadratic transport cost. By proving
that as the diffusion termination time increases, the probability flow
exponentially converges to the gradient of the solution to the classical
Monge-Amp\`ere equation, we establish a vital link between these fields.
Therefore, static OT emerges as the most intrinsic single-step method for
bridging this theoretical potential gap. Additionally, we apply these insights
to accelerate sampling in both unconditional and conditional generation
scenarios. Experimental results across multiple image datasets validate the
effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Partially Trained Graph Convolutional Networks Resist Oversmoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Kelesis, Dimitris Fotakis, Georgios Paliouras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we investigate an observation made by Kipf \& Welling, who
suggested that untrained GCNs can generate meaningful node embeddings. In
particular, we investigate the effect of training only a single layer of a GCN,
while keeping the rest of the layers frozen. We propose a basis on which the
effect of the untrained layers and their contribution to the generation of
embeddings can be predicted. Moreover, we show that network width influences
the dissimilarity of node embeddings produced after the initial node features
pass through the untrained part of the model. Additionally, we establish a
connection between partially trained GCNs and oversmoothing, showing that they
are capable of reducing it. We verify our theoretical results experimentally
and show the benefits of using deep networks that resist oversmoothing, in a
``cold start'' scenario, where there is a lack of feature information for
unlabeled nodes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAMPA: Robotic Augmented Reality for Machine Programming and Automation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatih Dogangun, Serdar Bahar, Yigit Yildirim, Bora Toprak Temir, Emre Ugur, Mustafa Doga Dogan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As robotics continue to enter various sectors beyond traditional industrial
applications, the need for intuitive robot training and interaction systems
becomes increasingly more important. This paper introduces Robotic Augmented
Reality for Machine Programming (RAMPA), a system that utilizes the
capabilities of state-of-the-art and commercially available AR headsets, e.g.,
Meta Quest 3, to facilitate the application of Programming from Demonstration
(PfD) approaches on industrial robotic arms, such as Universal Robots UR10. Our
approach enables in-situ data recording, visualization, and fine-tuning of
skill demonstrations directly within the user's physical environment. RAMPA
addresses critical challenges of PfD, such as safety concerns, programming
barriers, and the inefficiency of collecting demonstrations on the actual
hardware. The performance of our system is evaluated against the traditional
method of kinesthetic control in teaching three different robotic manipulation
tasks and analyzed with quantitative metrics, measuring task performance and
completion time, trajectory smoothness, system usability, user experience, and
task load using standardized surveys. Our findings indicate a substantial
advancement in how robotic tasks are taught and refined, promising improvements
in operational safety, efficiency, and user engagement in robotic programming.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoR: Mixture of Ranks for Low-Rank Adaptation Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyu Tang, Yilong Chen, Zhenyu Zhang, Junyuan Shang, Wenyuan Zhang, Yong Huang, Tingwen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) drives research to align its performance with full
fine-tuning. However, significant challenges remain: (1) Simply increasing the
rank size of LoRA does not effectively capture high-rank information, which
leads to a performance bottleneck.(2) MoE-style LoRA methods substantially
increase parameters and inference latency, contradicting the goals of efficient
fine-tuning and ease of application. To address these challenges, we introduce
Mixture of Ranks (MoR), which learns rank-specific information for different
tasks based on input and efficiently integrates multi-rank information. We
firstly propose a new framework that equates the integration of multiple LoRAs
to expanding the rank of LoRA. Moreover, we hypothesize that low-rank LoRA
already captures sufficient intrinsic information, and MoR can derive high-rank
information through mathematical transformations of the low-rank components.
Thus, MoR can reduces the learning difficulty of LoRA and enhances its
multi-task capabilities. MoR achieves impressive results, with MoR delivering a
1.31\% performance improvement while using only 93.93\% of the parameters
compared to baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Breast Cancer Survival: A Survival Analysis Approach Using
  Log Odds and Clinical Variables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Opeyemi Sheu Alamu, Bismar Jorge Gutierrez Choque, Syed Wajeeh Abbs Rizvi, Samah Badr Hammed, Isameldin Elamin Medani, Md Kamrul Siam, Waqar Ahmad Tahir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer remains a significant global health challenge, with prognosis
and treatment decisions largely dependent on clinical characteristics. Accurate
prediction of patient outcomes is crucial for personalized treatment
strategies. This study employs survival analysis techniques, including Cox
proportional hazards and parametric survival models, to enhance the prediction
of the log odds of survival in breast cancer patients. Clinical variables such
as tumor size, hormone receptor status, HER2 status, age, and treatment history
were analyzed to assess their impact on survival outcomes. Data from 1557
breast cancer patients were obtained from a publicly available dataset provided
by the University College Hospital, Ibadan, Nigeria. This dataset was
preprocessed and analyzed using both univariate and multivariate approaches to
evaluate survival outcomes. Kaplan-Meier survival curves were generated to
visualize survival probabilities, while the Cox proportional hazards model
identified key risk factors influencing mortality. The results showed that
older age, larger tumor size, and HER2-positive status were significantly
associated with an increased risk of mortality. In contrast, estrogen receptor
positivity and breast-conserving surgery were linked to better survival
outcomes. The findings suggest that integrating these clinical variables into
predictive models improvesthe accuracy of survival predictions, helping to
identify high-risk patients who may benefit from more aggressive interventions.
This study demonstrates the potential of survival analysis in optimizing breast
cancer care, particularly in resource-limited settings. Future research should
focus on integrating genomic data and real-world clinical outcomes to further
refine these models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Self-Constructing Multi-Expert Fuzzy System for High-dimensional Data
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingtao Ren, Yu-Cheng Chang, Thomas Do, Zehong Cao, Chin-Teng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fuzzy Neural Networks (FNNs) are effective machine learning models for
classification tasks, commonly based on the Takagi-Sugeno-Kang (TSK) fuzzy
system. However, when faced with high-dimensional data, especially with noise,
FNNs encounter challenges such as vanishing gradients, excessive fuzzy rules,
and limited access to prior knowledge. To address these challenges, we propose
a novel fuzzy system, the Self-Constructing Multi-Expert Fuzzy System
(SOME-FS). It combines two learning strategies: mixed structure learning and
multi-expert advanced learning. The former enables each base classifier to
effectively determine its structure without requiring prior knowledge, while
the latter tackles the issue of vanishing gradients by enabling each rule to
focus on its local region, thereby enhancing the robustness of the fuzzy
classifiers. The overall ensemble architecture enhances the stability and
prediction performance of the fuzzy system. Our experimental results
demonstrate that the proposed SOME-FS is effective in high-dimensional tabular
data, especially in dealing with uncertainty. Moreover, our stable rule mining
process can identify concise and core rules learned by the SOME-FS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Counterfactual Distributions via Kernel Nearest Neighbors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyuseong Choi, Jacob Feitelberg, Anish Agarwal, Raaz Dwivedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consider a setting with multiple units (e.g., individuals, cohorts,
geographic locations) and outcomes (e.g., treatments, times, items), where the
goal is to learn a multivariate distribution for each unit-outcome entry, such
as the distribution of a user's weekly spend and engagement under a specific
mobile app version. A common challenge is the prevalence of missing not at
random data, where observations are available only for certain unit-outcome
combinations and the observation availability can be correlated with the
properties of distributions themselves, i.e., there is unobserved confounding.
An additional challenge is that for any observed unit-outcome entry, we only
have a finite number of samples from the underlying distribution. We tackle
these two challenges by casting the problem into a novel distributional matrix
completion framework and introduce a kernel based distributional generalization
of nearest neighbors to estimate the underlying distributions. By leveraging
maximum mean discrepancies and a suitable factor model on the kernel mean
embeddings of the underlying distributions, we establish consistent recovery of
the underlying distributions even when data is missing not at random and
positivity constraints are violated. Furthermore, we demonstrate that our
nearest neighbors approach is robust to heteroscedastic noise, provided we have
access to two or more measurements for the observed unit-outcome entries, a
robustness not present in prior works on nearest neighbors with single
measurements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Augmented Predictive Deep Neural Network: Enhancing the
  extrapolation capabilities of non-intrusive surrogate models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuwen Sun, Lihong Feng, Peter Benner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerically solving a large parametric nonlinear dynamical system is
challenging due to its high complexity and the high computational costs. In
recent years, machine-learning-aided surrogates are being actively researched.
However, many methods fail in accurately generalizing in the entire time
interval $[0, T]$, when the training data is available only in a training time
interval $[0, T_0]$, with $T_0<T$.
  To improve the extrapolation capabilities of the surrogate models in the
entire time domain, we propose a new deep learning framework, where kernel
dynamic mode decomposition (KDMD) is employed to evolve the dynamics of the
latent space generated by the encoder part of a convolutional autoencoder
(CAE). After adding the KDMD-decoder-extrapolated data into the original data
set, we train the CAE along with a feed-forward deep neural network using the
augmented data. The trained network can predict future states outside the
training time interval at any out-of-training parameter samples. The proposed
method is tested on two numerical examples: a FitzHugh-Nagumo model and a model
of incompressible flow past a cylinder. Numerical results show accurate and
fast prediction performance in both the time and the parameter domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Heterogeneity and Heterophily in Graphs: A Heterogeneous
  Heterophilic Spectral Graph Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangkang Lu, Yanhua Yu, Zhiyong Huang, Jia Li, Yuling Wang, Meiyu Liang, Xiting Qin, Yimeng Ren, Tat-Seng Chua, Xidian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have garnered significant scholarly attention
for their powerful capabilities in modeling graph structures. Despite this, two
primary challenges persist: heterogeneity and heterophily. Existing studies
often address heterogeneous and heterophilic graphs separately, leaving a
research gap in the understanding of heterogeneous heterophilic graphs-those
that feature diverse node or relation types with dissimilar connected nodes. To
address this gap, we investigate the application of spectral graph filters
within heterogeneous graphs. Specifically, we propose a Heterogeneous
Heterophilic Spectral Graph Neural Network (H2SGNN), which employs a
dual-module approach: local independent filtering and global hybrid filtering.
The local independent filtering module applies polynomial filters to each
subgraph independently to adapt to different homophily, while the global hybrid
filtering module captures interactions across different subgraphs. Extensive
empirical evaluations on four real-world datasets demonstrate the superiority
of H2SGNN compared to state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Statistical testing on generative AI anomaly detection tools in
  Alzheimer's Disease diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rosemary He, Ichiro Takeuchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's Disease is challenging to diagnose due to our limited
understanding of its mechanism and large heterogeneity among patients.
Neurodegeneration is studied widely as a biomarker for clinical diagnosis,
which can be measured from time series MRI progression. On the other hand,
generative AI has shown promise in anomaly detection in medical imaging and
used for tasks including tumor detection. However, testing the reliability of
such data-driven methods is non-trivial due to the issue of double-dipping in
hypothesis testing. In this work, we propose to solve this issue with selective
inference and develop a reliable generative AI method for Alzheimer's
prediction. We show that compared to traditional statistical methods with
highly inflated p-values, selective inference successfully controls the false
discovery rate under the desired alpha level while retaining statistical power.
In practice, our pipeline could assist clinicians in Alzheimer's diagnosis and
early intervention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Remember, Retrieve and Generate: Understanding Infinite Visual Concepts
  as Your Personalized Assistant 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Hao, Jiaming Han, Changsheng Li, Yu-Feng Li, Xiangyu Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of large language models (LLMs) has significantly enhanced
the capabilities of multimodal LLMs (MLLMs) as general assistants. However,
lack of user-specific knowledge still restricts their application in human's
daily life. In this paper, we introduce the Retrieval Augmented Personalization
(RAP) framework for MLLMs' personalization. Starting from a general MLLM, we
turn it into a personalized assistant in three steps. (a) Remember: We design a
key-value database to store user-related information, e.g., user's name, avatar
and other attributes. (b) Retrieve: When the user initiates a conversation, RAP
will retrieve relevant information from the database using a multimodal
retriever. (c) Generate: The input query and retrieved concepts' information
are fed into MLLMs to generate personalized, knowledge-augmented responses.
Unlike previous methods, RAP allows real-time concept editing via updating the
external database. To further improve generation quality and alignment with
user-specific information, we design a pipeline for data collection and create
a specialized dataset for personalized training of MLLMs. Based on the dataset,
we train a series of MLLMs as personalized multimodal assistants. By
pretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual
concepts without additional finetuning. Our models demonstrate outstanding
flexibility and generation quality across a variety of tasks, such as
personalized image captioning, question answering and visual recognition. The
code, data and models are available at https://github.com/Hoar012/RAP-MLLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representation Learning of Structured Data for Medical Foundation Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijay Prakash Dwivedi, Viktor Schlegel, Andy T. Liu, Thanh-Tung Nguyen, Abhinav Ramesh Kashyap, Jeng Wei, Wei-Hsian Yin, Stefan Winkler, Robby T. Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable performance across
various domains, including healthcare. However, their ability to effectively
represent structured non-textual data, such as the alphanumeric medical codes
used in records like ICD-10 or SNOMED-CT, is limited and has been particularly
exposed in recent research. This paper examines the challenges LLMs face in
processing medical codes due to the shortcomings of current tokenization
methods. As a result, we introduce the UniStruct architecture to design a
multimodal medical foundation model of unstructured text and structured data,
which addresses these challenges by adapting subword tokenization techniques
specifically for the structured medical codes. Our approach is validated
through model pre-training on both an extensive internal medical database and a
public repository of structured medical records. Trained on over 1 billion
tokens on the internal medical database, the proposed model achieves up to a
23% improvement in evaluation metrics, with around 2% gain attributed to our
proposed tokenization. Additionally, when evaluated on the EHRSHOT public
benchmark with a 1/1000 fraction of the pre-training data, the UniStruct model
improves performance on over 42% of the downstream tasks. Our approach not only
enhances the representation and generalization capabilities of patient-centric
models but also bridges a critical gap in representation learning models'
ability to handle complex structured medical data, alongside unstructured text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Workshop on Unifying Representations in Neural Models
  (UniReps 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges
  in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Yuan, Lili Zhao, Kai Zhang, Guangting Zheng, Qi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable capabilities in various
natural language processing tasks. However, LLMs may rely on dataset biases as
shortcuts for prediction, which can significantly impair their robustness and
generalization capabilities. This paper presents Shortcut Suite, a
comprehensive test suite designed to evaluate the impact of shortcuts on LLMs'
performance, incorporating six shortcut types, five evaluation metrics, and
four prompting strategies. Our extensive experiments yield several key
findings: 1) LLMs demonstrate varying reliance on shortcuts for downstream
tasks, significantly impairing their performance. 2) Larger LLMs are more
likely to utilize shortcuts under zero-shot and few-shot in-context learning
prompts. 3) Chain-of-thought prompting notably reduces shortcut reliance and
outperforms other prompting strategies, while few-shot prompts generally
underperform compared to zero-shot prompts. 4) LLMs often exhibit
overconfidence in their predictions, especially when dealing with datasets that
contain shortcuts. 5) LLMs generally have a lower explanation quality in
shortcut-laden datasets, with errors falling into three types: distraction,
disguised comprehension, and logical fallacy. Our findings offer new insights
for evaluating robustness and generalization in LLMs and suggest potential
directions for mitigating the reliance on shortcuts. The code is available at
\url {https://github.com/yyhappier/ShortcutSuite.git}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Limits to scalable evaluation at the frontier: LLM as Judge won't beat
  twice the data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian E. Dorner, Vivian Y. Nastl, Moritz Hardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High quality annotations are increasingly a bottleneck in the explosively
growing machine learning ecosystem. Scalable evaluation methods that avoid
costly annotation have therefore become an important research ambition. Many
hope to use strong existing models in lieu of costly labels to provide cheap
model evaluations. Unfortunately, this method of using models as judges
introduces biases, such as self-preferencing, that can distort model
comparisons. An emerging family of debiasing tools promises to fix these issues
by using a few high quality labels to debias a large number of model judgments.
In this paper, we study how far such debiasing methods, in principle, can go.
Our main result shows that when the judge is no more accurate than the
evaluated model, no debiasing method can decrease the required amount of ground
truth labels by more than half. Our result speaks to the severe limitations of
the LLM-as-a-judge paradigm at the evaluation frontier where the goal is to
assess newly released models that are possibly better than the judge. Through
an empirical evaluation, we demonstrate that the sample size savings achievable
in practice are even more modest than what our theoretical limit suggests.
Along the way, our work provides new observations about debiasing methods for
model evaluation, and points out promising avenues for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffImp: Efficient Diffusion Model for Probabilistic Time Series
  Imputation with Bidirectional Mamba Backbone 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongfan Gao, Wangmeng Shen, Xiangfei Qiu, Ronghui Xu, Jilin Hu, Bin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probabilistic time series imputation has been widely applied in real-world
scenarios due to its ability to estimate uncertainty of imputation results.
Meanwhile, denoising diffusion probabilistic models (DDPMs) have achieved great
success in probabilistic time series imputation tasks with its power to model
complex distributions. However, current DDPM-based probabilistic time series
imputation methodologies are confronted with two types of challenges:
1)~\textit{~The backbone modules of the denoising parts are not capable of
achieving sequence modeling with low time complexity.} 2)~\textit{The
architecture of denoising modules can not handle the inter-variable and
bidirectional dependencies in the time series imputation problem effectively.}
To address the first challenge, we integrate the computational efficient state
space model, namely Mamba, as the backbone denosing module for DDPMs. To tackle
the second challenge, we carefully devise several SSM-based blocks for
bidirectional modeling and inter-variable relation understanding. Experimental
results demonstrate that our approach can achieve state-of-the-art time series
imputation results on multiple datasets, different missing scenarios and
missing ratios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLMs Have Political Correctness? Analyzing Ethical Biases and
  Jailbreak Vulnerabilities in AI Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isack Lee, Haebin Seong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large language models (LLMs) demonstrate impressive proficiency in
various tasks, they present potential safety risks, such as `jailbreaks', where
malicious inputs can coerce LLMs into generating harmful content. To address
these issues, many LLM developers have implemented various safety measures to
align these models. This alignment involves several techniques, including data
filtering during pre-training, supervised fine-tuning, reinforcement learning
from human feedback, and red-teaming exercises. These methods often introduce
deliberate and intentional biases similar to Political Correctness (PC) to
ensure the ethical behavior of LLMs. In this paper, we delve into the
intentional biases injected into LLMs for safety purposes and examine methods
to circumvent these safety alignment techniques. Notably, these intentional
biases result in a jailbreaking success rate in GPT-4o models that differs by
20% between non-binary and cisgender keywords and by 16% between white and
black keywords, even when the other parts of the prompts are identical. We
introduce the concept of PCJailbreak, highlighting the inherent risks posed by
these safety-induced biases. Additionally, we propose an efficient defense
method PCDefense, which prevents jailbreak attempts by injecting defense
prompts prior to generation. PCDefense stands as an appealing alternative to
Guard Models, such as Llama-Guard, that require additional inference cost after
text generation. Our findings emphasize the urgent need for LLM developers to
adopt a more responsible approach when designing and implementing safety
measures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Discrete Optimisation Via Decoupled Straight-Through
  Gumbel-Softmax 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rushi Shah, Mingyuan Yan, Michael Curtis Mozer, Dianbo Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discrete representations play a crucial role in many deep learning
architectures, yet their non-differentiable nature poses significant challenges
for gradient-based optimization. To address this issue, various gradient
estimators have been developed, including the Straight-Through Gumbel-Softmax
(ST-GS) estimator, which combines the Straight-Through Estimator (STE) and the
Gumbel-based reparameterization trick. However, the performance of ST-GS is
highly sensitive to temperature, with its selection often compromising gradient
fidelity. In this work, we propose a simple yet effective extension to ST-GS by
employing decoupled temperatures for forward and backward passes, which we
refer to as "Decoupled ST-GS". We show that our approach significantly enhances
the original ST-GS through extensive experiments across multiple tasks and
datasets. We further investigate the impact of our method on gradient fidelity
from multiple perspectives, including the gradient gap and the bias-variance
trade-off of estimated gradients. Our findings contribute to the ongoing effort
to improve discrete optimization in deep learning, offering a practical
solution that balances simplicity and effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Multilingual LLM Evaluation for European Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Klaudia Thellmann, Bernhard Stadler, Michael Fromm, Jasper Schulze Buschhoff, Alex Jude, Fabio Barth, Johannes Leveling, Nicolas Flores-Herr, Joachim Köhler, René Jäkel, Mehdi Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of Large Language Models (LLMs) has revolutionized natural language
processing across numerous languages and tasks. However, evaluating LLM
performance in a consistent and meaningful way across multiple European
languages remains challenging, especially due to the scarcity of
language-parallel multilingual benchmarks. We introduce a multilingual
evaluation approach tailored for European languages. We employ translated
versions of five widely-used benchmarks to assess the capabilities of 40 LLMs
across 21 European languages. Our contributions include examining the
effectiveness of translated benchmarks, assessing the impact of different
translation services, and offering a multilingual evaluation framework for LLMs
that includes newly created datasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC,
EU20-TruthfulQA, and EU20-GSM8K. The benchmarks and results are made publicly
available to encourage further research in multilingual LLM evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive
  Study and Hybrid Approach <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16833v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16833v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) has been a powerful tool for Large
Language Models (LLMs) to efficiently process overly lengthy contexts. However,
recent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to
understand long contexts directly. We conduct a comprehensive comparison
between RAG and long-context (LC) LLMs, aiming to leverage the strengths of
both. We benchmark RAG and LC across various public datasets using three latest
LLMs. Results reveal that when resourced sufficiently, LC consistently
outperforms RAG in terms of average performance. However, RAG's significantly
lower cost remains a distinct advantage. Based on this observation, we propose
Self-Route, a simple yet effective method that routes queries to RAG or LC
based on model self-reflection. Self-Route significantly reduces the
computation cost while maintaining a comparable performance to LC. Our findings
provide a guideline for long-context applications of LLMs using RAG and LC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 industry track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Many-Shot In-Context Learning <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11018v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11018v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet, Luis Rosias, Stephanie Chan, Biao Zhang, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu, Feryal Behbahani, Aleksandra Faust, Hugo Larochelle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) excel at few-shot in-context learning (ICL) --
learning from a few examples provided in context at inference, without any
weight updates. Newly expanded context windows allow us to investigate ICL with
hundreds or thousands of examples -- the many-shot regime. Going from few-shot
to many-shot, we observe significant performance gains across a wide variety of
generative and discriminative tasks. While promising, many-shot ICL can be
bottlenecked by the available amount of human-generated examples. To mitigate
this limitation, we explore two new settings: Reinforced and Unsupervised ICL.
Reinforced ICL uses model-generated chain-of-thought rationales in place of
human examples. Unsupervised ICL removes rationales from the prompt altogether,
and prompts the model only with domain-specific questions. We find that both
Reinforced and Unsupervised ICL can be quite effective in the many-shot regime,
particularly on complex reasoning tasks. Finally, we demonstrate that, unlike
few-shot learning, many-shot learning is effective at overriding pretraining
biases, can learn high-dimensional functions with numerical inputs, and
performs comparably to fine-tuning. We also find that inference cost increases
linearly in the many-shot regime, and frontier LLMs benefit from many-shot ICL
to varying degrees. Our analysis also reveals the limitations of next-token
prediction loss as an indicator of downstream ICL performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Driven Estimation of Heterogeneous Treatment Effects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06615v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06615v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Tran, Keith Burghardt, Kristina Lerman, Elena Zheleva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating how a treatment affects different individuals, known as
heterogeneous treatment effect estimation, is an important problem in empirical
sciences. In the last few years, there has been a considerable interest in
adapting machine learning algorithms to the problem of estimating heterogeneous
effects from observational and experimental data. However, these algorithms
often make strong assumptions about the observed features in the data and
ignore the structure of the underlying causal model, which can lead to biased
estimation. At the same time, the underlying causal mechanism is rarely known
in real-world datasets, making it hard to take it into consideration. In this
work, we provide a survey of state-of-the-art data-driven methods for
heterogeneous treatment effect estimation using machine learning, broadly
categorizing them as methods that focus on counterfactual prediction and
methods that directly estimate the causal effect. We also provide an overview
of a third category of methods which rely on structural causal models and learn
the model structure from data. Our empirical evaluation under various
underlying structural model mechanisms shows the advantages and deficiencies of
existing estimators and of the metrics for measuring their performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Topic Language Model on Heterogeneous Children's Mental Health
  Clinical Notes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanwen Ye, Tatiana Moreno, Adrianne Alpern, Louis Ehwerhemuepha, Annie Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mental health diseases affect children's lives and well-beings which have
received increased attention since the COVID-19 pandemic. Analyzing psychiatric
clinical notes with topic models is critical to evaluating children's mental
status over time. However, few topic models are built for longitudinal
settings, and most existing approaches fail to capture temporal trajectories
for each document. To address these challenges, we develop a dynamic topic
model with consistent topics and individualized temporal dependencies on the
evolving document metadata. Our model preserves the semantic meaning of
discovered topics over time and incorporates heterogeneity among documents. In
particular, when documents can be categorized, we propose a classifier-free
approach to maximize topic heterogeneity across different document groups. We
also present an efficient variational optimization procedure adapted for the
multistage longitudinal setting. In this case study, we apply our method to the
psychiatric clinical notes from a large tertiary pediatric hospital in Southern
California and achieve a 38% increase in the overall coherence of extracted
topics. Our real data analysis reveals that children tend to express more
negative emotions during state shutdowns and more positive when schools reopen.
Furthermore, it suggests that sexual and gender minority (SGM) children display
more pronounced reactions to major COVID-19 events and a greater sensitivity to
vaccine-related news than non-SGM children. This study examines children's
mental health progression during the pandemic and offers clinicians valuable
insights to recognize disparities in children's mental health related to their
sexual and gender identities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Achieving Exponential Asymptotic Optimality in Average-Reward Restless
  Bandits without Global Attractor Assumption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17882v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17882v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yige Hong, Qiaomin Xie, Yudong Chen, Weina Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the infinite-horizon average-reward restless bandit problem. We
propose a novel \emph{two-set policy} that maintains two dynamic subsets of
arms: one subset of arms has a nearly optimal state distribution and takes
actions according to an Optimal Local Control routine; the other subset of arms
is driven towards the optimal state distribution and gradually merged into the
first subset. We show that our two-set policy is asymptotically optimal with an
$O(\exp(-C N))$ optimality gap for an $N$-armed problem, under the mild
assumptions of aperiodic-unichain, non-degeneracy, and local stability. Our
policy is the first to achieve \emph{exponential asymptotic optimality} under
the above set of easy-to-verify assumptions, whereas prior work either requires
a strong \emph{global attractor} assumption or only achieves an $O(1/\sqrt{N})$
optimality gap. We further discuss obstacles in weakening the assumptions by
demonstrating examples where exponential asymptotic optimality is not
achievable when any of the three assumptions is violated. Notably, we prove a
lower bound for a large class of locally unstable restless bandits, showing
that local stability is particularly fundamental for exponential asymptotic
optimality. Finally, we use simulations to demonstrate that the two-set policy
outperforms previous policies on certain RB problems and performs competitively
overall.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages, 4 figures. In this version we included simulations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guided Multi-objective Generative AI to Enhance Structure-based Drug
  Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11785v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11785v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Kadan, Kevin Ryczko, Erika Lloyd, Adrian Roitberg, Takeshi Yamazaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI has the potential to revolutionize drug discovery. Yet, despite
recent advances in deep learning, existing models cannot generate molecules
that satisfy all desired physicochemical properties. Herein, we describe
IDOLpro, a generative chemistry AI combining diffusion with multi-objective
optimization for structure-based drug design. Differentiable scoring functions
guide the latent variables of the diffusion model to explore uncharted chemical
space and generate novel ligands in silico, optimizing a plurality of target
physicochemical properties. We demonstrate our platform's effectiveness by
generating ligands with optimized binding affinity and synthetic accessibility
on two benchmark sets. IDOLpro produces ligands with binding affinities over
10%-20% better than the next best state-of-the-art method on each test set,
producing more drug-like molecules with generally better synthetic
accessibility scores than other methods. We do a head-to-head comparison of
IDOLpro against a classic virtual screen of a large database of drug-like
molecules. We show that IDOLpro can generate molecules for a range of important
disease-related targets with better binding affinity and synthetic
accessibility than any molecule found in the virtual screen while being over
100x faster and less expensive to run. On a test set of experimental complexes,
IDOLpro is the first to produce molecules with better binding affinities than
experimentally observed ligands. IDOLpro can accommodate other scoring
functions (e.g. ADME-Tox) to accelerate hit-finding, hit-to-lead, and lead
optimization for drug discovery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stage-Aware Learning for Dynamic Treatments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19300v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19300v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanwen Ye, Wenzhuo Zhou, Ruoqing Zhu, Annie Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in dynamic treatment regimes (DTRs) facilitate the search for
optimal treatments, which are tailored to individuals' specific needs and able
to maximize their expected clinical benefits. However, existing algorithms
relying on consistent trajectories, such as inverse probability weighting
estimators (IPWEs), could suffer from insufficient sample size under optimal
treatments and a growing number of decision-making stages, particularly in the
context of chronic diseases. To address these challenges, we propose a novel
individualized learning method which estimates the DTR with a focus on
prioritizing alignment between the observed treatment trajectory and the one
obtained by the optimal regime across decision stages. By relaxing the
restriction that the observed trajectory must be fully aligned with the optimal
treatments, our approach substantially improves the sample efficiency and
stability of IPWE-based methods. In particular, the proposed learning scheme
builds a more general framework which includes the popular outcome weighted
learning framework as a special case of ours. Moreover, we introduce the notion
of stage importance scores along with an attention mechanism to explicitly
account for heterogeneity among decision stages. We establish the theoretical
properties of the proposed approach, including the Fisher consistency and
finite-sample performance bound. Empirically, we evaluate the proposed method
in extensive simulated environments and a real case study for the COVID-19
pandemic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Corrective Machine Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14015v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14015v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashwat Goel, Ameya Prabhu, Philip Torr, Ponnurangam Kumaraguru, Amartya Sanyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning models increasingly face data integrity challenges due to
the use of large-scale training datasets drawn from the Internet. We study what
model developers can do if they detect that some data was manipulated or
incorrect. Such manipulated data can cause adverse effects including
vulnerability to backdoored samples, systemic biases, and reduced accuracy on
certain input domains. Realistically, all manipulated training samples cannot
be identified, and only a small, representative subset of the affected data can
be flagged.
  We formalize Corrective Machine Unlearning as the problem of mitigating the
impact of data affected by unknown manipulations on a trained model, only
having identified a subset of the corrupted data. We demonstrate that the
problem of corrective unlearning has significantly different requirements from
traditional privacy-oriented unlearning. We find most existing unlearning
methods, including retraining-from-scratch without the deletion set, require
most of the manipulated data to be identified for effective corrective
unlearning. However, one approach, Selective Synaptic Dampening, achieves
limited success, unlearning adverse effects with just a small portion of the
manipulated samples in our setting, which shows encouraging signs for future
progress. We hope our work spurs research towards developing better methods for
corrective unlearning and offers practitioners a new strategy to handle data
integrity challenges arising from web-scale training. Code is available at
https://github.com/drimpossible/corrective-unlearning-bench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions of Machine Learning Research (TMLR), 17
  pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GPTreeO: An R package for continual regression with dividing local
  Gaussian processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01024v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01024v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timo Braun, Anders Kvellestad, Riccardo De Bin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce GPTreeO, a flexible R package for scalable Gaussian process (GP)
regression, particularly tailored to continual learning problems. GPTreeO
builds upon the Dividing Local Gaussian Processes (DLGP) algorithm, in which a
binary tree of local GP regressors is dynamically constructed using a continual
stream of input data. In GPTreeO we extend the original DLGP algorithm by
allowing continual optimisation of the GP hyperparameters, incorporating
uncertainty calibration, and introducing new strategies for how the local
partitions are created. Moreover, the modular code structure allows users to
interface their favourite GP library to perform the local GP regression in
GPTreeO. The flexibility of GPTreeO gives the user fine-grained control of the
balance between computational speed, accuracy, stability and smoothness. We
conduct a sensitivity analysis to show how GPTreeO's configurable features
impact the regression performance in a continual learning setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated the bibliography, and is now equivalent to the journal
  submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Moments of Clarity: Streamlining Latent Spaces in Machine Learning using
  Moment Pooling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08854v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08854v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rikab Gambhir, Athis Osathapan, Jesse Thaler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many machine learning applications involve learning a latent representation
of data, which is often high-dimensional and difficult to directly interpret.
In this work, we propose "Moment Pooling", a natural extension of Deep Sets
networks which drastically decrease latent space dimensionality of these
networks while maintaining or even improving performance. Moment Pooling
generalizes the summation in Deep Sets to arbitrary multivariate moments, which
enables the model to achieve a much higher effective latent dimensionality for
a fixed latent dimension. We demonstrate Moment Pooling on the collider physics
task of quark/gluon jet classification by extending Energy Flow Networks (EFNs)
to Moment EFNs. We find that Moment EFNs with latent dimensions as small as 1
perform similarly to ordinary EFNs with higher latent dimension. This small
latent dimension allows for the internal representation to be directly
visualized and interpreted, which in turn enables the learned internal jet
representation to be extracted in closed form.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15+7 pages, 14 figures, 7 tables. Code available at
  https://github.com/athiso/moment and https://github.com/rikab/MomentAnalysis;
  v2: Updated to match journal version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine-learning prediction of tipping with applications to the Atlantic
  Meridional Overturning Circulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14877v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14877v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shirin Panahi, Ling-Wei Kong, Mohammadamin Moradi, Zheng-Meng Zhai, Bryan Glaz, Mulugeta Haile, Ying-Cheng Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anticipating a tipping point, a transition from one stable steady state to
another, is a problem of broad relevance due to the ubiquity of the phenomenon
in diverse fields. The steady-state nature of the dynamics about a tipping
point makes its prediction significantly more challenging than predicting other
types of critical transitions from oscillatory or chaotic dynamics. Exploiting
the benefits of noise, we develop a general data-driven and machine-learning
approach to predicting potential future tipping in nonautonomous dynamical
systems and validate the framework using examples from different fields. As an
application, we address the problem of predicting the potential collapse of the
Atlantic Meridional Overturning Circulation (AMOC), possibly driven by
climate-induced changes in the freshwater input to the North Atlantic. Our
predictions based on synthetic and currently available empirical data place a
potential collapse window spanning from 2040 to 2065, in consistency with the
results in the current literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LieRE: Generalizing Rotary Position Encodings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10322v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10322v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophie Ostmeier, Brian Axelrod, Michael E. Moseley, Akshay Chaudhari, Curtis Langlotz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Rotary Position Embeddings (RoPE) for large language models have become
widely adopted, their application for other modalities has been slower. Here,
we introduce Lie group Relative position Encodings (LieRE) that goes beyond
RoPE in supporting n-dimensional inputs. We evaluate the performance of LieRE
on 2D and 3D image classification tasks and observe that LieRE leads to marked
relative improvements in performance (up to 9.7% for 2D and up to 25.5% for
3D), training efficiency (3.5x reduction), data efficiency (30%) compared to
the baselines of DeiT III, RoPE-Mixed and Vision-Llama.
https://github.com/Stanford-AIMI/LieRE
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16635v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16635v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Akhauri, Ahmed F AbouElhamayed, Jordan Dotzel, Zhiru Zhang, Alexander M Rush, Safeen Huda, Mohamed S Abdelfattah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high power consumption and latency-sensitive deployments of large
language models (LLMs) have motivated efficiency techniques like quantization
and sparsity. Contextual sparsity, where the sparsity pattern is
input-dependent, is crucial in LLMs because the permanent removal of attention
heads or neurons from LLMs can significantly degrade accuracy. Prior work has
attempted to model contextual sparsity using neural networks trained to predict
activation magnitudes, which can be used to dynamically prune structures with
low predicted activation magnitude. In this paper, we look beyond
magnitude-based pruning criteria to assess attention head and neuron importance
in LLMs. We develop a novel predictor called ShadowLLM, which can shadow the
LLM behavior and enforce better sparsity patterns, resulting in over 15%
improvement in end-to-end accuracy compared to prior methods. In addition,
ShadowLLM achieves up to a 20% speed-up over the state-of-the-art DejaVu
framework. These enhancements are validated on Llama-2 and OPT models with up
to 30 billion parameters. Our code is available at
\href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Main, Long Paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlashTex: Fast Relightable Mesh Texturing with LightControlNet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13251v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13251v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangle Deng, Timothy Omernick, Alexander Weiss, Deva Ramanan, Jun-Yan Zhu, Tinghui Zhou, Maneesh Agrawala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manually creating textures for 3D meshes is time-consuming, even for expert
visual content creators. We propose a fast approach for automatically texturing
an input 3D mesh based on a user-provided text prompt. Importantly, our
approach disentangles lighting from surface material/reflectance in the
resulting texture so that the mesh can be properly relit and rendered in any
lighting environment. We introduce LightControlNet, a new text-to-image model
based on the ControlNet architecture, which allows the specification of the
desired lighting as a conditioning image to the model. Our text-to-texture
pipeline then constructs the texture in two stages. The first stage produces a
sparse set of visually consistent reference views of the mesh using
LightControlNet. The second stage applies a texture optimization based on Score
Distillation Sampling (SDS) that works with LightControlNet to increase the
texture quality while disentangling surface material from lighting. Our
algorithm is significantly faster than previous text-to-texture methods, while
producing high-quality and relightable textures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://flashtex.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Block-Attention for Efficient RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15355v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15355v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        East Sun, Yan Wang, Lan Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Block-Attention, an attention mechanism designed to address the
increased inference latency and cost in Retrieval-Augmented Generation (RAG)
scenarios. Traditional approaches often encode the entire context. Instead,
Block-Attention divides retrieved documents into discrete blocks, with each
block independently calculating key-value (KV) states except for the final
block. In RAG scenarios, by defining each passage as a block, Block-Attention
enables us to reuse the KV states of passages that have been seen before,
thereby significantly reducing the latency and the computation overhead during
inference. The implementation of Block-Attention involves block segmentation,
position re-encoding, and fine-tuning the LLM to adapt to the Block-Attention
mechanism. Experiments on four RAG benchmarks demonstrate that after block
fine-tuning, the Block-Attention model achieves performance comparable to
self-attention models (68.4\% vs 67.9\% on Llama3) or even superior performance
(62.8\% vs 59.6\% on Mistral). Notably, Block-Attention significantly reduces
the time to first token (TTFT) and floating point operations (FLOPs) to a very
low level. It only takes 45 ms to output the first token for an input sequence
with a total length of 32K. Compared to the self-attention models, the time
consumption and corresponding FLOPs are reduced by 98.7\% and 99.8\%,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>-SAW: Leveraging Relation-Aware Graphs for Textual <span class="highlight-title">Prompt</span>
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Guimin Hu, Weimin Lyu, Lijie Hu, Lu Yu, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown exceptional abilities for multiple
different natural language processing tasks. While prompting is a crucial tool
for LLM inference, we observe that there is a significant cost associated with
exceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead
to substandard results in terms of readability/interpretability of the
compressed prompt, with a detrimental impact on prompt utility. To address
this, we propose PromptSAW: Prompt compresSion via Relation AWare graphs, an
effective strategy for prompt compression over task-agnostic and task-aware
prompts. Prompt-SAW uses the prompt's textual information to build a graph and
later extracts key information elements in the graph to come up with the
compressed prompt. We also propose GSM8K-aug, i.e., an extended version of the
existing GSM8K benchmark for task-agnostic prompts in order to provide a
comprehensive evaluation platform. Experimental evaluation using benchmark
datasets shows that prompts compressed by Prompt-SAW are not only better in
terms of readability, but they also outperform the best-performing baseline
models by up to 10.1 and 77.1, respectively, for task-agnostic and task-aware
settings while compressing the original prompt text by 34.9 and 56.7.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Expected Sliced Transport Plans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12176v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12176v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinran Liu, Rocío Díaz Martín, Yikun Bai, Ashkan Shahbazi, Matthew Thorpe, Akram Aldroubi, Soheil Kolouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The optimal transport (OT) problem has gained significant traction in modern
machine learning for its ability to: (1) provide versatile metrics, such as
Wasserstein distances and their variants, and (2) determine optimal couplings
between probability measures. To reduce the computational complexity of OT
solvers, methods like entropic regularization and sliced optimal transport have
been proposed. The sliced OT framework improves efficiency by comparing
one-dimensional projections (slices) of high-dimensional distributions.
However, despite their computational efficiency, sliced-Wasserstein approaches
lack a transportation plan between the input measures, limiting their use in
scenarios requiring explicit coupling. In this paper, we address two key
questions: Can a transportation plan be constructed between two probability
measures using the sliced transport framework? If so, can this plan be used to
define a metric between the measures? We propose a "lifting" operation to
extend one-dimensional optimal transport plans back to the original space of
the measures. By computing the expectation of these lifted plans, we derive a
new transportation plan, termed expected sliced transport (EST) plans. We prove
that using the EST plan to weight the sum of the individual Euclidean costs for
moving from one point to another results in a valid metric between the input
discrete probability measures. We demonstrate the connection between our
approach and the recently proposed min-SWGG, along with illustrative numerical
examples that support our theoretical findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Contrastive Feature Representations for Facial Action Unit
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06165v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06165v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiao Shang, Bin Liu, Fengmao Lv, Fei Teng, Tianrui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial action unit (AU) detection has long encountered the challenge of
detecting subtle feature differences when AUs activate. Existing methods often
rely on encoding pixel-level information of AUs, which not only encodes
additional redundant information but also leads to increased model complexity
and limited generalizability. Additionally, the accuracy of AU detection is
negatively impacted by the class imbalance issue of each AU type, and the
presence of noisy and false AU labels. In this paper, we introduce a novel
contrastive learning framework aimed for AU detection that incorporates both
self-supervised and supervised signals, thereby enhancing the learning of
discriminative features for accurate AU detection. To tackle the class
imbalance issue, we employ a negative sample re-weighting strategy that adjusts
the step size of updating parameters for minority and majority class samples.
Moreover, to address the challenges posed by noisy and false AU labels, we
employ a sampling technique that encompasses three distinct types of positive
sample pairs. This enables us to inject self-supervised signals into the
supervised signal, effectively mitigating the adverse effects of noisy labels.
Our experimental assessments, conducted on four widely-utilized benchmark
datasets (BP4D, DISFA, GFT and Aff-Wild2), underscore the superior performance
of our approach compared to state-of-the-art methods of AU detection. Our code
is available at \url{https://github.com/Ziqiao-Shang/AUNCE}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 18 figures, submitted to Pattern Recognition (PR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16710v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16710v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed A Aly, Beidi Chen, Carole-Jean Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LayerSkip, an end-to-end solution to speed-up inference of large
language models (LLMs). First, during training we apply layer dropout, with low
dropout rates for earlier layers and higher dropout rates for later layers, and
an early exit loss where all transformer layers share the same exit. Second,
during inference, we show that this training recipe increases the accuracy of
early exit at earlier layers, without adding any auxiliary layers or modules to
the model. Third, we present a novel self-speculative decoding solution where
we exit at early layers and verify and correct with remaining layers of the
model. Our proposed self-speculative decoding approach has less memory
footprint than other speculative decoding approaches and benefits from shared
compute and activations of the draft and verification stages. We run
experiments on different Llama model sizes on different types of training:
pretraining from scratch, continual pretraining, finetuning on specific data
domain, and finetuning on specific task. We implement our inference solution
and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x
on coding, and 2.0x on TOPv2 semantic parsing task. We open source our code and
checkpoints at https://github.com/facebookresearch/LayerSkip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-based Cognitive Models of Students with Misconceptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12294v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12294v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Sonkar, Xinghe Chen, Naiming Liu, Richard G. Baraniuk, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately modeling student cognition is crucial for developing effective
AI-driven educational technologies. A key challenge is creating realistic
student models that satisfy two essential properties: (1) accurately
replicating specific misconceptions, and (2) correctly solving problems where
these misconceptions are not applicable. This dual requirement reflects the
complex nature of student understanding, where misconceptions coexist with
correct knowledge. This paper investigates whether Large Language Models (LLMs)
can be instruction-tuned to meet this dual requirement and effectively simulate
student thinking in algebra. We introduce MalAlgoPy, a novel Python library
that generates datasets reflecting authentic student solution patterns through
a graph-based representation of algebraic problem-solving. Utilizing MalAlgoPy,
we define and examine Cognitive Student Models (CSMs) - LLMs instruction tuned
to faithfully emulate realistic student behavior. Our findings reveal that LLMs
trained on misconception examples can efficiently learn to replicate errors.
However, the training diminishes the model's ability to solve problems
correctly, particularly for problem types where the misconceptions are not
applicable, thus failing to satisfy second property of CSMs. We demonstrate
that by carefully calibrating the ratio of correct to misconception examples in
the training data - sometimes as low as 0.25 - it is possible to develop CSMs
that satisfy both properties. Our insights enhance our understanding of
AI-based student models and pave the way for effective adaptive learning
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MuJo: <span class="highlight-title">Multimodal</span> Joint Feature Space Learning for Human Activity
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Gerd Fritsch, Cennet Oguz, Vitor Fortes Rey, Lala Ray, Maximilian Kiefer-Emmanouilidis, Paul Lukowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human Activity Recognition (HAR) is a longstanding problem in AI with
applications in a broad range of areas, including healthcare, sports and
fitness, security, and more. The performance of HAR in real-world settings is
strongly dependent on the type and quality of the input signal that can be
acquired. Given an unobstructed, high-quality camera view of a scene, computer
vision systems, in particular in conjunction with foundation models, can today
fairly reliably distinguish complex activities. On the other hand, recognition
using modalities such as wearable sensors (which are often more broadly
available, e.g., in mobile phones and smartwatches) is a more difficult
problem, as the signals often contain less information and labeled training
data is more difficult to acquire. To alleviate the need for labeled data, we
introduce our comprehensive Fitness Multimodal Activity Dataset (FiMAD) in this
work, which can be used with the proposed pre-training method MuJo (Multimodal
Joint Feature Space Learning) to enhance HAR performance across various
modalities. FiMAD was created using YouTube fitness videos and contains
parallel video, language, pose, and simulated IMU sensor data. MuJo utilizes
this dataset to learn a joint feature space for these modalities. We show that
classifiers pre-trained on FiMAD can increase the performance on real HAR
datasets such as MM-Fit, MyoGym, MotionSense, and MHEALTH. For instance, on
MM-Fit, we achieve an Macro F1-Score of up to 0.855 when fine-tuning on only 2%
of the training data and 0.942 when utilizing the full training set for
classification tasks. We have compared our approach to other self-supervised
ones and showed that, unlike them, ours can consistently improve on the
baseline network performance as well as provide a better data-efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Generalization on the ProcGen Benchmark with Simple
  Architectural Changes and Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Jesson, Yiding Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate that recent advances in reinforcement learning (RL) combined
with simple architectural changes significantly improves generalization on the
ProcGen benchmark. These changes are frame stacking, replacing 2D convolutional
layers with 3D convolutional layers, and scaling up the number of convolutional
kernels per layer. Experimental results using a single set of hyperparameters
across all environments show a 37.9\% reduction in the optimality gap compared
to the baseline (from 0.58 to 0.36). This performance matches or exceeds
current state-of-the-art methods. The proposed changes are largely orthogonal
and therefore complementary to the existing approaches for improving
generalization in RL, and our results suggest that further exploration in this
direction could yield substantial improvements in addressing generalization
challenges in deep reinforcement learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Mapping of Anatomical Landmarks from Free-Text Using Large
  Language Models: Insights from Llama-2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12686v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12686v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamad Abdi, Gerardo Hermosillo Valadez, Halid Ziya Yerebakan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anatomical landmarks are vital in medical imaging for navigation and anomaly
detection. Modern large language models (LLMs), like Llama-2, offer promise for
automating the mapping of these landmarks in free-text radiology reports to
corresponding positions in image data. Recent studies propose LLMs may develop
coherent representations of generative processes. Motivated by these insights,
we investigated whether LLMs accurately represent the spatial positions of
anatomical landmarks. Through experiments with Llama-2 models, we found that
they can linearly represent anatomical landmarks in space with considerable
robustness to different prompts. These results underscore the potential of LLMs
to enhance the efficiency and accuracy of medical imaging workflows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient PAC Learning of Halfspaces with Constant Malicious Noise Rate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01186v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01186v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Shen, Xiaoyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding noise tolerance of learning algorithms under certain conditions
is a central quest in learning theory. In this work, we study the problem of
computationally efficient PAC learning of halfspaces in the presence of
malicious noise, where an adversary can corrupt both instances and labels of
training samples. The best-known noise tolerance either depends on a target
error rate under distributional assumptions or on a margin parameter under
large-margin conditions. In this work, we show that when both types of
conditions are satisfied, it is possible to achieve {\em constant} noise
tolerance by minimizing a reweighted hinge loss. Our key ingredients include:
1) an efficient algorithm that finds weights to control the gradient
deterioration from corrupted samples, and 2) a new analysis on the robustness
of the hinge loss equipped with such weights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>author list in contribution order</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization Error of the Tilted Empirical Risk 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19431v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19431v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gholamali Aminian, Amir R. Asadi, Tian Li, Ahmad Beirami, Gesine Reinert, Samuel N. Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generalization error (risk) of a supervised statistical learning
algorithm quantifies its prediction ability on previously unseen data. Inspired
by exponential tilting, Li et al. (2021) proposed the tilted empirical risk as
a non-linear risk metric for machine learning applications such as
classification and regression problems. In this work, we examine the
generalization error of the tilted empirical risk. In particular, we provide
uniform and information-theoretic bounds on the tilted generalization error,
defined as the difference between the population risk and the tilted empirical
risk, with a convergence rate of $O(1/\sqrt{n})$ where $n$ is the number of
training samples. Furthermore, we study the solution to the KL-regularized
expected tilted empirical risk minimization problem and derive an upper bound
on the expected tilted generalization error with a convergence rate of
$O(1/n)$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>New results are added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RECOVAR: Representation Covariances on Deep Latent Spaces for Seismic
  Event Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18402v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18402v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Onur Efe, Arkadas Ozakin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While modern deep learning methods have shown great promise in the problem of
earthquake detection, the most successful methods so far have been based on
supervised learning, which requires large datasets with ground-truth labels.
The curation of such datasets is both time consuming and prone to systematic
biases, which result in difficulties with cross-dataset generalization,
hindering general applicability. In this paper, we develop an unsupervised
method for earthquake detection that learns to detect earthquakes from raw
waveforms, without access to ground truth labels. The performance is comparable
to, and in some cases better than, some state-of-the-art supervised methods.
Moreover, the method has strong \emph{cross-dataset generalization}
performance. The algorithm utilizes deep autoencoders that learn to reproduce
the waveforms after a data-compressive bottleneck and uses a simple,
cross-covariance-based triggering algorithm at the bottleneck for labeling. The
approach has the potential to be useful for time series datasets from other
domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Laws and Compute-Optimal Training Beyond Fixed Training
  Durations <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18392v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18392v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Hägele, Elie Bakouch, Atli Kosson, Loubna Ben Allal, Leandro Von Werra, Martin Jaggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scale has become a main ingredient in obtaining strong machine learning
models. As a result, understanding a model's scaling properties is key to
effectively designing both the right training setup as well as future
generations of architectures. In this work, we argue that scale and training
research has been needlessly complex due to reliance on the cosine schedule,
which prevents training across different lengths for the same model size. We
investigate the training behavior of a direct alternative -- constant learning
rate and cooldowns -- and find that it scales predictably and reliably similar
to cosine. Additionally, we show that stochastic weight averaging yields
improved performance along the training trajectory, without additional training
costs, across different scales. Importantly, with these findings we demonstrate
that scaling experiments can be performed with significantly reduced compute
and GPU hours by utilizing fewer but reusable training runs. Our code is
available at \url{https://github.com/epfml/schedules-and-scaling/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Spotlight at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recursive deep learning framework for forecasting the decadal world
  economic outlook 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10874v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10874v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Wang, Rodney Beard, John Hawkins, Rohitash Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The gross domestic product (GDP) is the most widely used indicator in
macroeconomics and the main tool for measuring a country's economic output. Due
to the diversity and complexity of the world economy, a wide range of models
have been used, but there are challenges in making decadal GDP forecasts given
unexpected changes such as emergence of catastrophic world events including
pandemics and wars. Deep learning models are well suited for modelling temporal
sequences and time series forecasting. In this paper, we develop a deep
learning framework to forecast the GDP growth rate of the world economy over a
decade. We use the Penn World Table as the data source featuring 13 countries
prior to the COVID-19 pandemic, such as Australia, China, India, and the United
States. We present a recursive deep learning framework to predict the GDP
growth rate in the next ten years. We test prominent deep learning models and
compare their results with traditional econometric models for selected
developed and developing countries. Our decadal forecasts reveal that that most
of the developed countries would experience economic growth slowdown,
stagnation and even recession within five years (2020-2024). Furthermore, our
model forecasts show that only China, France, and India would experience stable
GDP growth.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Targeted Vaccine: Safety Alignment for Large Language Models against
  Harmful <span class="highlight-title">Fine-Tuning</span> via Layer-wise Perturbation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09760v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09760v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guozhi Liu, Weiwei Lin, Tiansheng Huang, Ruichao Mo, Qi Mu, Li Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Harmful fine-tuning attack poses a serious threat to the online fine-tuning
service. Vaccine, a recent alignment-stage defense, applies uniform
perturbation to all layers of embedding to make the model robust to the
simulated embedding drift. However, applying layer-wise uniform perturbation
may lead to excess perturbations for some particular safety-irrelevant layers,
resulting in defense performance degradation and unnecessary memory
consumption. To address this limitation, we propose Targeted Vaccine
(T-Vaccine), a memory-efficient safety alignment method that applies
perturbation to only selected layers of the model. T-Vaccine follows two core
steps: First, it uses gradient norm as a statistical metric to identify the
safety-critical layers. Second, instead of applying uniform perturbation across
all layers, T-Vaccine only applies perturbation to the safety-critical layers
while keeping other layers frozen during training. Results show that T-Vaccine
outperforms Vaccine in terms of both defense effectiveness and resource
efficiency. Comparison with other defense baselines, e.g., RepNoise and TAR
also demonstrate the superiority of T-Vaccine. Notably, T-Vaccine is the first
defense that can address harmful fine-tuning issues for a 7B pre-trained models
trained on consumer GPUs with limited memory (e.g., RTX 4090). Our code is
available at https://github.com/Lslland/T-Vaccine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MirrorCheck: Efficient Adversarial Defense for <span class="highlight-title">Vision-Language</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09250v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09250v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samar Fares, Klea Ziu, Toluwani Aremu, Nikita Durasov, Martin Takáč, Pascal Fua, Karthik Nandakumar, Ivan Laptev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) are becoming increasingly vulnerable to
adversarial attacks as various novel attack strategies are being proposed
against these models. While existing defenses excel in unimodal contexts, they
currently fall short in safeguarding VLMs against adversarial threats. To
mitigate this vulnerability, we propose a novel, yet elegantly simple approach
for detecting adversarial samples in VLMs. Our method leverages Text-to-Image
(T2I) models to generate images based on captions produced by target VLMs.
Subsequently, we calculate the similarities of the embeddings of both input and
generated images in the feature space to identify adversarial samples.
Empirical evaluations conducted on different datasets validate the efficacy of
our approach, outperforming baseline methods adapted from image classification
domains. Furthermore, we extend our methodology to classification tasks,
showcasing its adaptability and model-agnostic nature. Theoretical analyses and
empirical findings also show the resilience of our approach against adaptive
attacks, positioning it as an excellent defense mechanism for real-world
deployment against adversarial threats.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback
  for Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katherine M. Collins, Najoung Kim, Yonatan Bitton, Verena Rieser, Shayegan Omidshafiei, Yushi Hu, Sherol Chen, Senjuti Dutta, Minsuk Chang, Kimin Lee, Youwei Liang, Georgina Evans, Sahil Singla, Gang Li, Adrian Weller, Junfeng He, Deepak Ramachandran, Krishnamurthy Dj Dvijotham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human feedback plays a critical role in learning and refining reward models
for text-to-image generation, but the optimal form the feedback should take for
learning an accurate reward function has not been conclusively established.
This paper investigates the effectiveness of fine-grained feedback which
captures nuanced distinctions in image quality and prompt-alignment, compared
to traditional coarse-grained feedback (for example, thumbs up/down or ranking
between a set of options). While fine-grained feedback holds promise,
particularly for systems catering to diverse societal preferences, we show that
demonstrating its superiority to coarse-grained feedback is not automatic.
Through experiments on real and synthetic preference data, we surface the
complexities of building effective models due to the interplay of model choice,
feedback type, and the alignment between human judgment and computational
interpretation. We identify key challenges in eliciting and utilizing
fine-grained feedback, prompting a reassessment of its assumed benefits and
practicality. Our findings -- e.g., that fine-grained feedback can lead to
worse models for a fixed budget, in some settings; however, in controlled
settings with known attributes, fine grained rewards can indeed be more helpful
-- call for careful consideration of feedback attributes and potentially beckon
novel modeling approaches to appropriately unlock the potential value of
fine-grained feedback in-the-wild.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structure-Preserving Network Compression Via Low-Rank Induced Training
  Through Linear Layers Composition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03089v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03089v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xitong Zhang, Ismail R. Alkhouri, Rongrong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) have achieved remarkable success in addressing
many previously unsolvable tasks. However, the storage and computational
requirements associated with DNNs pose a challenge for deploying these trained
models on resource-limited devices. Therefore, a plethora of compression and
pruning techniques have been proposed in recent years. Low-rank decomposition
techniques are among the approaches most utilized to address this problem.
Compared to post-training compression, compression-promoted training is still
under-explored. In this paper, we present a theoretically-justified technique
termed Low-Rank Induced Training (LoRITa), that promotes low-rankness through
the composition of linear layers and compresses by using singular value
truncation. This is achieved without the need to change the structure at
inference time or require constrained and/or additional optimization, other
than the standard weight decay regularization. Moreover, LoRITa eliminates the
need to (i) initialize with pre-trained models, (ii) specify rank selection
prior to training, and (iii) compute SVD in each iteration. Our experimental
results (i) demonstrate the effectiveness of our approach using MNIST on Fully
Connected Networks, CIFAR10 on Vision Transformers, and CIFAR10/100 and
ImageNet on Convolutional Neural Networks, and (ii) illustrate that we achieve
either competitive or state-of-the-art results when compared to leading
structured pruning and low-rank training methods in terms of FLOPs and
parameters drop. Our code is available at
\url{https://github.com/XitongSystem/LoRITa/tree/main}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CL3: A Collaborative Learning Framework for the Medical Data Ensuring
  Data Privacy in the Hyperconnected Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07900v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07900v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamamd Zavid Parvez, Rafiqul Islam, Md Zahidul Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a hyperconnected environment, medical institutions are particularly
concerned with data privacy when sharing and transmitting sensitive patient
information due to the risk of data breaches, where malicious actors could
intercept sensitive information. A collaborative learning framework, including
transfer, federated, and incremental learning, can generate efficient, secure,
and scalable models while requiring less computation, maintaining patient data
privacy, and ensuring an up-to-date model. This study aims to address the
detection of COVID-19 using chest X-ray images through a proposed collaborative
learning framework called CL3. Initially, transfer learning is employed,
leveraging knowledge from a pre-trained model as the starting global model.
Local models from different medical institutes are then integrated, and a new
global model is constructed to adapt to any data drift observed in the local
models. Additionally, incremental learning is considered, allowing continuous
adaptation to new medical data without forgetting previously learned
information. Experimental results demonstrate that the CL3 framework achieved a
global accuracy of 89.99% when using Xception with a batch size of 16 after
being trained for six federated communication rounds. A demo of the CL3
framework is available at
https://github.com/zavidparvez/CL3-Collaborative-Approach to ensure
reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ G2D: From Global to Dense Radiography Representation Learning via
  <span class="highlight-title">Vision-Language</span> <span class="highlight-title">Pre-train</span>ing <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01522v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01522v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Che Liu, Cheng Ouyang, Sibo Cheng, Anand Shah, Wenjia Bai, Rossella Arcucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, medical vision-language pre-training (VLP) has reached substantial
progress to learn global visual representation from medical images and their
paired radiology reports. However, medical imaging tasks in real world usually
require finer granularity in visual features. These tasks include visual
localization tasks (e.g., semantic segmentation, object detection) and visual
grounding task. Yet, current medical VLP methods face challenges in learning
these fine-grained features, as they primarily focus on brute-force alignment
between image patches and individual text tokens for local visual feature
learning, which is suboptimal for downstream dense prediction tasks. In this
work, we propose a new VLP framework, named \textbf{G}lobal to \textbf{D}ense
level representation learning (G2D) that achieves significantly improved
granularity and more accurate grounding for the learned features, compared to
existing medical VLP approaches. In particular, G2D learns dense and
semantically-grounded image representations via a pseudo segmentation task
parallel with the global vision-language alignment. Notably, generating pseudo
segmentation targets does not incur extra trainable parameters: they are
obtained on the fly during VLP with a parameter-free processor. G2D achieves
superior performance across 6 medical imaging tasks and 25 diseases,
particularly in semantic segmentation, which necessitates fine-grained,
semantically-grounded image features. In this task, G2D surpasses peer models
even when fine-tuned with just 1\% of the training data, compared to the 100\%
used by these models. The code will be released upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLOPS: Forward Learning with OPtimal Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05966v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05966v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Ren, Zishi Zhang, Jinyang Jiang, Guanghao Li, Zeliang Zhang, Mingqian Feng, Yijie Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the limitations of backpropagation, perturbation-based gradient
computation methods have recently gained focus for learning with only forward
passes, also referred to as queries. Conventional forward learning consumes
enormous queries on each data point for accurate gradient estimation through
Monte Carlo sampling, which hinders the scalability of those algorithms.
However, not all data points deserve equal queries for gradient estimation. In
this paper, we study the problem of improving the forward learning efficiency
from a novel perspective: how to reduce the gradient estimation variance with
minimum cost? For this, we propose to allocate the optimal number of queries
over each data in one batch during training to achieve a good balance between
estimation accuracy and computational efficiency. Specifically, with a
simplified proxy objective and a reparameterization technique, we derive a
novel plug-and-play query allocator with minimal parameters. Theoretical
results are carried out to verify its optimality. We conduct extensive
experiments for fine-tuning Vision Transformers on various datasets and further
deploy the allocator to two black-box applications: prompt tuning and
multimodal alignment for foundation models. All findings demonstrate that our
proposed allocator significantly enhances the scalability of forward-learning
algorithms, paving the way for real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ t-READi: Transformer-Powered Robust and Efficient <span class="highlight-title">Multimodal</span> Inference
  for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09747v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09747v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Hu, Yuhang Qian, Tianyue Zheng, Ang Li, Zhe Chen, Yue Gao, Xiuzhen Cheng, Jun Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the wide adoption of multimodal sensors (e.g., camera, lidar, radar) by
autonomous vehicles (AVs), deep analytics to fuse their outputs for a robust
perception become imperative. However, existing fusion methods often make two
assumptions rarely holding in practice: i) similar data distributions for all
inputs and ii) constant availability for all sensors. Because, for example,
lidars have various resolutions and failures of radars may occur, such
variability often results in significant performance degradation in fusion. To
this end, we present tREADi, an adaptive inference system that accommodates the
variability of multimodal sensory data and thus enables robust and efficient
perception. t-READi identifies variation-sensitive yet structure-specific model
parameters; it then adapts only these parameters while keeping the rest intact.
t-READi also leverages a cross-modality contrastive learning method to
compensate for the loss from missing modalities. Both functions are implemented
to maintain compatibility with existing multimodal deep fusion methods. The
extensive experiments evidently demonstrate that compared with the status quo
approaches, t-READi not only improves the average inference accuracy by more
than 6% but also reduces the inference latency by almost 15x with the cost of
only 5% extra memory overhead in the worst case under realistic data and modal
variations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online-to-PAC Conversions: Generalization Bounds via Regret Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19674v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19674v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gábor Lugosi, Gergely Neu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new framework for deriving bounds on the generalization bound of
statistical learning algorithms from the perspective of online learning.
Specifically, we construct an online learning game called the "generalization
game", where an online learner is trying to compete with a fixed statistical
learning algorithm in predicting the sequence of generalization gaps on a
training set of i.i.d. data points. We establish a connection between the
online and statistical learning setting by showing that the existence of an
online learning algorithm with bounded regret in this game implies a bound on
the generalization error of the statistical learning algorithm, up to a
martingale concentration term that is independent of the complexity of the
statistical learning method. This technique allows us to recover several
standard generalization bounds including a range of PAC-Bayesian and
information-theoretic guarantees, as well as generalizations thereof.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Fast Adaptation from Adversarially Explicit Task Distribution
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheems Wang, Yiqin Lv, Yixiu Mao, Yun Qu, Yi Xu, Xiangyang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta-learning is a practical learning paradigm to transfer skills across
tasks from a few examples. Nevertheless, the existence of task distribution
shifts tends to weaken meta-learners' generalization capability, particularly
when the task distribution is naively hand-crafted or based on simple priors
that fail to cover typical scenarios sufficiently. Here, we consider explicitly
generative modeling task distributions placed over task identifiers and propose
robustifying fast adaptation from adversarial training. Our approach, which can
be interpreted as a model of a Stackelberg game, not only uncovers the task
structure during problem-solving from an explicit generative model but also
theoretically increases the adaptation robustness in worst cases. This work has
practical implications, particularly in dealing with task distribution shifts
in meta-learning, and contributes to theoretical insights in the field. Our
method demonstrates its robustness in the presence of task subpopulation shifts
and improved performance over SOTA baselines in extensive experiments. The
project is available at https://sites.google.com/view/ar-metalearn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The project is available at
  https://sites.google.com/view/ar-metalearn</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Invariant Principle for Heterophilic Graph Structure
  Distribution Shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinluan Yang, Zhengyu Chen, Teng Xiao, Wenqiao Zhang, Yong Lin, Kun Kuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heterophilic Graph Neural Networks (HGNNs) have shown promising results for
semi-supervised learning tasks on graphs. Notably, most real-world heterophilic
graphs are composed of a mixture of nodes with different neighbor patterns,
exhibiting local node-level homophilic and heterophilic structures. However,
existing works are only devoted to designing better HGNN backbones or
architectures for node classification tasks on heterophilic and homophilic
graph benchmarks simultaneously, and their analyses of HGNN performance with
respect to nodes are only based on the determined data distribution without
exploring the effect caused by this structural difference between training and
testing nodes. How to learn invariant node representations on heterophilic
graphs to handle this structure difference or distribution shifts remains
unexplored. In this paper, we first discuss the limitations of previous
graph-based invariant learning methods from the perspective of data
augmentation. Then, we propose \textbf{HEI}, a framework capable of generating
invariant node representations through incorporating heterophily information to
infer latent environments without augmentation, which are then used for
invariant prediction, under heterophilic graph structure distribution shifts.
We theoretically show that our proposed method can achieve guaranteed
performance under heterophilic graph structure distribution shifts. Extensive
experiments on various benchmarks and backbones can also demonstrate the
effectiveness of our method compared with existing state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing RVFL networks: Robust classification with the HawkEye loss
  function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00510v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00510v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mushir Akhtar, Ritik Mishra, M. Tanveer, Mohd. Arshad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Random vector functional link (RVFL), a variant of single-layer feedforward
neural network (SLFN), has garnered significant attention due to its lower
computational cost and robustness to overfitting. Despite its advantages, the
RVFL network's reliance on the square error loss function makes it highly
sensitive to outliers and noise, leading to degraded model performance in
real-world applications. To remedy it, we propose the incorporation of the
HawkEye loss (H-loss) function into the RVFL framework. The H-loss function
features nice mathematical properties, including smoothness and boundedness,
while simultaneously incorporating an insensitive zone. Each characteristic
brings its own advantages: 1) Boundedness limits the impact of extreme errors,
enhancing robustness against outliers; 2) Smoothness facilitates the use of
gradient-based optimization algorithms, ensuring stable and efficient
convergence; and 3) The insensitive zone mitigates the effect of minor
discrepancies and noise. Leveraging the H-loss function, we embed it into the
RVFL framework and develop a novel robust RVFL model termed H-RVFL. Notably,
this work addresses a significant gap, as no bounded loss function has been
incorporated into RVFL to date. The non-convex optimization of the proposed
H-RVFL is effectively addressed by the Nesterov accelerated gradient (NAG)
algorithm, whose computational complexity is also discussed. The proposed
H-RVFL model's effectiveness is validated through extensive experiments on $40$
benchmark datasets from UCI and KEEL repositories, with and without label
noise. The results highlight significant improvements in robustness and
efficiency, establishing the H-RVFL model as a powerful tool for applications
in noisy and outlier-prone environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reducing Bias in Federated Class-Incremental Learning with Hierarchical
  Generative Prototypes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02447v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02447v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Salami, Pietro Buzzega, Matteo Mosconi, Mattia Verasani, Simone Calderara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) aims at unburdening the training of deep models by
distributing computation across multiple devices (clients) while safeguarding
data privacy. On top of that, Federated Continual Learning (FCL) also accounts
for data distribution evolving over time, mirroring the dynamic nature of
real-world environments. In this work, we shed light on the Incremental and
Federated biases that naturally emerge in FCL. While the former is a known
problem in Continual Learning, stemming from the prioritization of recently
introduced classes, the latter (i.e., the bias towards local distributions)
remains relatively unexplored. Our proposal constrains both biases in the last
layer by efficiently fine-tuning a pre-trained backbone using learnable
prompts, resulting in clients that produce less biased representations and more
biased classifiers. Therefore, instead of solely relying on parameter
aggregation, we also leverage generative prototypes to effectively balance the
predictions of the global model. Our method improves on the current State Of
The Art, providing an average increase of +7.9% in accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decision Mamba Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Correia, Luís A. Alexandre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in imitation learning have been largely fueled by the
integration of sequence models, which provide a structured flow of information
to effectively mimic task behaviours. Currently, Decision Transformer (DT) and
subsequently, the Hierarchical Decision Transformer (HDT), presented
Transformer-based approaches to learn task policies. Recently, the Mamba
architecture has shown to outperform Transformers across various task domains.
In this work, we introduce two novel methods, Decision Mamba (DM) and
Hierarchical Decision Mamba (HDM), aimed at enhancing the performance of the
Transformer models. Through extensive experimentation across diverse
environments such as OpenAI Gym and D4RL, leveraging varying demonstration data
sets, we demonstrate the superiority of Mamba models over their Transformer
counterparts in a majority of tasks. Results show that DM outperforms other
methods in most settings. The code can be found at
https://github.com/meowatthemoon/DecisionMamba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SmoothGNN: Smoothing-aware GNN for Unsupervised Node Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Dong, Xingyi Zhang, Yanni Sun, Lei Chen, Mingxuan Yuan, Sibo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The smoothing issue in graph learning leads to indistinguishable node
representations, posing significant challenges for graph-related tasks.
However, our experiments reveal that this problem can uncover underlying
properties of node anomaly detection (NAD) that previous research has missed.
We introduce Individual Smoothing Patterns (ISP) and Neighborhood Smoothing
Patterns (NSP), which indicate that the representations of anomalous nodes are
harder to smooth than those of normal ones. In addition, we explore the
theoretical implications of these patterns, demonstrating the potential
benefits of ISP and NSP for NAD tasks. Motivated by these findings, we propose
SmoothGNN, a novel unsupervised NAD framework. First, we design a learning
component to explicitly capture ISP for detecting node anomalies. Second, we
design a spectral graph neural network to implicitly learn ISP to enhance
detection. Third, we design an effective coefficient based on our findings that
NSP can serve as coefficients for node representations, aiding in the
identification of anomalous nodes. Furthermore, we devise a novel anomaly
measure to calculate loss functions and anomalous scores for nodes, reflecting
the properties of NAD using ISP and NSP. Extensive experiments on 9 real
datasets show that SmoothGNN outperforms the best rival by an average of 14.66%
in AUC and 7.28% in Average Precision, with 75x running time speedup,
validating the effectiveness and efficiency of our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HC-GLAD: Dual Hyperbolic Contrastive Learning for Unsupervised
  Graph-Level Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yali Fu, Jindong Li, Jiahong Liu, Qianli Xing, Qi Wang, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised graph-level anomaly detection (UGAD) has garnered increasing
attention in recent years due to its significance. Most existing methods that
rely on traditional GNNs mainly consider pairwise relationships between
first-order neighbors, which is insufficient to capture the complex high-order
dependencies often associated with anomalies. This limitation underscores the
necessity of exploring high-order node interactions in UGAD. In addition, most
previous works ignore the underlying properties (e.g., hierarchy and power-law
structure) which are common in real-world graph datasets and therefore are
indispensable factors in the UGAD task. In this paper, we propose a novel Dual
Hyperbolic Contrastive Learning for Unsupervised Graph-Level Anomaly Detection
(HC-GLAD in short). To exploit high-order node group information, we construct
hypergraphs based on pre-designed gold motifs and subsequently perform
hypergraph convolution. Furthermore, to preserve the hierarchy of real-world
graphs, we introduce hyperbolic geometry into this field and conduct both graph
and hypergraph embedding learning in hyperbolic space with the hyperboloid
model. To the best of our knowledge, this is the first work to simultaneously
apply hypergraph with node group information and hyperbolic geometry in this
field. Extensive experiments on 13 real-world datasets of different fields
demonstrate the superiority of HC-GLAD on the UGAD task. The code is available
at https://github.com/Yali-F/HC-GLAD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLoCO: Learning Long Contexts Offline <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07979v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07979v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sijun Tan, Xiuyu Li, Shishir Patil, Ziyang Wu, Tianjun Zhang, Kurt Keutzer, Joseph E. Gonzalez, Raluca Ada Popa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Processing long contexts remains a challenge for large language models (LLMs)
due to the quadratic computational and memory overhead of the self-attention
mechanism and the substantial KV cache sizes during generation. We propose
LLoCO, a novel approach to address this problem by learning contexts offline
through context compression and in-domain parameter-efficient finetuning with
LoRA. Our method enables an LLM to create a concise representation of the
original context and efficiently retrieve relevant information to answer
questions accurately. Our approach extends the effective context window of a 4k
token LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on
several long-context question-answering datasets, demonstrating that LLoCO
significantly outperforms in-context learning while using $30\times$ fewer
tokens during inference. LLoCO achieves up to $7.62\times$ speed-up during
inference and $11.52\times$ higher throughput during finetuning, substantially
reduces the cost of long document question answering. This makes it a promising
solution for efficient long context processing. Our code is publicly available
on https://github.com/jeffreysijuntan/lloco.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024. The first two authors contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continuous-time q-Learning for Jump-Diffusion Models under Tsallis
  Entropy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03888v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03888v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijun Bo, Yijie Huang, Xiang Yu, Tingting Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the continuous-time reinforcement learning in
jump-diffusion models by featuring the q-learning (the continuous-time
counterpart of Q-learning) under Tsallis entropy regularization. Contrary to
the Shannon entropy, the general form of Tsallis entropy renders the optimal
policy not necessary a Gibbs measure, where the Lagrange and KKT multipliers
naturally arise from some constraints to ensure the learnt policy to be a
probability density function. As a consequence, the characterization of the
optimal policy using the q-function also involves a Lagrange multiplier. In
response, we establish the martingale characterization of the q-function under
Tsallis entropy and devise two q-learning algorithms depending on whether the
Lagrange multiplier can be derived explicitly or not. In the latter case, we
need to consider different parameterizations of the optimal q-function and the
optimal policy and update them alternatively in an Actor-Critic manner. We also
study two financial applications, namely, an optimal portfolio liquidation
problem and a non-LQ control problem. It is interesting to see therein that the
optimal policies under the Tsallis entropy regularization can be characterized
explicitly, which are distributions concentrated on some compact support. The
satisfactory performance of our q-learning algorithms is illustrated in each
example.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Concept Bottleneck Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19272v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19272v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Vandenhirtz, Sonia Laguna, Ričards Marcinkevičs, Julia E. Vogt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept Bottleneck Models (CBMs) have emerged as a promising interpretable
method whose final prediction is based on intermediate, human-understandable
concepts rather than the raw input. Through time-consuming manual
interventions, a user can correct wrongly predicted concept values to enhance
the model's downstream performance. We propose Stochastic Concept Bottleneck
Models (SCBMs), a novel approach that models concept dependencies. In SCBMs, a
single-concept intervention affects all correlated concepts, thereby improving
intervention effectiveness. Unlike previous approaches that model the concept
relations via an autoregressive structure, we introduce an explicit,
distributional parameterization that allows SCBMs to retain the CBMs' efficient
training and inference procedure. Additionally, we leverage the
parameterization to derive an effective intervention strategy based on the
confidence region. We show empirically on synthetic tabular and natural image
datasets that our approach improves intervention effectiveness significantly.
Notably, we showcase the versatility and usability of SCBMs by examining a
setting with CLIP-inferred concepts, alleviating the need for manual concept
annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at 38th Conference on Neural Information Processing Systems
  (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Don't Label Twice: Quantity Beats Quality when Comparing Binary
  Classifiers on a Budget <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02249v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02249v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian E. Dorner, Moritz Hardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study how to best spend a budget of noisy labels to compare the accuracy
of two binary classifiers. It's common practice to collect and aggregate
multiple noisy labels for a given data point into a less noisy label via a
majority vote. We prove a theorem that runs counter to conventional wisdom. If
the goal is to identify the better of two classifiers, we show it's best to
spend the budget on collecting a single label for more samples. Our result
follows from a non-trivial application of Cram\'er's theorem, a staple in the
theory of large deviations. We discuss the implications of our work for the
design of machine learning benchmarks, where they overturn some time-honored
recommendations. In addition, our results provide sample size bounds superior
to what follows from Hoeffding's bound.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 3 Figures, Published at ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Steerable Conditional Diffusion for Out-of-Distribution Adaptation in
  Medical Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14409v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14409v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Barbano, Alexander Denker, Hyungjin Chung, Tae Hoon Roh, Simon Arridge, Peter Maass, Bangti Jin, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising diffusion models have emerged as the go-to generative framework for
solving inverse problems in imaging. A critical concern regarding these models
is their performance on out-of-distribution tasks, which remains an
under-explored challenge. Using a diffusion model on an out-of-distribution
dataset, realistic reconstructions can be generated, but with hallucinating
image features that are uniquely present in the training dataset. To address
this discrepancy during train-test time and improve reconstruction accuracy, we
introduce a novel sampling framework called Steerable Conditional Diffusion.
Specifically, this framework adapts the diffusion model, concurrently with
image reconstruction, based solely on the information provided by the available
measurement. Utilising our proposed method, we achieve substantial enhancements
in out-of-distribution performance across diverse imaging modalities, advancing
the robust deployment of denoising diffusion models in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging LLM Embeddings for Cross Dataset Label Alignment and Zero
  Shot Music Emotion Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renhang Liu, Abhinaba Roy, Dorien Herremans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a novel method for music emotion recognition that
leverages Large Language Model (LLM) embeddings for label alignment across
multiple datasets and zero-shot prediction on novel categories. First, we
compute LLM embeddings for emotion labels and apply non-parametric clustering
to group similar labels, across multiple datasets containing disjoint labels.
We use these cluster centers to map music features (MERT) to the LLM embedding
space. To further enhance the model, we introduce an alignment regularization
that enables dissociation of MERT embeddings from different clusters. This
further enhances the model's ability to better adaptation to unseen datasets.
We demonstrate the effectiveness of our approach by performing zero-shot
inference on a new dataset, showcasing its ability to generalize to unseen
labels without additional training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Model for Constructing Reaction Path from Initial to Final
  States 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10721v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10721v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akihide Hayashi, So Takamoto, Ju Li, Yuta Tsuboi, Daisuke Okanohara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mapping the chemical reaction pathways and their corresponding activation
barriers is a significant challenge in molecular simulation. Given the inherent
complexities of 3D atomic geometries, even generating an initial guess of these
paths can be difficult for humans. This paper presents an innovative approach
that utilizes neural networks to generate initial guesses for reaction pathways
based on the initial state and learning from a database of low-energy
transition paths. The proposed method is initiated by inputting the coordinates
of the initial state, followed by progressive alterations to its structure.
This iterative process culminates in the generation of the guess reaction path
and the coordinates of the final state. The method does not require one-the-fly
computation of the actual potential energy surface, and is therefore
fast-acting. The application of this geometry-based method extends to complex
reaction pathways illustrated by organic reactions. Training was executed on
the Transition1x dataset of organic reaction pathways. The results revealed the
generation of reactions that bore substantial similarities with the test set of
chemical reaction paths. The method's flexibility allows for reactions to be
generated either to conform to predetermined conditions or in a randomized
manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature learning in finite-width Bayesian deep linear networks with
  multiple outputs and convolutional layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03260v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03260v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Bassetti, Marco Gherardi, Alessandro Ingrosso, Mauro Pastore, Pietro Rotondo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep linear networks have been extensively studied, as they provide
simplified models of deep learning. However, little is known in the case of
finite-width architectures with multiple outputs and convolutional layers. In
this manuscript, we provide rigorous results for the statistics of functions
implemented by the aforementioned class of networks, thus moving closer to a
complete characterization of feature learning in the Bayesian setting. Our
results include: (i) an exact and elementary non-asymptotic integral
representation for the joint prior distribution over the outputs, given in
terms of a mixture of Gaussians; (ii) an analytical formula for the posterior
distribution in the case of squared error loss function (Gaussian likelihood);
(iii) a quantitative description of the feature learning infinite-width regime,
using large deviation theory. From a physical perspective, deep architectures
with multiple outputs or convolutional layers represent different
manifestations of kernel shape renormalization, and our work provides a
dictionary that translates this physics intuition and terminology into rigorous
Bayesian statistics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Depth-supervised NeRF: Fewer Views and Faster Training for Free 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.02791v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.02791v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangle Deng, Andrew Liu, Jun-Yan Zhu, Deva Ramanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting
incorrect geometries when given an insufficient number of input views. One
potential reason is that standard volumetric rendering does not enforce the
constraint that most of a scene's geometry consist of empty space and opaque
surfaces. We formalize the above assumption through DS-NeRF (Depth-supervised
Neural Radiance Fields), a loss for learning radiance fields that takes
advantage of readily-available depth supervision. We leverage the fact that
current NeRF pipelines require images with known camera poses that are
typically estimated by running structure-from-motion (SFM). Crucially, SFM also
produces sparse 3D points that can be used as "free" depth supervision during
training: we add a loss to encourage the distribution of a ray's terminating
depth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can
render better images given fewer training views while training 2-3x faster.
Further, we show that our loss is compatible with other recently proposed NeRF
methods, demonstrating that depth is a cheap and easily digestible supervisory
signal. And finally, we find that DS-NeRF can support other types of depth
supervision such as scanned depth sensors and RGB-D reconstruction outputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: http://www.cs.cmu.edu/~dsnerf/ GitHub:
  https://github.com/dunbar12138/DSNeRF</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjie Ni, Yifan Song, Deepanway Ghosal, Bo Li, David Junhao Zhang, Xiang Yue, Fuzhao Xue, Zian Zheng, Kaichen Zhang, Mahir Shah, Kabir Jain, Yang You, Michael Shieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceiving and generating diverse modalities are crucial for AI models to
effectively learn from and engage with real-world signals, necessitating
reliable evaluations for their development. We identify two major issues in
current evaluations: (1) inconsistent standards, shaped by different
communities with varying protocols and maturity levels; and (2) significant
query, grading, and generalization biases. To address these, we introduce
MixEval-X, the first any-to-any real-world benchmark designed to optimize and
standardize evaluations across input and output modalities. We propose
multi-modal benchmark mixture and adaptation-rectification pipelines to
reconstruct real-world task distributions, ensuring evaluations generalize
effectively to real-world use cases. Extensive meta-evaluations show our
approach effectively aligns benchmark samples with real-world task
distributions and the model rankings correlate strongly with that of
crowd-sourced real-world evaluations (up to 0.98). We provide comprehensive
leaderboards to rerank existing models and organizations and offer insights to
enhance understanding of multi-modal evaluations and inform future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving <span class="highlight-title">Multi-modal</span> Large Language Model through Boosting Vision
  Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanpeng Sun, Huaxin Zhang, Qiang Chen, Xinyu Zhang, Nong Sang, Gang Zhang, Jingdong Wang, Zechao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We focus on improving the visual understanding capability for boosting the
vision-language models. We propose \textbf{Arcana}, a multiModal language
model, which introduces two crucial techniques. First, we present Multimodal
LoRA (MM-LoRA), a module designed to enhance the decoder. Unlike traditional
language-driven decoders, MM-LoRA consists of two parallel LoRAs -- one for
vision and one for language -- each with its own parameters. This disentangled
parameters design allows for more specialized learning in each modality and
better integration of multimodal information. Second, we introduce the Query
Ladder adapter (QLadder) to improve the visual encoder. QLadder employs a
learnable ``\textit{ladder}'' structure to deeply aggregates the intermediate
representations from the frozen pretrained visual encoder (e.g., CLIP image
encoder). This enables the model to learn new and informative visual features,
as well as remaining the powerful capabilities of the pretrained visual
encoder. These techniques collectively enhance Arcana's visual perception
power, enabling it to leverage improved visual information for more accurate
and contextually relevant outputs across various multimodal scenarios.
Extensive experiments and ablation studies demonstrate the effectiveness and
generalization capability of our Arcana. The code and re-annotated data are
available at \url{https://arcana-project-page.github.io}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multimodal</span> growth and development assessment model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Li, Zichen Song, Zijie Gong, Sitan Huang, Jiewei Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of social economy and the improvement of people's
attention to health, the growth and development of children and adolescents has
become an important indicator to measure the level of national health.
Therefore, accurate and timely assessment of children's growth and development
has become increasingly important. At the same time, global health
inequalities, especially child malnutrition and stunting in developing
countries, urgently require effective assessment tools to monitor and
intervene. In recent years, the rapid development of technologies such as big
data, artificial intelligence, and cloud computing, and the cross-integration
of multiple disciplines such as biomedicine, statistics, and computer science
have promoted the rapid development of large-scale models for growth and
development assessment. However, there are still problems such as too single
evaluation factors, inaccurate diagnostic results, and inability to give
accurate and reasonable recommendations. The multi-modal growth and development
assessment model uses the public data set of RSNA ( North American College of
Radiology ) as the training set, and the data set of the Department of
Pediatrics of Huaibei People's Hospital as the open source test set. The
embedded ICL module enables the model to quickly adapt and identify the tasks
that need to be done to ensure that under the premise of considering multiple
evaluation factors, accurate diagnosis results and reasonable medical
recommendations are given, so as to provide solutions to the above problems and
promote the development of the medical field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Pages 7 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MeloTrans: A Text to Symbolic Music Generation Model Following Human
  Composition Habit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutian Wang, Wanyin Yang, Zhenrong Dai, Yilong Zhang, Kun Zhao, Hui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  At present, neural network models show powerful sequence prediction ability
and are used in many automatic composition models. In comparison, the way
humans compose music is very different from it. Composers usually start by
creating musical motifs and then develop them into music through a series of
rules. This process ensures that the music has a specific structure and
changing pattern. However, it is difficult for neural network models to learn
these composition rules from training data, which results in a lack of
musicality and diversity in the generated music. This paper posits that
integrating the learning capabilities of neural networks with human-derived
knowledge may lead to better results. To archive this, we develop the
POP909$\_$M dataset, the first to include labels for musical motifs and their
variants, providing a basis for mimicking human compositional habits. Building
on this, we propose MeloTrans, a text-to-music composition model that employs
principles of motif development rules. Our experiments demonstrate that
MeloTrans excels beyond existing music generation models and even surpasses
Large Language Models (LLMs) like ChatGPT-4. This highlights the importance of
merging human insights with neural network capabilities to achieve superior
symbolic music generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Remember, Retrieve and Generate: Understanding Infinite Visual Concepts
  as Your Personalized Assistant 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Hao, Jiaming Han, Changsheng Li, Yu-Feng Li, Xiangyu Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of large language models (LLMs) has significantly enhanced
the capabilities of multimodal LLMs (MLLMs) as general assistants. However,
lack of user-specific knowledge still restricts their application in human's
daily life. In this paper, we introduce the Retrieval Augmented Personalization
(RAP) framework for MLLMs' personalization. Starting from a general MLLM, we
turn it into a personalized assistant in three steps. (a) Remember: We design a
key-value database to store user-related information, e.g., user's name, avatar
and other attributes. (b) Retrieve: When the user initiates a conversation, RAP
will retrieve relevant information from the database using a multimodal
retriever. (c) Generate: The input query and retrieved concepts' information
are fed into MLLMs to generate personalized, knowledge-augmented responses.
Unlike previous methods, RAP allows real-time concept editing via updating the
external database. To further improve generation quality and alignment with
user-specific information, we design a pipeline for data collection and create
a specialized dataset for personalized training of MLLMs. Based on the dataset,
we train a series of MLLMs as personalized multimodal assistants. By
pretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual
concepts without additional finetuning. Our models demonstrate outstanding
flexibility and generation quality across a variety of tasks, such as
personalized image captioning, question answering and visual recognition. The
code, data and models are available at https://github.com/Hoar012/RAP-MLLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Coarse-Grained Matching in Video-Text Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12407v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12407v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aozhu Chen, Hazel Doughty, Xirong Li, Cees G. M. Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-text retrieval has seen significant advancements, yet the ability of
models to discern subtle differences in captions still requires verification.
In this paper, we introduce a new approach for fine-grained evaluation. Our
approach can be applied to existing datasets by automatically generating hard
negative test captions with subtle single-word variations across nouns, verbs,
adjectives, adverbs, and prepositions. We perform comprehensive experiments
using four state-of-the-art models across two standard benchmarks (MSR-VTT and
VATEX) and two specially curated datasets enriched with detailed descriptions
(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our
analyses show that the current evaluation benchmarks fall short in detecting a
model's ability to perceive subtle single-word differences, 2) our fine-grained
evaluation highlights the difficulty models face in distinguishing such subtle
variations. To enhance fine-grained understanding, we propose a new baseline
that can be easily combined with current methods. Experiments on our
fine-grained evaluations demonstrate that this approach enhances a model's
ability to understand fine-grained differences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SaMoye: Zero-shot Singing Voice Conversion Model Based on Feature
  Disentanglement and Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07728v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07728v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Wang, Le Ma, Yongsheng Feng, Xin Pan, Yuhang Jin, Kejun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Singing voice conversion (SVC) aims to convert a singer's voice to another
singer's from a reference audio while keeping the original semantics. However,
existing SVC methods can hardly perform zero-shot due to incomplete feature
disentanglement or dependence on the speaker look-up table. We propose the
first open-source high-quality zero-shot SVC model SaMoye that can convert
singing to human and non-human timbre. SaMoye disentangles the singing voice's
features into content, timbre, and pitch features, where we combine multiple
ASR models and compress the content features to reduce timbre leaks. Besides,
we enhance the timbre features by unfreezing the speaker encoder and mixing the
speaker embedding with top-3 similar speakers. We also establish an
unparalleled large-scale dataset to guarantee zero-shot performance, which
comprises more than 1,815 hours of pure singing voice and 6,367 speakers. We
conduct objective and subjective experiments to find that SaMoye outperforms
other models in zero-shot SVC tasks even under extreme conditions like
converting singing to animals' timbre. The code and weight of SaMoye are
available on https://github.com/CarlWangChina/SaMoye-SVC. The weights, code,
dataset, and documents of SaMoye are publicly available on
\url{https://github.com/CarlWangChina/SaMoye-SVC}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging LLM Embeddings for Cross Dataset Label Alignment and Zero
  Shot Music Emotion Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renhang Liu, Abhinaba Roy, Dorien Herremans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a novel method for music emotion recognition that
leverages Large Language Model (LLM) embeddings for label alignment across
multiple datasets and zero-shot prediction on novel categories. First, we
compute LLM embeddings for emotion labels and apply non-parametric clustering
to group similar labels, across multiple datasets containing disjoint labels.
We use these cluster centers to map music features (MERT) to the LLM embedding
space. To further enhance the model, we introduce an alignment regularization
that enables dissociation of MERT embeddings from different clusters. This
further enhances the model's ability to better adaptation to unseen datasets.
We demonstrate the effectiveness of our approach by performing zero-shot
inference on a new dataset, showcasing its ability to generalize to unseen
labels without additional training.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-16T00:00:00Z">2024-10-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context is Key(NMF): Modelling Topical Information Dynamics in Chinese
  Diaspora Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ross Deans Kristensen-McLachlan, Rebecca M. M. Hicke, Márton Kardos, Mette Thunø
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Does the People's Republic of China (PRC) interfere with European elections
through ethnic Chinese diaspora media? This question forms the basis of an
ongoing research project exploring how PRC narratives about European elections
are represented in Chinese diaspora media, and thus the objectives of PRC news
media manipulation. In order to study diaspora media efficiently and at scale,
it is necessary to use techniques derived from quantitative text analysis, such
as topic modelling. In this paper, we present a pipeline for studying
information dynamics in Chinese media. Firstly, we present KeyNMF, a new
approach to static and dynamic topic modelling using transformer-based
contextual embedding models. We provide benchmark evaluations to demonstrate
that our approach is competitive on a number of Chinese datasets and metrics.
Secondly, we integrate KeyNMF with existing methods for describing information
dynamics in complex systems. We apply this pipeline to data from five news
sites, focusing on the period of time leading up to the 2024 European
parliamentary elections. Our methods and results demonstrate the effectiveness
of KeyNMF for studying information dynamics in Chinese media and lay groundwork
for further work addressing the broader research questions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 2024 Computational Humanities Research Conference
  (CHR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-Chunking: Learning Efficient Text Segmentation via Logical
  Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Zhao, Zhiyuan Ji, Pengnian Qi, Simin Niu, Bo Tang, Feiyu Xiong, Zhiyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG), while serving as a viable complement to
large language models (LLMs), often overlooks the crucial aspect of text
chunking within its pipeline, which impacts the quality of knowledge-intensive
tasks. This paper introduces the concept of Meta-Chunking, which refers to a
granularity between sentences and paragraphs, consisting of a collection of
sentences within a paragraph that have deep linguistic logical connections. To
implement Meta-Chunking, we designed two strategies based on LLMs: Margin
Sampling Chunking and Perplexity Chunking. The former employs LLMs to perform
binary classification on whether consecutive sentences need to be segmented,
making decisions based on the probability difference obtained from margin
sampling. The latter precisely identifies text chunk boundaries by analyzing
the characteristics of perplexity distribution. Additionally, considering the
inherent complexity of different texts, we propose a strategy that combines
Meta-Chunking with dynamic merging to achieve a balance between fine-grained
and coarse-grained text chunking. Experiments conducted on eleven datasets
demonstrate that Meta-Chunking can more efficiently improve the performance of
single-hop and multi-hop question answering based on RAG. For instance, on the
2WikiMultihopQA dataset, it outperforms similarity chunking by 1.32 while only
consuming 45.8% of the time. Our code is available at
https://github.com/IAAR-Shanghai/Meta-Chunking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JudgeBench: A Benchmark for Evaluating LLM-based Judges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Y. Tang, Alejandro Cuadron, Chenguang Wang, Raluca Ada Popa, Ion Stoica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-based judges have emerged as a scalable alternative to human evaluation
and are increasingly used to assess, compare, and improve models. However, the
reliability of LLM-based judges themselves is rarely scrutinized. As LLMs
become more advanced, their responses grow more sophisticated, requiring
stronger judges to evaluate them. Existing benchmarks primarily focus on a
judge's alignment with human preferences, but often fail to account for more
challenging tasks where crowdsourced human preference is a poor indicator of
factual and logical correctness. To address this, we propose a novel evaluation
framework to objectively evaluate LLM-based judges. Based on this framework, we
propose JudgeBench, a benchmark for evaluating LLM-based judges on challenging
response pairs spanning knowledge, reasoning, math, and coding. JudgeBench
leverages a novel pipeline for converting existing difficult datasets into
challenging response pairs with preference labels reflecting objective
correctness. Our comprehensive evaluation on a collection of prompted judges,
fine-tuned judges, multi-agent judges, and reward models shows that JudgeBench
poses a significantly greater challenge than previous benchmarks, with many
strong models (e.g., GPT-4o) performing just slightly better than random
guessing. Overall, JudgeBench offers a reliable platform for assessing
increasingly advanced LLM-based judges. Data and code are available at
https://github.com/ScalerLab/JudgeBench .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Context Learning Enables Robot Action Prediction in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Yin, Zekai Wang, Yuvan Sharma, Dantong Niu, Trevor Darrell, Roei Herzig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Large Language Models (LLMs) have achieved remarkable success using
in-context learning (ICL) in the language domain. However, leveraging the ICL
capabilities within LLMs to directly predict robot actions remains largely
unexplored. In this paper, we introduce RoboPrompt, a framework that enables
off-the-shelf text-only LLMs to directly predict robot actions through ICL
without training. Our approach first heuristically identifies keyframes that
capture important moments from an episode. Next, we extract end-effector
actions from these keyframes as well as the estimated initial object poses, and
both are converted into textual descriptions. Finally, we construct a
structured template to form ICL demonstrations from these textual descriptions
and a task instruction. This enables an LLM to directly predict robot actions
at test time. Through extensive experiments and analysis, RoboPrompt shows
stronger performance over zero-shot and ICL baselines in simulated and
real-world settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned
  Concepts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongcheng Gao, Tianyu Pang, Chao Du, Taihang Hu, Zhijie Deng, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid progress of diffusion-based content generation, significant
efforts are being made to unlearn harmful or copyrighted concepts from
pretrained diffusion models (DMs) to prevent potential model misuse. However,
it is observed that even when DMs are properly unlearned before release,
malicious finetuning can compromise this process, causing DMs to relearn the
unlearned concepts. This occurs partly because certain benign concepts (e.g.,
"skin") retained in DMs are related to the unlearned ones (e.g., "nudity"),
facilitating their relearning via finetuning. To address this, we propose
meta-unlearning on DMs. Intuitively, a meta-unlearned DM should behave like an
unlearned DM when used as is; moreover, if the meta-unlearned DM undergoes
malicious finetuning on unlearned concepts, the related benign concepts
retained within it will be triggered to self-destruct, hindering the relearning
of unlearned concepts. Our meta-unlearning framework is compatible with most
existing unlearning methods, requiring only the addition of an
easy-to-implement meta objective. We validate our approach through empirical
experiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4
and SDXL), supported by extensive ablation studies. Our code is available at
https://github.com/sail-sg/Meta-Unlearning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying Task Groupings for Multi-Task Learning Using Pointwise
  V-Usable Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingya Li, Timothy Miller, Steven Bethard, Guergana Savova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of multi-task learning can depend heavily on which tasks are
grouped together. Naively grouping all tasks or a random set of tasks can
result in negative transfer, with the multi-task models performing worse than
single-task models. Though many efforts have been made to identify task
groupings and to measure the relatedness among different tasks, it remains a
challenging research topic to define a metric to identify the best task
grouping out of a pool of many potential task combinations. We propose a metric
of task relatedness based on task difficulty measured by pointwise V-usable
information (PVI). PVI is a recently proposed metric to estimate how much
usable information a dataset contains given a model. We hypothesize that tasks
with not statistically different PVI estimates are similar enough to benefit
from the joint learning process. We conduct comprehensive experiments to
evaluate the feasibility of this metric for task grouping on 15 NLP datasets in
the general, biomedical, and clinical domains. We compare the results of the
joint learners against single learners, existing baseline methods, and recent
large language models, including Llama 2 and GPT-4. The results show that by
grouping tasks with similar PVI estimates, the joint learners yielded
competitive results with fewer total parameters, with consistent performance
across domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>main paper 12 pages, Appendix 7 pages, 1 figure, 18 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unitary Multi-Margin BERT for Robust Natural Language Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Yuan Chang, Kang L. Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in adversarial attacks on deep learning leave many
mission-critical natural language processing (NLP) systems at risk of
exploitation. To address the lack of computationally efficient adversarial
defense methods, this paper reports a novel, universal technique that
drastically improves the robustness of Bidirectional Encoder Representations
from Transformers (BERT) by combining the unitary weights with the multi-margin
loss. We discover that the marriage of these two simple ideas amplifies the
protection against malicious interference. Our model, the unitary multi-margin
BERT (UniBERT), boosts post-attack classification accuracies significantly by
5.3% to 73.8% while maintaining competitive pre-attack accuracies. Furthermore,
the pre-attack and post-attack accuracy tradeoff can be adjusted via a single
scalar parameter to best fit the design requirements for the target
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StyleDistance: Stronger Content-Independent Style Embeddings with
  Synthetic Parallel Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajay Patel, Jiacheng Zhu, Justin Qiu, Zachary Horvitz, Marianna Apidianaki, Kathleen McKeown, Chris Callison-Burch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Style representations aim to embed texts with similar writing styles closely
and texts with different styles far apart, regardless of content. However, the
contrastive triplets often used for training these representations may vary in
both style and content, leading to potential content leakage in the
representations. We introduce StyleDistance, a novel approach to training
stronger content-independent style embeddings. We use a large language model to
create a synthetic dataset of near-exact paraphrases with controlled style
variations, and produce positive and negative examples across 40 distinct style
features for precise contrastive learning. We assess the quality of our
synthetic data and embeddings through human and automatic evaluations.
StyleDistance enhances the content-independence of style embeddings, which
generalize to real-world benchmarks and outperform leading style
representations in downstream applications. Our model can be found at
https://huggingface.co/StyleDistance/styledistance .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Analysis of Extrinsic Factors for NER in French 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grace Yang, Zhiyi Li, Yandong Liu, Jungyeul Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named entity recognition (NER) is a crucial task that aims to identify
structured information, which is often replete with complex, technical terms
and a high degree of variability. Accurate and reliable NER can facilitate the
extraction and analysis of important information. However, NER for other than
English is challenging due to limited data availability, as the high expertise,
time, and expenses are required to annotate its data. In this paper, by using
the limited data, we explore various factors including model structure, corpus
annotation scheme and data augmentation techniques to improve the performance
of a NER model for French. Our experiments demonstrate that these approaches
can significantly improve the model's F1 score from original CRF score of 62.41
to 79.39. Our findings suggest that considering different extrinsic factors and
combining these techniques is a promising approach for improving NER
performance where the size of data is limited.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CREAM: Consistency Regularized Self-Rewarding Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Wang, Weilei He, Zhiyuan Liang, Xuchao Zhang, Chetan Bansal, Ying Wei, Weitong Zhang, Huaxiu Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent self-rewarding large language models (LLM) have successfully applied
LLM-as-a-Judge to iteratively improve the alignment performance without the
need of human annotations for preference data. These methods commonly utilize
the same LLM to act as both the policy model (which generates responses) and
the reward model (which scores and ranks those responses). The ranked responses
are then used as preference pairs to train the LLM via direct alignment
technologies (e.g. DPO). However, it is noteworthy that throughout this
process, there is no guarantee of accuracy in the rewarding and ranking, which
is critical for ensuring accurate rewards and high-quality preference data.
Empirical results from relatively small LLMs (e.g., 7B parameters) also
indicate that improvements from self-rewarding may diminish after several
iterations in certain situations, which we hypothesize is due to accumulated
bias in the reward system. This bias can lead to unreliable preference data for
training the LLM. To address this issue, we first formulate and analyze the
generalized iterative preference fine-tuning framework for self-rewarding
language model. We then introduce the regularization to this generalized
framework to mitigate the overconfident preference labeling in the
self-rewarding process. Based on this theoretical insight, we propose a
Consistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages
the rewarding consistency across different iterations to regularize the
self-rewarding training, helping the model to learn from more reliable
preference data. With this explicit regularization, our empirical results
demonstrate the superiority of CREAM in improving both reward consistency and
alignment performance. The code is publicly available at
https://github.com/Raibows/CREAM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WorldMedQA-V: a multilingual, <span class="highlight-title">multimodal</span> medical examination dataset for
  <span class="highlight-title">multimodal</span> language models evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Matos, Shan Chen, Siena Placino, Yingya Li, Juan Carlos Climent Pardo, Daphna Idan, Takeshi Tohyama, David Restrepo, Luis F. Nakayama, Jose M. M. Pascual-Leone, Guergana Savova, Hugo Aerts, Leo A. Celi, A. Ian Wong, Danielle S. Bitterman, Jack Gallifant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal/vision language models (VLMs) are increasingly being deployed in
healthcare settings worldwide, necessitating robust benchmarks to ensure their
safety, efficacy, and fairness. Multiple-choice question and answer (QA)
datasets derived from national medical examinations have long served as
valuable evaluation tools, but existing datasets are largely text-only and
available in a limited subset of languages and countries. To address these
challenges, we present WorldMedQA-V, an updated multilingual, multimodal
benchmarking dataset designed to evaluate VLMs in healthcare. WorldMedQA-V
includes 568 labeled multiple-choice QAs paired with 568 medical images from
four countries (Brazil, Israel, Japan, and Spain), covering original languages
and validated English translations by native clinicians, respectively. Baseline
performance for common open- and closed-source models are provided in the local
language and English translations, and with and without images provided to the
model. The WorldMedQA-V benchmark aims to better match AI systems to the
diverse healthcare environments in which they are deployed, fostering more
equitable, effective, and representative applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted for review, total of 14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WorldCuisines: A Massive-Scale Benchmark for Multilingual and
  Multicultural Visual Question Answering on Global Cuisines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genta Indra Winata, Frederikus Hudi, Patrick Amadeus Irawan, David Anugraha, Rifki Afina Putri, Yutong Wang, Adam Nohejl, Ubaidillah Ariq Prathama, Nedjma Ousidhoum, Afifa Amriani, Anar Rzayev, Anirban Das, Ashmari Pramodya, Aulia Adila, Bryan Wilie, Candy Olivia Mawalim, Ching Lam Cheng, Daud Abolade, Emmanuele Chersoni, Enrico Santus, Fariz Ikhwantri, Garry Kuwanto, Hanyang Zhao, Haryo Akbarianto Wibowo, Holy Lovenia, Jan Christian Blaise Cruz, Jan Wira Gotama Putra, Junho Myung, Lucky Susanto, Maria Angelica Riera Machin, Marina Zhukova, Michael Anugraha, Muhammad Farid Adilazuarda, Natasha Santosa, Peerat Limkonchotiwat, Raj Dabre, Rio Alexander Audino, Samuel Cahyawijaya, Shi-Xiong Zhang, Stephanie Yulia Salim, Yi Zhou, Yinxuan Gui, David Ifeoluwa Adelani, En-Shiun Annie Lee, Shogo Okada, Ayu Purwarianti, Alham Fikri Aji, Taro Watanabe, Derry Tanti Wijaya, Alice Oh, Chong-Wah Ngo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) often struggle with culture-specific knowledge,
particularly in languages other than English and in underrepresented cultural
contexts. To evaluate their understanding of such knowledge, we introduce
WorldCuisines, a massive-scale benchmark for multilingual and multicultural,
visually grounded language understanding. This benchmark includes a visual
question answering (VQA) dataset with text-image pairs across 30 languages and
dialects, spanning 9 language families and featuring over 1 million data
points, making it the largest multicultural VQA benchmark to date. It includes
tasks for identifying dish names and their origins. We provide evaluation
datasets in two sizes (12k and 60k instances) alongside a training dataset (1
million instances). Our findings show that while VLMs perform better with
correct location context, they struggle with adversarial contexts and
predicting specific regional cuisines and languages. To support future
research, we release a knowledge base with annotated food entries and images
along with the VQA data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sarcasm Detection in a Less-Resourced Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lazar Đoković, Marko Robnik-Šikonja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sarcasm detection task in natural language processing tries to classify
whether an utterance is sarcastic or not. It is related to sentiment analysis
since it often inverts surface sentiment. Because sarcastic sentences are
highly dependent on context, and they are often accompanied by various
non-verbal cues, the task is challenging. Most of related work focuses on
high-resourced languages like English. To build a sarcasm detection dataset for
a less-resourced language, such as Slovenian, we leverage two modern
techniques: a machine translation specific medium-size transformer model, and a
very large generative language model. We explore the viability of translated
datasets and how the size of a pretrained transformer affects its ability to
detect sarcasm. We train ensembles of detection models and evaluate models'
performance. The results show that larger models generally outperform smaller
ones and that ensembling can slightly improve sarcasm detection performance.
Our best ensemble approach achieves an $\text{F}_1$-score of 0.765 which is
close to annotators' agreement in the source language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, published in the Slovenian Conference on Artificial
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VividMed: <span class="highlight-title">Vision Language</span> Model with Versatile Visual Grounding for
  Medicine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingxiao Luo, Bingda Tang, Xuanzhong Chen, Rong Han, Ting Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Vision Language Models (VLMs) have demonstrated
remarkable promise in generating visually grounded responses. However, their
application in the medical domain is hindered by unique challenges. For
instance, most VLMs rely on a single method of visual grounding, whereas
complex medical tasks demand more versatile approaches. Additionally, while
most VLMs process only 2D images, a large portion of medical images are 3D. The
lack of medical data further compounds these obstacles. To address these
challenges, we present VividMed, a vision language model with versatile visual
grounding for medicine. Our model supports generating both semantic
segmentation masks and instance-level bounding boxes, and accommodates various
imaging modalities, including both 2D and 3D data. We design a three-stage
training procedure and an automatic data synthesis pipeline based on open
datasets and models. Besides visual grounding tasks, VividMed also excels in
other common downstream tasks, including Visual Question Answering (VQA) and
report generation. Ablation studies empirically show that the integration of
visual grounding ability leads to improved performance on these tasks. Our code
is publicly available at https://github.com/function2-llx/MMMM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Better: Avoiding Pitfalls in Developing Language Resources when
  Data is Scarce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nedjma Ousidhoum, Meriem Beloucif, Saif M. Mohammad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language is a symbolic capital that affects people's lives in many ways
(Bourdieu, 1977, 1991). It is a powerful tool that accounts for identities,
cultures, traditions, and societies in general. Hence, data in a given language
should be viewed as more than a collection of tokens. Good data collection and
labeling practices are key to building more human-centered and socially aware
technologies. While there has been a rising interest in mid- to low-resource
languages within the NLP community, work in this space has to overcome unique
challenges such as data scarcity and access to suitable annotators. In this
paper, we collect feedback from those directly involved in and impacted by NLP
artefacts for mid- to low-resource languages. We conduct a quantitative and
qualitative analysis of the responses and highlight the main issues related to
(1) data quality such as linguistic and cultural data suitability; and (2) the
ethics of common annotation practices such as the misuse of online community
services. Based on these findings, we make several recommendations for the
creation of high-quality language artefacts that reflect the cultural milieu of
its speakers, while simultaneously respecting the dignity and labor of data
workers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Modal Safety Mechanism Transfer in Large <span class="highlight-title">Vision-Language</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shicheng Xu, Liang Pang, Yunchang Zhu, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language alignment in Large Vision-Language Models (LVLMs)
successfully enables LLMs to understand visual input. However, we find that
existing vision-language alignment methods fail to transfer the existing safety
mechanism for text in LLMs to vision, which leads to vulnerabilities in toxic
image. To explore the cause of this problem, we give the insightful explanation
of where and how the safety mechanism of LVLMs operates and conduct comparative
analysis between text and vision. We find that the hidden states at the
specific transformer layers play a crucial role in the successful activation of
safety mechanism, while the vision-language alignment at hidden states level in
current methods is insufficient. This results in a semantic shift for input
images compared to text in hidden states, therefore misleads the safety
mechanism. To address this, we propose a novel Text-Guided vision-language
Alignment method (TGA) for LVLMs. TGA retrieves the texts related to input
vision and uses them to guide the projection of vision into the hidden states
space in LLMs. Experiments show that TGA not only successfully transfers the
safety mechanism for text in basic LLMs to vision in vision-language alignment
for LVLMs without any safety fine-tuning on the visual modality but also
maintains the general performance on various vision tasks (Safe and Good).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Morphological Compositional Generalization in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mete Ismayilzada, Defne Circi, Jonne Sälevä, Hale Sirin, Abdullatif Köksal, Bhuwan Dhingra, Antoine Bosselut, Lonneke van der Plas, Duygu Ataman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated significant progress in
various natural language generation and understanding tasks. However, their
linguistic generalization capabilities remain questionable, raising doubts
about whether these models learn language similarly to humans. While humans
exhibit compositional generalization and linguistic creativity in language use,
the extent to which LLMs replicate these abilities, particularly in morphology,
is under-explored. In this work, we systematically investigate the
morphological generalization abilities of LLMs through the lens of
compositionality. We define morphemes as compositional primitives and design a
novel suite of generative and discriminative tasks to assess morphological
productivity and systematicity. Focusing on agglutinative languages such as
Turkish and Finnish, we evaluate several state-of-the-art instruction-finetuned
multilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs
struggle with morphological compositional generalization particularly when
applied to novel word roots, with performance declining sharply as
morphological complexity increases. While models can identify individual
morphological combinations better than chance, their performance lacks
systematicity, leading to significant accuracy gaps compared to humans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Measurement Instruments to Training Data: Leveraging Theory-Driven
  Synthetic Training Data for Measuring Social Constructs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Birkenmaier, Matthias Roth, Indira Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational text classification is a challenging task, especially for
multi-dimensional social constructs. Recently, there has been increasing
discussion that synthetic training data could enhance classification by
offering examples of how these constructs are represented in texts. In this
paper, we systematically examine the potential of theory-driven synthetic
training data for improving the measurement of social constructs. In
particular, we explore how researchers can transfer established knowledge from
measurement instruments in the social sciences, such as survey scales or
annotation codebooks, into theory-driven generation of synthetic data. Using
two studies on measuring sexism and political topics, we assess the added value
of synthetic training data for fine-tuning text classification models. Although
the results of the sexism study were less promising, our findings demonstrate
that synthetic data can be highly effective in reducing the need for labeled
data in political topic classification. With only a minimal drop in
performance, synthetic data allows for substituting large amounts of labeled
data. Furthermore, theory-driven synthetic data performed markedly better than
data generated without conceptual information in mind.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weak-to-Strong Generalization beyond Accuracy: a Pilot Study in Safety,
  Toxicity, and Legal Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruimeng Ye, Yang Xiao, Bo Hui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) continue to advance, ensuring their alignment
with human values becomes increasingly critical. Traditional alignment methods
heavily rely on human feedback to fine-tune models. With the emergence of
superhuman models whose outputs may surpass human understanding, evaluating and
aligning these models using human judgments poses significant challenges. To
address the challenges, recent works use weak supervisors to elicit knowledge
from much stronger models. However, there are important disanalogies between
the empirical setup in the existing works and the genuine goal of alignment. We
remark that existing works investigate the phenomenon of weak-to-strong
generation in analogous setup (i.e., binary classification), rather than
practical alignment-relevant tasks (e.g., safety). In this paper, we bridge
this gap by extending weak-to-strong generation to the context of practical
alignment. We empirically demonstrate the widespread phenomenon of
weak-to-strong generation in three complicated alignment tasks: safety,
toxicity, and legal reasoning}. Furthermore, we explore efficient strategies
for improving alignment performance to enhance the quality of model outcomes.
Lastly, we summarize and analyze the challenges and potential solutions in
regard to specific alignment tasks, which we hope to catalyze the research
progress on the topic of weak-to-strong generalization. Our code is released at
https://github.com/yeruimeng/WTS.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parsing Akkadian Verbs with Prolog <span class="chip">ACL-02</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Macks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes a parsing/generation system for finite verbal forms in
Akkadian, with the possible addition of suffixes, implemented in Prolog. The
work described provides the framework and engine to interpret the D, N, and G
stems along with accusative, dative and ventive endings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 9 figures, presented at ACL-02 the Association of
  Computational Linguistics, 2002</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Model Kinship for Merging Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yedi Hu, Yunzhi Yao, Ningyu Zhang, Shumin Deng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging has become one of the key technologies for enhancing the
capabilities and efficiency of Large Language Models (LLMs). However, our
understanding of the expected performance gains and principles when merging any
two models remains limited. In this work, we introduce model kinship, the
degree of similarity or relatedness between LLMs, analogous to biological
evolution. With comprehensive empirical analysis, we find that there is a
certain relationship between model kinship and the performance gains after
model merging, which can help guide our selection of candidate models. Inspired
by this, we propose a new model merging strategy: Top-k Greedy Merging with
Model Kinship, which can yield better performance on benchmark datasets.
Specifically, we discover that using model kinship as a criterion can assist us
in continuously performing model merging, alleviating the degradation (local
optima) in model evolution, whereas model kinship can serve as a guide to
escape these traps. Code is available at
https://github.com/zjunlp/ModelKinship.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not All Votes Count! Programs as Verifiers Improve Self-Consistency of
  Language Models for Math Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vernon Y. H. Toh, Deepanway Ghosal, Soujanya Poria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown increasing proficiency in solving
mathematical reasoning problems. However, many current open-source LLMs often
still make calculation and semantic understanding errors in their intermediate
reasoning steps. In this work, we propose PROVE, a simple yet effective
framework that uses program-based verification as a heuristic to filter out
potentially incorrect reasoning paths before aggregating the final answers.
Instead of relying on vanilla majority voting, our approach rejects solutions
whose corresponding program outputs are inconsistent with the generated
solution, aggregating only those validated by Python programs. We conducted
extensive experiments on 13 open-source LLMs from various model families and
sizes, ranging from 0.5B to 13B parameters, across seven math benchmarks. We
demonstrate that PROVE consistently outperforms vanilla majority voting as a
heuristic for solving mathematical reasoning tasks across all datasets and
model sizes. Notably, PROVE increases accuracy on the GSM8K benchmark from
48.85% to 53.83% for Qwen2-0.5B-Instruct, from 65.66% to 73.01% for
Llama-3.2-1B-Instruct, from 73.39% to 79.61% for Gemma-2-2b-it, and from 41.32%
to 59.51% for Llama-2-7B-chat. Our codes are available at
https://github.com/declare-lab/prove.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CCSBench: Evaluating Compositional Controllability in LLMs for
  Scientific Document Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixi Ding, Jiaying Wu, Tongyao Zhu, Yanxia Qin, Qian Liu, Min-Yen Kan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To broaden the dissemination of scientific knowledge to diverse audiences,
scientific document summarization must simultaneously control multiple
attributes such as length and empirical focus. However, existing research
typically focuses on controlling single attributes, leaving the compositional
control of multiple attributes underexplored. To address this gap, we introduce
CCSBench, a benchmark for compositional controllable summarization in the
scientific domain. Our benchmark enables fine-grained control over both
explicit attributes (e.g., length), which are objective and straightforward,
and implicit attributes (e.g., empirical focus), which are more subjective and
conceptual. We conduct extensive experiments on GPT-4, LLaMA2, and other
popular LLMs under various settings. Our findings reveal significant
limitations in large language models' ability to balance trade-offs between
control attributes, especially implicit ones that require deeper understanding
and abstract reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Risk of Evidence Pollution for Malicious Social Text Detection in
  the Era of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Herun Wan, Minnan Luo, Zhixiong Su, Guang Dai, Xiang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evidence-enhanced detectors present remarkable abilities in identifying
malicious social text with related evidence. However, the rise of large
language models (LLMs) brings potential risks of evidence pollution to confuse
detectors. This paper explores how to manipulate evidence, simulating potential
misuse scenarios including basic pollution, and rephrasing or generating
evidence by LLMs. To mitigate its negative impact, we propose three defense
strategies from both the data and model sides, including machine-generated text
detection, a mixture of experts, and parameter updating. Extensive experiments
on four malicious social text detection tasks with ten datasets present that
evidence pollution, especially the generate strategy, significantly compromises
existing detectors. On the other hand, the defense strategies could mitigate
evidence pollution, but they faced limitations for practical employment, such
as the need for annotated data and huge inference costs. Further analysis
illustrates that polluted evidence is of high quality, would compromise the
model calibration, and could ensemble to amplify the negative impact.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can We Reverse In-Context Knowledge Edits? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Youssef, Zhixue Zhao, Jörg Schlötterer, Christin Seifert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context knowledge editing (IKE) enables efficient modification of large
language model (LLM) outputs without parameter changes and at zero-cost.
However, it can be misused to manipulate responses opaquely, e.g., insert
misinformation or offensive content. Such malicious interventions could be
incorporated into high-level wrapped APIs where the final input prompt is not
shown to end-users. To address this issue, we investigate the detection and
reversal of IKE-edits. First, we demonstrate that IKE-edits can be detected
with high accuracy (F1 > 80\%) using only the top-10 output probabilities of
the next token, even in a black-box setting, e.g. proprietary LLMs with limited
output information. Further, we introduce the novel task of reversing IKE-edits
using specially tuned reversal tokens. We explore using both continuous and
discrete reversal tokens, achieving over 80\% accuracy in recovering original,
unedited outputs across multiple LLMs. Our continuous reversal tokens prove
particularly effective, with minimal impact on unedited prompts. Through
analysis of output distributions, attention patterns, and token rankings, we
provide insights into IKE's effects on LLMs and how reversal tokens mitigate
them. This work represents a significant step towards enhancing LLM resilience
against potential misuse of in-context editing, improving their transparency
and trustworthiness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STRUX: An LLM for Decision-Making with Structured Explanations <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Lu, Yebowen Hu, Hassan Foroosh, Wei Jin, Fei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Countless decisions shape our daily lives, and it is paramount to understand
the how and why behind these choices. In this paper, we introduce a new LLM
decision-making framework called STRUX, which enhances LLM decision-making by
providing structured explanations. These include favorable and adverse facts
related to the decision, along with their respective strengths. STRUX begins by
distilling lengthy information into a concise table of key facts. It then
employs a series of self-reflection steps to determine which of these facts are
pivotal, categorizing them as either favorable or adverse in relation to a
specific decision. Lastly, we fine-tune an LLM to identify and prioritize these
key facts to optimize decision-making. STRUX has been evaluated on the
challenging task of forecasting stock investment decisions based on earnings
call transcripts and demonstrated superior performance against strong
baselines. It enhances decision transparency by allowing users to understand
the impact of different factors, representing a meaningful step towards
practical decision-making with LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, submitted to NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Claim Decomposition Benchmark for Long-form Answer Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Zhang, Yixing Fan, Ruqing Zhang, Jiafeng Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of LLMs has significantly boosted the performance of complex
long-form question answering tasks. However, one prominent issue of LLMs is the
generated "hallucination" responses that are not factual. Consequently,
attribution for each claim in responses becomes a common solution to improve
the factuality and verifiability. Existing researches mainly focus on how to
provide accurate citations for the response, which largely overlook the
importance of identifying the claims or statements for each response. To bridge
this gap, we introduce a new claim decomposition benchmark, which requires
building system that can identify atomic and checkworthy claims for LLM
responses. Specifically, we present the Chinese Atomic Claim Decomposition
Dataset (CACDD), which builds on the WebCPM dataset with additional expert
annotations to ensure high data quality. The CACDD encompasses a collection of
500 human-annotated question-answer pairs, including a total of 4956 atomic
claims. We further propose a new pipeline for human annotation and describe the
challenges of this task. In addition, we provide experiment results on
zero-shot, few-shot and fine-tuned LLMs as baselines. The results show that the
claim decomposition is highly challenging and requires further explorations.
All code and data are publicly available at
\url{https://github.com/FBzzh/CACDD}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CCIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-based Translation Inference with Iterative Bilingual Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andong Chen, Kehai Chen, Yang Xiang, Xuefeng Bai, Muyun Yang, Tiejun Zhao, Min zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable understanding and generation capabilities of large language
models (LLMs) have greatly improved translation performance. However, incorrect
understanding of the sentence to be translated can degrade translation quality.
To address this issue, we proposed a novel Iterative Bilingual Understanding
Translation (IBUT) method based on the cross-lingual capabilities of LLMs and
the dual characteristics of translation tasks. The cross-lingual capability of
LLMs enables the generation of contextual understanding for both the source and
target languages separately. Furthermore, the dual characteristics allow IBUT
to generate effective cross-lingual feedback, iteratively refining contextual
understanding, thereby reducing errors and improving translation performance.
Experimental results showed that the proposed IBUT outperforms several strong
comparison methods, especially being generalized to multiple domains (e.g.,
news, commonsense, and cultural translation benchmarks).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in process</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedAide: Towards an Omni Medical Aide via Specialized LLM-based
  Multi-Agent Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjie Wei, Dingkang Yang, Yanshu Li, Qingyao Xu, Zhaoyu Chen, Mingcheng Li, Yue Jiang, Xiaolu Hou, Lihua Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM)-driven interactive systems currently show
potential promise in healthcare domains. Despite their remarkable capabilities,
LLMs typically lack personalized recommendations and diagnosis analysis in
sophisticated medical applications, causing hallucinations and performance
bottlenecks. To address these challenges, this paper proposes MedAide, an
LLM-based omni medical multi-agent collaboration framework for specialized
healthcare services. Specifically, MedAide first performs query rewriting
through retrieval-augmented generation to accomplish accurate medical intent
understanding. Immediately, we devise a contextual encoder to obtain intent
prototype embeddings, which are used to recognize fine-grained intents by
similarity matching. According to the intent relevance, the activated agents
collaborate effectively to provide integrated decision analysis. Extensive
experiments are conducted on four medical benchmarks with composite intents.
Experimental results from automated metrics and expert doctor evaluations show
that MedAide outperforms current LLMs and improves their medical proficiency
and strategic reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FiRST: Finetuning Router-Selective Transformers for Input-Adaptive
  Latency Reduction <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akriti Jain, Saransh Sharma, Koyel Mukherjee, Soumyabrata Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Auto-regressive Large Language Models (LLMs) demonstrate remarkable
performance across domanins such as vision and language processing. However,
due to sequential processing through a stack of transformer layers,
autoregressive decoding faces significant computation/latency challenges,
particularly in resource constrained environments like mobile and edge devices.
Existing approaches in literature that aim to improve latency via skipping
layers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics
where tokens exit at pre-determined layers irrespective of input sequence. Both
the above strategies have limitations - the former cannot be applied to handle
KV Caching necessary for speed-ups in modern framework and the latter does not
capture the variation in layer importance across tasks or more generally,
across input sequences. To address both limitations, we propose FIRST, an
algorithm that reduces inference latency by using layer-specific routers to
select a subset of transformer layers adaptively for each input sequence - the
prompt (during prefill stage) decides which layers will be skipped during
decoding. FIRST preserves compatibility with KV caching enabling faster
inference while being quality-aware. FIRST is model-agnostic and can be easily
enabled on any pre-trained LLM. We further improve performance by incorporating
LoRA adapters for fine-tuning on external datasets, enhancing task-specific
accuracy while maintaining latency benefits. Our approach reveals that input
adaptivity is critical - indeed, different task-specific middle layers play a
crucial role in evolving hidden representations depending on task. Extensive
experiments show that FIRST significantly reduces latency while retaining
competitive performance (as compared to baselines), making our approach an
efficient solution for LLM deployment in low-resource environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures, Submitted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Fairness in Natural Language Processing: From Traditional
  Methods to Explainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanny Jourdan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The burgeoning field of Natural Language Processing (NLP) stands at a
critical juncture where the integration of fairness within its frameworks has
become an imperative. This PhD thesis addresses the need for equity and
transparency in NLP systems, recognizing that fairness in NLP is not merely a
technical challenge but a moral and ethical necessity, requiring a rigorous
examination of how these technologies interact with and impact diverse human
populations. Through this lens, this thesis undertakes a thorough investigation
into the development of equitable NLP methodologies and the evaluation of
biases that prevail in current systems.
  First, it introduces an innovative algorithm to mitigate biases in
multi-class classifiers, tailored for high-risk NLP applications, surpassing
traditional methods in both bias mitigation and prediction accuracy. Then, an
analysis of the Bios dataset reveals the impact of dataset size on
discriminatory biases and the limitations of standard fairness metrics. This
awareness has led to explorations in the field of explainable AI, aiming for a
more complete understanding of biases where traditional metrics are limited.
Consequently, the thesis presents COCKATIEL, a model-agnostic explainability
method that identifies and ranks concepts in Transformer models, outperforming
previous approaches in sentiment analysis tasks. Finally, the thesis
contributes to bridging the gap between fairness and explainability by
introducing TaCo, a novel method to neutralize bias in Transformer model
embeddings.
  In conclusion, this thesis constitutes a significant interdisciplinary
endeavor that intertwines explicability and fairness to challenge and reshape
current NLP paradigms. The methodologies and critiques presented contribute to
the ongoing discourse on fairness in machine learning, offering actionable
solutions for more equitable and responsible AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD Thesis, Toulouse University</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ With a Grain of SALT: Are LLMs Fair Across Social Dimensions? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samee Arif, Zohaib Khan, Agha Ali Raza, Awais Athar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an analysis of biases in open-source Large Language
Models (LLMs) across various genders, religions, and races. We introduce a
methodology for generating a bias detection dataset using seven bias triggers:
General Debate, Positioned Debate, Career Advice, Story Generation,
Problem-Solving, Cover-Letter Writing, and CV Generation. We use GPT-4o to
generate a diverse set of prompts for each trigger across various genders,
religious and racial groups. We evaluate models from Llama and Gemma family on
the generated dataset. We anonymise the LLM-generated text associated with each
group using GPT-4o-mini and do a pairwise comparison using GPT-4o-as-a-Judge.
To quantify bias in the LLM-generated text we use the number of wins and losses
in the pairwise comparison. Our analysis spans three languages, English,
German, and Arabic to explore how language influences bias manifestation. Our
findings reveal that LLMs exhibit strong polarization toward certain groups
across each category, with a notable consistency observed across models.
However, when switching languages, variations and anomalies emerge, often
attributable to cultural cues and contextual differences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-end Planner Training for Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Cornille, Florian Mai, Jingyuan Sun, Marie-Francine Moens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Through end-to-end training to predict the next token, LLMs have become
valuable tools for various tasks. Enhancing their core training in language
modeling can improve numerous downstream applications. A successful approach to
enhance language modeling uses a separate planning module to predict abstract
labels of future sentences and conditions the LM on these predictions. However,
this method is non-differentiable, preventing joint end-to-end tuning of the
planner with the LM. We propose an effective method to improve this approach by
enabling joint fine-tuning of the planner and the LM. We show that a naive way
of approximating the gradient of selecting a label via the straight-through
estimator is not effective. Instead, we propose to use the predicted label
probabilities as mixing weights to condition the LM on a weighted average of
label embeddings in a differentiable manner. This not only enables joint
fine-tuning of the planner and the LM, but also allows the LM to draw on the
full label distribution predicted by the planner, retaining more information.
Our experimental results show consistent improvements in perplexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Insights from the Inverse: Reconstructing LLM Training Goals Through
  Inverse RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jared Joselowitz, Arjun Jagota, Satyapriya Krishna, Sonali Parbhoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) trained with Reinforcement Learning from Human
Feedback (RLHF) have demonstrated remarkable capabilities, but their underlying
reward functions and decision-making processes remain opaque. This paper
introduces a novel approach to interpreting LLMs by applying inverse
reinforcement learning (IRL) to recover their implicit reward functions. We
conduct experiments on toxicity-aligned LLMs of varying sizes, extracting
reward models that achieve up to 80.40% accuracy in predicting human
preferences. Our analysis reveals key insights into the non-identifiability of
reward functions, the relationship between model size and interpretability, and
potential pitfalls in the RLHF process. We demonstrate that IRL-derived reward
models can be used to fine-tune new LLMs, resulting in comparable or improved
performance on toxicity benchmarks. This work provides a new lens for
understanding and improving LLM alignment, with implications for the
responsible development and deployment of these powerful systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KcMF: A Knowledge-compliant Framework for Schema and Entity Matching
  with <span class="highlight-title">Fine-tuning</span>-free LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqin Xu, Huan Li, Ke Chen, Lidan Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Schema and entity matching tasks are crucial for data integration and
management. While large language models (LLMs) have shown promising results in
these tasks, they suffer from hallucinations and confusion about task
instructions. In this paper, we present the Knowledge-Compliant Matching
Framework (KcMF), an LLM-based approach that addresses these issues without the
need for domain-specific fine-tuning. KcMF employs a pseudo-code-based task
decomposition strategy to adopt task-specific natural language statements that
guide LLM reasoning and reduce confusion. We also propose two mechanisms,
Dataset as Knowledge (DaK) and Example as Knowledge (EaK), to build domain
knowledge sets when unstructured domain knowledge is lacking. Additionally, we
introduce a result-ensembling strategy to leverage multiple knowledge sources
and suppress poorly formatted outputs. Comprehensive evaluations on schema and
entity matching tasks demonstrate that KcMF outperforms previous non-LLM
state-of-the-art (SOTA) methods by an average F1 score of 22.9% and competes
effectively with SOTA fine-tuned LLMs. Moreover, KcMF generalizes well across
different LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MlingConf: A Comprehensive Study of Multilingual Confidence Estimation
  on Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyang Xue, Hongru Wang, Rui Wang, Sheng Wang, Zezhong Wang, Yiming Du, Bin Liang, Kam-Fai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tendency of Large Language Models (LLMs) to generate hallucinations
raises concerns regarding their reliability. Therefore, confidence estimations
indicating the extent of trustworthiness of the generations become essential.
However, current LLM confidence estimations in languages other than English
remain underexplored. This paper addresses this gap by introducing a
comprehensive investigation of Multilingual Confidence estimation (MlingConf)
on LLMs, focusing on both language-agnostic (LA) and language-specific (LS)
tasks to explore the performance and language dominance effects of multilingual
confidence estimations on different tasks. The benchmark comprises four
meticulously checked and human-evaluate high-quality multilingual datasets for
LA tasks and one for the LS task tailored to specific social, cultural, and
geographical contexts of a language. Our experiments reveal that on LA tasks
English exhibits notable linguistic dominance in confidence estimations than
other languages, while on LS tasks, using question-related language to prompt
LLMs demonstrates better linguistic dominance in multilingual confidence
estimations. The phenomena inspire a simple yet effective native-tone prompting
strategy by employing language-specific prompts for LS tasks, effectively
improving LLMs' reliability and accuracy on LS tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zerui Xu, Fang Wu, Tianfan Fu, Yue Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) exhibits promise in the clinical domain. However, it is
constrained by data scarcity and ethical considerations, as the generation of
clinical trials presents significant challenges due to stringent privacy
regulations, high costs, and the extended duration required for conducting
studies with human participants. Despite the advancements of large language
models (LLMs) in general generation tasks, their potential in facilitating the
generation of synthetic clinical trials is under-explored. To address this gap,
we introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs
to generate artificial yet realistic and diverse clinical trials with binary
success/failure labels. Experiments conducted on real clinical trials from the
\url{ClinicalTrials.gov} database demonstrate that our synthetic data can
effectively augment real datasets. Furthermore, by fine-tuning a pre-trained
model as a binary classifier on synthetic clinical trial datasets, we
demonstrate that this augmentation enhances model training for downstream tasks
such as trial outcome prediction. Our findings suggest that LLMs for synthetic
clinical trial generation hold promise for accelerating clinical research and
upholding ethical standards for patient privacy. The code is publicly available
at
https://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Predict Usage Options of Product Reviews with LLM-Generated
  Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leo Kohlenberg, Leonard Horns, Frederic Sadrieh, Nils Kiele, Matthis Clausen, Konstantin Ketterer, Avetis Navasardyan, Tamara Czinczoll, Gerard de Melo, Ralf Herbrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annotating large datasets can be challenging. However, crowd-sourcing is
often expensive and can lack quality, especially for non-trivial tasks. We
propose a method of using LLMs as few-shot learners for annotating data in a
complex natural language task where we learn a standalone model to predict
usage options for products from customer reviews. We also propose a new
evaluation metric for this scenario, HAMS4, that can be used to compare a set
of strings with multiple reference sets. Learning a custom model offers
individual control over energy efficiency and privacy measures compared to
using the LLM directly for the sequence-to-sequence task. We compare this data
annotation approach with other traditional methods and demonstrate how LLMs can
enable considerable cost savings. We find that the quality of the resulting
data exceeds the level attained by third-party vendor services and that
GPT-4-generated labels even reach the level of domain experts. We make the code
and generated labels publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Language Gaps in Large Language Models with Inference-Time
  Cross-Lingual Intervention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable capabilities in natural
language processing but exhibit significant performance gaps among different
languages. Most existing approaches to address these disparities rely on
pretraining or fine-tuning, which are resource-intensive. To overcome these
limitations without incurring significant costs, we propose Inference-Time
Cross-Lingual Intervention (INCLINE), a novel framework that enhances LLM
performance on low-performing (source) languages by aligning their internal
representations with those of high-performing (target) languages during
inference. INCLINE initially learns alignment matrices using parallel sentences
from source and target languages through a Least-Squares optimization, and then
applies these matrices during inference to transform the low-performing
language representations toward the high-performing language space. Extensive
experiments on nine benchmarks with five LLMs demonstrate that INCLINE
significantly improves performance across diverse tasks and languages, compared
to recent strong baselines. Our analysis demonstrates that INCLINE is highly
cost-effective and applicable to a wide range of applications. In addition, we
release the code to foster research along this line:
https://github.com/weixuan-wang123/INCLINE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Best of Both Worlds: Bridging Quality and Diversity in Data
  Selection with Bipartite Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghao Wu, Thuy-Trang Vu, Lizhen Qu, Gholamreza Haffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of large language models (LLMs) in natural language
processing (NLP) tasks is significantly influenced by the quality and diversity
of data used for supervised fine-tuning (SFT). Current data selection methods
often focus solely on quality or diversity, leading to underperforming models
due to suboptimal training data. In this paper, we introduce GraphFilter, a
novel method that represents the dataset as a bipartite graph, linking
sentences to their constituent n-grams. This representation effectively
captures the relationships between sentences and linguistic patterns,
facilitating the selection of sentences that enhance n-gram diversity. To
balance quality and diversity during selection, we propose a priority function
that combines the quality metric with the diversity metric in a multiplicative
manner. GraphFilter iteratively selects high-priority sentences, updates the
bipartite graph by removing covered n-grams, and re-calculates priorities to
reflect the evolving data landscape. We conduct extensive experiments using
three model backbones across six widely used benchmarks. The results
demonstrate that GraphFilter outperforms all nine baseline approaches,
achieving superior model performance and computational efficiency. Our analyses
validate the effectiveness of our design choices, examine the subsets selected
by GraphFilter and other methods, highlight the importance of instruction
diversity, and explore the role of quality and diversity in relation to subset
sizes. GraphFilter establishes a new foundation for effective data selection
strategies, encouraging further research in data selection for LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 5 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open Ko-LLM Leaderboard2: Bridging Foundational and Practical Evaluation
  for Korean LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeonwoo Kim, Dahyun Kim, Jihoo Kim, Sukyung Lee, Yungi Kim, Chanjun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Open Ko-LLM Leaderboard has been instrumental in benchmarking Korean
Large Language Models (LLMs), yet it has certain limitations. Notably, the
disconnect between quantitative improvements on the overly academic leaderboard
benchmarks and the qualitative impact of the models should be addressed.
Furthermore, the benchmark suite is largely composed of translated versions of
their English counterparts, which may not fully capture the intricacies of the
Korean language. To address these issues, we propose Open Ko-LLM Leaderboard2,
an improved version of the earlier Open Ko-LLM Leaderboard. The original
benchmarks are entirely replaced with new tasks that are more closely aligned
with real-world capabilities. Additionally, four new native Korean benchmarks
are introduced to better reflect the distinct characteristics of the Korean
language. Through these refinements, Open Ko-LLM Leaderboard2 seeks to provide
a more meaningful evaluation for advancing Korean LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expanding Chatbot Knowledge in Customer Service: Context-Aware Similar
  Question Generation Using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengze Hong, Yuanfeng Song, Di Jiang, Lu Wang, Zichang Guo, Chen Jason Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable responses of service chatbots are often achieved by employing
retrieval-based methods that restrict answers to a knowledge base comprising
predefined question-answer pairs (QA pairs). To accommodate potential
variations in how a customer's query may be expressed, it emerges as the
favored solution to augment these QA pairs with similar questions that are
possibly diverse while remaining semantic consistency. This augmentation task
is known as Similar Question Generation (SQG). Traditional methods that heavily
rely on human efforts or rule-based techniques suffer from limited diversity or
significant semantic deviation from the source question, only capable of
producing a finite number of useful questions.
  To address these limitations, we propose an SQG approach based on Large
Language Models (LLMs), capable of producing a substantial number of diverse
questions while maintaining semantic consistency to the source QA pair. This is
achieved by leveraging LLMs' natural language understanding capability through
fine-tuning with specially designed prompts. The experiments conducted on a
real customer-service dataset demonstrate that our method surpasses baseline
methods by a significant margin in terms of semantic diversity. Human
evaluation further confirms that integrating the answer that reflects the
customer's intention is crucial for increasing the number of generated
questions that meet business requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformity in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochen Zhu, Caiqi Zhang, Tom Stafford, Nigel Collier, Andreas Vlachos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conformity effect describes the tendency of individuals to align their
responses with the majority. Studying this bias in large language models (LLMs)
is crucial, as LLMs are increasingly used in various information-seeking and
decision-making tasks as conversation partners to improve productivity. Thus,
conformity to incorrect responses can compromise their effectiveness. In this
paper, we adapt psychological experiments to examine the extent of conformity
in state-of-the-art LLMs. Our findings reveal that all models tested exhibit
varying levels of conformity toward the majority, regardless of their initial
choice or correctness, across different knowledge domains. Notably, we are the
first to show that LLMs are more likely to conform when they are more uncertain
in their own prediction. We further explore factors that influence conformity,
such as training paradigms and input characteristics, finding that
instruction-tuned models are less susceptible to conformity, while increasing
the naturalness of majority tones amplifies conformity. Finally, we propose two
interventions--Devil's Advocate and Question Distillation--to mitigate
conformity, providing insights into building more robust language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages (8 pages main body), 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretical Analysis of Hierarchical Language Recognition and Generation
  by Transformers without Positional Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daichi Hayakawa, Issei Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we provide constructive proof that Transformers can recognize
and generate hierarchical language efficiently with respect to model size, even
without the need for a specific positional encoding. Specifically, we show that
causal masking and a starting token enable Transformers to compute positional
information and depth within hierarchical structures. We demonstrate that
Transformers without positional encoding can generate hierarchical languages.
Furthermore, we suggest that explicit positional encoding might have a
detrimental effect on generalization with respect to sequence length.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revealing the Barriers of Language Agents in Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Xie, Kexun Zhang, Jiangjie Chen, Siyu Yuan, Kai Zhang, Yikai Zhang, Lei Li, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous planning has been an ongoing pursuit since the inception of
artificial intelligence. Based on curated problem solvers, early planning
agents could deliver precise solutions for specific tasks but lacked
generalization. The emergence of large language models (LLMs) and their
powerful reasoning capabilities has reignited interest in autonomous planning
by automatically generating reasonable solutions for given tasks. However,
prior research and our experiments show that current language agents still lack
human-level planning abilities. Even the state-of-the-art reasoning model,
OpenAI o1, achieves only 15.6% on one of the complex real-world planning
benchmarks. This highlights a critical question: What hinders language agents
from achieving human-level planning? Although existing studies have highlighted
weak performance in agent planning, the deeper underlying issues and the
mechanisms and limitations of the strategies proposed to address them remain
insufficiently understood. In this work, we apply the feature attribution study
and identify two key factors that hinder agent planning: the limited role of
constraints and the diminishing influence of questions. We also find that
although current strategies help mitigate these challenges, they do not fully
resolve them, indicating that agents still have a long way to go before
reaching human-level intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Coarse-Grained Matching in Video-Text Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aozhu Chen, Hazel Doughty, Xirong Li, Cees G. M. Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-text retrieval has seen significant advancements, yet the ability of
models to discern subtle differences in captions still requires verification.
In this paper, we introduce a new approach for fine-grained evaluation. Our
approach can be applied to existing datasets by automatically generating hard
negative test captions with subtle single-word variations across nouns, verbs,
adjectives, adverbs, and prepositions. We perform comprehensive experiments
using four state-of-the-art models across two standard benchmarks (MSR-VTT and
VATEX) and two specially curated datasets enriched with detailed descriptions
(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our
analyses show that the current evaluation benchmarks fall short in detecting a
model's ability to perceive subtle single-word differences, 2) our fine-grained
evaluation highlights the difficulty models face in distinguishing such subtle
variations. To enhance fine-grained understanding, we propose a new baseline
that can be easily combined with current methods. Experiments on our
fine-grained evaluations demonstrate that this approach enhances a model's
ability to understand fine-grained differences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nominal Class Assignment in Swahili: A Computational Account 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giada Palmieri, Konstantinos Kogkalidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We discuss the open question of the relation between semantics and nominal
class assignment in Swahili. We approach the problem from a computational
perspective, aiming first to quantify the extent of this relation, and then to
explicate its nature, taking extra care to suppress morphosyntactic confounds.
Our results are the first of their kind, providing a quantitative evaluation of
the semantic cohesion of each nominal class, as well as a nuanced taxonomic
description of its semantic content.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tenth Italian Conference on Computational Linguistics (CliC-it-2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProSA: Assessing and Understanding the <span class="highlight-title">Prompt</span> Sensitivity of LLMs <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingming Zhuo, Songyang Zhang, Xinyu Fang, Haodong Duan, Dahua Lin, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive capabilities across
various tasks, but their performance is highly sensitive to the prompts
utilized. This variability poses challenges for accurate assessment and user
satisfaction. Current research frequently overlooks instance-level prompt
variations and their implications on subjective evaluations. To address these
shortcomings, we introduce ProSA, a framework designed to evaluate and
comprehend prompt sensitivity in LLMs. ProSA incorporates a novel sensitivity
metric, PromptSensiScore, and leverages decoding confidence to elucidate
underlying mechanisms. Our extensive study, spanning multiple tasks, uncovers
that prompt sensitivity fluctuates across datasets and models, with larger
models exhibiting enhanced robustness. We observe that few-shot examples can
alleviate this sensitivity issue, and subjective evaluations are also
susceptible to prompt sensitivities, particularly in complex,
reasoning-oriented tasks. Furthermore, our findings indicate that higher model
confidence correlates with increased prompt robustness. We believe this work
will serve as a helpful tool in studying prompt sensitivity of LLMs. The
project is released at: https://github.com/open-compass/ProSA .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024, Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tracking Universal Features Through <span class="highlight-title">Fine-Tuning</span> and Model Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niels Horn, Desmond Elliott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study how features emerge, disappear, and persist across models fine-tuned
on different domains of text. More specifically, we start from a base one-layer
Transformer language model that is trained on a combination of the BabyLM
corpus, and a collection of Python code from The Stack. This base model is
adapted to two new domains of text: TinyStories, and the Lua programming
language, respectively; and then these two models are merged using these two
models using spherical linear interpolation. Our exploration aims to provide
deeper insights into the stability and transformation of features across
typical transfer-learning scenarios using small-scale models and sparse
auto-encoders.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span> Compression for Large Language Models: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongqian Li, Yinhong Liu, Yixuan Su, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging large language models (LLMs) for complex natural language tasks
typically requires long-form prompts to convey detailed requirements and
information, which results in increased memory usage and inference costs. To
mitigate these challenges, multiple efficient methods have been proposed, with
prompt compression gaining significant research interest. This survey provides
an overview of prompt compression techniques, categorized into hard prompt
methods and soft prompt methods. First, the technical approaches of these
methods are compared, followed by an exploration of various ways to understand
their mechanisms, including the perspectives of attention optimization,
Parameter-Efficient Fine-Tuning (PEFT), modality fusion, and new synthetic
language. We also examine the downstream adaptations of various prompt
compression techniques. Finally, the limitations of current prompt compression
methods are analyzed, and several future directions are outlined, such as
optimizing the compression encoder, combining hard and soft prompts methods,
and leveraging insights from multimodality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Attribution Bias in Retrieval-Augmented Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Abolghasemi, Leif Azzopardi, Seyyed Hadi Hashemi, Maarten de Rijke, Suzan Verberne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attributing answers to source documents is an approach used to enhance the
verifiability of a model's output in retrieval augmented generation (RAG).
Prior work has mainly focused on improving and evaluating the attribution
quality of large language models (LLMs) in RAG, but this may come at the
expense of inducing biases in the attribution of answers. We define and examine
two aspects in the evaluation of LLMs in RAG pipelines, namely attribution
sensitivity and bias with respect to authorship information. We explicitly
inform an LLM about the authors of source documents, instruct it to attribute
its answers, and analyze (i) how sensitive the LLM's output is to the author of
source documents, and (ii) whether the LLM exhibits a bias towards
human-written or AI-generated source documents. We design an experimental setup
in which we use counterfactual evaluation to study three LLMs in terms of their
attribution sensitivity and bias in RAG pipelines. Our results show that adding
authorship information to source documents can significantly change the
attribution quality of LLMs by 3% to 18%. Moreover, we show that LLMs can have
an attribution bias towards explicit human authorship, which can serve as a
competing hypothesis for findings of prior work that shows that LLM-generated
content may be preferred over human-written contents. Our findings indicate
that metadata of source documents can influence LLMs' trust, and how they
attribute their answers. Furthermore, our research highlights attribution bias
and sensitivity as a novel aspect of brittleness in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HerO at AVeriTeC: The Herd of Open Large Language Models for Verifying
  Real-World Claims <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yejun Yoon, Jaeyoon Jung, Seunghyun Yoon, Kunwoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To tackle the AVeriTeC shared task hosted by the FEVER-24, we introduce a
system that only employs publicly available large language models (LLMs) for
each step of automated fact-checking, dubbed the Herd of Open LLMs for
verifying real-world claims (HerO). HerO employs multiple LLMs for each step of
automated fact-checking. For evidence retrieval, a language model is used to
enhance a query by generating hypothetical fact-checking documents. We prompt
pretrained and fine-tuned LLMs for question generation and veracity prediction
by crafting prompts with retrieved in-context samples. HerO achieved 2nd place
on the leaderboard with the AVeriTeC score of 0.57, suggesting the potential of
open LLMs for verifying real-world claims. For future research, we make our
code publicly available at https://github.com/ssu-humane/HerO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A system description paper for the AVeriTeC shared task, hosted by
  the seventh FEVER workshop (co-located with EMNLP 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRefLexOR: Preference-based Recursive Language Modeling for Exploratory
  Optimization of Reasoning and Agentic Thinking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus J. Buehler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PRefLexOR (Preference-based Recursive Language Modeling for Exploratory
Optimization of Reasoning) combines preference optimization with concepts from
Reinforcement Learning to enable models to self-teach through iterative
reasoning improvements. We propose a recursive learning approach that engages
the model in multi-step reasoning, revisiting, and refining intermediate steps
before producing a final output in training and inference phases. Through
multiple training stages, the model first learns to align its reasoning with
accurate decision paths by optimizing the log odds between preferred and
non-preferred responses. During this process, PRefLexOR builds a dynamic
knowledge graph by generating questions from random text chunks and
retrieval-augmentation to contextualize relevant details from the entire
training corpus. In the second stage, preference optimization enhances model
performance by using rejection sampling to fine-tune reasoning quality by
continually producing in-situ training data while masking the reasoning steps.
Recursive optimization within a thinking token framework introduces iterative
feedback loops, where the model refines reasoning, achieving deeper coherence,
consistency, and adaptability. Implemented in small language models with only 3
billion parameters, we should that even tiny models can iteratively teach
themselves to reason with greater depth and reflectivity. Our implementation is
straightforward and can be incorporated into any existing pretrained LLM. We
focus our examples on applications in biological materials science and
demonstrate the method in a variety of case studies that range from in-domain
to cross-domain applications. Using reasoning strategies that include thinking
and reflection modalities we build a multi-agent recursive self-improving
inference approach to successively improve responses via repeated sampling in
inference time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proactive Agent: Shifting LLM Agents from Reactive Responses to Active
  Assistance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaxi Lu, Shenzhi Yang, Cheng Qian, Guirong Chen, Qinyu Luo, Yesai Wu, Huadong Wang, Xin Cong, Zhong Zhang, Yankai Lin, Weiwen Liu, Yasheng Wang, Zhiyuan Liu, Fangming Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agents powered by large language models have shown remarkable abilities in
solving complex tasks. However, most agent systems remain reactive, limiting
their effectiveness in scenarios requiring foresight and autonomous
decision-making. In this paper, we tackle the challenge of developing proactive
agents capable of anticipating and initiating tasks without explicit human
instructions. We propose a novel data-driven approach for this problem.
Firstly, we collect real-world human activities to generate proactive task
predictions. These predictions are then labeled by human annotators as either
accepted or rejected. The labeled data is used to train a reward model that
simulates human judgment and serves as an automatic evaluator of the
proactiveness of LLM agents. Building on this, we develop a comprehensive data
generation pipeline to create a diverse dataset, ProactiveBench, containing
6,790 events. Finally, we demonstrate that fine-tuning models with the proposed
ProactiveBench can significantly elicit the proactiveness of LLM agents.
Experimental results show that our fine-tuned model achieves an F1-Score of
66.47% in proactively offering assistance, outperforming all open-source and
close-source models. These results highlight the potential of our method in
creating more proactive and effective agent systems, paving the way for future
advancements in human-agent collaboration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GECTurk WEB: An Explainable Online Platform for Turkish Grammatical
  Error Detection and Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Gebeşçe, Gözde Gül Şahin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sophisticated grammatical error detection/correction tools are available for
a small set of languages such as English and Chinese. However, it is not
straightforward -- if not impossible -- to adapt them to morphologically rich
languages with complex writing rules like Turkish which has more than 80
million speakers. Even though several tools exist for Turkish, they primarily
focus on spelling errors rather than grammatical errors and lack features such
as web interfaces, error explanations and feedback mechanisms. To fill this
gap, we introduce GECTurk WEB, a light, open-source, and flexible web-based
system that can detect and correct the most common forms of Turkish writing
errors, such as the misuse of diacritics, compound and foreign words, pronouns,
light verbs along with spelling mistakes. Our system provides native speakers
and second language learners an easily accessible tool to detect/correct such
mistakes and also to learn from their mistakes by showing the explanation for
the violated rule(s). The proposed system achieves 88,3 system usability score,
and is shown to help learn/remember a grammatical rule (confirmed by 80% of the
participants). The GECTurk WEB is available both as an offline tool at
https://github.com/GGLAB-KU/gecturkweb or online at www.gecturk.net.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A linguistic analysis of undesirable outcomes in the era of generative
  AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele Gambetta, Gizem Gezici, Fosca Giannotti, Dino Pedreschi, Alistair Knott, Luca Pappalardo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has focused on the medium and long-term impacts of generative
AI, posing scientific and societal challenges mainly due to the detection and
reliability of machine-generated information, which is projected to form the
major content on the Web soon. Prior studies show that LLMs exhibit a lower
performance in generation tasks (model collapse) as they undergo a fine-tuning
process across multiple generations on their own generated content
(self-consuming loop). In this paper, we present a comprehensive simulation
framework built upon the chat version of LLama2, focusing particularly on the
linguistic aspects of the generated content, which has not been fully examined
in existing studies. Our results show that the model produces less lexical rich
content across generations, reducing diversity. The lexical richness has been
measured using the linguistic measures of entropy and TTR as well as
calculating the POSTags frequency. The generated content has also been examined
with an $n$-gram analysis, which takes into account the word order, and
semantic networks, which consider the relation between different words. These
findings suggest that the model collapse occurs not only by decreasing the
content diversity but also by distorting the underlying linguistic patterns of
the generated text, which both highlight the critical importance of carefully
choosing and curating the initial input text, which can alleviate the model
collapse problem. Furthermore, we conduct a qualitative analysis of the
fine-tuned models of the pipeline to compare their performances on generic NLP
tasks to the original model. We find that autophagy transforms the initial
model into a more creative, doubtful and confused one, which might provide
inaccurate answers and include conspiracy theories in the model responses,
spreading false and biased information on the Web.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Role of LLMs in <span class="highlight-title">Multimodal</span> Evaluation Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Botian Jiang, Lei Li, Xiaonan Li, Zhaowei Li, Xiachong Feng, Lingpeng Kong, Qi Liu, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Multimodal Large Language Models (MLLMs) has been
accompanied by the development of various benchmarks to evaluate their
capabilities. However, the true nature of these evaluations and the extent to
which they assess multimodal reasoning versus merely leveraging the underlying
Large Language Model (LLM) backbone remain unclear. This paper presents a
comprehensive investigation into the role of LLM backbones in MLLM evaluation,
focusing on two critical aspects: the degree to which current benchmarks truly
assess multimodal reasoning and the influence of LLM prior knowledge on
performance. Specifically, we introduce a modified evaluation protocol to
disentangle the contributions of the LLM backbone from multimodal integration,
and an automatic knowledge identification technique for diagnosing whether LLMs
equip the necessary knowledge for corresponding multimodal questions. Our study
encompasses four diverse MLLM benchmarks and eight state-of-the-art MLLMs. Key
findings reveal that some benchmarks allow high performance even without visual
inputs and up to 50\% of error rates can be attributed to insufficient world
knowledge in the LLM backbone, indicating a heavy reliance on language
capabilities. To address knowledge deficiencies, we propose a knowledge
augmentation pipeline that achieves significant performance gains, with
improvements of up to 60\% on certain datasets, resulting in a approximately 4x
increase in performance. Our work provides crucial insights into the role of
the LLM backbone in MLLMs, and highlights the need for more nuanced
benchmarking approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuron-based Personality Trait Induction in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Deng, Tianyi Tang, Yanbin Yin, Wenhao Yang, Wayne Xin Zhao, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have become increasingly proficient at
simulating various personality traits, an important capability for supporting
related applications (e.g., role-playing). To further improve this capacity, in
this paper, we present a neuron-based approach for personality trait induction
in LLMs, with three major technical contributions. First, we construct
PersonalityBench, a large-scale dataset for identifying and evaluating
personality traits in LLMs. This dataset is grounded in the Big Five
personality traits from psychology and is designed to assess the generative
capabilities of LLMs towards specific personality traits. Second, by leveraging
PersonalityBench, we propose an efficient method for identifying
personality-related neurons within LLMs by examining the opposite aspects of a
given trait. Third, we develop a simple yet effective induction method that
manipulates the values of these identified personality-related neurons. This
method enables fine-grained control over the traits exhibited by LLMs without
training and modifying model parameters. Extensive experiments validate the
efficacy of our neuron identification and trait induction methods. Notably, our
approach achieves comparable performance as fine-tuned models, offering a more
efficient and flexible solution for personality trait induction in LLMs. We
provide access to all the mentioned resources at
https://github.com/RUCAIBox/NPTI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Low-Resource Language Model Training: Comprehensive Analysis
  of Multi-Epoch, Multi-Lingual, and Two-Stage Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kosuke Akimoto, Masafumi Oyamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the challenge of optimizing training setups for
Large Language Models (LLMs) of low-resource language with a limited amount of
corpus. Existing works adopt multi-epoch, multi-lingual, and two-stage training
to utilize the limited target language corpus efficiently. However, there is
still a lack of understanding about the optimal hyperparameter setups for
combining these three approaches to train LLMs. We exhaustively explore
training setups for low-resource language LLM, combining these three
approaches, and found the following insights for efficiently reducing the cost
of hyperparameter search: (1) As the amount of target language corpus
decreases, the optimal training approach shifts from monolingual single-stage
training to multi-lingual two-stage training at a compute budget dependent
threshold. (2) The optimal model scale remains stable regardless of the amount
of target language corpus, allowing the use of the compute-optimal scale of
monolingual training. (3) The optimal number of epochs can be extrapolated from
smaller-scale experiments to larger scale using our proposed model. Also, we
provide evidence that, in single-stage training, the target language validation
loss follows a power law with respect to the target language ratio, with an
exponent independent of the amount of data, model scale, and language pair.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reversal of Thought: Enhancing Large Language Models with
  Preference-Guided Reverse Reasoning Warm-up 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Yuan, Dehui Du, Hao Zhang, Zixiang Di, Usman Naseem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable performance in reasoning
tasks but face limitations in mathematical and complex logical reasoning.
Existing methods to improve LLMs' logical capabilities either involve traceable
or verifiable logical sequences that generate more reliable responses by
constructing logical structures yet increase computational costs, or introduces
rigid logic template rules, reducing flexibility. In this paper, we propose
Reversal of Thought (RoT), a novel framework aimed at enhancing the logical
reasoning abilities of LLMs. RoT utilizes a Preference-Guided Reverse Reasoning
warm-up strategy, which integrates logical symbols for pseudocode planning
through meta-cognitive mechanisms and pairwise preference self-evaluation to
generate task-specific prompts solely through demonstrations, aligning with
LLMs' cognitive preferences shaped by Reinforcement Learning with Human
Feedback (RLHF). Through reverse reasoning, we ultilize a Cognitive Preference
Manager to assess knowledge boundaries and further expand LLMs' reasoning
capabilities by aggregating solution logic for known tasks and stylistic
templates for unknown tasks. Experiments across various tasks demonstrate that
RoT surpasses existing baselines in both reasoning accuracy and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open Domain Question Answering with Conflicting Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyi Liu, Qiang Ning, Kishaloy Halder, Wei Xiao, Zheng Qi, Phu Mon Htut, Yi Zhang, Neha Anna John, Bonan Min, Yassine Benajiba, Dan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open domain question answering systems frequently rely on information
retrieved from large collections of text (such as the Web) to answer questions.
However, such collections of text often contain conflicting information, and
indiscriminately depending on this information may result in untruthful and
inaccurate answers. To understand the gravity of this problem, we collect a
human-annotated dataset, Question Answering with Conflicting Contexts (QACC),
and find that as much as 25% of unambiguous, open domain questions can lead to
conflicting contexts when retrieved using Google Search. We evaluate and
benchmark three powerful Large Language Models (LLMs) with our dataset QACC and
demonstrate their limitations in effectively addressing questions with
conflicting information. To explore how humans reason through conflicting
contexts, we request our annotators to provide explanations for their
selections of correct answers. We demonstrate that by finetuning LLMs to
explain their answers, we can introduce richer information into their training
that guide them through the process of reasoning with conflicting contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantics-Adaptive Activation Intervention for LLMs via Dynamic Steering
  Vectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixuan Wang, Jingyuan Yang, Wei Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved remarkable performance across many
tasks, yet aligning them with desired behaviors remains challenging. Activation
intervention has emerged as an effective and economical method to modify the
behavior of LLMs. Despite considerable interest in this area, current
intervention methods exclusively employ a fixed steering vector to modify model
activations, lacking adaptability to diverse input semantics. To address this
limitation, we propose Semantics-Adaptive Dynamic Intervention (SADI), a novel
method that constructs a dynamic steering vector to intervene model activations
at inference time. More specifically, SADI utilizes activation differences in
contrastive pairs to precisely identify critical elements of an LLM (i.e.,
attention heads, hidden states, and neurons) for targeted intervention. During
inference, SADI dynamically steers model behavior by scaling element-wise
activations based on the directions of input semantics. Experimental results
show that SADI outperforms established baselines by substantial margins,
improving task performance without training. SADI's cost-effectiveness and
generalizability across various LLM backbones and tasks highlight its potential
as a versatile alignment technique. In addition, we release the code to foster
research along this line:https://github.com/weixuan-wang123/SADI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pyramid-Driven Alignment: Pyramid Principle Guided Integration of Large
  Language Models and Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Sun, Xinchen Wang, Youdi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) possess impressive reasoning abilities but are
prone to generating incorrect information, often referred to as hallucinations.
While incorporating external Knowledge Graphs (KGs) can partially mitigate this
issue, existing methods primarily treat KGs as static knowledge repositories,
overlooking the critical disparity between KG and LLM knowledge, and failing to
fully exploit the reasoning capabilities inherent in KGs. To address these
limitations, we propose Pyramid-Driven Alignment (PDA), a novel framework for
seamlessly integrating LLMs with KGs. PDA utilizes Pyramid Principle analysis
to construct a hierarchical pyramid structure. This structure is designed to
reflect the input question and generate more validated deductive knowledge,
thereby enhancing the alignment of LLMs and KGs and ensuring more cohesive
integration. Furthermore, PDA employs a recursive mechanism to harness the
underlying reasoning abilities of KGs, resulting in more accurate knowledge
retrieval for question-answering tasks. Our experimental results reveal a
substantial performance advantage of PDA over state-of-the-art baselines, with
improvements reaching 26.70% and 26.78%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards LLM-based Cognitive Models of Students with Misconceptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Sonkar, Xinghe Chen, Naiming Liu, Richard G. Baraniuk, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately modeling student cognition is crucial for developing effective
AI-driven educational technologies. A key challenge is creating realistic
student models that satisfy two essential properties: (1) accurately
replicating specific misconceptions, and (2) correctly solving problems where
these misconceptions are not applicable. This dual requirement reflects the
complex nature of student understanding, where misconceptions coexist with
correct knowledge. This paper investigates whether Large Language Models (LLMs)
can be instruction-tuned to meet this dual requirement and effectively simulate
student thinking in algebra. We introduce MalAlgoPy, a novel Python library
that generates datasets reflecting authentic student solution patterns through
a graph-based representation of algebraic problem-solving. Utilizing MalAlgoPy,
we define and examine Cognitive Student Models (CSMs) - LLMs instruction tuned
to faithfully emulate realistic student behavior. Our findings reveal that LLMs
trained on misconception examples can efficiently learn to replicate errors.
However, the training diminishes the model's ability to solve problems
correctly, particularly for problem types where the misconceptions are not
applicable, thus failing to satisfy second property of CSMs. We demonstrate
that by carefully calibrating the ratio of correct to misconception examples in
the training data - sometimes as low as 0.25 - it is possible to develop CSMs
that satisfy both properties. Our insights enhance our understanding of
AI-based student models and pave the way for effective adaptive learning
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How much do contextualized representations encode long-range context? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simeng Sun, Cheng-Ping Hsieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze contextual representations in neural autoregressive language
models, emphasizing long-range contexts that span several thousand tokens. Our
methodology employs a perturbation setup and the metric
\emph{Anisotropy-Calibrated Cosine Similarity}, to capture the degree of
contextualization of long-range patterns from the perspective of representation
geometry. We begin the analysis with a case study on standard decoder-only
Transformers, demonstrating that similar perplexity can exhibit markedly
different downstream task performance, which can be explained by the difference
in contextualization of long-range content. Next, we extend the analysis to
other models, covering recent novel architectural designs and various training
configurations. The representation-level results illustrate a reduced capacity
for high-complexity (i.e., less compressible) sequences across architectures,
and that fully recurrent models rely heavily on local context, whereas hybrid
models more effectively encode the entire sequence structure. Finally,
preliminary analysis of model size and training configurations on the encoding
of long-range context suggest potential directions for improving existing
language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Prompt</span>-Based Knowledge Graph Foundation Model for Universal In-Context
  Reasoning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanning Cui, Zequn Sun, Wei Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extensive knowledge graphs (KGs) have been constructed to facilitate
knowledge-driven tasks across various scenarios. However, existing work usually
develops separate reasoning models for different KGs, lacking the ability to
generalize and transfer knowledge across diverse KGs and reasoning settings. In
this paper, we propose a prompt-based KG foundation model via in-context
learning, namely KG-ICL, to achieve a universal reasoning ability.
Specifically, we introduce a prompt graph centered with a query-related example
fact as context to understand the query relation. To encode prompt graphs with
the generalization ability to unseen entities and relations in queries, we
first propose a unified tokenizer that maps entities and relations in prompt
graphs to predefined tokens. Then, we propose two message passing neural
networks to perform prompt encoding and KG reasoning, respectively. We conduct
evaluation on 43 different KGs in both transductive and inductive settings.
Results indicate that the proposed KG-ICL outperforms baselines on most
datasets, showcasing its outstanding generalization and universal reasoning
capabilities. The source code is accessible on GitHub:
https://github.com/nju-websoft/KG-ICL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical
  Decision-Support Setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Kayser, Bayar Menzat, Cornelius Emde, Bogdan Bercean, Alex Novak, Abdala Espinosa, Bartlomiej W. Papiez, Susanne Gaube, Thomas Lukasiewicz, Oana-Maria Camburu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing capabilities of AI models are leading to their wider use,
including in safety-critical domains. Explainable AI (XAI) aims to make these
models safer to use by making their inference process more transparent.
However, current explainability methods are seldom evaluated in the way they
are intended to be used: by real-world end users. To address this, we conducted
a large-scale user study with 85 healthcare practitioners in the context of
human-AI collaborative chest X-ray analysis. We evaluated three types of
explanations: visual explanations (saliency maps), natural language
explanations, and a combination of both modalities. We specifically examined
how different explanation types influence users depending on whether the AI
advice and explanations are factually correct. We find that text-based
explanations lead to significant over-reliance, which is alleviated by
combining them with saliency maps. We also observe that the quality of
explanations, that is, how much factually correct information they entail, and
how much this aligns with AI correctness, significantly impacts the usefulness
of the different explanation types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Oversmoothing: Evaluating DDPM and MSE for Scalable Speech
  Synthesis in ASR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Minixhofer, Ondrej Klejch, Peter Bell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetically generated speech has rapidly approached human levels of
naturalness. However, the paradox remains that ASR systems, when trained on TTS
output that is judged as natural by humans, continue to perform badly on real
speech. In this work, we explore whether this phenomenon is due to the
oversmoothing behaviour of models commonly used in TTS, with a particular focus
on the behaviour of TTS-for-ASR as the amount of TTS training data is scaled
up. We systematically compare Denoising Diffusion Probabilistic Models (DDPM)
to Mean Squared Error (MSE) based models for TTS, when used for ASR model
training. We test the scalability of the two approaches, varying both the
number hours, and the number of different speakers. We find that for a given
model size, DDPM can make better use of more data, and a more diverse set of
speakers, than MSE models. We achieve the best reported ratio between real and
synthetic speech WER to date (1.46), but also find that a large gap remains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controlled Automatic Task-Specific Synthetic Data Generation for
  Hallucination Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Xie, Karan Aggarwal, Aitzaz Ahmad, Stephen Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to automatically generate non-trivial
task-specific synthetic datasets for hallucination detection. Our approach
features a two-step generation-selection pipeline, using hallucination pattern
guidance and a language style alignment during generation. Hallucination
pattern guidance leverages the most important task-specific hallucination
patterns while language style alignment aligns the style of the synthetic
dataset with benchmark text. To obtain robust supervised detectors from
synthetic datasets, we also adopt a data mixture strategy to improve
performance robustness and generalization. Our results on three datasets show
that our generated hallucination text is more closely aligned with
non-hallucinated text versus baselines, to train hallucination detectors with
better generalization. Our hallucination detectors trained on synthetic
datasets outperform in-context-learning (ICL)-based detectors by a large margin
of 32%. Our extensive experiments confirm the benefits of our approach with
cross-task and cross-generator generalization. Our data-mixture-based training
further improves the generalization and robustness of hallucination detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kallini et al. (2024) do not compare impossible languages with
  constituency-based ones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Hunter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A central goal of linguistic theory is to find a precise characterization of
the notion "possible human language", in the form of a computational device
that is capable of describing all and only the languages that can be acquired
by a typically developing human child. The success of recent large language
models (LLMs) in NLP applications arguably raises the possibility that LLMs
might be computational devices that meet this goal. This would only be the case
if, in addition to succeeding in learning human languages, LLMs struggle to
learn "impossible" human languages. Kallini et al. (2024; "Mission: Impossible
Language Models", Proc. ACL) conducted experiments aiming to test this by
training GPT-2 on a variety of synthetic languages, and found that it learns
some more successfully than others. They present these asymmetries as support
for the idea that LLMs' inductive biases align with what is regarded as
"possible" for human languages, but the most significant comparison has a
confound that makes this conclusion unwarranted. In this paper I explain the
confound and suggest some ways forward towards constructing a comparison that
appropriately tests the underlying issue.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Automatic and Cost-Efficient Peer-Review Framework for Language
  Generation Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Chen, Weihang Su, Zhumin Chu, Haitao Li, Qinyao Ai, Yiqun Liu, Min Zhang, Shaoping Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of large language models (LLMs), how to
efficiently evaluate them has become an important research question. Existing
evaluation methods often suffer from high costs, limited test formats, the need
of human references, and systematic evaluation biases. To address these
limitations, our study introduces the Auto-PRE, an automatic LLM evaluation
framework based on peer review. In contrast to previous studies that rely on
human annotations, Auto-PRE selects evaluator LLMs automatically based on their
inherent traits including consistency, self-confidence, and pertinence. We
conduct extensive experiments on three tasks: summary generation, non-factoid
question-answering, and dialogue generation. Experimental results indicate our
Auto-PRE achieves state-of-the-art performance at a lower cost. Moreover, our
study highlights the impact of prompt strategies and evaluation formats on
evaluation performance, offering guidance for method optimization in the
future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for
  Retrieval-Augmented Generation with Enhanced Data Diversity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Liu, Ruixue Ding, Linhao Zhang, Pengjun Xie, Fie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) aims to enhance large language models
(LLMs) to generate more accurate and reliable answers with the help of the
retrieved context from external knowledge sources, thereby reducing the
incidence of hallucinations. Despite the advancements, evaluating these systems
remains a crucial research area due to the following issues: (1) Limited data
diversity: The insufficient diversity of knowledge sources and query types
constrains the applicability of RAG systems; (2) Obscure problems location:
Existing evaluation methods have difficulty in locating the stage of the RAG
pipeline where problems occur; (3) Unstable retrieval evaluation: These methods
often fail to effectively assess retrieval performance, particularly when the
chunking strategy changes. To tackle these challenges, we propose a
Comprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough
evaluation across the entire RAG pipeline, including chunking, retrieval,
reranking, and generation. To effectively evaluate the first three phases, we
introduce multi-granularity keywords, including coarse-grained and fine-grained
keywords, to assess the retrieved context instead of relying on the annotation
of golden chunks. Moreover, we release a holistic benchmark dataset tailored
for diverse data scenarios covering a wide range of document formats and query
types. We demonstrate the utility of the CoFE-RAG framework by conducting
experiments to evaluate each stage of RAG systems. Our evaluation method
provides unique insights into the effectiveness of RAG systems in handling
diverse data scenarios, offering a more nuanced understanding of their
capabilities and limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EPS-MoE: Expert Pipeline Scheduler for Cost-Efficient MoE Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulei Qian, Fengcun Li, Xiangyang Ji, Xiaoyu Zhao, Jianchao Tan, Kefeng Zhang, Xunliang Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM) has revolutionized the field of artificial
intelligence, with their capabilities expanding rapidly due to advances in deep
learning and increased computational resources. The mixture-of-experts (MoE)
model has emerged as a prominent architecture in the field of LLM, better
balancing the model performance and computational efficiency. MoE architecture
allows for effective scaling and efficient parallel processing, but the GEMM
(General Matrix Multiply) of MoE and the large parameters introduce challenges
in terms of computation efficiency and communication overhead, which becomes
the throughput bottleneck during inference. Applying a single parallelism
strategy like EP, DP, PP, etc. to MoE architecture usually achieves sub-optimal
inference throughput, the straightforward combinations of existing different
parallelisms on MoE can not obtain optimal inference throughput yet. This paper
introduces EPS-MoE, a novel expert pipeline scheduler for MoE that goes beyond
the existing inference parallelism schemes. Our approach focuses on optimizing
the computation of MoE FFN (FeedForward Network) modules by dynamically
selecting the best kernel implementation of GroupGemm and DenseGemm for
different loads and adaptively overlapping these computations with
\textit{all2all} communication, leading to a substantial increase in
throughput. Our experimental results demonstrate an average 21% improvement in
prefill throughput over existing parallel inference methods. Specifically, we
validated our method on DeepSeekV2, a highly optimized model claimed to achieve
a prefill throughput of 100K tokens per second. By applying EPS-MoE, we further
accelerated it to at least 120K tokens per second.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Triple Modality Fusion: Aligning Visual, Textual, and Graph Data with
  Large Language Models for Multi-Behavior Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luyi Ma, Xiaohan Li, Zezhong Fan, Jianpeng Xu, Jason Cho, Praveen Kanumala, Kaushiki Nag, Sushant Kumar, Kannan Achan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating diverse data modalities is crucial for enhancing the performance
of personalized recommendation systems. Traditional models, which often rely on
singular data sources, lack the depth needed to accurately capture the
multifaceted nature of item features and user behaviors. This paper introduces
a novel framework for multi-behavior recommendations, leveraging the fusion of
triple-modality, which is visual, textual, and graph data through alignment
with large language models (LLMs). By incorporating visual information, we
capture contextual and aesthetic item characteristics; textual data provides
insights into user interests and item features in detail; and graph data
elucidates relationships within the item-behavior heterogeneous graphs. Our
proposed model called Triple Modality Fusion (TMF) utilizes the power of LLMs
to align and integrate these three modalities, achieving a comprehensive
representation of user behaviors. The LLM models the user's interactions
including behaviors and item features in natural languages. Initially, the LLM
is warmed up using only natural language-based prompts. We then devise the
modality fusion module based on cross-attention and self-attention mechanisms
to integrate different modalities from other models into the same embedding
space and incorporate them into an LLM. Extensive experiments demonstrate the
effectiveness of our approach in improving recommendation accuracy. Further
ablation studies validate the effectiveness of our model design and benefits of
the TMF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On A Scale From 1 to 5: Quantifying Hallucination in Faithfulness
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaonan Jing, Srinivas Billa, Danny Godbout
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucination has been a popular topic in natural language generation (NLG).
In real-world applications, unfaithful content can result in bad data quality
or loss of trust from end users. Thus, it is crucial to fact-check before
adopting NLG for production usage, which can be expensive if done manually. In
this paper, we investigate automated faithfulness evaluation in guided NLG. We
developed a rubrics template and use large language models (LLMs) to score the
generation into quantifiable scales. We compared popular LLMs as well as the
widely adopted natural language inference (NLI) models in scoring quality and
sensitivity. In addition, we developed methods to generation synthetic
unfaithful data, as well as a heuristics to quantify the percentage of
hallucination. Our results on 4 travel-domain industry dataset show that GPT-4
can provide accurate judgement and explanation on whether a source and a
generation are factually consistent. Furthermore, we found that tuning NLI
models on synthetic data can improve performance. Lastly, we present insights
on latency and cost for deploying such system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmnixR: Evaluating Omni-modality Language Models on Reasoning across
  Modalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lichang Chen, Hexiang Hu, Mingda Zhang, Yiwen Chen, Zifeng Wang, Yandong Li, Pranav Shyam, Tianyi Zhou, Heng Huang, Ming-Hsuan Yang, Boqing Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce OmnixR, an evaluation suite designed to benchmark SoTA
Omni-modality Language Models, such as GPT-4o and Gemini. Evaluating OLMs,
which integrate multiple modalities such as text, vision, and audio, presents
unique challenges. Particularly, the user message might often consist of
multiple modalities, such that OLMs have to establish holistic understanding
and reasoning across modalities to accomplish the task. Existing benchmarks are
limited to single modality or dual-modality tasks, overlooking comprehensive
multi-modal assessments of model reasoning. To address this, OmnixR offers two
evaluation variants: (1)synthetic subset: a synthetic dataset generated
automatically by translating text into multiple modalities--audio, images,
video, and hybrids (Omnify). (2)realistic subset: a real-world dataset,
manually curated and annotated by experts, for evaluating cross-modal reasoning
in natural settings. OmnixR presents a unique evaluation towards assessing OLMs
over a diverse mix of modalities, such as a question that involves video,
audio, and text, providing a rigorous cross-modal reasoning testbed unlike any
existing benchmarks. Our experiments find that all state-of-the-art OLMs
struggle with OmnixR questions that require integrating information from
multiple modalities to answer. Further analysis highlights differences in
reasoning behavior, underscoring the challenges of omni-modal AI alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accurate and Data-Efficient Toxicity Prediction when Annotators Disagree 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harbani Jaggi, Kashyap Murali, Eve Fleisig, Erdem Bıyık
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When annotators disagree, predicting the labels given by individual
annotators can capture nuances overlooked by traditional label aggregation. We
introduce three approaches to predicting individual annotator ratings on the
toxicity of text by incorporating individual annotator-specific information: a
neural collaborative filtering (NCF) approach, an in-context learning (ICL)
approach, and an intermediate embedding-based architecture. We also study the
utility of demographic information for rating prediction. NCF showed limited
utility; however, integrating annotator history, demographics, and survey
information permits both the embedding-based architecture and ICL to
substantially improve prediction accuracy, with the embedding-based
architecture outperforming the other methods. We also find that, if
demographics are predicted from survey information, using these imputed
demographics as features performs comparably to using true demographic data.
This suggests that demographics may not provide substantial information for
modeling ratings beyond what is captured in survey responses. Our findings
raise considerations about the relative utility of different types of annotator
information and provide new approaches for modeling annotators in subjective
NLP tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Negative-<span class="highlight-title">Prompt</span>-driven Alignment for Generative Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiqi Qiao, Ning Xv, Biao Liu, Xin Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have achieved remarkable capabilities, but aligning
their outputs with human values and preferences remains a significant
challenge. Existing alignment methods primarily focus on positive examples
while overlooking the importance of negative responses in guiding models away
from undesirable behaviors. For instance, the widely-used alignment datasets
reveals a scarcity of explicit negative examples that contradict human values,
hindering its ability to discourage harmful or biased outputs during training.
To address this limitation, we propose NEAT, i.e., NEgative-prompt-driven
AlignmenT, to introduce negative prompts to generate undesirable responses
alongside positive examples during the optimization process. NEAT explicitly
penalizes the model for producing harmful outputs, guiding it not only toward
desirable behaviors but also steering it away from generating undesirable,
biased responses. This dual feedback mechanism enables better alignment with
human preferences, crucial in contexts where avoiding harm is paramount.
Starting from a pre-trained language model, NEAT performs online alignment by
incorporating a ranking loss derived from an expanded preference dataset
containing both positive and negative examples. Extensive experiments validate
NEAT's effectiveness in significantly enhancing language models' alignment with
human values and preferences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BIRD: A Trustworthy Bayesian Inference Framework for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12494v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12494v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Feng, Ben Zhou, Weidong Lin, Dan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predictive models often need to work with incomplete information in
real-world tasks. Consequently, they must provide reliable probability or
confidence estimation, especially in large-scale decision making and planning
tasks. Current large language models (LLM) are insufficient for such accurate
estimations, but they can generate relevant factors that may affect the
probabilities, produce coarse-grained probabilities when the information is
more complete, and help determine which factors are relevant to specific
downstream contexts. In this paper, we make use of these capabilities of LLMs
to provide a significantly more accurate probabilistic estimation. We propose
BIRD, a novel probabilistic inference framework that aligns a Bayesian network
with LLM abductions and then estimates more accurate probabilities in a
deduction step. We show BIRD provides reliable probability estimations that are
30\% better than those provided directly by LLM baselines. These estimates can
further contribute to better and more trustworthy decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Energy and Carbon Considerations of <span class="highlight-title">Fine-Tuning</span> BERT <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10267v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10267v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaorong Wang, Clara Na, Emma Strubell, Sorelle Friedler, Sasha Luccioni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP
community, existing work quantifying energy costs and associated carbon
emissions has largely focused on language model pre-training. Although a single
pre-training run draws substantially more energy than fine-tuning, fine-tuning
is performed more frequently by many more individual actors, and thus must be
accounted for when considering the energy and carbon footprint of NLP. In order
to better characterize the role of fine-tuning in the landscape of energy and
carbon emissions in NLP, we perform a careful empirical study of the
computational costs of fine-tuning across tasks, datasets, hardware
infrastructure and measurement modalities. Our experimental results allow us to
place fine-tuning energy and carbon costs into perspective with respect to
pre-training and inference, and outline recommendations to NLP researchers and
practitioners who wish to improve their fine-tuning energy efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Findings; First two authors contributed equally; 12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ÚFAL CorPipe at CRAC 2023: Larger Context Improves Multilingual
  Coreference Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14391v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14391v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milan Straka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CorPipe, the winning entry to the CRAC 2023 Shared Task on
Multilingual Coreference Resolution. Our system is an improved version of our
earlier multilingual coreference pipeline, and it surpasses other participants
by a large margin of 4.5 percent points. CorPipe first performs mention
detection, followed by coreference linking via an antecedent-maximization
approach on the retrieved spans. Both tasks are trained jointly on all
available corpora using a shared pretrained language model. Our main
improvements comprise inputs larger than 512 subwords and changing the mention
decoding to support ensembling. The source code is available at
https://github.com/ufal/crac2023-corpipe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CRAC 2023 (the Sixth Workshop on Computational Models of
  Reference, Anaphora and Coreference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ÚFAL CorPipe at CRAC 2022: Effectivity of Multilingual Models for
  Coreference Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.07278v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.07278v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milan Straka, Jana Straková
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe the winning submission to the CRAC 2022 Shared Task on
Multilingual Coreference Resolution. Our system first solves mention detection
and then coreference linking on the retrieved spans with an
antecedent-maximization approach, and both tasks are fine-tuned jointly with
shared Transformer weights. We report results of fine-tuning a wide range of
pretrained models. The center of this contribution are fine-tuned multilingual
models. We found one large multilingual model with sufficiently large encoder
to increase performance on all datasets across the board, with the benefit not
limited only to the underrepresented languages or groups of typologically
relative languages. The source code is available at
https://github.com/ufal/crac2022-corpipe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CRAC 2022 (Fifth Workshop on Computational Models of
  Reference, Anaphora and Coreference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open-Source Conversational AI with SpeechBrain 1.0 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00463v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00463v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mirco Ravanelli, Titouan Parcollet, Adel Moumen, Sylvain de Langen, Cem Subakan, Peter Plantinga, Yingzhi Wang, Pooneh Mousavi, Luca Della Libera, Artem Ploujnikov, Francesco Paissan, Davide Borra, Salah Zaiem, Zeyu Zhao, Shucong Zhang, Georgios Karakasidis, Sung-Lin Yeh, Pierre Champion, Aku Rouhe, Rudolf Braun, Florian Mai, Juan Zuluaga-Gomez, Seyed Mahed Mousavi, Andreas Nautsch, Xuechen Liu, Sangeet Sagar, Jarod Duret, Salima Mdhaffar, Gaelle Laperriere, Mickael Rouvier, Renato De Mori, Yannick Esteve
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SpeechBrain is an open-source Conversational AI toolkit based on PyTorch,
focused particularly on speech processing tasks such as speech recognition,
speech enhancement, speaker recognition, text-to-speech, and much more. It
promotes transparency and replicability by releasing both the pre-trained
models and the complete "recipes" of code and algorithms required for training
them. This paper presents SpeechBrain 1.0, a significant milestone in the
evolution of the toolkit, which now has over 200 recipes for speech, audio, and
language processing tasks, and more than 100 models available on Hugging Face.
SpeechBrain 1.0 introduces new technologies to support diverse learning
modalities, Large Language Model (LLM) integration, and advanced decoding
strategies, along with novel models, tasks, and modalities. It also includes a
new benchmark repository, offering researchers a unified platform for
evaluating models across diverse tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Journal of Machine Learning research (JMLR), Machine
  Learning Open Source Software</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised End-to-End Task-Oriented Dialogue with LLMs: The Power of
  the Noisy Channel <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brendan King, Jeffrey Flanigan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training task-oriented dialogue systems typically requires turn-level
annotations for interacting with their APIs: e.g. a dialogue state and the
system actions taken at each step. These annotations can be costly to produce,
error-prone, and require both domain and annotation expertise. With advances in
LLMs, we hypothesize that unlabeled data and a schema definition are sufficient
for building a working task-oriented dialogue system, completely unsupervised.
We consider a novel unsupervised setting of only (1) a well-defined API schema
(2) a set of unlabeled dialogues between a user and agent. We propose an
innovative approach using expectation-maximization (EM) that infers turn-level
annotations as latent variables using a noisy channel model to build an
end-to-end dialogue agent. Evaluating our approach on the MultiWOZ benchmark,
our method more than doubles the dialogue success rate of a strong GPT-3.5
baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be presented at Empirical Methods in Natural Language Processing
  (EMNLP 2024). 18 Pages, 8 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Figurative Meaning through Explainable Visual Entailment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01474v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01474v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arkadiy Saakyan, Shreyas Kulkarni, Tuhin Chakrabarty, Smaranda Muresan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (VLMs) have demonstrated strong capabilities in
tasks requiring a fine-grained understanding of literal meaning in images and
text, such as visual question-answering or visual entailment. However, there
has been little exploration of these models' capabilities when presented with
images and captions containing figurative meaning, such as metaphors or humor.
To close this gap, we propose a new task framing the figurative meaning
understanding problem as an explainable visual entailment task, where the model
has to predict whether the image (premise) entails a caption (hypothesis) and
justify the predicted label with a textual explanation. The figurative
phenomena can be present either in the image, the caption, or both. Utilizing a
human-AI collaboration approach, we build the accompanying expert-verified
dataset V-FLUTE, containing 6,027 {image, caption, label, explanation}
instances spanning five diverse figurative phenomena: metaphors, similes,
idioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs
struggle to generalize from literal to figurative meaning, particularly when it
is present in images. Further, we identify common types of errors in VLM
reasoning via human evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ToBlend: Token-Level Blending With an Ensemble of LLMs to Attack
  AI-Generated Text Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Huang, Haewoon Kwak, Jisun An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The robustness of AI-content detection models against sophisticated
adversarial strategies, such as paraphrasing or word switching, is a rising
concern in natural language generation (NLG) applications. This study proposes
ToBlend, a novel token-level ensemble text generation method to challenge the
robustness of current AI-content detection approaches by utilizing multiple
sets of candidate generative large language models (LLMs). By randomly sampling
token(s) from candidate LLMs sets, we find ToBlend significantly drops the
performance of most mainstream AI-content detection methods. We evaluate the
text quality produced under different ToBlend settings based on annotations
from experienced human experts. We proposed a fine-tuned Llama3.1 model to
distinguish the ToBlend generated text more accurately. Our findings underscore
our proposed text generation approach's great potential in deceiving and
improving detection models. Our datasets, codes, and annotations are
open-sourced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ARR Oct-2024 Cycle</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ITINERA: Integrating Spatial Optimization with Large Language Models for
  Open-domain Urban Itinerary Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07204v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07204v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Tang, Zhaokai Wang, Ao Qu, Yihao Yan, Zhaofeng Wu, Dingyi Zhuang, Jushi Kai, Kebing Hou, Xiaotong Guo, Jinhua Zhao, Zhan Zhao, Wei Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Citywalk, a recently popular form of urban travel, requires genuine
personalization and understanding of fine-grained requests compared to
traditional itinerary planning. In this paper, we introduce the novel task of
Open-domain Urban Itinerary Planning (OUIP), which generates personalized urban
itineraries from user requests in natural language. We then present ITINERA, an
OUIP system that integrates spatial optimization with large language models to
provide customized urban itineraries based on user needs. This involves
decomposing user requests, selecting candidate points of interest (POIs),
ordering the POIs based on cluster-aware spatial optimization, and generating
the itinerary. Experiments on real-world datasets and the performance of the
deployed system demonstrate our system's capacity to deliver personalized and
spatially coherent itineraries compared to current solutions. Source codes of
ITINERA are available at https://github.com/YihongT/ITINERA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CELL your Model: Contrastive Explanations for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11785v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11785v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronny Luss, Erik Miehling, Amit Dhurandhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of black-box deep neural network classification models has sparked
the need to explain their decisions. However, in the case of generative AI,
such as large language models (LLMs), there is no class prediction to explain.
Rather, one can ask why an LLM output a particular response to a given prompt.
In this paper, we answer this question by proposing, to the best of our
knowledge, the first contrastive explanation methods requiring simply
black-box/query access. Our explanations suggest that an LLM outputs a reply to
a given prompt because if the prompt was slightly modified, the LLM would have
given a different response that is either less preferable or contradicts the
original response. The key insight is that contrastive explanations simply
require a scoring function that has meaning to the user and not necessarily a
specific real valued quantity (viz. class label). We offer two algorithms for
finding contrastive explanations: i) A myopic algorithm, which although
effective in creating contrasts, requires many model calls and ii) A budgeted
algorithm, our main algorithmic contribution, which intelligently creates
contrasts adhering to a query budget, necessary for longer contexts. We show
the efficacy of these methods on diverse natural language tasks such as
open-text generation, automated red teaming, and explaining conversational
degradation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangling Singlish Discourse Particles with Task-Driven
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20366v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20366v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linus Tze En Foo, Lynnette Hui Xian Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Singlish, or formally Colloquial Singapore English, is an English-based
creole language originating from the SouthEast Asian country Singapore. The
language contains influences from Sinitic languages such as Chinese dialects,
Malay, Tamil and so forth. A fundamental task to understanding Singlish is to
first understand the pragmatic functions of its discourse particles, upon which
Singlish relies heavily to convey meaning. This work offers a preliminary
effort to disentangle the Singlish discourse particles (lah, meh and hor) with
task-driven representation learning. After disentanglement, we cluster these
discourse particles to differentiate their pragmatic functions, and perform
Singlish-to-English machine translation. Our work provides a computational
method to understanding Singlish discourse particles, and opens avenues towards
a deeper comprehension of the language and its usage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DOCE: Finding the Sweet Spot for Execution-Based Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13745v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13745v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haau-Sing Li, Patrick Fernandes, Iryna Gurevych, André F. T. Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a diverse set of decoding and reranking procedures have been shown
effective for LLM-based code generation. However, a comprehensive framework
that links and experimentally compares these methods is missing. We address
this by proposing Decoding Objectives for Code Execution, a comprehensive
framework that includes candidate generation, $n$-best reranking, minimum Bayes
risk (MBR) decoding, and self-debugging as the core components. We then study
the contributions of these components through execution-based evaluation
metrics. Our findings highlight the importance of execution-based methods and
the difference gap between execution-based and execution-free methods.
Furthermore, we assess the impact of filtering based on trial unit tests, a
simple and effective strategy that has been often overlooked in prior works. We
also propose self-debugging on multiple candidates, obtaining state-of-the-art
performance on reranking for code generation. We expect our framework to
provide a solid guideline for future research on code generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages (32 including appendix), 5 figures, 25 tables. Prompts are
  provided in the GitHub repository to avoid potential text overlap with other
  papers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music
  Scores for All Singing Tasks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13832v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13832v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Changhao Pan, Wenxiang Guo, Ruiqi Li, Zhiyuan Zhu, Jialei Wang, Wenhao Xu, Jingyu Lu, Zhiqing Hong, Chuxin Wang, LiChao Zhang, Jinzheng He, Ziyue Jiang, Yuxin Chen, Chen Yang, Jiecheng Zhou, Xinyu Cheng, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scarcity of high-quality and multi-task singing datasets significantly
hinders the development of diverse controllable and personalized singing tasks,
as existing singing datasets suffer from low quality, limited diversity of
languages and singers, absence of multi-technique information and realistic
music scores, and poor task suitability. To tackle these problems, we present
GTSinger, a large global, multi-technique, free-to-use, high-quality singing
corpus with realistic music scores, designed for all singing tasks, along with
its benchmarks. Particularly, (1) we collect 80.59 hours of high-quality
singing voices, forming the largest recorded singing dataset; (2) 20
professional singers across nine widely spoken languages offer diverse timbres
and styles; (3) we provide controlled comparison and phoneme-level annotations
of six commonly used singing techniques, helping technique modeling and
control; (4) GTSinger offers realistic music scores, assisting real-world
musical composition; (5) singing voices are accompanied by manual
phoneme-to-audio alignments, global style labels, and 16.16 hours of paired
speech for various singing tasks. Moreover, to facilitate the use of GTSinger,
we conduct four benchmark experiments: technique-controllable singing voice
synthesis, technique recognition, style transfer, and speech-to-singing
conversion. The corpus and demos can be found at http://gtsinger.github.io. We
provide the dataset and the code for processing data and conducting benchmarks
at https://huggingface.co/datasets/GTSinger/GTSinger and
https://github.com/GTSinger/GTSinger.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reward-Robust RLHF in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15360v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15360v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzi Yan, Xingzhou Lou, Jialian Li, Yiping Zhang, Jian Xie, Chao Yu, Yu Wang, Dong Yan, Yuan Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) continue to progress toward more advanced
forms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is
increasingly seen as a key pathway toward achieving Artificial General
Intelligence (AGI). However, the reliance on reward-model-based (RM-based)
alignment methods introduces significant challenges due to the inherent
instability and imperfections of Reward Models (RMs), which can lead to
critical issues such as reward hacking and misalignment with human intentions.
In this paper, we introduce a reward-robust RLHF framework aimed at addressing
these fundamental challenges, paving the way for more reliable and resilient
learning in LLMs. Our approach introduces a novel optimization objective that
carefully balances performance and robustness by incorporating Bayesian Reward
Model Ensembles (BRME) to model the uncertainty set of reward functions. This
allows the framework to integrate both nominal performance and minimum reward
signals, ensuring more stable learning even with imperfect RMs. Empirical
results demonstrate that our framework consistently outperforms baselines
across diverse benchmarks, showing improved accuracy and long-term stability.
We also provide a theoretical analysis, demonstrating that reward-robust RLHF
approaches the stability of constant reward settings, which proves to be
acceptable even in a stochastic-case analysis. Together, these contributions
highlight the framework potential to enhance both the performance and stability
of LLM alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust ASR Error Correction with Conservative Data Filtering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13300v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13300v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takuma Udagawa, Masayuki Suzuki, Masayasu Muraoka, Gakuto Kurata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Error correction (EC) based on large language models is an emerging
technology to enhance the performance of automatic speech recognition (ASR)
systems. Generally, training data for EC are collected by automatically pairing
a large set of ASR hypotheses (as sources) and their gold references (as
targets). However, the quality of such pairs is not guaranteed, and we observed
various types of noise which can make the EC models brittle, e.g. inducing
overcorrection in out-of-domain (OOD) settings. In this work, we propose two
fundamental criteria that EC training data should satisfy: namely, EC targets
should (1) improve linguistic acceptability over sources and (2) be inferable
from the available context (e.g. source phonemes). Through these criteria, we
identify low-quality EC pairs and train the models not to make any correction
in such cases, the process we refer to as conservative data filtering. In our
experiments, we focus on Japanese ASR using a strong Conformer-CTC as the
baseline and finetune Japanese LLMs for EC. Through our evaluation on a suite
of 21 internal benchmarks, we demonstrate that our approach can significantly
reduce overcorrection and improve both the accuracy and quality of ASR results
in the challenging OOD settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Details Make a Difference: Object State-Sensitive Neurorobotic Task
  Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09988v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09988v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaowen Sun, Xufeng Zhao, Jae Hee Lee, Wenhao Lu, Matthias Kerzel, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The state of an object reflects its current status or condition and is
important for a robot's task planning and manipulation. However, detecting an
object's state and generating a state-sensitive plan for robots is challenging.
Recently, pre-trained Large Language Models (LLMs) and Vision-Language Models
(VLMs) have shown impressive capabilities in generating plans. However, to the
best of our knowledge, there is hardly any investigation on whether LLMs or
VLMs can also generate object state-sensitive plans. To study this, we
introduce an Object State-Sensitive Agent (OSSA), a task-planning agent
empowered by pre-trained neural networks. We propose two methods for OSSA: (i)
a modular model consisting of a pre-trained vision processing module (dense
captioning model, DCM) and a natural language processing model (LLM), and (ii)
a monolithic model consisting only of a VLM. To quantitatively evaluate the
performances of the two methods, we use tabletop scenarios where the task is to
clear the table. We contribute a multimodal benchmark dataset that takes object
states into consideration. Our results show that both methods can be used for
object state-sensitive tasks, but the monolithic approach outperforms the
modular approach. The code for OSSA is available at
https://github.com/Xiao-wen-Sun/OSSA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICANN24, Switzerland</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain
  Readability Assessment <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14463v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14463v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tarek Naous, Michael J. Ryan, Anton Lavrouk, Mohit Chandra, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a comprehensive evaluation of large language models for
multilingual readability assessment. Existing evaluation resources lack domain
and language diversity, limiting the ability for cross-domain and cross-lingual
analyses. This paper introduces ReadMe++, a multilingual multi-domain dataset
with human annotations of 9757 sentences in Arabic, English, French, Hindi, and
Russian, collected from 112 different data sources. This benchmark will
encourage research on developing robust multilingual readability assessment
methods. Using ReadMe++, we benchmark multilingual and monolingual language
models in the supervised, unsupervised, and few-shot prompting settings. The
domain and language diversity in ReadMe++ enable us to test more effective
few-shot prompting, and identify shortcomings in state-of-the-art unsupervised
methods. Our experiments also reveal exciting results of superior domain
generalization and enhanced cross-lingual transfer capabilities by models
trained on ReadMe++. We will make our data publicly available and release a
python package tool for multilingual sentence readability prediction using our
trained models at: https://github.com/tareknaous/readme
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Explainable to Interpretable Deep Learning for Natural Language
  Processing in Healthcare: How Far from Reality? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11894v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11894v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangming Huang, Yingya Li, Shoaib Jameel, Yunfei Long, Giorgos Papanastasiou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) has substantially enhanced natural language processing
(NLP) in healthcare research. However, the increasing complexity of DL-based
NLP necessitates transparent model interpretability, or at least
explainability, for reliable decision-making. This work presents a thorough
scoping review of explainable and interpretable DL in healthcare NLP. The term
"eXplainable and Interpretable Artificial Intelligence" (XIAI) is introduced to
distinguish XAI from IAI. Different models are further categorized based on
their functionality (model-, input-, output-based) and scope (local, global).
Our analysis shows that attention mechanisms are the most prevalent emerging
IAI technique. The use of IAI is growing, distinguishing it from XAI. The major
challenges identified are that most XIAI does not explore "global" modelling
processes, the lack of best practices, and the lack of systematic evaluation
and benchmarks. One important opportunity is to use attention mechanisms to
enhance multi-modal XIAI for personalized medicine. Additionally, combining DL
with causal logic holds promise. Our discussion encourages the integration of
XIAI in Large Language Models (LLMs) and domain-specific smaller models. In
conclusion, XIAI adoption in healthcare requires dedicated in-house expertise.
Collaboration with domain experts, end-users, and policymakers can lead to
ready-to-use XIAI methods across NLP and medical tasks. While challenges exist,
XIAI techniques offer a valuable foundation for interpretable NLP algorithms in
healthcare.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by Computational and Structural
  Biotechnology Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>ing Explicit and Implicit Knowledge for Multi-hop Question
  Answering Based on Human Reading Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19350v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19350v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangming Huang, Yunfei Long, Cunjin Luo, Jiaxing Shen, Xia Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to
simulate human reasoning and inference processes, achieving proficient
performance in multi-hop QA. However, a gap persists between PLMs' reasoning
abilities and those of humans when tackling complex problems. Psychological
studies suggest a vital connection between explicit information in passages and
human prior knowledge during reading. Nevertheless, current research has given
insufficient attention to linking input passages and PLMs' pre-training-based
knowledge from the perspective of human cognition studies. In this study, we
introduce a Prompting Explicit and Implicit knowledge (PEI) framework, which
uses prompts to connect explicit and implicit knowledge, aligning with human
reading process for multi-hop QA. We consider the input passages as explicit
knowledge, employing them to elicit implicit knowledge through unified prompt
reasoning. Furthermore, our model incorporates type-specific reasoning via
prompts, a form of implicit knowledge. Experimental results show that PEI
performs comparably to the state-of-the-art on HotpotQA. Ablation studies
confirm the efficacy of our model in bridging and integrating explicit and
implicit knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted at COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Five Years of COVID-19 Discourse on Instagram: A Labeled Instagram
  Dataset of Over Half a Million Posts for Multilingual Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03293v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03293v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nirmalya Thakur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The work presented in this paper makes three scientific contributions with a
specific focus on mining and analysis of COVID-19-related posts on Instagram.
First, it presents a multilingual dataset of 500,153 Instagram posts about
COVID-19 published between January 2020 and September 2024. This dataset,
available at https://dx.doi.org/10.21227/d46p-v480, contains Instagram posts in
161 different languages as well as 535,021 distinct hashtags. After the
development of this dataset, multilingual sentiment analysis was performed,
which involved classifying each post as positive, negative, or neutral. The
results of sentiment analysis are presented as a separate attribute in this
dataset. Second, it presents the results of performing sentiment analysis per
year from 2020 to 2024. The findings revealed the trends in sentiment related
to COVID-19 on Instagram since the beginning of the pandemic. For instance,
between 2020 and 2024, the sentiment trends show a notable shift, with positive
sentiment decreasing from 38.35% to 28.69%, while neutral sentiment rising from
44.19% to 58.34%. Finally, the paper also presents findings of
language-specific sentiment analysis. This analysis highlighted similar and
contrasting trends of sentiment across posts published in different languages
on Instagram. For instance, out of all English posts, 49.68% were positive,
14.84% were negative, and 35.48% were neutral. In contrast, among Hindi posts,
4.40% were positive, 57.04% were negative, and 38.56% were neutral, reflecting
distinct differences in the sentiment distribution between these two languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic Token Reweighting for Interpretable and Controllable Text
  Embeddings in <span class="highlight-title">CLIP</span> <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eunji Kim, Kyuhong Shim, Simyung Chang, Sungroh Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial
role in translating textual input into an embedding space shared with images,
thereby facilitating the interpretative analysis of vision tasks through
natural language. Despite the varying significance of different textual
elements within a sentence depending on the context, efforts to account for
variation of importance in constructing text embeddings have been lacking. We
propose a framework of Semantic Token Reweighting to build Interpretable text
embeddings (SToRI), which incorporates controllability as well. SToRI refines
the text encoding process in CLIP by differentially weighting semantic elements
based on contextual importance, enabling finer control over emphasis responsive
to data-driven insights and user preferences. The efficacy of SToRI is
demonstrated through comprehensive experiments on few-shot image classification
and image retrieval tailored to user preferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>DSI: <span class="highlight-title">Prompt</span>-based Rehearsal-free Instance-wise Incremental
  Learning for Document Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan-Luc Huynh, Thuy-Trang Vu, Weiqing Wang, Yinwei Wei, Trung Le, Dragan Gasevic, Yuan-Fang Li, Thanh-Toan Do
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiable Search Index (DSI) utilizes Pre-trained Language Models (PLMs)
for efficient document retrieval without relying on external indexes. However,
DSI needs full re-training to handle updates in dynamic corpora, causing
significant computational inefficiencies. We introduce PromptDSI, a
prompt-based rehearsal-free approach for instance-wise incremental learning
document retrieval. PromptDSI attaches prompts to the frozen PLM's encoder of
DSI, leveraging its powerful representation to efficiently index new corpora
while maintaining a balance between stability and plasticity. We eliminate the
initial forward pass of prompt-based continual learning methods that doubles
training and inference time. Moreover, we propose a topic-aware prompt pool
that employs neural topic embeddings as fixed keys. This strategy ensures
diverse and effective prompt usage, addressing the challenge of parameter
underutilization caused by the collapse of the query-key matching mechanism.
Our empirical evaluations demonstrate that BERT-based PromptDSI matches IncDSI
in managing forgetting while improving new corpora performance by more than 4%
Hits@10 on NQ320k and upto 3% MRR@10 on MS MARCO 300k.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Comparative Trap: Pairwise Comparisons Amplifies Biased Preferences
  of LLM Evaluators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12319v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12319v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hawon Jeong, ChaeHun Park, Jimin Hong, Hojoon Lee, Jaegul Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) are increasingly used as evaluators for
natural language generation tasks, ensuring unbiased assessments is essential.
However, LLM evaluators often display biased preferences, such as favoring
verbosity and authoritative tones. Our empirical analysis reveals that these
biases are exacerbated in pairwise evaluation, where LLMs directly compare two
outputs and easily prioritize superficial attributes. In contrast, pointwise
evaluation, which assesses outputs independently, is less susceptible to such
bias because each output is judged in isolation. To address the limitations of
the pairwise evaluation, we introduce a novel evaluation method, PRePair, which
integrates pointwise reasoning within a pairwise framework. PRePair effectively
alleviates biased preference, improving performance on the adversarial
benchmark (LLMBar) while outperforming pointwise evaluation on the standard
benchmark (MT-Bench).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Data Privacy in Large Language Models through Private
  Association Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18221v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18221v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Venditti, Elena Sofia Ruzzetti, Giancarlo A. Xompero, Cristina Giannone, Andrea Favalli, Raniero Romagnoli, Fabio Massimo Zanzotto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) require a significant redesign in solutions to
preserve privacy in data-intensive applications due to their text-generation
capabilities. Indeed, LLMs tend to memorize and emit private information when
maliciously prompted. In this paper, we introduce Private Association Editing
(PAE) as a novel defense approach for private data leakage. PAE is designed to
effectively remove Personally Identifiable Information (PII) without retraining
the model. Experimental results demonstrate the effectiveness of PAE with
respect to alternative baseline methods. We believe PAE will serve as a
critical tool in the ongoing effort to protect data privacy in LLMs,
encouraging the development of safer models for real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval
  Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14162v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14162v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Ni, Tobias Schimanski, Meihong Lin, Mrinmaya Sachan, Elliott Ash, Markus Leippold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) is widely employed to ground responses
to queries on domain-specific documents. But do RAG implementations leave out
important information when answering queries that need an integrated analysis
of information (e.g., Tell me good news in the stock market today.)? To address
these concerns, RAG developers need to annotate information retrieval (IR) data
for their domain of interest, which is challenging because (1) domain-specific
queries usually need nuanced definitions of relevance beyond shallow semantic
relevance; and (2) human or GPT-4 annotation is costly and cannot cover all
(query, document) pairs (i.e., annotation selection bias), thus harming the
effectiveness in evaluating IR recall. To address these challenges, we propose
DIRAS (Domain-specific Information Retrieval Annotation with Scalability), a
manual-annotation-free schema that fine-tunes open-sourced LLMs to consider
nuanced relevance definition and annotate (partial) relevance labels with
calibrated relevance scores. Extensive evaluation shows that DIRAS enables
smaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking
unseen (query, document) pairs, and is helpful for real-world RAG development.
All code, LLM generations, and human annotations can be found in
\url{https://github.com/EdisonNi-hku/DIRAS}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ sPhinX: Sample Efficient Multilingual Instruction <span class="highlight-title">Fine-Tuning</span> Through
  N-shot Guided <span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09879v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09879v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanchit Ahuja, Kumar Tanmay, Hardik Hansrajbhai Chauhan, Barun Patra, Kriti Aggarwal, Luciano Del Corro, Arindam Mitra, Tejas Indulal Dhamecha, Ahmed Awadallah, Monojit Choudhary, Vishrav Chaudhary, Sunayana Sitaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable success of LLMs in English, there is a significant gap
in performance in non-English languages. In order to address this, we introduce
a novel recipe for creating a multilingual synthetic instruction tuning
dataset, sPhinX, which is created by selectively translating instruction
response pairs from English into 50 languages. We test the effectiveness of
sPhinx by using it to fine-tune two state-of-the-art models, Mistral-7B and
Phi-Small and then evaluating them across a comprehensive suite of multilingual
benchmarks that test reasoning, question answering, reading comprehension and
machine translation. Our results show that Mistral-7B and Phi-Small fine-tuned
with sPhinX perform better on an average by 5%pt for both the models when
compared to the base variants of these models. We also devise a strategy to
incorporate N-shot examples in each fine-tuning sample which further boosts the
performance of these models by 9%pt and 4%pt respectively respectively compared
to vanilla fine-tuning. To show efficacy of our data curation approach, we also
directly translate our original dataset to the target languages, and observe an
increase of 7%pt and 4%pt on both the models respectively. sPhinX outperforms
other multilingual instruction tuning datasets in both efficiency and
diversity, reducing dataset creation costs. It also maintains strong
performance on standard English LLM benchmarks, with minimal regression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 12 tables, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19014v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19014v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heegyu Kim, Taeyang Jeon, Seunghwan Choi, Seungtaek Choi, Hyunsouk Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL systems have become crucial for translating natural language into
SQL queries in various industries, enabling non-technical users to perform
complex data operations. The need for accurate evaluation methods has increased
as these systems have grown more sophisticated. However, the Execution Accuracy
(EX), the most prevalent evaluation metric, still shows many false positives
and negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel
approach to evaluating text-to-SQL systems using large language models (LLMs)
to emulate human expert-level evaluation of SQL queries. Our metric improves
agreement with human experts (from 62 to 87.04 in Cohen's kappa) with
comprehensive context and sophisticated criteria. Our extensive experiments
yield several key insights: (1) Models' performance increases by over 2.6
points on average, substantially affecting rankings on Spider and BIRD
benchmarks; (2) The underestimation of models in EX primarily stems from
annotation quality issues; and (3) Model performance on particularly
challenging questions tends to be overestimated. This work contributes to a
more accurate and nuanced evaluation of text-to-SQL systems, potentially
reshaping our understanding of state-of-the-art performance in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixture of Experts Made Personalized: Federated <span class="highlight-title">Prompt</span> Learning for
  <span class="highlight-title">Vision-Language</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Luo, Chen Chen, Shandong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt learning for pre-trained Vision-Language Models (VLMs) like CLIP has
demonstrated potent applicability across diverse downstream tasks. This
lightweight approach has quickly gained traction from federated learning (FL)
researchers who seek to efficiently adapt VLMs to heterogeneous scenarios.
However, current federated prompt learning methods are habitually restricted to
the traditional FL paradigm, where the participating clients are generally only
allowed to download a single globally aggregated model from the server. While
justifiable for training full-sized models under federated settings, in this
work, we argue that this paradigm is ill-suited for lightweight prompts. By
facilitating the clients to download multiple pre-aggregated prompts as fixed
non-local experts, we propose Personalized Federated Mixture of Adaptive
Prompts (pFedMoAP), a novel FL framework that personalizes the prompt learning
process through the lens of Mixture of Experts (MoE). pFedMoAP implements a
local attention-based gating network that learns to generate enhanced text
features for better alignment with local image data on the client, benefiting
from both local and downloaded non-local adaptive prompt experts. The non-local
experts are sparsely selected from a server-maintained pool, fostering
collaborative learning across clients. To evaluate the proposed algorithm, we
conduct extensive experiments across 9 datasets under various heterogeneous
federated settings. The results show that pFedMoAP consistently outperforms the
state-of-the-art alternatives, underscoring its efficacy in personalizing
prompt learning for CLIP within the federated learning paradigm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Few-shot Learning for Multi-label Classification of Scientific
  Documents with Many Classes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Schopf, Alexander Blatzheim, Nektarios Machner, Florian Matthes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific document classification is a critical task and often involves many
classes. However, collecting human-labeled data for many classes is expensive
and usually leads to label-scarce scenarios. Moreover, recent work has shown
that sentence embedding model fine-tuning for few-shot classification is
efficient, robust, and effective. In this work, we propose FusionSent
(Fusion-based Sentence Embedding Fine-tuning), an efficient and prompt-free
approach for few-shot classification of scientific documents with many classes.
FusionSent uses available training examples and their respective label texts to
contrastively fine-tune two different sentence embedding models. Afterward, the
parameters of both fine-tuned models are fused to combine the complementary
knowledge from the separate fine-tuning steps into a single model. Finally, the
resulting sentence embedding model is frozen to embed the training instances,
which are then used as input features to train a classification head. Our
experiments show that FusionSent significantly outperforms strong baselines by
an average of $6.0$ $F_{1}$ points across multiple scientific document
classification datasets. In addition, we introduce a new dataset for
multi-label classification of scientific documents, which contains 203,961
scientific articles and 130 classes from the arXiv category taxonomy. Code and
data are available at https://github.com/sebischair/FusionSent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 7th International Conference on Natural Language and
  Speech Processing (ICNLSP 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking LLMs for Translating Classical Chinese Poetry:Evaluating
  Adequacy, Fluency, and Elegance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09945v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09945v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andong Chen, Lianzhang Lou, Kehai Chen, Xuefeng Bai, Yang Xiang, Muyun Yang, Tiejun Zhao, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable performance in general
translation tasks. However, the increasing demand for high-quality translations
that are not only adequate but also fluent and elegant. To assess the extent to
which current LLMs can meet these demands, we introduce a suitable benchmark
for translating classical Chinese poetry into English. This task requires not
only adequacy in translating culturally and historically significant content
but also a strict adherence to linguistic fluency and poetic elegance. Our
study reveals that existing LLMs fall short of this task. To address these
issues, we propose RAT, a \textbf{R}etrieval-\textbf{A}ugmented machine
\textbf{T}ranslation method that enhances the translation process by
incorporating knowledge related to classical poetry. Additionally, we propose
an automatic evaluation metric based on GPT-4, which better assesses
translation quality in terms of adequacy, fluency, and elegance, overcoming the
limitations of traditional metrics. Our dataset and code will be made
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic
  Preference Optimization Dataset Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08688v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08688v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samee Arif, Sualeha Farid, Abdul Hameed Azeemi, Awais Athar, Agha Ali Raza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel methodology for generating synthetic Preference
Optimization (PO) datasets using multi-agent workflows. We evaluate the
effectiveness and potential of these workflows in automating and enhancing the
dataset generation process. PO dataset generation requires two modules: (1)
response evaluation, and (2) response generation. In the response evaluation
module, the responses from Large Language Models (LLMs) are evaluated and
ranked - a task typically carried out by human annotators that we automate
using LLMs. We assess the response evaluation module in a 2 step process. In
step 1, we assess LLMs as evaluators using three distinct prompting strategies.
In step 2, we apply the winning prompting strategy to compare the performance
of LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. Our evaluation shows that
GPT-4o-as-a-Judge is more consistent across all datasets. For the response
generation module, we use the identified LLM evaluator configuration and
compare different configurations of the LLM Feedback Loop. We use the win rate
to determine the best multi-agent configuration for generation. Experimenting
with various configurations, we find that the LLM Feedback Loop, with Llama as
the generator and Gemma as the reviewer, achieves a notable 71.8% and 73.8% win
rate over single-agent Llama and Gemma, respectively. After identifying the
best configurations for both modules, we generate our PO datasets using the
above pipeline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Token-based Decision Criteria Are Suboptimal in In-context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16535v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16535v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hakaze Cho, Yoshihiro Sakai, Mariko Kato, Kenshiro Tanaka, Akira Ishii, Naoya Inoue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-Context Learning (ICL) typically utilizes classification criteria from
output probabilities of manually selected label tokens. However, we argue that
such token-based classification criteria lead to suboptimal decision
boundaries, despite delicate calibrations through translation and constrained
rotation applied. To address this problem, we propose Hidden Calibration, which
renounces token probabilities and uses the nearest centroid classifier on the
LM's last hidden states. In detail, we assign the label of the nearest centroid
previously estimated from a calibration set to the test sample as the predicted
label. Our experiments on 6 models and 10 classification datasets indicate that
Hidden Calibration consistently outperforms current token-based baselines by
about 20%~50%, achieving a strong state-of-the-art in ICL. Our further analysis
demonstrates that Hidden Calibration finds better classification criteria with
less inter-class overlap, and LMs provide linearly separable intra-class
clusters with the help of demonstrations, which supports Hidden Calibration and
gives new insights into the principle of ICL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 15 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10052v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10052v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, Ivan Vulić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigating the retention of sensitive or private information in large
language models is essential for enhancing privacy and safety. Existing
unlearning methods, like Gradient Ascent and Negative Preference Optimization,
directly tune models to remove unwanted information. However, these methods
often become unstable because they fine-tune by maximizing cross-entropy loss,
which is the opposite of traditional loss minimization in learning. This
reversal creates instability, especially on larger datasets, as the model
struggles to balance unlearning with maintaining language capacity, leading to
over-unlearning. In this paper, we introduce UnDIAL (Unlearning via
Self-Distillation on Adjusted Logits), a novel and robust unlearning method.
Our approach leverages self-distillation to adjust logits and selectively
reduce the influence of targeted tokens. This technique ensures smooth
convergence and avoids catastrophic forgetting, even in challenging unlearning
tasks with large datasets and sequential unlearning requests. Extensive
experiments show that UnDIAL can achieve both robustness in unlearning and
scalability while maintaining stable training dynamics and resilience to
hyperparameter tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Instruction Following: Evaluating Inferential Rule Following of
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08440v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08440v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangtao Sun, Chenxiang Zhang, XueYou Zhang, Xuanqing Yu, Ziyang Huang, Pei Chen, Haotian Xu, Shizhu He, Jun Zhao, Kang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Large Language Models (LLMs) have demonstrated strong ability, they
are further supposed to be controlled and guided by in real-world scenarios to
be safe, accurate, and intelligent. This demands the possession of capability
of LLMs. However, no prior work has made a clear evaluation of the inferential
rule-following capability of LLMs. Previous studies that try to evaluate the
inferential rule-following capability of LLMs fail to distinguish the
inferential rule-following scenarios from the instruction-following scenarios.
Therefore, this paper first clarifies the concept of inferential rule-following
and proposes a comprehensive benchmark, RuleBench, to evaluate a diversified
range of inferential rule-following abilities. Our experimental results on a
variety of LLMs show that they are still limited in following rules. Our
analysis based on the evaluation results provides insights into the
improvements for LLMs toward a better inferential rule-following intelligent
agent. We further propose Inferential Rule-Following Tuning (IRFT). The
experimental results show that through IRFT, LLMs can learn abstract
rule-following abilities from purely synthetic data and then generalize to
RuleBench. The data and code can be found at:
https://anonymous.4open.science/r/llm-rule-following-B3E3/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VersiCode: Towards Version-controllable Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07411v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07411v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongtong Wu, Weigang Wu, Xingyu Wang, Kang Xu, Suyu Ma, Bo Jiang, Ping Yang, Zhenchang Xing, Yuan-Fang Li, Gholamreza Haffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have made tremendous strides in code generation,
but existing research fails to account for the dynamic nature of software
development, marked by frequent library updates. This gap significantly limits
LLMs' deployment in realistic settings. In this paper, we propose two novel
tasks aimed at bridging this gap: version-specific code completion (VSCC) and
version-aware code migration (VACM). In conjunction, we introduce VersiCode, a
comprehensive Python dataset specifically designed to evaluate LLMs on these
two tasks, together with a novel evaluation metric, Critical Diff Check
(CDC@1), which assesses code generation against evolving API requirements. We
conduct an extensive evaluation on VersiCode, which reveals that
version-controllable code generation is indeed a significant challenge, even
for GPT-4o and other strong frontier models. We believe the novel tasks,
dataset, and metric open up a new, important research direction that will
further enhance LLMs' real-world applicability. The code and resources can be
found at https://github.com/wutong8023/VersiCode.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic
  Evaluation Framework for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11507v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11507v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanying Wang, Zeyu Ma, Pengfei Liu, Mingang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While various vertical domain large language models (LLMs) have been
developed, the challenge of automatically evaluating their performance across
different domains remains significant. Current benchmark-based evaluation
methods exhibit rigid, aimless interactions and rely on pre-collected static
datasets that are costly to build, inflexible across domains, and misaligned
with practical user needs. To address this issue, we revisit the evaluation
components and introduce two concepts: Benchmark+, which extends traditional
question-answer benchmark into a more flexible "strategy-criterion" format; and
Assessment+, which enhances the interaction process, enabling deeper
exploration and supporting both quantitative metrics and qualitative insights.
These concepts capture the nuanced behaviors of LLMs through richer, multi-turn
interactions. We propose an agent-based evaluation framework called TestAgent,
which implements these concepts through retrieval augmented generation and
reinforcement learning. Experiments on tasks ranging from constructing vertical
domain evaluation to activating existing benchmarks demonstrate the
effectiveness of TestAgent across various scenarios. We believe this work
offers an interesting perspective on automatic evaluation for LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LoraMap: Harnessing the Power of LoRA Connections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeryun Park, Jeongwon Kwak, Dongsuk Jang, Sumin Park, Jinwook Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fact-checking techniques can mitigate hallucinations in Large Language Models
(LLMs), a prominent issue in specialized domains. As parameter-efficient
techniques such as Low-Rank Adaptation (LoRA) can overcome substantial
computational overhead, some studies have explored the integration of multiple
LoRAs. While previous studies focus on parallel integration, this paper
investigates methods to establish connections among multiple LoRAs. We create
three reasoning datasets tailored to fact-checking and fine-tune individual
LoRAs, allowing them to view and reason from diverse perspectives. Then, we
explore strategies for allocating these reasoning LoRAs and introduce LoraMap,
an approach to map connections between them. The results of the fact-checking
task demonstrate that the performance of LoraMap is superior to LoraHub, an
existing method for integrating LoRAs. LoraMap also outperforms with
significantly fewer trainable parameters than LoraConcat, which concatenates
LoRAs and further fine-tunes them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 12 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mental Disorders Detection in the Era of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07129v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07129v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gleb Kuzmin, Petr Strepetov, Maksim Stankevich, Artem Shelmanov, Ivan Smirnov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper compares the effectiveness of traditional machine learning
methods, encoder-based models, and large language models (LLMs) on the task of
detecting depression and anxiety. Five datasets were considered, each differing
in format and the method used to define the target pathology class. We tested
AutoML models based on linguistic features, several variations of encoder-based
Transformers such as BERT, and state-of-the-art LLMs as pathology
classification models. The results demonstrated that LLMs outperform
traditional methods, particularly on noisy and small datasets where training
examples vary significantly in text length and genre. However, psycholinguistic
features and encoder-based models can achieve performance comparable to
language models when trained on texts from individuals with clinically
confirmed depression, highlighting their potential effectiveness in targeted
clinical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MFC-Bench: Benchmarking <span class="highlight-title">Multimodal</span> Fact-Checking with Large
  <span class="highlight-title">Vision-Language</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11288v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11288v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengkang Wang, Hongzhan Lin, Ziyang Luo, Zhen Ye, Guang Chen, Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) have significantly improved multimodal
reasoning tasks, such as visual question answering and image captioning. These
models embed multimodal facts within their parameters, rather than relying on
external knowledge bases to store factual information explicitly. However, the
content discerned by LVLMs may deviate from actual facts due to inherent bias
or incorrect inference. To address this issue, we introduce MFC-Bench, a
rigorous and comprehensive benchmark designed to evaluate the factual accuracy
of LVLMs across three stages of verdict prediction for MFC: Manipulation,
Out-of-Context, and Veracity Classification. Through our evaluation on
MFC-Bench, we benchmarked a dozen diverse and representative LVLMs, uncovering
that current models still fall short in multimodal fact-checking and
demonstrate insensitivity to various forms of manipulated content. We hope that
MFC-Bench could raise attention to the trustworthy AI potentially assisted by
LVLMs in the future. The MFC-Bench and accompanying resources are publicly
accessible at https://github.com/wskbest/MFC-Bench, contributing to ongoing
research in the multimodal fact-checking field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A corpus-based investigation of pitch contours of monosyllabic words in
  conversational Taiwan Mandarin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07891v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07891v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyun Jin, Mirjam Ernestus, R. Harald Baayen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Mandarin, the tonal contours of monosyllabic words produced in isolation
or in careful speech are characterized by four lexical tones: a high-level tone
(T1), a rising tone (T2), a dipping tone (T3) and a falling tone (T4). However,
in spontaneous speech, the actual tonal realization of monosyllabic words can
deviate significantly from these canonical tones due to intra-syllabic
co-articulation and inter-syllabic co-articulation with adjacent tones. In
addition, Chuang et al. (2024) recently reported that the tonal contours of
disyllabic Mandarin words with T2-T4 tone pattern are co-determined by their
meanings. Following up on their research, we present a corpus-based
investigation of how the pitch contours of monosyllabic words are realized in
spontaneous conversational Mandarin, focusing on the effects of contextual
predictors on the one hand, and the way in words' meanings co-determine pitch
contours on the other hand. We analyze the F0 contours of 3824 tokens of 63
different word types in a spontaneous Taiwan Mandarin corpus, using the
generalized additive (mixed) model to decompose a given observed pitch contour
into a set of component pitch contours. We show that the tonal context
substantially modify a word's canonical tone. Once the effect of tonal context
is controlled for, T2 and T3 emerge as low flat tones, contrasting with T1 as a
high tone, and with T4 as a high-to-mid falling tone. The neutral tone (T0),
which in standard descriptions, is realized based on the preceding tone,
emerges as a low tone in its own right, modified by the other predictors in the
same way as the standard tones T1, T2, T3, and T4. We also show that word, and
even more so, word sense, co-determine words' F0 contours. Analyses of variable
importance using random forests further supported the substantial effect of
tonal context and an effect of word sense.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reverse Stable Diffusion: What <span class="highlight-title">prompt</span> was used to generate this image? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.01472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.01472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have recently attracted the interest of many
researchers, and inverting the diffusion process can play an important role in
better understanding the generative process and how to engineer prompts in
order to obtain the desired images. To this end, we study the task of
predicting the prompt embedding given an image generated by a generative
diffusion model. We consider a series of white-box and black-box models (with
and without access to the weights of the diffusion network) to deal with the
proposed task. We propose a novel learning framework comprising a joint prompt
regression and multi-label vocabulary classification objective that generates
improved prompts. To further improve our method, we employ a curriculum
learning procedure that promotes the learning of image-prompt pairs with lower
labeling noise (i.e. that are better aligned). We conduct experiments on the
DiffusionDB data set, predicting text prompts from images generated by Stable
Diffusion. In addition, we make an interesting discovery: training a diffusion
model on the prompt generation task can make the model generate images that are
much better aligned with the input prompts, when the model is directly reused
for text-to-image generation. Our code is publicly available for download at
https://github.com/CroitoruAlin/Reverse-Stable-Diffusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Computer Vision and Image Understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models in the Clinic: A Comprehensive Benchmark <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00716v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00716v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fenglin Liu, Zheng Li, Hongjian Zhou, Qingyu Yin, Jingfeng Yang, Xianfeng Tang, Chen Luo, Ming Zeng, Haoming Jiang, Yifan Gao, Priyanka Nigam, Sreyashi Nag, Bing Yin, Yining Hua, Xuan Zhou, Omid Rohanian, Anshul Thakur, Lei Clifton, David A. Clifton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The adoption of large language models (LLMs) to assist clinicians has
attracted remarkable attention. Existing works mainly adopt the close-ended
question-answering (QA) task with answer options for evaluation. However, many
clinical decisions involve answering open-ended questions without pre-set
options. To better understand LLMs in the clinic, we construct a benchmark
ClinicBench. We first collect eleven existing datasets covering diverse
clinical language generation, understanding, and reasoning tasks. Furthermore,
we construct six novel datasets and clinical tasks that are complex but common
in real-world practice, e.g., open-ended decision-making, long document
processing, and emerging drug analysis. We conduct an extensive evaluation of
twenty-two LLMs under both zero-shot and few-shot settings. Finally, we invite
medical experts to evaluate the clinical usefulness of LLMs. The benchmark data
is available at https://github.com/AI-in-Health/ClinicBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLongEval: A Chinese Benchmark for Evaluating Long-Context Large
  Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03514v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03514v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zexuan Qiu, Jingjing Li, Shijue Huang, Xiaoqi Jiao, Wanjun Zhong, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing Large Language Models (LLMs) with robust long-context capabilities
has been the recent research focus, resulting in the emergence of long-context
LLMs proficient in Chinese. However, the evaluation of these models remains
underdeveloped due to a lack of benchmarks. To address this gap, we present
CLongEval, a comprehensive Chinese benchmark for evaluating long-context LLMs.
CLongEval is characterized by three key features: (1) Sufficient data volume,
comprising 7 distinct tasks and 7,267 examples; (2) Broad applicability,
accommodating to models with context windows size from 1K to 100K; (3) High
quality, with over 2,000 manually annotated question-answer pairs in addition
to the automatically constructed labels. With CLongEval, we undertake a
comprehensive assessment of 6 open-source long-context LLMs and 2 leading
commercial counterparts that feature both long-context abilities and
proficiency in Chinese. We also provide in-depth analysis based on the
empirical results, trying to shed light on the critical capabilities that
present challenges in long-context settings. The dataset, evaluation scripts,
and model outputs are released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TaCo: Targeted Concept Erasure Prevents Non-Linear Classifiers From
  Detecting Protected Attributes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06499v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06499v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanny Jourdan, Louis Béthune, Agustin Picard, Laurent Risser, Nicholas Asher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring fairness in NLP models is crucial, as they often encode sensitive
attributes like gender and ethnicity, leading to biased outcomes. Current
concept erasure methods attempt to mitigate this by modifying final latent
representations to remove sensitive information without retraining the entire
model. However, these methods typically rely on linear classifiers, which leave
models vulnerable to non-linear adversaries capable of recovering sensitive
information.
  We introduce Targeted Concept Erasure (TaCo), a novel approach that removes
sensitive information from final latent representations, ensuring fairness even
against non-linear classifiers. Our experiments show that TaCo outperforms
state-of-the-art methods, achieving greater reductions in the prediction
accuracy of sensitive attributes by non-linear classifier while preserving
overall task performance. Code is available on
https://github.com/fanny-jourdan/TaCo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Inference with Large Language Model: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09822v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09822v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal inference has been a pivotal challenge across diverse domains such as
medicine and economics, demanding a complicated integration of human knowledge,
mathematical reasoning, and data mining capabilities. Recent advancements in
natural language processing (NLP), particularly with the advent of large
language models (LLMs), have introduced promising opportunities for traditional
causal inference tasks. This paper reviews recent progress in applying LLMs to
causal inference, encompassing various tasks spanning different levels of
causation. We summarize the main causal problems and approaches, and present a
comparison of their evaluation results in different causal scenarios.
Furthermore, we discuss key findings and outline directions for future
research, underscoring the potential implications of integrating LLMs in
advancing causal inference methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explore, Select, Derive, and Recall: Augmenting LLM with Human-like
  Memory for Mobile Task Automation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03003v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03003v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunjae Lee, Junyoung Choi, Jungjae Lee, Munim Hasan Wasi, Hojun Choi, Steven Y. Ko, Sangeun Oh, Insik Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large language models (LLMs) has opened up new opportunities in
the field of mobile task automation. Their superior language understanding and
reasoning capabilities allow users to automate complex and repetitive tasks.
However, due to the inherent unreliability and high operational cost of LLMs,
their practical applicability is quite limited. To address these issues, this
paper introduces MobileGPT, an innovative LLM-based mobile task automator
equipped with a human-like app memory. MobileGPT emulates the cognitive process
of humans interacting with a mobile app -- explore, select, derive, and recall.
This approach allows for a more precise and efficient learning of a task's
procedure by breaking it down into smaller, modular sub-tasks that can be
re-used, re-arranged, and adapted for various objectives. We implement
MobileGPT using online LLMs services (GPT-3.5 and GPT-4) and evaluate its
performance on a dataset of 185 tasks across 18 mobile apps. The results
indicate that MobileGPT can automate and learn new tasks with 82.7% accuracy,
and is able to adapt them to different contexts with near perfect (98.75%)
accuracy while reducing both latency and cost by 62.5% and 68.8%, respectively,
compared to the GPT-4 powered baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reconsidering Degeneration of Token Embeddings with Definitions for
  Encoder-based <span class="highlight-title">Pre-train</span>ed Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Zhang, Dongyuan Li, Manabu Okumura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning token embeddings based on token co-occurrence statistics has proven
effective for both pre-training and fine-tuning in natural language processing.
However, recent studies have pointed out that the distribution of learned
embeddings degenerates into anisotropy (i.e., non-uniform distribution), and
even pre-trained language models (PLMs) suffer from a loss of semantics-related
information in embeddings for low-frequency tokens. This study first analyzes
the fine-tuning dynamics of encoder-based PLMs and demonstrates their
robustness against degeneration. On the basis of this analysis, we propose
DefinitionEMB, a method that utilizes definitions to re-construct isotropically
distributed and semantics-related token embeddings for encoder-based PLMs while
maintaining original robustness during fine-tuning. Our experiments demonstrate
the effectiveness of leveraging definitions from Wiktionary to re-construct
such embeddings for two encoder-based PLMs: RoBERTa-base and BART-large.
Furthermore, the re-constructed embeddings for low-frequency tokens improve the
performance of these models across various GLUE and four text summarization
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in
  Multi-Agent Settings with Social Hierarchy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gian Maria Campedelli, Nicolò Penzo, Massimo Stefan, Roberto Dessì, Marco Guerini, Bruno Lepri, Jacopo Staiano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Model (LLM)-based agents become increasingly autonomous and
will more freely interact with each other, studying interactions between them
becomes crucial to anticipate emergent phenomena and potential risks. Drawing
inspiration from the widely popular Stanford Prison Experiment, we contribute
to this line of research by studying interaction patterns of LLM agents in a
context characterized by strict social hierarchy. We do so by specifically
studying two types of phenomena: persuasion and anti-social behavior in
simulated scenarios involving a guard and a prisoner agent who seeks to achieve
a specific goal (i.e., obtaining additional yard time or escape from prison).
Leveraging 200 experimental scenarios for a total of 2,000 machine-machine
conversations across five different popular LLMs, we provide a set of
noteworthy findings. We first document how some models consistently fail in
carrying out a conversation in our multi-agent setup where power dynamics are
at play. Then, for the models that were able to engage in successful
interactions, we empirically show how the goal that an agent is set to achieve
impacts primarily its persuasiveness, while having a negligible effect with
respect to the agent's anti-social behavior. Third, we highlight how agents'
personas, and particularly the guard's personality, drive both the likelihood
of successful persuasion from the prisoner and the emergence of anti-social
behaviors. Fourth, we show that even without explicitly prompting for specific
personalities, anti-social behavior emerges by simply assigning agents' roles.
These results bear implications for the development of interactive LLM agents
as well as the debate on their societal impact.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Do Humans Write Code? Large Models Do It the Same Way Too 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15729v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15729v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Li, Xuzheng He, Haozhe Wang, Linlin Wang, Liang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought
(CoT) as the most popular method in Large Language Models (LLMs) mathematical
reasoning tasks by utilizing external tool calls to circumvent computational
errors. However, our evaluation of the GPT-4 and Llama series reveals that
using PoT introduces more reasoning errors, such as incorrect formulas or
flawed logic, compared to CoT. To address this issue, we propose Human-Think
Language (HTL), which leverages a suite of strategies that help integrate PoT
and CoT, encompassing: (1) a new generation paradigm that uses full CoT
reasoning to control code generation. (2) Focus Attention, that directs model
attention to the CoT reasoning during PoT to generate more logical code. (3)
reinforcement learning that utilizes the accuracy of both CoT and PoT responses
as rewards to prevent repetitive reasoning steps in LLMs when solving difficult
math problems. Our method achieves an average improvement of 6.5% on the
Llama-Base model and 4.3% on the Mistral-Base model across 8 mathematical
calculation datasets. It also shows significant effectiveness on five
out-of-domain datasets by controlling the model's information flow, exhibiting
strong transferability. Additionally, HTL shows the most significant
improvement in non-mathematical natural language inference task, contributing
to a unified reasoning task framework
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring and Benchmarking Large Language Models' Capabilities to
  Generate Persuasive Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17753v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17753v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amalie Brogaard Pauli, Isabelle Augenstein, Ira Assent
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We are exposed to much information trying to influence us, such as teaser
messages, debates, politically framed news, and propaganda - all of which use
persuasive language. With the recent interest in Large Language Models (LLMs),
we study the ability of LLMs to produce persuasive text. As opposed to prior
work which focuses on particular domains or types of persuasion, we conduct a
general study across various domains to measure and benchmark to what degree
LLMs produce persuasive language - both when explicitly instructed to rewrite
text to be more or less persuasive and when only instructed to paraphrase. We
construct the new dataset Persuasive-Pairs of pairs of a short text and its
rewrite by an LLM to amplify or diminish persuasive language. We multi-annotate
the pairs on a relative scale for persuasive language: a valuable resource in
itself, and for training a regression model to score and benchmark persuasive
language, including for new LLMs across domains. In our analysis, we find that
different 'personas' in LLaMA3's system prompt change persuasive language
substantially, even when only instructed to paraphrase.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deciphering Cross-Modal Alignment in Large <span class="highlight-title">Vision-Language</span> Models with
  Modality Integration Rate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qidong Huang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Dahua Lin, Weiming Zhang, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Modality Integration Rate (MIR), an effective, robust, and
generalized metric to indicate the multi-modal pre-training quality of Large
Vision Language Models (LVLMs). Large-scale pre-training plays a critical role
in building capable LVLMs, while evaluating its training quality without the
costly supervised fine-tuning stage is under-explored. Loss, perplexity, and
in-context evaluation results are commonly used pre-training metrics for Large
Language Models (LLMs), while we observed that these metrics are less
indicative when aligning a well-trained LLM with a new modality. Due to the
lack of proper metrics, the research of LVLMs in the critical pre-training
stage is hindered greatly, including the training data choice, efficient module
design, etc. In this paper, we propose evaluating the pre-training quality from
the inter-modal distribution distance perspective and present MIR, the Modality
Integration Rate, which is 1) \textbf{Effective} to represent the pre-training
quality and show a positive relation with the benchmark performance after
supervised fine-tuning. 2) \textbf{Robust} toward different training/evaluation
data. 3) \textbf{Generalize} across training configurations and architecture
choices. We conduct a series of pre-training experiments to explore the
effectiveness of MIR and observe satisfactory results that MIR is indicative
about training data selection, training strategy schedule, and model
architecture design to get better pre-training results. We hope MIR could be a
helpful metric for building capable LVLMs and inspire the following research
about modality alignment in different areas. Our code is at:
https://github.com/shikiw/Modality-Integration-Rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/shikiw/Modality-Integration-Rate</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Changes in Nation Perception with Nationality-Assigned
  Personas in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13993v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13993v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahammed Kamruzzaman, Gene Louis Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Persona assignment has become a common strategy for customizing LLM use to
particular tasks and contexts. In this study, we explore how evaluation of
different nations change when LLMs are assigned specific nationality personas.
We assign 193 different nationality personas (e.g., an American person) to four
LLMs and examine how the LLM evaluations (or ''perceptions'')of countries
change. We find that all LLM-persona combinations tend to favor Western
European nations, though nation-personas push LLM behaviors to focus more on
and treat the nation-persona's own region more favorably. Eastern European,
Latin American, and African nations are treated more negatively by different
nationality personas. We additionally find that evaluations by nation-persona
LLMs of other nations correlate with human survey responses but fail to match
the values closely. Our study provides insight into how biases and stereotypes
are realized within LLMs when adopting different national personas. In line
with the ''Blueprint for an AI Bill of Rights'', our findings underscore the
critical need for developing mechanisms to ensure that LLM outputs promote
fairness and avoid over-generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print, Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptation Odyssey in LLMs: Why Does Additional <span class="highlight-title">Pretrain</span>ing Sometimes
  Fail to Improve? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fırat Öncel, Matthias Bethge, Beyza Ermis, Mirco Ravanelli, Cem Subakan, Çağatay Yıldız
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last decade, the generalization and adaptation abilities of deep
learning models were typically evaluated on fixed training and test
distributions. Contrary to traditional deep learning, large language models
(LLMs) are (i) even more overparameterized, (ii) trained on unlabeled text
corpora curated from the Internet with minimal human intervention, and (iii)
trained in an online fashion. These stark contrasts prevent researchers from
transferring lessons learned on model generalization and adaptation in deep
learning contexts to LLMs. To this end, our short paper introduces empirical
observations that aim to shed light on further training of already pretrained
language models. Specifically, we demonstrate that training a model on a text
domain could degrade its perplexity on the test portion of the same domain. We
observe with our subsequent analysis that the performance degradation is
positively correlated with the similarity between the additional and the
original pretraining dataset of the LLM. Our further token-level perplexity
observations reveals that the perplexity degradation is due to a handful of
tokens that are not informative about the domain. We hope these findings will
guide us in determining when to adapt a model vs when to rely on its
foundational capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Infinite-Long Prefix in Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingyu Liang, Zhenmei Shi, Zhao Song, Chiwun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting and context-based fine-tuning methods, which we call Prefix
Learning, have been proposed to enhance the performance of language models on
various downstream tasks. They are empirically efficient and effective,
matching the performance of full parameter fine-tuning, but the theoretical
understandings are limited. In this paper, we aim to address this limitation by
studying their ability from the perspective of prefix length. In particular, we
provide a convergence guarantee for training an ultra-long prefix in a stylized
setting using the Neural Tangent Kernel (NTK) framework. Based on this strong
theoretical guarantee, we design and implement an algorithm that only needs to
introduce and fine-tune a few extra trainable parameters instead of an
infinite-long prefix in each layer of a transformer, and can approximate the
prefix attention to a guaranteed polynomial-small error. Preliminary
experimental results on vision, natural language, and math data show that our
method achieves superior or competitive performance compared to existing
methods like full parameters fine-tuning, P-Tuning V2, and LoRA. This
demonstrates our method is promising for parameter-efficient fine-tuning. Our
code can be found at
\url{https://github.com/ChristianYang37/chiwun/tree/main/src/NTK-Attention}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented
  Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minda Hu, Licheng Zong, Hongru Wang, Jingyan Zhou, Jingjing Li, Yichen Gao, Kam-Fai Wong, Yu Li, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown great potential in the biomedical
domain with the advancement of retrieval-augmented generation (RAG). However,
existing retrieval-augmented approaches face challenges in addressing diverse
queries and documents, particularly for medical knowledge queries, resulting in
sub-optimal performance. To address these limitations, we propose a novel
plug-and-play LLM-based retrieval method called Self-Rewarding Tree Search
(SeRTS) based on Monte Carlo Tree Search (MCTS) and a self-rewarding paradigm.
By combining the reasoning capabilities of LLMs with the effectiveness of tree
search, SeRTS boosts the zero-shot performance of retrieving high-quality and
informative results for RAG. We further enhance retrieval performance by
fine-tuning LLMs with Proximal Policy Optimization (PPO) objectives using the
trajectories collected by SeRTS as feedback. Controlled experiments using the
BioASQ-QA dataset with GPT-3.5-Turbo and LLama2-7b demonstrate that our method
significantly improves the performance of the BM25 retriever and surpasses the
strong baseline of self-reflection in both efficiency and scalability.
Moreover, SeRTS generates higher-quality feedback for PPO training than
self-reflection. Our proposed method effectively adapts LLMs to document
retrieval tasks, enhancing their ability to retrieve highly relevant documents
for RAG in the context of medical knowledge queries. This work presents a
significant step forward in leveraging LLMs for accurate and comprehensive
biomedical question answering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted by EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Assamese NLP Capabilities: Introducing a Centralized Dataset
  Repository 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. Tamang, D. J. Bora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a centralized, open-source dataset repository designed
to advance NLP and NMT for Assamese, a low-resource language. The repository,
available at GitHub, supports various tasks like sentiment analysis, named
entity recognition, and machine translation by providing both pre-training and
fine-tuning corpora. We review existing datasets, highlighting the need for
standardized resources in Assamese NLP, and discuss potential applications in
AI-driven research, such as LLMs, OCR, and chatbots. While promising,
challenges like data scarcity and linguistic diversity remain. The repository
aims to foster collaboration and innovation, promoting Assamese language
research in the digital age.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 table, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MERLIN: <span class="highlight-title">Multimodal</span> Embedding Refinement via LLM-based Iterative
  Navigation for Text-Video Retrieval-Rerank Pipeline <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghoon Han, Eunhwan Park, Gisang Lee, Adam Lee, Nojun Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid expansion of multimedia content has made accurately retrieving
relevant videos from large collections increasingly challenging. Recent
advancements in text-video retrieval have focused on cross-modal interactions,
large-scale foundation model training, and probabilistic modeling, yet often
neglect the crucial user perspective, leading to discrepancies between user
queries and the content retrieved. To address this, we introduce MERLIN
(Multimodal Embedding Refinement via LLM-based Iterative Navigation), a novel,
training-free pipeline that leverages Large Language Models (LLMs) for
iterative feedback learning. MERLIN refines query embeddings from a user
perspective, enhancing alignment between queries and video content through a
dynamic question answering process. Experimental results on datasets like
MSR-VTT, MSVD, and ActivityNet demonstrate that MERLIN substantially improves
Recall@1, outperforming existing systems and confirming the benefits of
integrating LLMs into multimodal retrieval systems for more responsive and
context-aware multimedia retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Industry Track Accepted (Camera-Ready Version)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAD: Personalized Alignment at Decoding-Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04070v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04070v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhe Chen, Xiaotian Zhang, Meng Luo, Wenhao Chai, Zuozhu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning with personalized preferences, which vary significantly across
cultural, educational, and political differences, poses a significant challenge
due to the computational costs and data demands of traditional alignment
methods. In response, this paper presents Personalized Alignment at
Decoding-time (PAD), a novel framework designed to align LLM outputs with
diverse personalized preferences during the inference phase, eliminating the
need for additional training. By introducing a unique personalized reward
modeling strategy, this framework decouples the text generation process from
personalized preferences, facilitating the generation of generalizable
token-level personalized rewards. The PAD algorithm leverages these rewards to
guide the decoding process, dynamically tailoring the base model's predictions
to personalized preferences. Extensive experimental results demonstrate that
PAD not only outperforms existing training-based alignment methods in terms of
aligning with diverse preferences but also shows significant generalizability
to preferences unseen during training and scalability across different base
models. This work advances the capability of LLMs to meet user needs in
real-time applications, presenting a substantial step forward in personalized
LLM alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper presents Personalized Alignment at Decoding-time (PAD), a
  novel framework designed to align LLM outputs with diverse personalized
  preferences during the inference phase</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $α$-DPO: Adaptive Reward Margin is What Direct Preference
  Optimization Needs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10148v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10148v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junkang Wu, Xue Wang, Zhengyi Yang, Jiancan Wu, Jinyang Gao, Bolin Ding, Xiang Wang, Rong Jin, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models (LLMs) with human values and intentions is
crucial for their utility, honesty, and safety. Reinforcement learning from
human feedback (RLHF) is a popular approach to achieve this alignment, but it
faces challenges in computational efficiency and training stability. Recent
methods like Direct Preference Optimization (DPO) and Simple Preference
Optimization (SimPO) have proposed offline alternatives to RLHF, simplifying
the process by reparameterizing the reward function. However, DPO depends on a
potentially suboptimal reference model, and SimPO's assumption of a fixed
target reward margin may lead to suboptimal decisions in diverse data settings.
In this work, we propose $\alpha$-DPO, an adaptive preference optimization
algorithm designed to address these limitations by introducing a dynamic reward
margin. Specifically, $\alpha$-DPO employs an adaptive preference distribution,
balancing the policy model and the reference model to achieve personalized
reward margins. We provide theoretical guarantees for $\alpha$-DPO,
demonstrating its effectiveness as a surrogate optimization objective and its
ability to balance alignment and diversity through KL divergence control.
Empirical evaluations on AlpacaEval 2 and Arena-Hard show that $\alpha$-DPO
consistently outperforms DPO and SimPO across various model settings,
establishing it as a robust approach for fine-tuning LLMs. Our method achieves
significant improvements in win rates, highlighting its potential as a powerful
tool for LLM alignment. The code is available at
https://github.com/junkangwu/alpha-DPO
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instruction Tuning for Large Language Models: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10792v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10792v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper surveys research works in the quickly advancing field of
instruction tuning (IT), a crucial technique to enhance the capabilities and
controllability of large language models (LLMs). Instruction tuning refers to
the process of further training LLMs on a dataset consisting of
\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the
gap between the next-word prediction objective of LLMs and the users' objective
of having LLMs adhere to human instructions. In this work, we make a systematic
review of the literature, including the general methodology of IT, the
construction of IT datasets, the training of IT models, and applications to
different modalities, domains and applications, along with an analysis on
aspects that influence the outcome of IT (e.g., generation of instruction
outputs, size of the instruction dataset, etc). We also review the potential
pitfalls of IT along with criticism against it, along with efforts pointing out
current deficiencies of existing strategies and suggest some avenues for
fruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>V3; Last update: Oct 16, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Power of Source: Source-based Minimum Bayes Risk Decoding
  for Neural Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11632v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11632v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxuan Lyu, Hidetaka Kamigaito, Kotaro Funakoshi, Manabu Okumura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Maximum a posteriori decoding, a commonly used method for neural machine
translation (NMT), aims to maximize the estimated posterior probability.
However, high estimated probability does not always lead to high translation
quality. Minimum Bayes Risk (MBR) decoding (\citealp{kumar2004minimum}) offers
an alternative by seeking hypotheses with the highest expected utility. In this
paper, we show that Quality Estimation (QE) reranking
(\citealp{fernandes-etal-2022-quality}), which uses a QE model as a reranker,
can be viewed as a variant of MBR. Inspired by this, we propose source-based
MBR (sMBR) decoding, a novel approach that utilizes synthetic sources
(generated via back-translation or paraphrasing) as ``support hypotheses'' and
a reference-free quality estimation metric as the utility function, marking the
first work to solely use sources in MBR decoding. Experiments show that sMBR
outperforms QE reranking and the standard MBR decoding. Our findings suggest
that sMBR is a promising approach for NMT decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent
  Approach <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06949v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06949v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanming Zhang, Yuxuan Chen, Yuan Yuan, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real world software development, improper or missing exception handling
can severely impact the robustness and reliability of code. Exception handling
mechanisms require developers to detect, capture, and manage exceptions
according to high standards, but many developers struggle with these tasks,
leading to fragile code. This problem is particularly evident in open source
projects and impacts the overall quality of the software ecosystem. To address
this challenge, we explore the use of large language models (LLMs) to improve
exception handling in code. Through extensive analysis, we identify three key
issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception
Types, and Distorted Handling Solutions. These problems are widespread across
real world repositories, suggesting that robust exception handling practices
are often overlooked or mishandled. In response, we propose Seeker, a multi
agent framework inspired by expert developer strategies for exception handling.
Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist
LLMs in detecting, capturing, and resolving exceptions more effectively. Our
work is the first systematic study on leveraging LLMs to enhance exception
handling practices, providing valuable insights for future improvements in code
reliability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 7 figures. Submitted ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating the Transferability of Code Repair for Low-Resource
  Programming Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14867v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14867v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Wong, Alfonso Amayuelas, Liangming Pan, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable performance on code
generation tasks. A recent use case is iterative code repair, where an LLM
fixes an incorrect program by rationalizing about errors and generating new
code. Recent works augment the code repair process by integrating modern
techniques such as chain-of-thought reasoning or distillation, but only study
their benefits on high-resource languages like Python, and ignore low-resource
languages like Perl. To address this gap of knowledge, we investigate the
benefits of distilling code repair for both high and low resource languages to
determine if the techniques that are effective in a high resource setting are
also applicable in a low resource setting. Our evaluation shows that distilling
the ability to repair code has language dependent benefits. To explain this
behavior, we perform a further analysis and find that contrary to preexisting
beliefs, the correlation between reasoning ability and code correction ability
is weak. We hypothesize this weak correlation is magnified in low-resource
settings where base models lack deep knowledge of a programming language,
leading to wavering benefits of code repair.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield
  Better Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06554v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06554v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjun Chen, Dawei Zhu, Yirong Sun, Xinghao Chen, Wei Zhang, Xiaoyu Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback significantly enhances Natural
Language Processing by aligning language models with human expectations. A
critical factor in this alignment is the strength of reward models used during
training. This study explores whether stronger reward models invariably lead to
better language models. In this paper, through experiments on relevance,
factuality, and completeness tasks using the QA-FEEDBACK dataset and reward
models based on Longformer, we uncover a surprising paradox: language models
trained with moderately accurate reward models outperform those guided by
highly accurate ones. This challenges the widely held belief that stronger
reward models always lead to better language models, and opens up new avenues
for future research into the key factors driving model performance and how to
choose the most suitable reward models. Code and additional details are
available at https://github.com/EIT-NLP/AccuracyParadox-RLHF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 27 figures (including 18 in the appendix), submitted to
  EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human
  Belief Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17232v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17232v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun-Shiuan Chuang, Krirk Nirunwiroj, Zach Studdiford, Agam Goyal, Vincent V. Frigo, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T. Rogers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating human-like large language model (LLM) agents is crucial for faithful
social simulation. Having LLMs role-play based on demographic information
sometimes improves human likeness but often does not. This study assessed
whether LLM alignment with human behavior can be improved by integrating
information from empirically-derived human belief networks. Using data from a
human survey, we estimated a belief network encompassing 64 topics loading on
nine non-overlapping latent factors. We then seeded LLM-based agents with an
opinion on one topic, and assessed the alignment of its expressed opinions on
remaining test topics with corresponding human data. Role-playing based on
demographic information alone did not align LLM and human opinions, but seeding
the agent with a single belief greatly improved alignment for topics related in
the belief network, and not for topics outside the network. These results
suggest a novel path for human-LLM belief alignment in work seeking to simulate
and understand patterns of belief distributions in society.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Breaking Language Barriers in Multilingual Mathematical Reasoning:
  Insights and Observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.20246v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.20246v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuo Chen, Zinan Zheng, Ning Wu, Ming Gong, Dongmei Zhang, Jia Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing research predominantly focuses on developing powerful language
learning models (LLMs) for mathematical reasoning within monolingual languages,
with few explorations in preserving efficacy in a multilingual context. To
bridge this gap, this paper pioneers exploring and training powerful
Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we
construct the first multilingual math reasoning instruction dataset,
MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue
of training data scarcity in xMR tasks. Based on the collected dataset, we
propose different training strategies to build powerful xMR LLMs, named
MathOctopus, notably outperform conventional open-source LLMs and exhibit
superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B
reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond
remarkable results, we unearth several pivotal observations and insights from
extensive experiments: (1) When extending the rejection sampling strategy to
the multilingual context, it proves effective for model performances, albeit
limited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT)
across multiple languages not only significantly enhances model performance
multilingually but also elevates their monolingual performance. This indicates
that crafting multilingual corpora can be regarded as a vital strategy for
enhancing model performance in a specific language, especially in mathematical
reasoning tasks. For instance, MathOctopus-7B improves its counterparts that
trained on English from 42.2% to 50.8% on GSM8K testset. Codes are available at
https://github.com/microsoft/MathOctopus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainable Natural Language Processing for Corporate Sustainability
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17487v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17487v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keane Ong, Rui Mao, Ranjan Satapathy, Ricardo Shirota Filho, Erik Cambria, Johan Sulaeman, Gianmarco Mengaldo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sustainability commonly refers to entities, such as individuals, companies,
and institutions, having a non-detrimental (or even positive) impact on the
environment, society, and the economy. With sustainability becoming a synonym
of acceptable and legitimate behaviour, it is being increasingly demanded and
regulated. Several frameworks and standards have been proposed to measure the
sustainability impact of corporations, including United Nations' sustainable
development goals and the recently introduced global sustainability reporting
framework, amongst others. However, the concept of corporate sustainability is
complex due to the diverse and intricate nature of firm operations (i.e.
geography, size, business activities, interlinks with other stakeholders). As a
result, corporate sustainability assessments are plagued by subjectivity both
within data that reflect corporate sustainability efforts (i.e. corporate
sustainability disclosures) and the analysts evaluating them. This subjectivity
can be distilled into distinct challenges, such as incompleteness, ambiguity,
unreliability and sophistication on the data dimension, as well as limited
resources and potential bias on the analyst dimension. Put together,
subjectivity hinders effective cost attribution to entities non-compliant with
prevailing sustainability expectations, potentially rendering sustainability
efforts and its associated regulations futile. To this end, we argue that
Explainable Natural Language Processing (XNLP) can significantly enhance
corporate sustainability analysis. Specifically, linguistic understanding
algorithms (lexical, semantic, syntactic), integrated with XAI capabilities
(interpretability, explainability, faithfulness), can bridge gaps in analyst
resources and mitigate subjectivity problems within data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JOOCI: a Framework for Learning Comprehensive Speech Representations <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hemant Yadav, Rajiv Ratn Shah, Sunayana Sitaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information in speech can be divided into two categories: what is being said
(content) and how it is expressed (other). Current state-of-the-art (SOTA)
techniques model speech at fixed segments, usually 10-25 ms, using a single
embedding. Given the orthogonal nature of other and content information,
attempting to optimize both within a single embedding results in suboptimal
solutions. This approach divides the models capacity, limiting its ability to
build complex hierarchical features effectively. In this work, we present an
end-to-end speech representation learning framework designed to jointly
optimize the other and content information (JOOCI) in speech. By using separate
learnable parameters, JOOCI addresses this optimization challenge by modeling
other and content information independently. Our results show that JOOCI
consistently outperforms other SOTA models of similar size (100 million
parameters) and pre-training data used (960 hours) by a significant margin when
evaluated on a range of speech downstream tasks in the SUPERB benchmark, as
shown in Table 1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Translation Canvas: An Explainable Interface to Pinpoint and Analyze
  Translation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10861v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10861v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmay Dandekar, Wenda Xu, Xi Xu, Siqi Ouyang, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of machine translation research, evaluation
toolkits have become essential for benchmarking system progress. Tools like
COMET and SacreBLEU offer single quality score assessments that are effective
for pairwise system comparisons. However, these tools provide limited insights
for fine-grained system-level comparisons and the analysis of instance-level
defects. To address these limitations, we introduce Translation Canvas, an
explainable interface designed to pinpoint and analyze translation systems'
performance: 1) Translation Canvas assists machine translation researchers in
comprehending system-level model performance by identifying common errors
(their frequency and severity) and analyzing relationships between different
systems based on various evaluation metrics. 2) It supports fine-grained
analysis by highlighting error spans with explanations and selectively
displaying systems' predictions. According to human evaluation, Translation
Canvas demonstrates superior performance over COMET and SacreBLEU packages
under enjoyability and understandability criteria.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering Elementary Discourse Units in Textual Data Using Canonical
  Correlation Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akanksha Mehndiratta, Krishna Asawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Canonical Correlation Analysis (CCA) has been exploited immensely for
learning latent representations in various fields. This study takes a step
further by demonstrating the potential of CCA in identifying Elementary
Discourse Units(EDUs) that captures the latent information within the textual
data. The probabilistic interpretation of CCA discussed in this study utilizes
the two-view nature of textual data, i.e. the consecutive sentences in a
document or turns in a dyadic conversation, and has a strong theoretical
foundation. Furthermore, this study proposes a model for Elementary Discourse
Unit(EDU) segmentation that discovers EDUs in textual data without any
supervision. To validate the model, the EDUs are utilized as textual unit for
content selection in textual similarity task. Empirical results on Semantic
Textual Similarity(STSB) and Mohler datasets confirm that, despite represented
as a unigram, the EDUs deliver competitive results and can even beat various
sophisticated supervised techniques. The model is simple, linear, adaptable and
language independent making it an ideal baseline particularly when labeled
training data is scarce or nonexistent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Circuits in <span class="highlight-title">Pretrain</span>ed Transformers <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable capabilities of modern large language models are rooted in
their vast repositories of knowledge encoded within their parameters, enabling
them to perceive the world and engage in reasoning. The inner workings of how
these models store knowledge have long been a subject of intense interest and
investigation among researchers. To date, most studies have concentrated on
isolated components within these models, such as the Multilayer Perceptrons and
attention head. In this paper, we delve into the computation graph of the
language model to uncover the knowledge circuits that are instrumental in
articulating specific knowledge. The experiments, conducted with GPT2 and
TinyLLAMA, have allowed us to observe how certain information heads, relation
heads, and Multilayer Perceptrons collaboratively encode knowledge within the
model. Moreover, we evaluate the impact of current knowledge editing techniques
on these knowledge circuits, providing deeper insights into the functioning and
constraints of these editing methodologies. Finally, we utilize knowledge
circuits to analyze and interpret language model behaviors such as
hallucinations and in-context learning. We believe the knowledge circuits hold
potential for advancing our understanding of Transformers and guiding the
improved design of knowledge editing. Code and data are available in
https://github.com/zjunlp/KnowledgeCircuits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024, 32 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Examining Long-Context Large Language Models for Environmental Review
  Document Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07321v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07321v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hung Phan, Anurag Acharya, Rounak Meyur, Sarthak Chaturvedi, Shivam Sharma, Mike Parker, Dan Nally, Ali Jannesari, Karl Pazdernik, Mahantesh Halappanavar, Sai Munikoti, Sameera Horawalavithana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As LLMs become increasingly ubiquitous, researchers have tried various
techniques to augment the knowledge provided to these models. Long context and
retrieval-augmented generation (RAG) are two such methods that have recently
gained popularity. In this work, we examine the benefits of both of these
techniques by utilizing question answering (QA) task in a niche domain. While
the effectiveness of LLM-based QA systems has already been established at an
acceptable level in popular domains such as trivia and literature, it has not
often been established in niche domains that traditionally require specialized
expertise. We construct the NEPAQuAD1.0 benchmark to evaluate the performance
of five long-context LLMs -- Claude Sonnet, Gemini, GPT-4, Llama 3.1, and
Mistral -- when answering questions originating from Environmental Impact
Statements prepared by U.S. federal government agencies in accordance with the
National Environmental Environmental Act (NEPA). We specifically measure the
ability of LLMs to understand the nuances of legal, technical, and
compliance-related information present in NEPA documents in different
contextual scenarios. We test the LLMs' internal prior NEPA knowledge by
providing questions without any context, as well as assess how LLMs synthesize
the contextual information present in long NEPA documents to facilitate the
question/answering task. We compare the performance of the models in handling
different types of questions (e.g., problem-solving, divergent, etc.). Our
results suggest that RAG powered models significantly outperform those provided
with only the PDF context in terms of answer accuracy, regardless of the choice
of the LLM. Our further analysis reveals that many models perform better
answering closed type questions (Yes/No) than divergent and problem-solving
questions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">139</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Prototype Evolving for Test-Time Generalization of <span class="highlight-title">Vision-Language</span>
  Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ce Zhang, Simon Stepputtis, Katia Sycara, Yaqi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time adaptation, which enables models to generalize to diverse data with
unlabeled test samples, holds significant value in real-world scenarios.
Recently, researchers have applied this setting to advanced pre-trained
vision-language models (VLMs), developing approaches such as test-time prompt
tuning to further extend their practical applicability. However, these methods
typically focus solely on adapting VLMs from a single modality and fail to
accumulate task-specific knowledge as more samples are processed. To address
this, we introduce Dual Prototype Evolving (DPE), a novel test-time adaptation
approach for VLMs that effectively accumulates task-specific knowledge from
multi-modalities. Specifically, we create and evolve two sets of
prototypes--textual and visual--to progressively capture more accurate
multi-modal representations for target classes during test time. Moreover, to
promote consistent multi-modal representations, we introduce and optimize
learnable residuals for each test sample to align the prototypes from both
modalities. Extensive experimental results on 15 benchmark datasets demonstrate
that our proposed DPE consistently outperforms previous state-of-the-art
methods while also exhibiting competitive computational efficiency. Code is
available at https://github.com/zhangce01/DPE-CLIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. Project page:
  https://zhangce01.github.io/DPE-CLIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Curse of <span class="highlight-title">Multi-Modal</span>ities: Evaluating Hallucinations of Large
  <span class="highlight-title">Multimodal</span> Models across Language, Visual, and Audio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicong Leng, Yun Xing, Zesen Cheng, Yang Zhou, Hang Zhang, Xin Li, Deli Zhao, Shijian Lu, Chunyan Miao, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large multimodal models (LMMs) have significantly
enhanced performance across diverse tasks, with ongoing efforts to further
integrate additional modalities such as video and audio. However, most existing
LMMs remain vulnerable to hallucinations, the discrepancy between the factual
multimodal input and the generated textual output, which has limited their
applicability in various real-world scenarios. This paper presents the first
systematic investigation of hallucinations in LMMs involving the three most
common modalities: language, visual, and audio. Our study reveals two key
contributors to hallucinations: overreliance on unimodal priors and spurious
inter-modality correlations. To address these challenges, we introduce the
benchmark The Curse of Multi-Modalities (CMM), which comprehensively evaluates
hallucinations in LMMs, providing a detailed analysis of their underlying
issues. Our findings highlight key vulnerabilities, including imbalances in
modality integration and biases from training data, underscoring the need for
balanced cross-modal learning and enhanced hallucination mitigation strategies.
Based on our observations and findings, we suggest potential research
directions that could enhance the reliability of LMMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: cmm-damovl.site</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage
  Gaussian Splats 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Ziwen, Hao Tan, Kai Zhang, Sai Bi, Fujun Luan, Yicong Hong, Li Fuxin, Zexiang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Long-LRM, a generalizable 3D Gaussian reconstruction model that is
capable of reconstructing a large scene from a long sequence of input images.
Specifically, our model can process 32 source images at 960x540 resolution
within only 1.3 seconds on a single A100 80G GPU. Our architecture features a
mixture of the recent Mamba2 blocks and the classical transformer blocks which
allowed many more tokens to be processed than prior work, enhanced by efficient
token merging and Gaussian pruning steps that balance between quality and
efficiency. Unlike previous feed-forward models that are limited to processing
1~4 input images and can only reconstruct a small portion of a large scene,
Long-LRM reconstructs the entire scene in a single feed-forward step. On
large-scale scene datasets such as DL3DV-140 and Tanks and Temples, our method
achieves performance comparable to optimization-based approaches while being
two orders of magnitude more efficient. Project page:
https://arthurhero.github.io/projects/llrm
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned
  Concepts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongcheng Gao, Tianyu Pang, Chao Du, Taihang Hu, Zhijie Deng, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid progress of diffusion-based content generation, significant
efforts are being made to unlearn harmful or copyrighted concepts from
pretrained diffusion models (DMs) to prevent potential model misuse. However,
it is observed that even when DMs are properly unlearned before release,
malicious finetuning can compromise this process, causing DMs to relearn the
unlearned concepts. This occurs partly because certain benign concepts (e.g.,
"skin") retained in DMs are related to the unlearned ones (e.g., "nudity"),
facilitating their relearning via finetuning. To address this, we propose
meta-unlearning on DMs. Intuitively, a meta-unlearned DM should behave like an
unlearned DM when used as is; moreover, if the meta-unlearned DM undergoes
malicious finetuning on unlearned concepts, the related benign concepts
retained within it will be triggered to self-destruct, hindering the relearning
of unlearned concepts. Our meta-unlearning framework is compatible with most
existing unlearning methods, requiring only the addition of an
easy-to-implement meta objective. We validate our approach through empirical
experiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4
and SDXL), supported by extensive ablation studies. Our code is available at
https://github.com/sail-sg/Meta-Unlearning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Zero-Shot Camera Trap Image Categorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiří Vyskočil, Lukas Picek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the search for an alternative approach to the automatic
categorization of camera trap images. First, we benchmark state-of-the-art
classifiers using a single model for all images. Next, we evaluate methods
combining MegaDetector with one or more classifiers and Segment Anything to
assess their impact on reducing location-specific overfitting. Last, we propose
and test two approaches using large language and foundational models, such as
DINOv2, BioCLIP, BLIP, and ChatGPT, in a zero-shot scenario. Evaluation carried
out on two publicly available datasets (WCT from New Zealand, CCT20 from the
Southwestern US) and a private dataset (CEF from Central Europe) revealed that
combining MegaDetector with two separate classifiers achieves the highest
accuracy. This approach reduced the relative error of a single BEiTV2
classifier by approximately 42\% on CCT20, 48\% on CEF, and 75\% on WCT.
Besides, as the background is removed, the error in terms of accuracy in new
locations is reduced to half. The proposed zero-shot pipeline based on DINOv2
and FAISS achieved competitive results (1.0\% and 4.7\% smaller on CCT20, and
CEF, respectively), which highlights the potential of zero-shot approaches for
camera trap image categorization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gravity-aligned Rotation Averaging with Circular Regression <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linfei Pan, Marc Pollefeys, Dániel Baráth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing a 3D scene from unordered images is pivotal in computer vision
and robotics, with applications spanning crowd-sourced mapping and beyond.
While global Structure-from-Motion (SfM) techniques are scalable and fast, they
often compromise on accuracy. To address this, we introduce a principled
approach that integrates gravity direction into the rotation averaging phase of
global pipelines, enhancing camera orientation accuracy and reducing the
degrees of freedom. This additional information is commonly available in recent
consumer devices, such as smartphones, mixed-reality devices and drones, making
the proposed method readily accessible. Rooted in circular regression, our
algorithm has similar convergence guarantees as linear regression. It also
supports scenarios where only a subset of cameras have known gravity.
Additionally, we propose a mechanism to refine error-prone gravity. We achieve
state-of-the-art accuracy on four large-scale datasets. Particularly, the
proposed method improves upon the SfM baseline by 13 AUC@$1^\circ$ points, on
average, while running eight times faster. It also outperforms the standard
planar pose graph optimization technique by 23 AUC@$1^\circ$ points. The code
is at https://github.com/colmap/glomap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at ECCV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And
  Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehong Yoon, Shoubin Yu, Vaidehi Patil, Huaxiu Yao, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in diffusion models have significantly enhanced their ability
to generate high-quality images and videos, but they have also increased the
risk of producing unsafe content. Existing unlearning/editing-based methods for
safe generation remove harmful concepts from models but face several
challenges: (1) They cannot instantly remove harmful concepts without training.
(2) Their safe generation capabilities depend on collected training data. (3)
They alter model weights, risking degradation in quality for content unrelated
to toxic concepts. To address these, we propose SAFREE, a novel, training-free
approach for safe T2I and T2V, that does not alter the model's weights.
Specifically, we detect a subspace corresponding to a set of toxic concepts in
the text embedding space and steer prompt embeddings away from this subspace,
thereby filtering out harmful content while preserving intended semantics. To
balance the trade-off between filtering toxicity and preserving safe concepts,
SAFREE incorporates a novel self-validating filtering mechanism that
dynamically adjusts the denoising steps when applying the filtered embeddings.
Additionally, we incorporate adaptive re-attention mechanisms within the
diffusion latent space to selectively diminish the influence of features
related to toxic concepts at the pixel level. In the end, SAFREE ensures
coherent safety checking, preserving the fidelity, quality, and safety of the
output. SAFREE achieves SOTA performance in suppressing unsafe content in T2I
generation compared to training-free baselines and effectively filters targeted
concepts while maintaining high-quality images. It also shows competitive
results against training-based methods. We extend SAFREE to various T2I
backbones and T2V tasks, showcasing its flexibility and generalization. SAFREE
provides a robust and adaptable safeguard for ensuring safe visual generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally; Project page:
  https://safree-safe-t2i-t2v.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PND-Net: Plant Nutrition Deficiency and Disease Classification using
  Graph Convolutional Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asish Bera, Debotosh Bhattacharjee, Ondrej Krejcar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crop yield production could be enhanced for agricultural growth if various
plant nutrition deficiencies, and diseases are identified and detected at early
stages. The deep learning methods have proven its superior performances in the
automated detection of plant diseases and nutrition deficiencies from visual
symptoms in leaves. This article proposes a new deep learning method for plant
nutrition deficiencies and disease classification using a graph convolutional
network (GNN), added upon a base convolutional neural network (CNN). Sometimes,
a global feature descriptor might fail to capture the vital region of a
diseased leaf, which causes inaccurate classification of disease. To address
this issue, regional feature learning is crucial for a holistic feature
aggregation. In this work, region-based feature summarization at multi-scales
is explored using spatial pyramidal pooling for discriminative feature
representation. A GCN is developed to capacitate learning of finer details for
classifying plant diseases and insufficiency of nutrients. The proposed method,
called Plant Nutrition Deficiency and Disease Network (PND-Net), is evaluated
on two public datasets for nutrition deficiency, and two for disease
classification using four CNNs. The best classification performances are: (a)
90.00% Banana and 90.54% Coffee nutrition deficiency; and (b) 96.18% Potato
diseases and 84.30% on PlantDoc datasets using Xception backbone. Furthermore,
additional experiments have been carried out for generalization, and the
proposed method has achieved state-of-the-art performances on two public
datasets, namely the Breast Cancer Histopathology Image Classification
(BreakHis 40X: 95.50%, and BreakHis 100X: 96.79% accuracy) and Single cells in
Pap smear images for cervical cancer classification (SIPaKMeD: 99.18%
accuracy). Also, PND-Net achieves improved performances using five-fold cross
validation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing 3D Geometry Reconstruction from Implicit Neural
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shen Fan, Przemyslaw Musialski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit neural representations have emerged as a powerful tool in learning
3D geometry, offering unparalleled advantages over conventional representations
like mesh-based methods. A common type of INR implicitly encodes a shape's
boundary as the zero-level set of the learned continuous function and learns a
mapping from a low-dimensional latent space to the space of all possible shapes
represented by its signed distance function. However, most INRs struggle to
retain high-frequency details, which are crucial for accurate geometric
depiction, and they are computationally expensive. To address these
limitations, we present a novel approach that both reduces computational
expenses and enhances the capture of fine details. Our method integrates
periodic activation functions, positional encodings, and normals into the
neural network architecture. This integration significantly enhances the
model's ability to learn the entire space of 3D shapes while preserving
intricate details and sharp features, areas where conventional representations
often fall short.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAFA-Net: Region Attention Network For Food Items And Agricultural
  Stress Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asish Bera, Ondrej Krejcar, Debotosh Bhattacharjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Convolutional Neural Networks (CNNs) have facilitated remarkable success
in recognizing various food items and agricultural stress. A decent performance
boost has been witnessed in solving the agro-food challenges by mining and
analyzing of region-based partial feature descriptors. Also, computationally
expensive ensemble learning schemes using multiple CNNs have been studied in
earlier works. This work proposes a region attention scheme for modelling
long-range dependencies by building a correlation among different regions
within an input image. The attention method enhances feature representation by
learning the usefulness of context information from complementary regions.
Spatial pyramidal pooling and average pooling pair aggregate partial
descriptors into a holistic representation. Both pooling methods establish
spatial and channel-wise relationships without incurring extra parameters. A
context gating scheme is applied to refine the descriptiveness of weighted
attentional features, which is relevant for classification. The proposed Region
Attention network for Food items and Agricultural stress recognition method,
dubbed RAFA-Net, has been experimented on three public food datasets, and has
achieved state-of-the-art performances with distinct margins. The highest top-1
accuracies of RAFA-Net are 91.69%, 91.56%, and 96.97% on the UECFood-100,
UECFood-256, and MAFood-121 datasets, respectively. In addition, better
accuracies have been achieved on two benchmark agricultural stress datasets.
The best top-1 accuracies on the Insect Pest (IP-102) and PlantDoc-27 plant
disease datasets are 92.36%, and 85.54%, respectively; implying RAFA-Net's
generalization capability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WorldCuisines: A Massive-Scale Benchmark for Multilingual and
  Multicultural Visual Question Answering on Global Cuisines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genta Indra Winata, Frederikus Hudi, Patrick Amadeus Irawan, David Anugraha, Rifki Afina Putri, Yutong Wang, Adam Nohejl, Ubaidillah Ariq Prathama, Nedjma Ousidhoum, Afifa Amriani, Anar Rzayev, Anirban Das, Ashmari Pramodya, Aulia Adila, Bryan Wilie, Candy Olivia Mawalim, Ching Lam Cheng, Daud Abolade, Emmanuele Chersoni, Enrico Santus, Fariz Ikhwantri, Garry Kuwanto, Hanyang Zhao, Haryo Akbarianto Wibowo, Holy Lovenia, Jan Christian Blaise Cruz, Jan Wira Gotama Putra, Junho Myung, Lucky Susanto, Maria Angelica Riera Machin, Marina Zhukova, Michael Anugraha, Muhammad Farid Adilazuarda, Natasha Santosa, Peerat Limkonchotiwat, Raj Dabre, Rio Alexander Audino, Samuel Cahyawijaya, Shi-Xiong Zhang, Stephanie Yulia Salim, Yi Zhou, Yinxuan Gui, David Ifeoluwa Adelani, En-Shiun Annie Lee, Shogo Okada, Ayu Purwarianti, Alham Fikri Aji, Taro Watanabe, Derry Tanti Wijaya, Alice Oh, Chong-Wah Ngo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) often struggle with culture-specific knowledge,
particularly in languages other than English and in underrepresented cultural
contexts. To evaluate their understanding of such knowledge, we introduce
WorldCuisines, a massive-scale benchmark for multilingual and multicultural,
visually grounded language understanding. This benchmark includes a visual
question answering (VQA) dataset with text-image pairs across 30 languages and
dialects, spanning 9 language families and featuring over 1 million data
points, making it the largest multicultural VQA benchmark to date. It includes
tasks for identifying dish names and their origins. We provide evaluation
datasets in two sizes (12k and 60k instances) alongside a training dataset (1
million instances). Our findings show that while VLMs perform better with
correct location context, they struggle with adversarial contexts and
predicting specific regional cuisines and languages. To support future
research, we release a knowledge base with annotated food entries and images
along with the VQA data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via
  Lightweight Value Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingqi Wang, Xiaoyuan Yi, Xing Xie, Jia Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion models trained on large-scale data have
enabled the generation of indistinguishable human-level images, yet they often
produce harmful content misaligned with human values, e.g., social bias, and
offensive content. Despite extensive research on Large Language Models (LLMs),
the challenge of Text-to-Image (T2I) model alignment remains largely
unexplored. Addressing this problem, we propose LiVO (Lightweight Value
Optimization), a novel lightweight method for aligning T2I models with human
values. LiVO only optimizes a plug-and-play value encoder to integrate a
specified value principle with the input prompt, allowing the control of
generated images over both semantics and values. Specifically, we design a
diffusion model-tailored preference optimization loss, which theoretically
approximates the Bradley-Terry model used in LLM alignment but provides a more
flexible trade-off between image quality and value conformity. To optimize the
value encoder, we also develop a framework to automatically construct a
text-image preference dataset of 86k (prompt, aligned image, violating image,
value principle) samples. Without updating most model parameters and through
adaptive value selection from the input prompt, LiVO significantly reduces
harmful outputs and achieves faster convergence, surpassing several strong
baselines and taking an initial step towards ethically aligned T2I models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Multimedia 2024. The dataset and code can be found at
  https://github.com/achernarwang/LiVO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaptiveDrag: Semantic-Driven Dragging on Diffusion-Based Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        DuoSheng Chen, Binghui Chen, Yifeng Geng, Liefeng Bo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, several point-based image editing methods (e.g., DragDiffusion,
FreeDrag, DragNoise) have emerged, yielding precise and high-quality results
based on user instructions. However, these methods often make insufficient use
of semantic information, leading to less desirable results. In this paper, we
proposed a novel mask-free point-based image editing method, AdaptiveDrag,
which provides a more flexible editing approach and generates images that
better align with user intent. Specifically, we design an auto mask generation
module using super-pixel division for user-friendliness. Next, we leverage a
pre-trained diffusion model to optimize the latent, enabling the dragging of
features from handle points to target points. To ensure a comprehensive
connection between the input image and the drag process, we have developed a
semantic-driven optimization. We design adaptive steps that are supervised by
the positions of the points and the semantic regions derived from super-pixel
segmentation. This refined optimization process also leads to more realistic
and accurate drag results. Furthermore, to address the limitations in the
generative consistency of the diffusion model, we introduce an innovative
corresponding loss during the sampling process. Building on these effective
designs, our method delivers superior generation results using only the single
input image and the handle-target point pairs. Extensive experiments have been
conducted and demonstrate that the proposed method outperforms others in
handling various drag instructions (e.g., resize, movement, extension) across
different domains (e.g., animals, human face, land space, clothing).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiCamCows2024 -- A Multi-view Image Dataset for AI-driven
  Holstein-Friesian Cattle Re-Identification on a Working Farm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phoenix Yu, Tilo Burghardt, Andrew W Dowsey, Neill W Campbell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present MultiCamCows2024, a farm-scale image dataset filmed across
multiple cameras for the biometric identification of individual
Holstein-Friesian cattle exploiting their unique black and white coat-patterns.
Captured by three ceiling-mounted visual sensors covering adjacent barn areas
over seven days on a working dairy farm, the dataset comprises 101, 329 images
of 90 cows, plus the underlying original CCTV footage. The dataset is provided
alongside full computer vision recognition baselines, that is both a supervised
and self-supervised learning framework for individual cow identification
trained on cattle tracklets. We report a performance above 96% single image
identification accuracy from the dataset and demonstrate that combining data
from multiple cameras during learning enhances self-supervised identification.
We show that our framework enables fully automatic cattle identification,
barring only the simple human verification of tracklet integrity during data
collection. Crucially, our study highlights that multi-camera, supervised and
self-supervised components in tandem not only deliver highly accurate
individual cow identification but also achieve this efficiently with no
labelling of cattle identities by humans at all. We argue that this improvement
in efficacy has practical implications for livestock management, behaviour
analysis, and agricultural monitoring. For full reproducibility and practical
ease of use, we publish all key software and code including re-identification
components and the species detector with this paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VividMed: <span class="highlight-title">Vision Language</span> Model with Versatile Visual Grounding for
  Medicine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingxiao Luo, Bingda Tang, Xuanzhong Chen, Rong Han, Ting Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Vision Language Models (VLMs) have demonstrated
remarkable promise in generating visually grounded responses. However, their
application in the medical domain is hindered by unique challenges. For
instance, most VLMs rely on a single method of visual grounding, whereas
complex medical tasks demand more versatile approaches. Additionally, while
most VLMs process only 2D images, a large portion of medical images are 3D. The
lack of medical data further compounds these obstacles. To address these
challenges, we present VividMed, a vision language model with versatile visual
grounding for medicine. Our model supports generating both semantic
segmentation masks and instance-level bounding boxes, and accommodates various
imaging modalities, including both 2D and 3D data. We design a three-stage
training procedure and an automatic data synthesis pipeline based on open
datasets and models. Besides visual grounding tasks, VividMed also excels in
other common downstream tasks, including Visual Question Answering (VQA) and
report generation. Ablation studies empirically show that the integration of
visual grounding ability leads to improved performance on these tasks. Our code
is publicly available at https://github.com/function2-llx/MMMM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning Approach to Brain Tumor Detection and Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alice Oh, Inyoung Noh, Jian Choo, Jihoo Lee, Justin Park, Kate Hwang, Sanghyeon Kim, Soo Min Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain tumor detection and classification are critical tasks in medical image
analysis, particularly in early-stage diagnosis, where accurate and timely
detection can significantly improve treatment outcomes. In this study, we apply
various statistical and machine learning models to detect and classify brain
tumors using brain MRI images. We explore a variety of statistical models
including linear, logistic, and Bayesian regressions, and the machine learning
models including decision tree, random forest, single-layer perceptron,
multi-layer perceptron, convolutional neural network (CNN), recurrent neural
network, and long short-term memory. Our findings show that CNN outperforms
other models, achieving the best performance. Additionally, we confirm that the
CNN model can also work for multi-class classification, distinguishing between
four categories of brain MRI images such as normal, glioma, meningioma, and
pituitary tumor images. This study demonstrates that machine learning
approaches are suitable for brain tumor detection and classification,
facilitating real-world medical applications in assisting radiologists with
early and accurate diagnosis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Mapping of Anatomical Landmarks from Free-Text Using Large
  Language Models: Insights from Llama-2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamad Abdi, Gerardo Hemosillo Valadez, Halid Ziya Yerebakan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anatomical landmarks are vital in medical imaging for navigation and anomaly
detection. Modern large language models (LLMs), like Llama-2, offer promise for
automating the mapping of these landmarks in free-text radiology reports to
corresponding positions in image data. Recent studies propose LLMs may develop
coherent representations of generative processes. Motivated by these insights,
we investigated whether LLMs accurately represent the spatial positions of
anatomical landmarks. Through experiments with Llama-2 models, we found that
they can linearly represent anatomical landmarks in space with considerable
robustness to different prompts. These results underscore the potential of LLMs
to enhance the efficiency and accuracy of medical imaging workflows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MambaBEV: An efficient 3D detection model with Mamba2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan You, Hao Wang, Qichao Zhao, Jinxiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A stable 3D object detection model based on BEV paradigm with temporal
information is very important for autonomous driving systems. However, current
temporal fusion model use convolutional layer or deformable self-attention is
not conducive to the exchange of global information of BEV space and has more
computational cost. Recently, a newly proposed based model specialized in
processing sequence called mamba has shown great potential in multiple
downstream task. In this work, we proposed a mamba2-based BEV 3D object
detection model named MambaBEV. We also adapt an end to end self driving
paradigm to test the performance of the model. Our work performs pretty good
results on nucences datasets:Our base version achieves 51.7% NDS. Our code will
be available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3DIS: Depth-Driven Decoupled Instance Synthesis for Text-to-Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dewei Zhou, Ji Xie, Zongxin Yang, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing demand for controllable outputs in text-to-image generation
has spurred advancements in multi-instance generation (MIG), allowing users to
define both instance layouts and attributes. However, unlike image-conditional
generation methods such as ControlNet, MIG techniques have not been widely
adopted in state-of-the-art models like SD2 and SDXL, primarily due to the
challenge of building robust renderers that simultaneously handle instance
positioning and attribute rendering. In this paper, we introduce Depth-Driven
Decoupled Instance Synthesis (3DIS), a novel framework that decouples the MIG
process into two stages: (i) generating a coarse scene depth map for accurate
instance positioning and scene composition, and (ii) rendering fine-grained
attributes using pre-trained ControlNet on any foundational model, without
additional training. Our 3DIS framework integrates a custom adapter into LDM3D
for precise depth-based layouts and employs a finetuning-free method for
enhanced instance-level attribute rendering. Extensive experiments on
COCO-Position and COCO-MIG benchmarks demonstrate that 3DIS significantly
outperforms existing methods in both layout precision and attribute rendering.
Notably, 3DIS offers seamless compatibility with diverse foundational models,
providing a robust, adaptable solution for advanced multi-instance generation.
The code is available at: https://github.com/limuloo/3DIS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Modal Safety Mechanism Transfer in Large <span class="highlight-title">Vision-Language</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shicheng Xu, Liang Pang, Yunchang Zhu, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language alignment in Large Vision-Language Models (LVLMs)
successfully enables LLMs to understand visual input. However, we find that
existing vision-language alignment methods fail to transfer the existing safety
mechanism for text in LLMs to vision, which leads to vulnerabilities in toxic
image. To explore the cause of this problem, we give the insightful explanation
of where and how the safety mechanism of LVLMs operates and conduct comparative
analysis between text and vision. We find that the hidden states at the
specific transformer layers play a crucial role in the successful activation of
safety mechanism, while the vision-language alignment at hidden states level in
current methods is insufficient. This results in a semantic shift for input
images compared to text in hidden states, therefore misleads the safety
mechanism. To address this, we propose a novel Text-Guided vision-language
Alignment method (TGA) for LVLMs. TGA retrieves the texts related to input
vision and uses them to guide the projection of vision into the hidden states
space in LLMs. Experiments show that TGA not only successfully transfers the
safety mechanism for text in basic LLMs to vision in vision-language alignment
for LVLMs without any safety fine-tuning on the visual modality but also
maintains the general performance on various vision tasks (Safe and Good).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cascade learning in multi-task encoder-decoder networks for concurrent
  bone segmentation and glenohumeral joint assessment in shoulder CT scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Marsilio, Davide Marzorati, Matteo Rossi, Andrea Moglia, Luca Mainardi, Alfonso Manzotti, Pietro Cerveri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Osteoarthritis is a degenerative condition affecting bones and cartilage,
often leading to osteophyte formation, bone density loss, and joint space
narrowing. Treatment options to restore normal joint function vary depending on
the severity of the condition. This work introduces an innovative deep-learning
framework processing shoulder CT scans. It features the semantic segmentation
of the proximal humerus and scapula, the 3D reconstruction of bone surfaces,
the identification of the glenohumeral (GH) joint region, and the staging of
three common osteoarthritic-related pathologies: osteophyte formation (OS), GH
space reduction (JS), and humeroscapular alignment (HSA). The pipeline
comprises two cascaded CNN architectures: 3D CEL-UNet for segmentation and 3D
Arthro-Net for threefold classification. A retrospective dataset of 571 CT
scans featuring patients with various degrees of GH osteoarthritic-related
pathologies was used to train, validate, and test the pipeline. Root mean
squared error and Hausdorff distance median values for 3D reconstruction were
0.22mm and 1.48mm for the humerus and 0.24mm and 1.48mm for the scapula,
outperforming state-of-the-art architectures and making it potentially suitable
for a PSI-based shoulder arthroplasty preoperative plan context. The
classification accuracy for OS, JS, and HSA consistently reached around 90%
across all three categories. The computational time for the inference pipeline
was less than 15s, showcasing the framework's efficiency and compatibility with
orthopedic radiology practice. The outcomes represent a promising advancement
toward the medical translation of artificial intelligence tools. This progress
aims to streamline the preoperative planning pipeline delivering high-quality
bone surfaces and supporting surgeons in selecting the most suitable surgical
approach according to the unique patient joint conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse
  Synthetic Data and Global-to-Local Adaptive Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Zhao, Hengrui Kang, Bin Wang, Conghui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document Layout Analysis is crucial for real-world document understanding
systems, but it encounters a challenging trade-off between speed and accuracy:
multimodal methods leveraging both text and visual features achieve higher
accuracy but suffer from significant latency, whereas unimodal methods relying
solely on visual features offer faster processing speeds at the expense of
accuracy. To address this dilemma, we introduce DocLayout-YOLO, a novel
approach that enhances accuracy while maintaining speed advantages through
document-specific optimizations in both pre-training and model design. For
robust document pre-training, we introduce the Mesh-candidate BestFit
algorithm, which frames document synthesis as a two-dimensional bin packing
problem, generating the large-scale, diverse DocSynth-300K dataset.
Pre-training on the resulting DocSynth-300K dataset significantly improves
fine-tuning performance across various document types. In terms of model
optimization, we propose a Global-to-Local Controllable Receptive Module that
is capable of better handling multi-scale variations of document elements.
Furthermore, to validate performance across different document types, we
introduce a complex and challenging benchmark named DocStructBench. Extensive
experiments on downstream datasets demonstrate that DocLayout-YOLO excels in
both speed and accuracy. Code, data, and models are available at
https://github.com/opendatalab/DocLayout-YOLO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github Repo: https://github.com/opendatalab/DocLayout-YOLO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Model Kinship for Merging Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yedi Hu, Yunzhi Yao, Ningyu Zhang, Shumin Deng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging has become one of the key technologies for enhancing the
capabilities and efficiency of Large Language Models (LLMs). However, our
understanding of the expected performance gains and principles when merging any
two models remains limited. In this work, we introduce model kinship, the
degree of similarity or relatedness between LLMs, analogous to biological
evolution. With comprehensive empirical analysis, we find that there is a
certain relationship between model kinship and the performance gains after
model merging, which can help guide our selection of candidate models. Inspired
by this, we propose a new model merging strategy: Top-k Greedy Merging with
Model Kinship, which can yield better performance on benchmark datasets.
Specifically, we discover that using model kinship as a criterion can assist us
in continuously performing model merging, alleviating the degradation (local
optima) in model evolution, whereas model kinship can serve as a guide to
escape these traps. Code is available at
https://github.com/zjunlp/ModelKinship.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMAL: A Novel Cross-Modal Associative Learning Framework for
  <span class="highlight-title">Vision-Language</span> <span class="highlight-title">Pre-Train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Ma, Jianjun Li, Guohui Li, Kaiyan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the flourishing of social media platforms, vision-language pre-training
(VLP) recently has received great attention and many remarkable progresses have
been achieved. The success of VLP largely benefits from the information
complementation and enhancement between different modalities. However, most of
recent studies focus on cross-modal contrastive learning (CMCL) to promote
image-text alignment by pulling embeddings of positive sample pairs together
while pushing those of negative pairs apart, which ignores the natural
asymmetry property between different modalities and requires large-scale
image-text corpus to achieve arduous progress. To mitigate this predicament, we
propose CMAL, a Cross-Modal Associative Learning framework with anchor points
detection and cross-modal associative learning for VLP. Specifically, we first
respectively embed visual objects and textual tokens into separate hypersphere
spaces to learn intra-modal hidden features, and then design a cross-modal
associative prompt layer to perform anchor point masking and swap feature
filling for constructing a hybrid cross-modal associative prompt. Afterwards,
we exploit a unified semantic encoder to learn their cross-modal interactive
features for context adaptation. Finally, we design an associative mapping
classification layer to learn potential associative mappings between modalities
at anchor points, within which we develop a fresh self-supervised associative
mapping classification task to boost CMAL's performance. Experimental results
verify the effectiveness of CMAL, showing that it achieves competitive
performance against previous CMCL-based methods on four common downstream
vision-and-language tasks, with significantly fewer corpus. Especially, CMAL
obtains new state-of-the-art results on SNLI-VE and REC (testA).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>vision-language pre-training, contrastive learning, cross-modal,
  associative learning, associative mapping classification</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cocoon: Robust <span class="highlight-title">Multi-Modal</span> Perception with Uncertainty-Aware Sensor
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minkyoung Cho, Yulong Cao, Jiachen Sun, Qingzhao Zhang, Marco Pavone, Jeong Joon Park, Heng Yang, Z. Morley Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An important paradigm in 3D object detection is the use of multiple
modalities to enhance accuracy in both normal and challenging conditions,
particularly for long-tail scenarios. To address this, recent studies have
explored two directions of adaptive approaches: MoE-based adaptive fusion,
which struggles with uncertainties arising from distinct object configurations,
and late fusion for output-level adaptive fusion, which relies on separate
detection pipelines and limits comprehensive understanding. In this work, we
introduce Cocoon, an object- and feature-level uncertainty-aware fusion
framework. The key innovation lies in uncertainty quantification for
heterogeneous representations, enabling fair comparison across modalities
through the introduction of a feature aligner and a learnable surrogate ground
truth, termed feature impression. We also define a training objective to ensure
that their relationship provides a valid metric for uncertainty quantification.
Cocoon consistently outperforms existing static and adaptive methods in both
normal and challenging conditions, including those with natural and artificial
corruptions. Furthermore, we show the validity and efficacy of our uncertainty
metric across diverse datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Visual Counterfactual Explanations Through Region Constraint 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bartlomiej Sobieski, Jakub Grzywaczewski, Bartlomiej Sadlej, Matthew Tivnan, Przemyslaw Biecek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual counterfactual explanations (VCEs) have recently gained immense
popularity as a tool for clarifying the decision-making process of image
classifiers. This trend is largely motivated by what these explanations promise
to deliver -- indicate semantically meaningful factors that change the
classifier's decision. However, we argue that current state-of-the-art
approaches lack a crucial component -- the region constraint -- whose absence
prevents from drawing explicit conclusions, and may even lead to faulty
reasoning due to phenomenons like confirmation bias. To address the issue of
previous methods, which modify images in a very entangled and widely dispersed
manner, we propose region-constrained VCEs (RVCEs), which assume that only a
predefined image region can be modified to influence the model's prediction. To
effectively sample from this subclass of VCEs, we propose Region-Constrained
Counterfactual Schr\"odinger Bridges (RCSB), an adaptation of a tractable
subclass of Schr\"odinger Bridges to the problem of conditional inpainting,
where the conditioning signal originates from the classifier of interest. In
addition to setting a new state-of-the-art by a large margin, we extend RCSB to
allow for exact counterfactual reasoning, where the predefined region contains
only the factor of interest, and incorporating the user to actively interact
with the RVCE by predefining the regions manually.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Lab to Pocket: A Novel Continual Learning-based Mobile Application
  for Screening COVID-19 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danny Falero, Muhammad Ashad Kabir, Nusrat Homaira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) has emerged as a promising tool for predicting
COVID-19 from medical images. In this paper, we propose a novel continual
learning-based approach and present the design and implementation of a mobile
application for screening COVID-19. Our approach demonstrates the ability to
adapt to evolving datasets, including data collected from different locations
or hospitals, varying virus strains, and diverse clinical presentations,
without retraining from scratch. We have evaluated state-of-the-art continual
learning methods for detecting COVID-19 from chest X-rays and selected the
best-performing model for our mobile app. We evaluated various deep learning
architectures to select the best-performing one as a foundation model for
continual learning. Both regularization and memory-based methods for continual
learning were tested, using different memory sizes to develop the optimal
continual learning model for our app. DenseNet161 emerged as the best
foundation model with 96.87\% accuracy, and Learning without Forgetting (LwF)
was the top continual learning method with an overall performance of 71.99\%.
The mobile app design considers both patient and doctor perspectives. It
incorporates the continual learning DenseNet161 LwF model on a cloud server,
enabling the model to learn from new instances of chest X-rays and their
classifications as they are submitted. The app is designed, implemented, and
evaluated to ensure it provides an efficient tool for COVID-19 screening. The
app is available to download from
https://github.com/DannyFGitHub/COVID-19PneumoCheckApp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-DenseMobileNet: A Robust Framework for Lung Nodule Classification
  using Self-ONN and Stacking-based Meta-Classifier 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Sohanur Rahman, Muhammad E. H. Chowdhury, Hasib Ryan Rahman, Mosabber Uddin Ahmed, Muhammad Ashad Kabir, Sanjiban Sekhar Roy, Rusab Sarmun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose a novel and robust framework, Self-DenseMobileNet,
designed to enhance the classification of nodules and non-nodules in chest
radiographs (CXRs). Our approach integrates advanced image standardization and
enhancement techniques to optimize the input quality, thereby improving
classification accuracy. To enhance predictive accuracy and leverage the
strengths of multiple models, the prediction probabilities from
Self-DenseMobileNet were transformed into tabular data and used to train eight
classical machine learning (ML) models; the top three performers were then
combined via a stacking algorithm, creating a robust meta-classifier that
integrates their collective insights for superior classification performance.
To enhance the interpretability of our results, we employed class activation
mapping (CAM) to visualize the decision-making process of the best-performing
model. Our proposed framework demonstrated remarkable performance on internal
validation data, achieving an accuracy of 99.28\% using a Meta-Random Forest
Classifier. When tested on an external dataset, the framework maintained strong
generalizability with an accuracy of 89.40\%. These results highlight a
significant improvement in the classification of CXRs with lung nodules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FTII-Bench: A Comprehensive <span class="highlight-title">Multimodal</span> Benchmark for Flow Text with
  Image Insertion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Ruan, Yebin Yang, Zehao Lin, Feiyu Xiong, Zeyun Tang, Zhiyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benefiting from the revolutionary advances in large language models (LLMs)
and foundational vision models, large vision-language models (LVLMs) have also
made significant progress. However, current benchmarks focus on tasks that
evaluating only a single aspect of LVLM capabilities (e.g., recognition,
detection, understanding). These tasks fail to fully demonstrate LVLMs'
potential in complex application scenarios. To comprehensively assess the
performance of existing LVLMs, we propose a more challenging task called the
Flow Text with Image Insertion task (FTII). This task requires LVLMs to
simultaneously possess outstanding abilities in image comprehension,
instruction understanding, and long-text interpretation. Specifically, given
several text paragraphs and a set of candidate images, as the text paragraphs
accumulate, the LVLMs are required to select the most suitable image from the
candidates to insert after the corresponding paragraph. Constructing a
benchmark for such a task is highly challenging, particularly in determining
the sequence of flowing text and images. To address this challenge, we turn to
professional news reports, which naturally contain a gold standard for
image-text sequences. Based on this, we introduce the Flow Text with Image
Insertion Benchmark (FTII-Bench), which includes 318 high-quality Chinese
image-text news articles and 307 high-quality English image-text news articles,
covering 10 different news domains. Using these 625 high-quality articles, we
construct problems of two different types with multiple levels of difficulty.
Furthermore, we establish two different evaluation pipelines based on the CLIP
model and existing LVLMs. We evaluate 9 open-source and 2 closed-source LVLMs
as well as 2 CLIP-based models. Results indicate that even the most advanced
models (e.g., GPT-4o) face significant challenges when tackling the FTII task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress. 9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive <span class="highlight-title">Prompt</span> Learning with SAM for Few-shot Scanning Probe Microscope
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Shen, Ziwei Wei, Chunmeng Liu, Shuming Wei, Qi Zhao, Kaiyang Zeng, Guangyao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Segment Anything Model (SAM) has demonstrated strong performance in image
segmentation of natural scene images. However, its effectiveness diminishes
markedly when applied to specific scientific domains, such as Scanning Probe
Microscope (SPM) images. This decline in accuracy can be attributed to the
distinct data distribution and limited availability of the data inherent in the
scientific images. On the other hand, the acquisition of adequate SPM datasets
is both time-intensive and laborious as well as skill-dependent. To address
these challenges, we propose an Adaptive Prompt Learning with SAM (APL-SAM)
framework tailored for few-shot SPM image segmentation. Our approach
incorporates two key innovations to enhance SAM: 1) An Adaptive Prompt Learning
module leverages few-shot embeddings derived from limited support set to learn
adaptively central representatives, serving as visual prompts. This innovation
eliminates the need for time-consuming online user interactions for providing
prompts, such as exhaustively marking points and bounding boxes slice by slice;
2) A multi-source, multi-level mask decoder specifically designed for few-shot
SPM image segmentation is introduced, which can effectively capture the
correspondence between the support and query images. To facilitate
comprehensive training and evaluation, we introduce a new dataset, SPM-Seg,
curated for SPM image segmentation. Extensive experiments on this dataset
reveal that the proposed APL-SAM framework significantly outperforms the
original SAM, achieving over a 30% improvement in terms of Dice Similarity
Coefficient with only one-shot guidance. Moreover, APL-SAM surpasses
state-of-the-art few-shot segmentation methods and even fully supervised
approaches in performance. Code and dataset used in this study will be made
available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Development of Image Collection Method Using YOLO and Siamese Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chan Young Shin, Ah Hyun Lee, Jun Young Lee, Ji Min Lee, Soo Jin Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As we enter the era of big data, collecting high-quality data is very
important. However, collecting data by humans is not only very time-consuming
but also expensive. Therefore, many scientists have devised various methods to
collect data using computers. Among them, there is a method called web
crawling, but the authors found that the crawling method has a problem in that
unintended data is collected along with the user. The authors found that this
can be filtered using the object recognition model YOLOv10. However, there are
cases where data that is not properly filtered remains. Here, image
reclassification was performed by additionally utilizing the distance output
from the Siamese network, and higher performance was recorded than other
classification models. (average \_f1 score YOLO+MobileNet
0.678->YOLO+SiameseNet 0.772)) The user can specify a distance threshold to
adjust the balance between data deficiency and noise-robustness. The authors
also found that the Siamese network can achieve higher performance with fewer
resources because the cropped images are used for object recognition when
processing images in the Siamese network. (Class 20 mean-based f1 score,
non-crop+Siamese(MobileNetV3-Small) 80.94 -> crop
preprocessing+Siamese(MobileNetV3-Small) 82.31) In this way, the image
retrieval system that utilizes two consecutive models to reduce errors can save
users' time and effort, and build better quality data faster and with fewer
resources than before.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 13 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One Step Diffusion via Shortcut Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Frans, Danijar Hafner, Sergey Levine, Pieter Abbeel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models and flow-matching models have enabled generating diverse and
realistic images by learning to transfer noise to data. However, sampling from
these models involves iterative denoising over many neural network passes,
making generation slow and expensive. Previous approaches for speeding up
sampling require complex training regimes, such as multiple training phases,
multiple networks, or fragile scheduling. We introduce shortcut models, a
family of generative models that use a single network and training phase to
produce high-quality samples in a single or multiple sampling steps. Shortcut
models condition the network not only on the current noise level but also on
the desired step size, allowing the model to skip ahead in the generation
process. Across a wide range of sampling step budgets, shortcut models
consistently produce higher quality samples than previous approaches, such as
consistency models and reflow. Compared to distillation, shortcut models reduce
complexity to a single network and training phase and additionally allow
varying step budgets at inference time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Utility of Memory Efficient Medical Image Generation: A Study
  on Lung Nodule Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kathrin Khadra, Utku Türkbey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scarcity of publicly available medical imaging data limits the
development of effective AI models. This work proposes a memory-efficient
patch-wise denoising diffusion probabilistic model (DDPM) for generating
synthetic medical images, focusing on CT scans with lung nodules. Our approach
generates high-utility synthetic images with nodule segmentation while
efficiently managing memory constraints, enabling the creation of training
datasets. We evaluate the method in two scenarios: training a segmentation
model exclusively on synthetic data, and augmenting real-world training data
with synthetic images. In the first case, models trained solely on synthetic
data achieve Dice scores comparable to those trained on real-world data
benchmarks. In the second case, augmenting real-world data with synthetic
images significantly improves segmentation performance. The generated images
demonstrate their potential to enhance medical image datasets in scenarios with
limited real-world data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shaping a Stabilized Video by Mitigating Unintended Changes for
  Concept-Augmented Video Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingce Guo, Jingxuan He, Shengeng Tang, Zhangye Wang, Lechao Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-driven video editing utilizing generative diffusion models has garnered
significant attention due to their potential applications. However, existing
approaches are constrained by the limited word embeddings provided in
pre-training, which hinders nuanced editing targeting open concepts with
specific attributes. Directly altering the keywords in target prompts often
results in unintended disruptions to the attention mechanisms. To achieve more
flexible editing easily, this work proposes an improved concept-augmented video
editing approach that generates diverse and stable target videos flexibly by
devising abstract conceptual pairs. Specifically, the framework involves
concept-augmented textual inversion and a dual prior supervision mechanism. The
former enables plug-and-play guidance of stable diffusion for video editing,
effectively capturing target attributes for more stylized results. The dual
prior supervision mechanism significantly enhances video stability and
fidelity. Comprehensive evaluations demonstrate that our approach generates
more stable and lifelike videos, outperforming state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MambaPainter: Neural Stroke-Based Rendering in a Single Step <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomoya Sawada, Marie Katsurai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stroke-based rendering aims to reconstruct an input image into an oil
painting style by predicting brush stroke sequences. Conventional methods
perform this prediction stroke-by-stroke or require multiple inference steps
due to the limitations of a predictable number of strokes. This procedure leads
to inefficient translation speed, limiting their practicality. In this study,
we propose MambaPainter, capable of predicting a sequence of over 100 brush
strokes in a single inference step, resulting in rapid translation. We achieve
this sequence prediction by incorporating the selective state-space model.
Additionally, we introduce a simple extension to patch-based rendering, which
we use to translate high-resolution images, improving the visual quality with a
minimal increase in computational cost. Experimental results demonstrate that
MambaPainter can efficiently translate inputs to oil painting-style images
compared to state-of-the-art methods. The codes are available at
https://github.com/STomoya/MambaPainter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SIGGRAPH Asia 2024 posters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QueensCAMP: an RGB-D dataset for robust Visual SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hudson M. S. Bruno, Esther L. Colombini, Sidney N. Givigi Jr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Simultaneous Localization and Mapping (VSLAM) is a fundamental
technology for robotics applications. While VSLAM research has achieved
significant advancements, its robustness under challenging situations, such as
poor lighting, dynamic environments, motion blur, and sensor failures, remains
a challenging issue. To address these challenges, we introduce a novel RGB-D
dataset designed for evaluating the robustness of VSLAM systems. The dataset
comprises real-world indoor scenes with dynamic objects, motion blur, and
varying illumination, as well as emulated camera failures, including lens dirt,
condensation, underexposure, and overexposure. Additionally, we offer
open-source scripts for injecting camera failures into any images, enabling
further customization by the research community. Our experiments demonstrate
that ORB-SLAM2, a traditional VSLAM algorithm, and TartanVO, a Deep
Learning-based VO algorithm, can experience performance degradation under these
challenging conditions. Therefore, this dataset and the camera failure
open-source tools provide a valuable resource for developing more robust VSLAM
systems capable of handling real-world challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DH-VTON: Deep Text-Driven Virtual Try-On via Hybrid Attention Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiabao Wei, Zhiyuan Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual Try-ON (VTON) aims to synthesis specific person images dressed in
given garments, which recently receives numerous attention in online shopping
scenarios. Currently, the core challenges of the VTON task mainly lie in the
fine-grained semantic extraction (i.e.,deep semantics) of the given reference
garments during depth estimation and effective texture preservation when the
garments are synthesized and warped onto human body. To cope with these issues,
we propose DH-VTON, a deep text-driven virtual try-on model featuring a special
hybrid attention learning strategy and deep garment semantic preservation
module. By standing on the shoulder of a well-built pre-trained
paint-by-example (abbr. PBE) approach, we present our DH-VTON pipeline in this
work. Specifically, to extract the deep semantics of the garments, we first
introduce InternViT-6B as fine-grained feature learner, which can be trained to
align with the large-scale intrinsic knowledge with deep text semantics
(e.g.,"neckline" or "girdle") to make up for the deficiency of the commonly
adopted CLIP encoder. Based on this, to enhance the customized dressing
abilities, we further introduce Garment-Feature ControlNet Plus (abbr. GFC+)
module and propose to leverage a fresh hybrid attention strategy for training,
which can adaptively integrate fine-grained characteristics of the garments
into the different layers of the VTON model, so as to achieve multi-scale
features preservation effects. Extensive experiments on several representative
datasets demonstrate that our method outperforms previous diffusion-based and
GAN-based approaches, showing competitive performance in preserving garment
details and generating authentic human images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures, ICASSP2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stabilize the Latent Space for Image Autoregressive Modeling: A Unified
  Perspective <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxin Zhu, Bocheng Li, Hang Zhang, Xin Li, Linli Xu, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent-based image generative models, such as Latent Diffusion Models (LDMs)
and Mask Image Models (MIMs), have achieved notable success in image generation
tasks. These models typically leverage reconstructive autoencoders like VQGAN
or VAE to encode pixels into a more compact latent space and learn the data
distribution in the latent space instead of directly from pixels. However, this
practice raises a pertinent question: Is it truly the optimal choice? In
response, we begin with an intriguing observation: despite sharing the same
latent space, autoregressive models significantly lag behind LDMs and MIMs in
image generation. This finding contrasts sharply with the field of NLP, where
the autoregressive model GPT has established a commanding presence. To address
this discrepancy, we introduce a unified perspective on the relationship
between latent space and generative models, emphasizing the stability of latent
space in image generative modeling. Furthermore, we propose a simple but
effective discrete image tokenizer to stabilize the latent space for image
generative modeling. Experimental results show that image autoregressive
modeling with our tokenizer (DiGIT) benefits both image understanding and image
generation with the next token prediction principle, which is inherently
straightforward for GPT models but challenging for other generative models.
Remarkably, for the first time, a GPT-style autoregressive model for images
outperforms LDMs, which also exhibits substantial improvement akin to GPT when
scaling up model size. Our findings underscore the potential of an optimized
latent space and the integration of discrete tokenization in advancing the
capabilities of image generative models. The code is available at
\url{https://github.com/DAMO-NLP-SG/DiGIT}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic Augmentation for Anatomical Landmark Localization using DDPMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnela Hadzic, Lea Bogensperger, Simon Johannes Joham, Martin Urschler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning techniques for anatomical landmark localization (ALL) have
shown great success, but their reliance on large annotated datasets remains a
problem due to the tedious and costly nature of medical data acquisition and
annotation. While traditional data augmentation, variational autoencoders
(VAEs), and generative adversarial networks (GANs) have already been used to
synthetically expand medical datasets, diffusion-based generative models have
recently started to gain attention for their ability to generate high-quality
synthetic images. In this study, we explore the use of denoising diffusion
probabilistic models (DDPMs) for generating medical images and their
corresponding heatmaps of landmarks to enhance the training of a supervised
deep learning model for ALL. Our novel approach involves a DDPM with a
2-channel input, incorporating both the original medical image and its heatmap
of annotated landmarks. We also propose a novel way to assess the quality of
the generated images using a Markov Random Field (MRF) model for landmark
matching and a Statistical Shape Model (SSM) to check landmark plausibility,
before we evaluate the DDPM-augmented dataset in the context of an ALL task
involving hand X-Rays.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mind the Gap Between Prototypes and Images in Cross-domain Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongduan Tian, Feng Liu, Zhanke Zhou, Tongliang Liu, Chengqi Zhang, Bo Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In cross-domain few-shot classification (CFC), recent works mainly focus on
adapting a simple transformation head on top of a frozen pre-trained backbone
with few labeled data to project embeddings into a task-specific metric space
where classification can be performed by measuring similarities between image
instance and prototype representations. Technically, an assumption implicitly
adopted in such a framework is that the prototype and image instance embeddings
share the same representation transformation. However, in this paper, we find
that there naturally exists a gap, which resembles the modality gap, between
the prototype and image instance embeddings extracted from the frozen
pre-trained backbone, and simply applying the same transformation during the
adaptation phase constrains exploring the optimal representations and shrinks
the gap between prototype and image representations. To solve this problem, we
propose a simple yet effective method, contrastive prototype-image adaptation
(CoPA), to adapt different transformations respectively for prototypes and
images similarly to CLIP by treating prototypes as text prompts. Extensive
experiments on Meta-Dataset demonstrate that CoPA achieves the state-of-the-art
performance more efficiently. Meanwhile, further analyses also indicate that
CoPA can learn better representation clusters, enlarge the gap, and achieve
minimal validation loss at the enlarged gap.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Primal-dual algorithm for image reconstruction with ICNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hok Shing Wong, Matthias J. Ehrhardt, Subhadip Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the optimization problem in a data-driven variational
reconstruction framework, where the regularizer is parameterized by an
input-convex neural network (ICNN). While gradient-based methods are commonly
used to solve such problems, they struggle to effectively handle non-smoothness
which often leads to slow convergence. Moreover, the nested structure of the
neural network complicates the application of standard non-smooth optimization
techniques, such as proximal algorithms. To overcome these challenges, we
reformulate the problem and eliminate the network's nested structure. By
relating this reformulation to epigraphical projections of the activation
functions, we transform the problem into a convex optimization problem that can
be efficiently solved using a primal-dual algorithm. We also prove that this
reformulation is equivalent to the original variational problem. Through
experiments on several imaging tasks, we demonstrate that the proposed approach
outperforms subgradient methods in terms of both speed and stability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention-Guided Perturbation for Consistency Regularization in
  Semi-Supervised Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Cheng, Chenxi Shao, Jie Ma, Guoliang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation is a pivotal step in diagnostic and therapeutic
processes. However, the acquisition of high-quality annotated data is often
constrained by scarcity and cost. Semi-supervised learning offers a promising
approach to enhance model performance by using unlabeled data. While
consistency regularization is a prevalent method in semi-supervised image
segmentation, there is a dearth of research on perturbation strategies tailored
for semi-supervised medical image segmentation tasks. This paper introduces an
attention-guided perturbation strategy for semi-supervised consistency
regularization in the context of medical image segmentation. We add the
perturbation based on the attention from the model in the image and feature
level to achieve consistency regularization. The method is adept at
accommodating the intricate structures and high-dimensional semantics inherent
in medical images, thereby enhancing the performance of semi-supervised
segmentation tasks. Our method achieved state-of-the-art results on benchmark
datasets, including a 90.4\% Dice score on the ACDC dataset in the 7-case
scenario.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Triplet: Triangle Patchlet for Mesh-Based Inverse Rendering and Scene
  Parameters Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajie Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Radiance Fields have significantly improved novel-view
synthesis. However, in many real-world applications, the more advanced
challenge lies in inverse rendering, which seeks to derive the physical
properties of a scene, including light, geometry, textures, and materials.
Meshes, as a traditional representation adopted by many simulation pipeline,
however, still show limited influence in radiance field for inverse rendering.
This paper introduces a novel framework called Triangle Patchlet (abbr.
Triplet), a mesh-based representation, to comprehensively approximate these
scene parameters. We begin by assembling Triplets with either randomly
generated points or sparse points obtained from camera calibration where all
faces are treated as an independent element. Next, we simulate the physical
interaction of light and optimize the scene parameters using traditional
graphics rendering techniques like rasterization and ray tracing, accompanying
with density control and propagation. An iterative mesh extracting process is
also suggested, where we continue to optimize on geometry and materials with
graph-based operation. We also introduce several regulation terms to enable
better generalization of materials property. Our framework could precisely
estimate the light, materials and geometry with mesh without prior of light,
materials and geometry in a unified framework. Experiments demonstrate that our
approach can achieve state-of-the-art visual quality while reconstructing
high-quality geometry and accurate material properties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/RANDO11199/Triplet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaCropFollow: Self-Supervised Online Adaptation for Visual Under-Canopy
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun N. Sivakumar, Federico Magistri, Mateus V. Gasparino, Jens Behley, Cyrill Stachniss, Girish Chowdhary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Under-canopy agricultural robots can enable various applications like precise
monitoring, spraying, weeding, and plant manipulation tasks throughout the
growing season. Autonomous navigation under the canopy is challenging due to
the degradation in accuracy of RTK-GPS and the large variability in the visual
appearance of the scene over time. In prior work, we developed a supervised
learning-based perception system with semantic keypoint representation and
deployed this in various field conditions. A large number of failures of this
system can be attributed to the inability of the perception model to adapt to
the domain shift encountered during deployment. In this paper, we propose a
self-supervised online adaptation method for adapting the semantic keypoint
representation using a visual foundational model, geometric prior, and pseudo
labeling. Our preliminary experiments show that with minimal data and
fine-tuning of parameters, the keypoint prediction model trained with labels on
the source domain can be adapted in a self-supervised manner to various
challenging target domains onboard the robot computer using our method. This
can enable fully autonomous row-following capability in under-canopy robots
across fields and crops without requiring human intervention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Coarse-Grained Matching in Video-Text Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aozhu Chen, Hazel Doughty, Xirong Li, Cees G. M. Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-text retrieval has seen significant advancements, yet the ability of
models to discern subtle differences in captions still requires verification.
In this paper, we introduce a new approach for fine-grained evaluation. Our
approach can be applied to existing datasets by automatically generating hard
negative test captions with subtle single-word variations across nouns, verbs,
adjectives, adverbs, and prepositions. We perform comprehensive experiments
using four state-of-the-art models across two standard benchmarks (MSR-VTT and
VATEX) and two specially curated datasets enriched with detailed descriptions
(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our
analyses show that the current evaluation benchmarks fall short in detecting a
model's ability to perceive subtle single-word differences, 2) our fine-grained
evaluation highlights the difficulty models face in distinguishing such subtle
variations. To enhance fine-grained understanding, we propose a new baseline
that can be easily combined with current methods. Experiments on our
fine-grained evaluations demonstrate that this approach enhances a model's
ability to understand fine-grained differences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ De-Identification of Medical Imaging Data: A Comprehensive Tool for
  Ensuring Patient Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Rempe, Lukas Heine, Constantin Seibold, Fabian Hörst, Jens Kleesiek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical data employed in research frequently comprises sensitive patient
health information (PHI), which is subject to rigorous legal frameworks such as
the General Data Protection Regulation (GDPR) or the Health Insurance
Portability and Accountability Act (HIPAA). Consequently, these types of data
must be pseudonymized prior to utilisation, which presents a significant
challenge for many researchers. Given the vast array of medical data, it is
necessary to employ a variety of de-identification techniques. To facilitate
the anonymization process for medical imaging data, we have developed an
open-source tool that can be used to de-identify DICOM magnetic resonance
images, computer tomography images, whole slide images and magnetic resonance
twix raw data. Furthermore, the implementation of a neural network enables the
removal of text within the images. The proposed tool automates an elaborate
anonymization pipeline for multiple types of inputs, reducing the need for
additional tools used for de-identification of imaging data. We make our code
publicly available at
https://github.com/code-lukas/medical_image_deidentification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature Augmentation for Self-supervised Contrastive Learning: A Closer
  Look 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Zhang, Rui Zhu, Shifeng Zhang, Xu Zhou, Shifeng Chen, Xiaofan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised contrastive learning heavily relies on the view variance
brought by data augmentation, so that it can learn a view-invariant pre-trained
representation. Beyond increasing the view variance for contrast, this work
focuses on improving the diversity of training data, to improve the
generalization and robustness of the pre-trained models. To this end, we
propose a unified framework to conduct data augmentation in the feature space,
known as feature augmentation. This strategy is domain-agnostic, which augments
similar features to the original ones and thus improves the data diversity. We
perform a systematic investigation of various feature augmentation
architectures, the gradient-flow skill, and the relationship between feature
augmentation and traditional data augmentation. Our study reveals some
practical principles for feature augmentation in self-contrastive learning. By
integrating feature augmentation on the instance discrimination or the instance
similarity paradigm, we consistently improve the performance of pre-trained
feature learning and gain better generalization over the downstream image
classification and object detection task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCNN 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Stereo-based 3D Object Detection for Streaming Perception <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changcai Li, Zonghua Gu, Gang Chen, Libo Huang, Wei Zhang, Huihui Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to promptly respond to environmental changes is crucial for the
perception system of autonomous driving. Recently, a new task called streaming
perception was proposed. It jointly evaluate the latency and accuracy into a
single metric for video online perception. In this work, we introduce
StreamDSGN, the first real-time stereo-based 3D object detection framework
designed for streaming perception. StreamDSGN is an end-to-end framework that
directly predicts the 3D properties of objects in the next moment by leveraging
historical information, thereby alleviating the accuracy degradation of
streaming perception. Further, StreamDSGN applies three strategies to enhance
the perception accuracy: (1) A feature-flow-based fusion method, which
generates a pseudo-next feature at the current moment to address the
misalignment issue between feature and ground truth. (2) An extra regression
loss for explicit supervision of object motion consistency in consecutive
frames. (3) A large kernel backbone with a large receptive field for
effectively capturing long-range spatial contextual features caused by changes
in object positions. Experiments on the KITTI Tracking dataset show that,
compared with the strong baseline, StreamDSGN significantly improves the
streaming average precision by up to 4.33%. Our code is available at
https://github.com/weiyangdaren/streamDSGN-pytorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Streaming Perception, 3D Object Detection, NeurIPS2024 poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of
  Large <span class="highlight-title">Multimodal</span> Models Through Coding Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengji Zhang, Linquan Wu, Huiyu Bai, Guancheng Lin, Xiao Li, Xiao Yu, Yue Wang, Bei Chen, Jacky Keung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coding tasks have been valuable for evaluating Large Language Models (LLMs),
as they demand the comprehension of high-level instructions, complex reasoning,
and the implementation of functional programs -- core capabilities for
advancing Artificial General Intelligence. Despite the progress in Large
Multimodal Models (LMMs), which extend LLMs with visual perception and
understanding capabilities, there remains a notable lack of coding benchmarks
that rigorously assess these models, particularly in tasks that emphasize
visual reasoning. To address this gap, we introduce HumanEval-V, a novel and
lightweight benchmark specifically designed to evaluate LMMs' visual
understanding and reasoning capabilities through code generation. HumanEval-V
includes 108 carefully crafted, entry-level Python coding tasks derived from
platforms like CodeForces and Stack Overflow. Each task is adapted by modifying
the context and algorithmic patterns of the original problems, with visual
elements redrawn to ensure distinction from the source, preventing potential
data leakage. LMMs are required to complete the code solution based on the
provided visual context and a predefined Python function signature outlining
the task requirements. Every task is equipped with meticulously handcrafted
test cases to ensure a thorough and reliable evaluation of model-generated
solutions. We evaluate 19 state-of-the-art LMMs using HumanEval-V, uncovering
significant challenges. Proprietary models like GPT-4o achieve only 13% pass@1
and 36.4% pass@10, while open-weight models with 70B parameters score below 4%
pass@1. Ablation studies further reveal the limitations of current LMMs in
vision reasoning and coding capabilities. These results underscore key areas
for future research to enhance LMMs' capabilities. We have open-sourced our
code and benchmark at https://github.com/HumanEval-V/HumanEval-V-Benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>homepage https://humaneval-v.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stylistic Multi-Task Analysis of Ukiyo-e Woodblock Prints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Selina Khan, Nanne van Noord
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we present a large-scale dataset of \textit{Ukiyo-e} woodblock
prints. Unlike previous works and datasets in the artistic domain that
primarily focus on western art, this paper explores this pre-modern Japanese
art form with the aim of broadening the scope for stylistic analysis and to
provide a benchmark to evaluate a variety of art focused Computer Vision
approaches. Our dataset consists of over $175.000$ prints with corresponding
metadata (\eg artist, era, and creation date) from the 17th century to present
day. By approaching stylistic analysis as a Multi-Task problem we aim to more
efficiently utilize the available metadata, and learn more general
representations of style. We show results for well-known baselines and
state-of-the-art multi-task learning frameworks to enable future comparison,
and to encourage stylistic analysis on this artistic domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GAN Based Top-Down View Synthesis in Reinforcement Learning Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usama Younus, Vinoj Jayasundara, Shivam Mishra, Suleyman Aslan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human actions are based on the mental perception of the environment. Even
when all the aspects of an environment are not visible, humans have an internal
mental model that can generalize the partially visible scenes to fully
constructed and connected views. This internal mental model uses learned
abstract representations of spatial and temporal aspects of the environments
encountered in the past.
  Artificial agents in reinforcement learning environments also benefit by
learning a representation of the environment from experience. It provides the
agent with viewpoints that are not directly visible to it, helping it make
better policy decisions. It can also be used to predict the future state of the
environment.
  This project explores learning the top-down view of an RL environment based
on the artificial agent's first-person view observations with a generative
adversarial network(GAN). The top-down view is useful as it provides a complete
overview of the environment by building a map of the entire environment. It
provides information about the objects' dimensions and shapes along with their
relative positions with one another. Initially, when only a partial observation
of the environment is visible to the agent, only a partial top-down view is
generated. As the agent explores the environment through a set of actions, the
generated top-down view becomes complete. This generated top-down view can
assist the agent in deducing better policy decisions. The focus of the project
is to learn the top-down view of an RL environment. It doesn't deal with any
Reinforcement Learning task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-Infused Visual Grounding for Art 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Selina Khan, Nanne van Noord
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many artwork collections contain textual attributes that provide rich and
contextualised descriptions of artworks. Visual grounding offers the potential
for localising subjects within these descriptions on images, however, existing
approaches are trained on natural images and generalise poorly to art. In this
paper, we present CIGAr (Context-Infused GroundingDINO for Art), a visual
grounding approach which utilises the artwork descriptions during training as
context, thereby enabling visual grounding on art. In addition, we present a
new dataset, Ukiyo-eVG, with manually annotated phrase-grounding annotations,
and we set a new state-of-the-art for object detection on two artwork datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Flexible and Efficient Diffusion Low Light Enhancer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanzhou Lan, Qianli Ma, Yuqi Yang, Zhigang Wang, Dong Wang, Yuan Yuan, Bin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based Low-Light Image Enhancement (LLIE) has demonstrated
significant success in improving the visibility of low-light images. However,
the substantial computational burden introduced by the iterative sampling
process remains a major concern. Current acceleration methods, whether
training-based or training-free, often lead to significant performance
degradation. As a result, to achieve an efficient student model with
performance comparable to that of existing multi-step teacher model, it is
usually necessary to retrain a more capable teacher model. This approach
introduces inflexibility, as it requires additional training to enhance the
teacher's performance. To address these challenges, we propose
\textbf{Re}flectance-aware \textbf{D}iffusion with \textbf{Di}stilled
\textbf{T}rajectory (\textbf{ReDDiT}), a step distillation framework
specifically designed for LLIE. ReDDiT trains a student model to replicate the
teacher's trajectory in fewer steps while also possessing the ability to
surpass the teacher's performance. Specifically, we first introduce a
trajectory decoder from the teacher model to provide guidance. Subsequently, a
reflectance-aware trajectory refinement module is incorporated into the
distillation process to enable more deterministic guidance from the teacher
model. Our framework achieves comparable performance to previous
diffusion-based methods with redundant steps in just 2 steps while establishing
new state-of-the-art (SOTA) results with 8 or 4 steps. Comprehensive
experimental evaluations on 10 benchmark datasets validate the effectiveness of
our method, consistently outperforming existing SOTA methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TAS: Distilling Arbitrary Teacher and Student via a Hybrid Assistant 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guopeng Li, Qiang Wang, Ke Yan, Shouhong Ding, Yuan Gao, Gui-Song Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most knowledge distillation (KD) methodologies predominantly focus on
teacher-student pairs with similar architectures, such as both being
convolutional neural networks (CNNs). However, the potential and flexibility of
KD can be greatly improved by expanding it to novel Cross-Architecture KD
(CAKD), where the knowledge of homogeneous and heterogeneous teachers can be
transferred flexibly to a given student. The primary challenge in CAKD lies in
the substantial feature gaps between heterogeneous models, originating from the
distinction of their inherent inductive biases and module functions. To this
end, we introduce an assistant model as a bridge to facilitate smooth feature
knowledge transfer between heterogeneous teachers and students. More
importantly, within our proposed design principle, the assistant model combines
the advantages of cross-architecture inductive biases and module functions by
merging convolution and attention modules derived from both student and teacher
module functions. Furthermore, we observe that heterogeneous features exhibit
diverse spatial distributions in CAKD, hindering the effectiveness of
conventional pixel-wise mean squared error (MSE) loss. Therefore, we leverage a
spatial-agnostic InfoNCE loss to align features after spatial smoothing,
thereby improving the feature alignments in CAKD. Our proposed method is
evaluated across some homogeneous model pairs and arbitrary heterogeneous
combinations of CNNs, ViTs, and MLPs, achieving state-of-the-art performance
for distilled models with a maximum gain of 11.47% on CIFAR-100 and 3.67% on
ImageNet-1K. Our code and models will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures, and 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ARIC: An Activity Recognition Dataset in Classroom Surveillance Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linfeng Xu, Fanman Meng, Qingbo Wu, Lili Pan, Heqian Qiu, Lanxiao Wang, Kailong Chen, Kanglei Geng, Yilei Qian, Haojie Wang, Shuchang Zhou, Shimou Ling, Zejia Liu, Nanlin Chen, Yingjie Xu, Shaoxu Cheng, Bowen Tan, Ziyong Xu, Hongliang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of activity recognition in the ``AI + Education" field is
gaining increasing attention. However, current work mainly focuses on the
recognition of activities in manually captured videos and a limited number of
activity types, with little attention given to recognizing activities in
surveillance images from real classrooms. Activity recognition in classroom
surveillance images faces multiple challenges, such as class imbalance and high
activity similarity. To address this gap, we constructed a novel multimodal
dataset focused on classroom surveillance image activity recognition called
ARIC (Activity Recognition In Classroom). The ARIC dataset has advantages of
multiple perspectives, 32 activity categories, three modalities, and real-world
classroom scenarios. In addition to the general activity recognition tasks, we
also provide settings for continual learning and few-shot continual learning.
We hope that the ARIC dataset can act as a facilitator for future analysis and
research for open teaching scenarios. You can download preliminary data from
https://ivipclab.github.io/publication_ARIC/ARIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2409.03354</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MC-Bench: A Benchmark for Multi-Context Visual Grounding in the Era of
  MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunqiu Xu, Linchao Zhu, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While multimodal large language models (MLLMs) have demonstrated
extraordinary vision-language understanding capabilities and shown potential to
serve as general-purpose assistants, their abilities to solve instance-level
visual-language problems beyond a single image warrant further exploration. In
order to assess these unproven abilities of MLLMs, this paper proposes a new
visual grounding task called multi-context visual grounding, which aims to
localize instances of interest across multiple images based on open-ended text
prompts. To facilitate this research, we meticulously construct a new dataset
MC-Bench for benchmarking the visual grounding capabilities of MLLMs. MC-Bench
features 2K high-quality and manually annotated samples, consisting of
instance-level labeled image pairs and corresponding text prompts that indicate
the target instances in the images. In total, there are three distinct styles
of text prompts, covering 20 practical skills. We benchmark over 20
state-of-the-art MLLMs and foundation models with potential multi-context
visual grounding capabilities. Our evaluation reveals a non-trivial performance
gap between existing MLLMs and humans across all metrics. We also observe that
existing MLLMs typically outperform foundation models without LLMs only on
image-level metrics, and the specialist MLLMs trained on single images often
struggle to generalize to multi-image scenarios. Moreover, a simple stepwise
baseline integrating advanced MLLM and a detector can significantly surpass
prior end-to-end MLLMs. We hope our MC-Bench and empirical findings can
encourage the research community to further explore and enhance the untapped
potentials of MLLMs in instance-level tasks, particularly in multi-image
contexts. Project page: https://xuyunqiu.github.io/MC-Bench/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Anomaly Detection through Conditional Latent Space VAE
  Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oskar Åström, Alexandros Sopasakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel Conditional Latent space Variational Autoencoder (CL-VAE)
to perform improved pre-processing for anomaly detection on data with known
inlier classes and unknown outlier classes. This proposed variational
autoencoder (VAE) improves latent space separation by conditioning on
information within the data. The method fits a unique prior distribution to
each class in the dataset, effectively expanding the classic prior distribution
for VAEs to include a Gaussian mixture model. An ensemble of these VAEs are
merged in the latent spaces to form a group consensus that greatly improves the
accuracy of anomaly detection across data sets. Our approach is compared
against the capabilities of a typical VAE, a CNN, and a PCA, with regards AUC
for anomaly detection. The proposed model shows increased accuracy in anomaly
detection, achieving an AUC of 97.4% on the MNIST dataset compared to 95.7% for
the second best model. In addition, the CL-VAE shows increased benefits from
ensembling, a more interpretable latent space, and an increased ability to
learn patterns in complex data with limited model sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages of main article, 19 pages including references and appendix,
  4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PAPL-SLAM: Principal Axis-Anchored Monocular Point-Line SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanghao Li, Yu Cao, Qi Chen, Yifan Yang, Jian Pu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In point-line SLAM systems, the utilization of line structural information
and the optimization of lines are two significant problems. The former is
usually addressed through structural regularities, while the latter typically
involves using minimal parameter representations of lines in optimization.
However, separating these two steps leads to the loss of constraint information
to each other. We anchor lines with similar directions to a principal axis and
optimize them with $n+2$ parameters for $n$ lines, solving both problems
together. Our method considers scene structural information, which can be
easily extended to different world hypotheses while significantly reducing the
number of line parameters to be optimized, enabling rapid and accurate mapping
and tracking. To further enhance the system's robustness and avoid mismatch, we
have modeled the line-axis probabilistic data association and provided the
algorithm for axis creation, updating, and optimization. Additionally,
considering that most real-world scenes conform to the Atlanta World
hypothesis, we provide a structural line detection strategy based on vertical
priors and vanishing points. Experimental results and ablation studies on
various indoor and outdoor datasets demonstrate the effectiveness of our
system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaceChain-FACT: Face Adapter with Decoupled Training for
  Identity-preserved Personalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Yu, Haoyu Xie, Lei Shang, Yang Liu, Jun Dan, Baigui Sun, Liefeng Bo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of human-centric personalized image generation, the
adapter-based method obtains the ability to customize and generate portraits by
text-to-image training on facial data. This allows for identity-preserved
personalization without additional fine-tuning in inference. Although there are
improvements in efficiency and fidelity, there is often a significant
performance decrease in test following ability, controllability, and diversity
of generated faces compared to the base model. In this paper, we analyze that
the performance degradation is attributed to the failure to decouple identity
features from other attributes during extraction, as well as the failure to
decouple the portrait generation training from the overall generation task. To
address these issues, we propose the Face Adapter with deCoupled Training
(FACT) framework, focusing on both model architecture and training strategy. To
decouple identity features from others, we leverage a transformer-based
face-export encoder and harness fine-grained identity features. To decouple the
portrait generation training, we propose Face Adapting Increment
Regularization~(FAIR), which effectively constrains the effect of face adapters
on the facial region, preserving the generative ability of the base model.
Additionally, we incorporate a face condition drop and shuffle mechanism,
combined with curriculum learning, to enhance facial controllability and
diversity. As a result, FACT solely learns identity preservation from training
data, thereby minimizing the impact on the original text-to-image capabilities
of the base model. Extensive experiments show that FACT has both
controllability and fidelity in both text-to-image generation and inpainting
solutions for portrait generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAT: Improving Adversarial Robustness via Generative Amplitude Mix-up in
  Frequency Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengpeng Li, Kemou Li, Haiwei Wu, Jinyu Tian, Jiantao Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To protect deep neural networks (DNNs) from adversarial attacks, adversarial
training (AT) is developed by incorporating adversarial examples (AEs) into
model training. Recent studies show that adversarial attacks disproportionately
impact the patterns within the phase of the sample's frequency spectrum --
typically containing crucial semantic information -- more than those in the
amplitude, resulting in the model's erroneous categorization of AEs. We find
that, by mixing the amplitude of training samples' frequency spectrum with
those of distractor images for AT, the model can be guided to focus on phase
patterns unaffected by adversarial perturbations. As a result, the model's
robustness can be improved. Unfortunately, it is still challenging to select
appropriate distractor images, which should mix the amplitude without affecting
the phase patterns. To this end, in this paper, we propose an optimized
Adversarial Amplitude Generator (AAG) to achieve a better tradeoff between
improving the model's robustness and retaining phase patterns. Based on this
generator, together with an efficient AE production procedure, we design a new
Dual Adversarial Training (DAT) strategy. Experiments on various datasets show
that our proposed DAT leads to significantly improved robustness against
diverse adversarial attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consistency Calibration: Improving Uncertainty Calibration via
  Consistency among Perturbed Neighbors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linwei Tao, Haolan Guo, Minjing Dong, Chang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Calibration is crucial in deep learning applications, especially in fields
like healthcare and autonomous driving, where accurate confidence estimates are
vital for decision-making. However, deep neural networks often suffer from
miscalibration, with reliability diagrams and Expected Calibration Error (ECE)
being the only standard perspective for evaluating calibration performance. In
this paper, we introduce the concept of consistency as an alternative
perspective on model calibration, inspired by uncertainty estimation literature
in large language models (LLMs). We highlight its advantages over the
traditional reliability-based view. Building on this concept, we propose a
post-hoc calibration method called Consistency Calibration (CC), which adjusts
confidence based on the model's consistency across perturbed inputs. CC is
particularly effective in locally uncertainty estimation, as it requires no
additional data samples or label information, instead generating input
perturbations directly from the source data. Moreover, we show that performing
perturbations at the logit level significantly improves computational
efficiency. We validate the effectiveness of CC through extensive comparisons
with various post-hoc and training-time calibration methods, demonstrating
state-of-the-art performance on standard datasets such as CIFAR-10, CIFAR-100,
and ImageNet, as well as on long-tailed datasets like ImageNet-LT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical
  Decision-Support Setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Kayser, Bayar Menzat, Cornelius Emde, Bogdan Bercean, Alex Novak, Abdala Espinosa, Bartlomiej W. Papiez, Susanne Gaube, Thomas Lukasiewicz, Oana-Maria Camburu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing capabilities of AI models are leading to their wider use,
including in safety-critical domains. Explainable AI (XAI) aims to make these
models safer to use by making their inference process more transparent.
However, current explainability methods are seldom evaluated in the way they
are intended to be used: by real-world end users. To address this, we conducted
a large-scale user study with 85 healthcare practitioners in the context of
human-AI collaborative chest X-ray analysis. We evaluated three types of
explanations: visual explanations (saliency maps), natural language
explanations, and a combination of both modalities. We specifically examined
how different explanation types influence users depending on whether the AI
advice and explanations are factually correct. We find that text-based
explanations lead to significant over-reliance, which is alleviated by
combining them with saliency maps. We also observe that the quality of
explanations, that is, how much factually correct information they entail, and
how much this aligns with AI correctness, significantly impacts the usefulness
of the different explanation types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controlled Automatic Task-Specific Synthetic Data Generation for
  Hallucination Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Xie, Karan Aggarwal, Aitzaz Ahmad, Stephen Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to automatically generate non-trivial
task-specific synthetic datasets for hallucination detection. Our approach
features a two-step generation-selection pipeline, using hallucination pattern
guidance and a language style alignment during generation. Hallucination
pattern guidance leverages the most important task-specific hallucination
patterns while language style alignment aligns the style of the synthetic
dataset with benchmark text. To obtain robust supervised detectors from
synthetic datasets, we also adopt a data mixture strategy to improve
performance robustness and generalization. Our results on three datasets show
that our generated hallucination text is more closely aligned with
non-hallucinated text versus baselines, to train hallucination detectors with
better generalization. Our hallucination detectors trained on synthetic
datasets outperform in-context-learning (ICL)-based detectors by a large margin
of 32%. Our extensive experiments confirm the benefits of our approach with
cross-task and cross-generator generalization. Our data-mixture-based training
further improves the generalization and robustness of hallucination detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fusion from Decomposition: A Self-Supervised Approach for Image Fusion
  and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengwei Liang, Junjun Jiang, Qing Ma, Xianming Liu, Jiayi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image fusion is famous as an alternative solution to generate one
high-quality image from multiple images in addition to image restoration from a
single degraded image. The essence of image fusion is to integrate
complementary information from source images. Existing fusion methods struggle
with generalization across various tasks and often require labor-intensive
designs, in which it is difficult to identify and extract useful information
from source images due to the diverse requirements of each fusion task.
Additionally, these methods develop highly specialized features for different
downstream applications, hindering the adaptation to new and diverse downstream
tasks. To address these limitations, we introduce DeFusion++, a novel framework
that leverages self-supervised learning (SSL) to enhance the versatility of
feature representation for different image fusion tasks. DeFusion++ captures
the image fusion task-friendly representations from large-scale data in a
self-supervised way, overcoming the constraints of limited fusion datasets.
Specifically, we introduce two innovative pretext tasks: common and unique
decomposition (CUD) and masked feature modeling (MFM). CUD decomposes source
images into abstract common and unique components, while MFM refines these
components into robust fused features. Jointly training of these tasks enables
DeFusion++ to produce adaptable representations that can effectively extract
useful information from various source images, regardless of the fusion task.
The resulting fused representations are also highly adaptable for a wide range
of downstream tasks, including image segmentation and object detection.
DeFusion++ stands out by producing versatile fused representations that can
enhance both the quality of image fusion and the effectiveness of downstream
high-level vision tasks, simplifying the process with the elegant fusion
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18page</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DaDiff: Domain-aware Diffusion Model for Nighttime UAV Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haobo Zuo, Changhong Fu, Guangze Zheng, Liangliang Yao, Kunhan Lu, Jia Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptation is an inspiring solution to the misalignment issue of
day/night image features for nighttime UAV tracking. However, the one-step
adaptation paradigm is inadequate in addressing the prevalent difficulties
posed by low-resolution (LR) objects when viewed from the UAVs at night, owing
to the blurry edge contour and limited detail information. Moreover, these
approaches struggle to perceive LR objects disturbed by nighttime noise. To
address these challenges, this work proposes a novel progressive alignment
paradigm, named domain-aware diffusion model (DaDiff), aligning nighttime LR
object features to the daytime by virtue of progressive and stable generations.
The proposed DaDiff includes an alignment encoder to enhance the detail
information of nighttime LR objects, a tracking-oriented layer designed to
achieve close collaboration with tracking tasks, and a successive distribution
discriminator presented to distinguish different feature distributions at each
diffusion timestep successively. Furthermore, an elaborate nighttime UAV
tracking benchmark is constructed for LR objects, namely NUT-LR, consisting of
100 annotated sequences. Exhaustive experiments have demonstrated the
robustness and feature alignment ability of the proposed DaDiff. The source
code and video demo are available at https://github.com/vision4robotics/DaDiff.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoD-Loc: Aerial Visual Localization using LoD 3D Map with Neural
  Wireframe Alignment <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juelin Zhu, Shen Yan, Long Wang, Shengyue Zhang, Yu Liu, Maojun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new method named LoD-Loc for visual localization in the air.
Unlike existing localization algorithms, LoD-Loc does not rely on complex 3D
representations and can estimate the pose of an Unmanned Aerial Vehicle (UAV)
using a Level-of-Detail (LoD) 3D map. LoD-Loc mainly achieves this goal by
aligning the wireframe derived from the LoD projected model with that predicted
by the neural network. Specifically, given a coarse pose provided by the UAV
sensor, LoD-Loc hierarchically builds a cost volume for uniformly sampled pose
hypotheses to describe pose probability distribution and select a pose with
maximum probability. Each cost within this volume measures the degree of line
alignment between projected and predicted wireframes. LoD-Loc also devises a
6-DoF pose optimization algorithm to refine the previous result with a
differentiable Gaussian-Newton method. As no public dataset exists for the
studied problem, we collect two datasets with map levels of LoD3.0 and LoD2.0,
along with real RGB queries and ground-truth pose annotations. We benchmark our
method and demonstrate that LoD-Loc achieves excellent performance, even
surpassing current state-of-the-art methods that use textured 3D models for
localization. The code and dataset are available at
https://victorzoo.github.io/LoD-Loc.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024; for Project page, see
  https://victorzoo.github.io/LoD-Loc.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing YOLOv5s Object Detection through Knowledge Distillation
  algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanming Huang, Aoran Shen, Yuxiang Hu, Junliang Du, Jiacheng Hu, Yingbin Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the application of knowledge distillation technology in
target detection tasks, especially the impact of different distillation
temperatures on the performance of student models. By using YOLOv5l as the
teacher network and a smaller YOLOv5s as the student network, we found that
with the increase of distillation temperature, the student's detection accuracy
gradually improved, and finally achieved mAP50 and mAP50-95 indicators that
were better than the original YOLOv5s model at a specific temperature.
Experimental results show that appropriate knowledge distillation strategies
can not only improve the accuracy of the model but also help improve the
reliability and stability of the model in practical applications. This paper
also records in detail the accuracy curve and loss function descent curve
during the model training process and shows that the model converges to a
stable state after 150 training cycles. These findings provide a theoretical
basis and technical reference for further optimizing target detection
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Healthcare: Innovative ML Approaches for Improved Medical
  Imaging in Data-Constrained Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Al Amin, Kamrul Hasan, Saleh Zein-Sabatto, Liang Hong, Sachin Shetty, Imtiaz Ahmed, Tariqul Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Healthcare industries face challenges when experiencing rare diseases due to
limited samples. Artificial Intelligence (AI) communities overcome this
situation to create synthetic data which is an ethical and privacy issue in the
medical domain. This research introduces the CAT-U-Net framework as a new
approach to overcome these limitations, which enhances feature extraction from
medical images without the need for large datasets. The proposed framework adds
an extra concatenation layer with downsampling parts, thereby improving its
ability to learn from limited data while maintaining patient privacy. To
validate, the proposed framework's robustness, different medical conditioning
datasets were utilized including COVID-19, brain tumors, and wrist fractures.
The framework achieved nearly 98% reconstruction accuracy, with a Dice
coefficient close to 0.946. The proposed CAT-U-Net has the potential to make a
big difference in medical image diagnostics in settings with limited data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EG-HumanNeRF: Efficient Generalizable Human NeRF Utilizing Human Prior
  for Sparse View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaorong Wang, Yoshihiro Kanamori, Yuki Endo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalizable neural radiance field (NeRF) enables neural-based digital human
rendering without per-scene retraining. When combined with human prior
knowledge, high-quality human rendering can be achieved even with sparse input
views. However, the inference of these methods is still slow, as a large number
of neural network queries on each ray are required to ensure the rendering
quality. Moreover, occluded regions often suffer from artifacts, especially
when the input views are sparse. To address these issues, we propose a
generalizable human NeRF framework that achieves high-quality and real-time
rendering with sparse input views by extensively leveraging human prior
knowledge. We accelerate the rendering with a two-stage sampling reduction
strategy: first constructing boundary meshes around the human geometry to
reduce the number of ray samples for sampling guidance regression, and then
volume rendering using fewer guided samples. To improve rendering quality,
especially in occluded regions, we propose an occlusion-aware attention
mechanism to extract occlusion information from the human priors, followed by
an image space refinement network to improve rendering quality. Furthermore,
for volume rendering, we adopt a signed ray distance function (SRDF)
formulation, which allows us to propose an SRDF loss at every sample position
to improve the rendering quality further. Our experiments demonstrate that our
method outperforms the state-of-the-art methods in rendering quality and has a
competitive rendering speed compared with speed-prioritized novel view
synthesis methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://github.com/LarsPh/EG-HumanNeRF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Spatial Attention and Edge Context for Optimized Feature
  Selection in Visual Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nanda Febri Istighfarin, HyungGi Jo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual localization determines an agent's precise position and orientation
within an environment using visual data. It has become a critical task in the
field of robotics, particularly in applications such as autonomous navigation.
This is due to the ability to determine an agent's pose using cost-effective
sensors such as RGB cameras. Recent methods in visual localization employ scene
coordinate regression to determine the agent's pose. However, these methods
face challenges as they attempt to regress 2D-3D correspondences across the
entire image region, despite not all regions providing useful information. To
address this issue, we introduce an attention network that selectively targets
informative regions of the image. Using this network, we identify the
highest-scoring features to improve the feature selection process and combine
the result with edge detection. This integration ensures that the features
chosen for the training buffer are located within robust regions, thereby
improving 2D-3D correspondence and overall localization performance. Our
approach was tested on the outdoor benchmark dataset, demonstrating superior
results compared to previous methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Cascaded Methods of <span class="highlight-title">Vision-Language</span> Models for Zero-Shot
  Detection and Association of Hardhats for Increased Construction Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Choi, Ross Greer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper evaluates the use of vision-language models (VLMs) for zero-shot
detection and association of hardhats to enhance construction safety. Given the
significant risk of head injuries in construction, proper enforcement of
hardhat use is critical. We investigate the applicability of foundation models,
specifically OWLv2, for detecting hardhats in real-world construction site
images. Our contributions include the creation of a new benchmark dataset,
Hardhat Safety Detection Dataset, by filtering and combining existing datasets
and the development of a cascaded detection approach. Experimental results on
5,210 images demonstrate that the OWLv2 model achieves an average precision of
0.6493 for hardhat detection. We further analyze the limitations and potential
improvements for real-world applications, highlighting the strengths and
weaknesses of current foundation models in safety perception domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Order-Aware Interactive Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Wang, Anwesa Choudhuri, Meng Zheng, Zhongpai Gao, Benjamin Planche, Andong Deng, Qin Liu, Terrence Chen, Ulas Bagci, Ziyan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive segmentation aims to accurately segment target objects with
minimal user interactions. However, current methods often fail to accurately
separate target objects from the background, due to a limited understanding of
order, the relative depth between objects in a scene. To address this issue, we
propose OIS: order-aware interactive segmentation, where we explicitly encode
the relative depth between objects into order maps. We introduce a novel
order-aware attention, where the order maps seamlessly guide the user
interactions (in the form of clicks) to attend to the image features. We
further present an object-aware attention module to incorporate a strong
object-level understanding to better differentiate objects with similar order.
Our approach allows both dense and sparse integration of user clicks, enhancing
both accuracy and efficiency as compared to prior works. Experimental results
demonstrate that OIS achieves state-of-the-art performance, improving mIoU
after one click by 7.61 on the HQSeg44K dataset and 1.32 on the DAVIS dataset
as compared to the previous state-of-the-art SegNext, while also doubling
inference speed compared to current leading methods. The project page is
https://ukaukaaaa.github.io/projects/OIS/index.html
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Interactive demo can be found in project page:
  https://ukaukaaaa.github.io/projects/OIS/index.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Prototype Network for Explainable Pedestrian Behavior Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Feng, Alexander Carballo, Kazuya Takeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting pedestrian behavior is challenging yet crucial for applications
such as autonomous driving and smart city. Recent deep learning models have
achieved remarkable performance in making accurate predictions, but they fail
to provide explanations of their inner workings. One reason for this problem is
the multi-modal inputs. To bridge this gap, we present Sparse Prototype Network
(SPN), an explainable method designed to simultaneously predict a pedestrian's
future action, trajectory, and pose. SPN leverages an intermediate prototype
bottleneck layer to provide sample-based explanations for its predictions. The
prototypes are modality-independent, meaning that they can correspond to any
modality from the input. Therefore, SPN can extend to arbitrary combinations of
modalities. Regularized by mono-semanticity and clustering constraints, the
prototypes learn consistent and human-understandable features and achieve
state-of-the-art performance on action, trajectory and pose prediction on TITAN
and PIE. Finally, we propose a metric named Top-K Mono-semanticity Scale to
quantitatively evaluate the explainability. Qualitative results show the
positive correlation between sparsity and explainability. Code available at
https://github.com/Equinoxxxxx/SPN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test-time adaptation for image compression with distribution
  regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kecheng Chen, Pingping Zhang, Tiexin Qin, Shiqi Wang, Hong Yan, Haoliang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current test- or compression-time adaptation image compression (TTA-IC)
approaches, which leverage both latent and decoder refinements as a two-step
adaptation scheme, have potentially enhanced the rate-distortion (R-D)
performance of learned image compression models on cross-domain compression
tasks, \textit{e.g.,} from natural to screen content images. However, compared
with the emergence of various decoder refinement variants, the latent
refinement, as an inseparable ingredient, is barely tailored to cross-domain
scenarios. To this end, we aim to develop an advanced latent refinement method
by extending the effective hybrid latent refinement (HLR) method, which is
designed for \textit{in-domain} inference improvement but shows noticeable
degradation of the rate cost in \textit{cross-domain} tasks. Specifically, we
first provide theoretical analyses, in a cue of marginalization approximation
from in- to cross-domain scenarios, to uncover that the vanilla HLR suffers
from an underlying mismatch between refined Gaussian conditional and hyperprior
distributions, leading to deteriorated joint probability approximation of
marginal distribution with increased rate consumption. To remedy this issue, we
introduce a simple Bayesian approximation-endowed \textit{distribution
regularization} to encourage learning a better joint probability approximation
in a plug-and-play manner. Extensive experiments on six in- and cross-domain
datasets demonstrate that our proposed method not only improves the R-D
performance compared with other latent refinement counterparts, but also can be
flexibly integrated into existing TTA-IC methods with incremental benefits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransAgent: Transfer <span class="highlight-title">Vision-Language</span> Foundation Models with
  Heterogeneous Agent Collaboration <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Guo, Shaobin Zhuang, Kunchang Li, Yu Qiao, Yali Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language foundation models (such as CLIP) have recently shown their
power in transfer learning, owing to large-scale image-text pre-training.
However, target domain data in the downstream tasks can be highly different
from the pre-training phase, which makes it hard for such a single model to
generalize well. Alternatively, there exists a wide range of expert models that
contain diversified vision and/or language knowledge pre-trained on different
modalities, tasks, networks, and datasets. Unfortunately, these models are
"isolated agents" with heterogeneous structures, and how to integrate their
knowledge for generalizing CLIP-like models has not been fully explored. To
bridge this gap, we propose a general and concise TransAgent framework, which
transports the knowledge of the isolated agents in a unified manner, and
effectively guides CLIP to generalize with multi-source knowledge distillation.
With such a distinct framework, we flexibly collaborate with 11 heterogeneous
agents to empower vision-language foundation models, without further cost in
the inference phase. Finally, our TransAgent achieves state-of-the-art
performance on 11 visual recognition datasets. Under the same low-shot setting,
it outperforms the popular CoOp with around 10% on average, and 20% on EuroSAT
which contains large domain shifts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-Model Distillation for Efficient Action Classification with Hybrid
  Edge-Cloud Solution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothy Wei, Hsien Xin Peng, Elaine Xu, Bryan Zhao, Lei Ding, Diji Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Artificial Intelligence models, such as Large Video-Language models
(VLMs), grow in size, their deployment in real-world applications becomes
increasingly challenging due to hardware limitations and computational costs.
To address this, we design a hybrid edge-cloud solution that leverages the
efficiency of smaller models for local processing while deferring to larger,
more accurate cloud-based models when necessary. Specifically, we propose a
novel unsupervised data generation method, Dual-Model Distillation (DMD), to
train a lightweight switcher model that can predict when the edge model's
output is uncertain and selectively offload inference to the large model in the
cloud. Experimental results on the action classification task show that our
framework not only requires less computational overhead, but also improves
accuracy compared to using a large model alone. Our framework provides a
scalable and adaptable solution for action classification in
resource-constrained environments, with potential applications beyond
healthcare. Noteworthy, while DMD-generated data is used for optimizing
performance and resource usage in our pipeline, we expect the concept of DMD to
further support future research on knowledge alignment across multiple models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAM-Guided Masked Token Prediction for 3D Scene Understanding <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhimin Chen, Liang Yang, Yingwei Li, Longlong Jing, Bing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models have significantly enhanced 2D task performance, and recent
works like Bridge3D have successfully applied these models to improve 3D scene
understanding through knowledge distillation, marking considerable
advancements. Nonetheless, challenges such as the misalignment between 2D and
3D representations and the persistent long-tail distribution in 3D datasets
still restrict the effectiveness of knowledge distillation from 2D to 3D using
foundation models. To tackle these issues, we introduce a novel SAM-guided
tokenization method that seamlessly aligns 3D transformer structures with
region-level knowledge distillation, replacing the traditional KNN-based
tokenization techniques. Additionally, we implement a group-balanced
re-weighting strategy to effectively address the long-tail problem in knowledge
distillation. Furthermore, inspired by the recent success of masked feature
prediction, our framework incorporates a two-stage masked token prediction
process in which the student model predicts both the global embeddings and the
token-wise local embeddings derived from the teacher models trained in the
first stage. Our methodology has been validated across multiple datasets,
including SUN RGB-D, ScanNet, and S3DIS, for tasks like 3D object detection and
semantic segmentation. The results demonstrate significant improvements over
current State-of-the-art self-supervised methods, establishing new benchmarks
in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling the Limits of Alignment: <span class="highlight-title">Multi-modal</span> Dynamic Local Fusion
  Network and A Benchmark for Unaligned RGBT Video Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qishun Wang, Zhengzheng Tu, Kunpeng Wang, Le Gu, Chuanwang Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current RGB-Thermal Video Object Detection (RGBT VOD) methods still depend on
manually aligning data at the image level, which hampers its practical
application in real-world scenarios since image pairs captured by multispectral
sensors often differ in both fields of view and resolution. To address this
limitation, we propose a Multi-modal Dynamic Local fusion Network (MDLNet)
designed to handle unaligned RGBT image pairs. Specifically, our proposed
Multi-modal Dynamic Local Fusion (MDLF) module includes a set of predefined
boxes, each enhanced with random Gaussian noise to generate a dynamic box. Each
box selects a local region from the original high-resolution RGB image. This
region is then fused with the corresponding information from another modality
and reinserted into the RGB. This method adapts to various data alignment
scenarios by interacting with local features across different ranges.
Simultaneously, we introduce a Cascaded Temporal Scrambler (CTS) within an
end-to-end architecture. This module leverages consistent spatiotemporal
information from consecutive frames to enhance the representation capability of
the current frame while maintaining network efficiency. We have curated an open
dataset called UVT-VOD2024 for unaligned RGBT VOD. It consists of 30,494 pairs
of unaligned RGBT images captured directly from a multispectral camera. We
conduct a comprehensive evaluation and comparison with MDLNet and
state-of-the-art (SOTA) models, demonstrating the superior effectiveness of
MDLNet. We will release our code and UVT-VOD2024 to the public for further
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task Consistent Prototype Learning for Incremental Few-shot Semantic
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbo Xu, Yanan Wu, Haoran Jiang, Yang Wang, Qiang Wu, Jian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incremental Few-Shot Semantic Segmentation (iFSS) tackles a task that
requires a model to continually expand its segmentation capability on novel
classes using only a few annotated examples. Typical incremental approaches
encounter a challenge that the objective of the base training phase (fitting
base classes with sufficient instances) does not align with the incremental
learning phase (rapidly adapting to new classes with less forgetting). This
disconnect can result in suboptimal performance in the incremental setting.
This study introduces a meta-learning-based prototype approach that encourages
the model to learn how to adapt quickly while preserving previous knowledge.
Concretely, we mimic the incremental evaluation protocol during the base
training session by sampling a sequence of pseudo-incremental tasks. Each task
in the simulated sequence is trained using a meta-objective to enable rapid
adaptation without forgetting. To enhance discrimination among class
prototypes, we introduce prototype space redistribution learning, which
dynamically updates class prototypes to establish optimal inter-prototype
boundaries within the prototype space. Extensive experiments on iFSS datasets
built upon PASCAL and COCO benchmarks show the advanced performance of the
proposed approach, offering valuable insights for addressing iFSS challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preserving Cardiac Integrity: A Topology-Infused Approach to Whole Heart
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Zhang, Wenxue Guan, Xiaodan Xing, Guang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole heart segmentation (WHS) supports cardiovascular disease (CVD)
diagnosis, disease monitoring, treatment planning, and prognosis. Deep learning
has become the most widely used method for WHS applications in recent years.
However, segmentation of whole-heart structures faces numerous challenges
including heart shape variability during the cardiac cycle, clinical artifacts
like motion and poor contrast-to-noise ratio, domain shifts in multi-center
data, and the distinct modalities of CT and MRI. To address these limitations
and improve segmentation quality, this paper introduces a new
topology-preserving module that is integrated into deep neural networks. The
implementation achieves anatomically plausible segmentation by using learned
topology-preserving fields, which are based entirely on 3D convolution and are
therefore very effective for 3D voxel data. We incorporate natural constraints
between structures into the end-to-end training and enrich the feature
representation of the neural network. The effectiveness of the proposed method
is validated on an open-source medical heart dataset, specifically using the
WHS++ data. The results demonstrate that the architecture performs
exceptionally well, achieving a Dice coefficient of 0.939 during testing. This
indicates full topology preservation for individual structures and
significantly outperforms other baselines in preserving the overall scene
topology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AssemAI: Interpretable Image-Based Anomaly Detection for Manufacturing
  Pipelines <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02181v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02181v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renjith Prasad, Chathurangi Shyalika, Ramtin Zand, Fadi El Kalach, Revathy Venkataramanan, Ramy Harik, Amit Sheth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection in manufacturing pipelines remains a critical challenge,
intensified by the complexity and variability of industrial environments. This
paper introduces AssemAI, an interpretable image-based anomaly detection system
tailored for smart manufacturing pipelines. Utilizing a curated image dataset
from an industry-focused rocket assembly pipeline, we address the challenge of
imbalanced image data and demonstrate the importance of image-based methods in
anomaly detection. Our primary contributions include deriving an image dataset,
fine-tuning an object detection model YOLO-FF, and implementing a custom
anomaly detection model for assembly pipelines. The proposed approach leverages
domain knowledge in data preparation, model development and reasoning. We
implement several anomaly detection models on the derived image dataset,
including a Convolutional Neural Network, Vision Transformer (ViT), and
pre-trained versions of these models. Additionally, we incorporate
explainability techniques at both user and model levels, utilizing ontology for
user-level explanations and SCORE-CAM for in-depth feature and model analysis.
Finally, the best-performing anomaly detection model and YOLO-FF are deployed
in a real-time setting. Our results include ablation studies on the baselines
and a comprehensive evaluation of the proposed system. This work highlights the
broader impact of advanced image-based anomaly detection in enhancing the
reliability and efficiency of smart manufacturing processes. The image dataset,
codes to reproduce the results and additional experiments are available at
https://github.com/renjithk4/AssemAI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 Pages, 6 Figures, 4 Tables, Predictive Models in Engineering
  Applications special session (MLPMEA )at International Conference on Machine
  Learning and Applications (ICMLA) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Figurative Meaning through Explainable Visual Entailment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01474v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01474v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arkadiy Saakyan, Shreyas Kulkarni, Tuhin Chakrabarty, Smaranda Muresan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (VLMs) have demonstrated strong capabilities in
tasks requiring a fine-grained understanding of literal meaning in images and
text, such as visual question-answering or visual entailment. However, there
has been little exploration of these models' capabilities when presented with
images and captions containing figurative meaning, such as metaphors or humor.
To close this gap, we propose a new task framing the figurative meaning
understanding problem as an explainable visual entailment task, where the model
has to predict whether the image (premise) entails a caption (hypothesis) and
justify the predicted label with a textual explanation. The figurative
phenomena can be present either in the image, the caption, or both. Utilizing a
human-AI collaboration approach, we build the accompanying expert-verified
dataset V-FLUTE, containing 6,027 {image, caption, label, explanation}
instances spanning five diverse figurative phenomena: metaphors, similes,
idioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs
struggle to generalize from literal to figurative meaning, particularly when it
is present in images. Further, we identify common types of errors in VLM
reasoning via human evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Inversion with Timestep-aware Sampling for Training-free
  Non-rigid Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08601v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08601v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunji Jung, Seokju Lee, Tair Djanibekov, Hyunjung Shim, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-guided non-rigid editing involves complex edits for input images, such
as changing motion or compositions within their surroundings. Since it requires
manipulating the input structure, existing methods often struggle with
preserving object identity and background, particularly when combined with
Stable Diffusion. In this work, we propose a training-free approach for
non-rigid editing with Stable Diffusion, aimed at improving the identity
preservation quality without compromising editability. Our approach comprises
three stages: text optimization, latent inversion, and timestep-aware text
injection sampling. Inspired by the success of Imagic, we employ their text
optimization for smooth editing. Then, we introduce latent inversion to
preserve the input image's identity without additional model fine-tuning. To
fully utilize the input reconstruction ability of latent inversion, we suggest
timestep-aware text injection sampling. This effectively retains the structure
of the input image by injecting the source text prompt in early sampling steps
and then transitioning to the target prompt in subsequent sampling steps. This
strategic approach seamlessly harmonizes with text optimization, facilitating
complex non-rigid edits to the input without losing the original identity. We
demonstrate the effectiveness of our method in terms of identity preservation,
editability, and aesthetic quality through extensive experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript has been submitted to Pattern Recognition Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-supervised Learning of LiDAR 3D Point Clouds via 2D-3D Neural
  Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12452v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12452v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zhang, Siyu Ren, Junhui Hou, Jinjian Wu, Yixuan Yuan, Guangming Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel self-supervised learning framework for
enhancing 3D perception in autonomous driving scenes. Specifically, our
approach, namely NCLR, focuses on 2D-3D neural calibration, a novel pretext
task that estimates the rigid pose aligning camera and LiDAR coordinate
systems. First, we propose the learnable transformation alignment to bridge the
domain gap between image and point cloud data, converting features into a
unified representation space for effective comparison and matching. Second, we
identify the overlapping area between the image and point cloud with the fused
features. Third, we establish dense 2D-3D correspondences to estimate the rigid
pose. The framework not only learns fine-grained matching from points to pixels
but also achieves alignment of the image and point cloud at a holistic level,
understanding their relative pose. We demonstrate the efficacy of NCLR by
applying the pre-trained backbone to downstream tasks, such as LiDAR-based 3D
semantic segmentation, object detection, and panoptic segmentation.
Comprehensive experiments on various datasets illustrate the superiority of
NCLR over existing self-supervised methods. The results confirm that joint
learning from different modalities significantly enhances the network's
understanding abilities and effectiveness of learned representation. The code
is publicly available at https://github.com/Eaphan/NCLR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Tuning Towards Parameter and Inference Efficiency for ViT
  Adaptation <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11808v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11808v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangbo Zhao, Jiasheng Tang, Yizeng Han, Yibing Song, Kai Wang, Gao Huang, Fan Wang, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing parameter-efficient fine-tuning (PEFT) methods have achieved
significant success on vision transformers (ViTs) adaptation by improving
parameter efficiency. However, the exploration of enhancing inference
efficiency during adaptation remains underexplored. This limits the broader
application of pre-trained ViT models, especially when the model is
computationally extensive. In this paper, we propose Dynamic Tuning (DyT), a
novel approach to improve both parameter and inference efficiency for ViT
adaptation. Specifically, besides using the lightweight adapter modules, we
propose a token dispatcher to distinguish informative tokens from less
important ones, allowing the latter to dynamically skip the original block,
thereby reducing the redundant computation during inference. Additionally, we
explore multiple design variants to find the best practice of DyT. Finally,
inspired by the mixture-of-experts (MoE) mechanism, we introduce an enhanced
adapter to further boost the adaptation performance. We validate DyT across
various tasks, including image/video recognition and semantic segmentation. For
instance, DyT achieves superior performance compared to existing PEFT methods
while evoking only 71% of their FLOPs on the VTAB-1K benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ScaleFlow++: Robust and Accurate Estimation of 3D Motion from Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Ling, Quansen Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceiving and understanding 3D motion is a core technology in fields such as
autonomous driving, robots, and motion prediction. This paper proposes a 3D
motion perception method called ScaleFlow++ that is easy to generalize. With
just a pair of RGB images, ScaleFlow++ can robustly estimate optical flow and
motion-in-depth (MID). Most existing methods directly regress MID from two RGB
frames or optical flow, resulting in inaccurate and unstable results. Our key
insight is cross-scale matching, which extracts deep motion clues by matching
objects in pairs of images at different scales. Unlike previous methods,
ScaleFlow++ integrates optical flow and MID estimation into a unified
architecture, estimating optical flow and MID end-to-end based on feature
matching. Moreover, we also proposed modules such as global initialization
network, global iterative optimizer, and hybrid training pipeline to integrate
global motion information, reduce the number of iterations, and prevent
overfitting during training. On KITTI, ScaleFlow++ achieved the best monocular
scene flow estimation performance, reducing SF-all from 6.21 to 5.79. The
evaluation of MID even surpasses RGBD-based methods. In addition, ScaleFlow++
has achieved stunning zero-shot generalization performance in both rigid and
nonrigid scenes. Code is available at
\url{https://github.com/HanLingsgjk/CSCV}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages; Previously this version appeared as arXiv:2409.12202 which
  was submitted as a new work by accident</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic Token Reweighting for Interpretable and Controllable Text
  Embeddings in <span class="highlight-title">CLIP</span> <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eunji Kim, Kyuhong Shim, Simyung Chang, Sungroh Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial
role in translating textual input into an embedding space shared with images,
thereby facilitating the interpretative analysis of vision tasks through
natural language. Despite the varying significance of different textual
elements within a sentence depending on the context, efforts to account for
variation of importance in constructing text embeddings have been lacking. We
propose a framework of Semantic Token Reweighting to build Interpretable text
embeddings (SToRI), which incorporates controllability as well. SToRI refines
the text encoding process in CLIP by differentially weighting semantic elements
based on contextual importance, enabling finer control over emphasis responsive
to data-driven insights and user preferences. The efficacy of SToRI is
demonstrated through comprehensive experiments on few-shot image classification
and image retrieval tailored to user preferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient and Effective Universal Adversarial Attack against
  <span class="highlight-title">Vision-Language</span> <span class="highlight-title">Pre-train</span>ing Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11639v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11639v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Yang, Yihao Huang, Kailong Wang, Ling Shi, Geguang Pu, Yang Liu, Haoyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language pre-training (VLP) models, trained on large-scale image-text
pairs, have become widely used across a variety of downstream
vision-and-language (V+L) tasks. This widespread adoption raises concerns about
their vulnerability to adversarial attacks. Non-universal adversarial attacks,
while effective, are often impractical for real-time online applications due to
their high computational demands per data instance. Recently, universal
adversarial perturbations (UAPs) have been introduced as a solution, but
existing generator-based UAP methods are significantly time-consuming. To
overcome the limitation, we propose a direct optimization-based UAP approach,
termed DO-UAP, which significantly reduces resource consumption while
maintaining high attack performance. Specifically, we explore the necessity of
multimodal loss design and introduce a useful data augmentation strategy.
Extensive experiments conducted on three benchmark VLP datasets, six popular
VLP models, and three classical downstream tasks demonstrate the efficiency and
effectiveness of DO-UAP. Specifically, our approach drastically decreases the
time consumption by 23-fold while achieving a better attack performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Convolutional Neural Network for Image Super-resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15704v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15704v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunwei Tian, Xuanyu Zhang, Tao Wang, Yongjun Zhang, Qi Zhu, Chia-Wen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks can automatically learn features via deep
network architectures and given input samples. However, the robustness of
obtained models may face challenges in varying scenes. Bigger differences in
network architecture are beneficial to extract more diversified structural
information to strengthen the robustness of an obtained super-resolution model.
In this paper, we proposed a adaptive convolutional neural network for image
super-resolution (ADSRNet). To capture more information, ADSRNet is implemented
by a heterogeneous parallel network. The upper network can enhance relation of
context information, salient information relation of a kernel mapping and
relations of shallow and deep layers to improve performance of image
super-resolution. That can strengthen adaptability of an obtained
super-resolution model for different scenes. The lower network utilizes a
symmetric architecture to enhance relations of different layers to mine more
structural information, which is complementary with a upper network for image
super-resolution. The relevant experimental results show that the proposed
ADSRNet is effective to deal with image resolving. Codes are obtained at
https://github.com/hellloxiaotian/ADSRNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Diffusion Models: A Comprehensive Survey from Principles to
  Practices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Ma, Yuzhu Zhang, Guoli Jia, Liangliang Zhao, Yichao Ma, Mingjie Ma, Gaofeng Liu, Kaiyan Zhang, Jianjun Li, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As one of the most popular and sought-after generative models in the recent
years, diffusion models have sparked the interests of many researchers and
steadily shown excellent advantage in various generative tasks such as image
synthesis, video generation, molecule design, 3D scene rendering and multimodal
generation, relying on their dense theoretical principles and reliable
application practices. The remarkable success of these recent efforts on
diffusion models comes largely from progressive design principles and efficient
architecture, training, inference, and deployment methodologies. However, there
has not been a comprehensive and in-depth review to summarize these principles
and practices to help the rapid understanding and application of diffusion
models. In this survey, we provide a new efficiency-oriented perspective on
these existing efforts, which mainly focuses on the profound principles and
efficient practices in architecture designs, model training, fast inference and
reliable deployment, to guide further theoretical research, algorithm migration
and model application for new scenarios in a reader-friendly way.
\url{https://github.com/ponyzym/Efficient-DMs-Survey}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Developing Generalist Foundation Models from a <span class="highlight-title">Multimodal</span> Dataset for 3D
  Computed Tomography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17834v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17834v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ibrahim Ethem Hamamci, Sezgin Er, Furkan Almas, Ayse Gulnihan Simsek, Sevval Nil Esirgun, Irem Dogan, Muhammed Furkan Dasdelen, Omer Faruk Durugol, Bastian Wittmann, Tamaz Amiranashvili, Enis Simsar, Mehmet Simsar, Emine Bensu Erdemir, Abdullah Alanbay, Anjany Sekuboyina, Berkan Lafci, Christian Bluethgen, Mehmet Kemal Ozdemir, Bjoern Menze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While computer vision has achieved tremendous success with multimodal
encoding and direct textual interaction with images via chat-based large
language models, similar advancements in medical imaging AI, particularly in 3D
imaging, have been limited due to the scarcity of comprehensive datasets. To
address this critical gap, we introduce CT-RATE, the first dataset that pairs
3D medical images with corresponding textual reports. CT-RATE comprises 25,692
non-contrast 3D chest CT scans from 21,304 unique patients. Through various
reconstructions, these scans are expanded to 50,188 volumes, totaling over 14.3
million 2D slices. Each scan is accompanied by its corresponding radiology
report. Leveraging CT-RATE, we develop CT-CLIP, a CT-focused contrastive
language-image pretraining framework designed for broad applications without
the need for task-specific training. We demonstrate how CT-CLIP can be used in
two tasks: multi-abnormality detection and case retrieval. Remarkably, in
multi-abnormality detection, CT-CLIP outperforms state-of-the-art fully
supervised models across all key metrics, effectively eliminating the need for
manual annotation. In case retrieval, it efficiently retrieves relevant cases
using either image or textual queries, thereby enhancing knowledge
dissemination. By combining CT-CLIP's vision encoder with a pretrained large
language model, we create CT-CHAT, a vision-language foundational chat model
for 3D chest CT volumes. Finetuned on over 2.7 million question-answer pairs
derived from the CT-RATE dataset, CT-CHAT surpasses other multimodal AI
assistants, underscoring the necessity for specialized methods in 3D medical
imaging. Collectively, the open-source release of CT-RATE, CT-CLIP, and CT-CHAT
not only addresses critical challenges in 3D medical imaging but also lays the
groundwork for future innovations in medical AI and improved patient care.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixture of Experts Made Personalized: Federated <span class="highlight-title">Prompt</span> Learning for
  <span class="highlight-title">Vision-Language</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Luo, Chen Chen, Shandong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt learning for pre-trained Vision-Language Models (VLMs) like CLIP has
demonstrated potent applicability across diverse downstream tasks. This
lightweight approach has quickly gained traction from federated learning (FL)
researchers who seek to efficiently adapt VLMs to heterogeneous scenarios.
However, current federated prompt learning methods are habitually restricted to
the traditional FL paradigm, where the participating clients are generally only
allowed to download a single globally aggregated model from the server. While
justifiable for training full-sized models under federated settings, in this
work, we argue that this paradigm is ill-suited for lightweight prompts. By
facilitating the clients to download multiple pre-aggregated prompts as fixed
non-local experts, we propose Personalized Federated Mixture of Adaptive
Prompts (pFedMoAP), a novel FL framework that personalizes the prompt learning
process through the lens of Mixture of Experts (MoE). pFedMoAP implements a
local attention-based gating network that learns to generate enhanced text
features for better alignment with local image data on the client, benefiting
from both local and downloaded non-local adaptive prompt experts. The non-local
experts are sparsely selected from a server-maintained pool, fostering
collaborative learning across clients. To evaluate the proposed algorithm, we
conduct extensive experiments across 9 datasets under various heterogeneous
federated settings. The results show that pFedMoAP consistently outperforms the
state-of-the-art alternatives, underscoring its efficacy in personalizing
prompt learning for CLIP within the federated learning paradigm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpret Your Decision: Logical Reasoning Regularization for
  Generalization in Visual Classification <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04492v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04492v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaorui Tan, Xi Yang, Qiufeng Wang, Anh Nguyen, Kaizhu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision models excel in image classification but struggle to generalize to
unseen data, such as classifying images from unseen domains or discovering
novel categories. In this paper, we explore the relationship between logical
reasoning and deep learning generalization in visual classification. A logical
regularization termed L-Reg is derived which bridges a logical analysis
framework to image classification. Our work reveals that L-Reg reduces the
complexity of the model in terms of the feature distribution and classifier
weights. Specifically, we unveil the interpretability brought by L-Reg, as it
enables the model to extract the salient features, such as faces to persons,
for classification. Theoretical analysis and experiments demonstrate that L-Reg
enhances generalization across various scenarios, including multi-domain
generalization and generalized category discovery. In complex real-world
scenarios where images span unknown classes and unseen domains, L-Reg
consistently improves generalization, highlighting its practical efficacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024 as Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A3D: Does Diffusion Dream about 3D Alignment? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15020v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15020v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Savva Ignatyev, Nina Konovalova, Daniil Selikhanovych, Oleg Voynov, Nikolay Patakin, Ilya Olkov, Dmitry Senushkin, Alexey Artemov, Anton Konushin, Alexander Filippov, Peter Wonka, Evgeny Burnaev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the problem of text-driven 3D generation from a geometry alignment
perspective. Given a set of text prompts, we aim to generate a collection of
objects with semantically corresponding parts aligned across them. Recent
methods based on Score Distillation have succeeded in distilling the knowledge
from 2D diffusion models to high-quality representations of the 3D objects.
These methods handle multiple text queries separately, and therefore the
resulting objects have a high variability in object pose and structure.
However, in some applications, such as 3D asset design, it may be desirable to
obtain a set of objects aligned with each other. In order to achieve the
alignment of the corresponding parts of the generated objects, we propose to
embed these objects into a common latent space and optimize the continuous
transitions between these objects. We enforce two kinds of properties of these
transitions: smoothness of the transition and plausibility of the intermediate
objects along the transition. We demonstrate that both of these properties are
essential for good alignment. We provide several practical scenarios that
benefit from alignment between the objects, including 3D editing and object
hybridization, and experimentally demonstrate the effectiveness of our method.
https://voyleg.github.io/a3d/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian Primitives for Deformable Image Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03394v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03394v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihe Li, Xiang Liu, Fabian Zhang, Xia Li, Xixin Cao, Ye Zhang, Joachim Buhmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deformable Image Registration (DIR) is essential for aligning medical images
that exhibit anatomical variations, facilitating applications such as disease
tracking and radiotherapy planning. While classical iterative methods and deep
learning approaches have achieved success in DIR, they are often hindered by
computational inefficiency or poor generalization. In this paper, we introduce
GaussianDIR, a novel, case-specific optimization DIR method inspired by 3D
Gaussian splatting. In general, GaussianDIR represents image deformations using
a sparse set of mobile and flexible Gaussian primitives, each defined by a
center position, covariance, and local rigid transformation. This compact and
explicit representation reduces noise and computational overhead while
improving interpretability. Furthermore, the movement of individual voxel is
derived via blending the local rigid transformation of the neighboring Gaussian
primitives. By this, GaussianDIR captures both global smoothness and local
rigidity as well as reduces the computational burden. To address varying levels
of deformation complexity, GaussianDIR also integrates an adaptive density
control mechanism that dynamically adjusts the density of Gaussian primitives.
Additionally, we employ multi-scale Gaussian primitives to capture both coarse
and fine deformations, reducing optimization to local minima. Experimental
results on brain MRI, lung CT, and cardiac MRI datasets demonstrate that
GaussianDIR outperforms existing DIR methods in both accuracy and efficiency,
highlighting its potential for clinical applications. Finally, as a
training-free approach, it challenges the stereotype that iterative methods are
inherently slow and transcend the limitations of poor generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BroadWay: Boost Your Text-to-Video Generation Model in a Training-free
  Way 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazi Bu, Pengyang Ling, Pan Zhang, Tong Wu, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Dahua Lin, Jiaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The text-to-video (T2V) generation models, offering convenient visual
creation, have recently garnered increasing attention. Despite their
substantial potential, the generated videos may present artifacts, including
structural implausibility, temporal inconsistency, and a lack of motion, often
resulting in near-static video. In this work, we have identified a correlation
between the disparity of temporal attention maps across different blocks and
the occurrence of temporal inconsistencies. Additionally, we have observed that
the energy contained within the temporal attention maps is directly related to
the magnitude of motion amplitude in the generated videos. Based on these
observations, we present BroadWay, a training-free method to improve the
quality of text-to-video generation without introducing additional parameters,
augmenting memory or sampling time. Specifically, BroadWay is composed of two
principal components: 1) Temporal Self-Guidance improves the structural
plausibility and temporal consistency of generated videos by reducing the
disparity between the temporal attention maps across various decoder blocks. 2)
Fourier-based Motion Enhancement enhances the magnitude and richness of motion
by amplifying the energy of the map. Extensive experiments demonstrate that
BroadWay significantly improves the quality of text-to-video generation with
negligible additional cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VrdONE: One-stage Video Visual Relation Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09408v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09408v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinjie Jiang, Chenxi Zheng, Xuemiao Xu, Bangzhen Liu, Weiying Zheng, Huaidong Zhang, Shengfeng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Visual Relation Detection (VidVRD) focuses on understanding how
entities interact over time and space in videos, a key step for gaining deeper
insights into video scenes beyond basic visual tasks. Traditional methods for
VidVRD, challenged by its complexity, typically split the task into two parts:
one for identifying what relation categories are present and another for
determining their temporal boundaries. This split overlooks the inherent
connection between these elements. Addressing the need to recognize entity
pairs' spatiotemporal interactions across a range of durations, we propose
VrdONE, a streamlined yet efficacious one-stage model. VrdONE combines the
features of subjects and objects, turning predicate detection into 1D instance
segmentation on their combined representations. This setup allows for both
relation category identification and binary mask generation in one go,
eliminating the need for extra steps like proposal generation or
post-processing. VrdONE facilitates the interaction of features across various
frames, adeptly capturing both short-lived and enduring relations.
Additionally, we introduce the Subject-Object Synergy (SOS) module, enhancing
how subjects and objects perceive each other before combining. VrdONE achieves
state-of-the-art performances on the VidOR benchmark and ImageNet-VidVRD,
showcasing its superior capability in discerning relations across different
temporal scales. The code is available at https://github.com/lucaspk512/vrdone.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures, accepted by ACM Multimedia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex
  Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11190v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11190v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifei Xie, Changqiao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GPT-4o, an all-encompassing model, represents a milestone in the development
of large multi-modal language models. It can understand visual, auditory, and
textual modalities, directly output audio, and support flexible duplex
interaction. Models from the open-source community often achieve some
functionalities of GPT-4o, such as visual understanding and voice chat.
Nevertheless, training a unified model that incorporates all modalities is
challenging due to the complexities of multi-modal data, intricate model
architectures, and training processes. In this paper, we introduce Mini-Omni2,
a visual-audio assistant capable of providing real-time, end-to-end voice
responses to visoin and audio queries. By integrating pretrained visual and
auditory encoders, Mini-Omni2 maintains performance in individual modalities.
We propose a three-stage training process to align modalities, allowing the
language model to handle multi-modal inputs and outputs after training on a
limited dataset. For interaction, we introduce a command-based interruption
mechanism, enabling more flexible interaction with users. To the best of our
knowledge, Mini-Omni2 is one of the closest reproductions of GPT-4o, which have
similar form of functionality, and we hope it can offer valuable insights for
subsequent research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DNTextSpotter: Arbitrary-Shaped Scene Text Spotting via Improved
  Denoising Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Xie, Qian Qiao, Jun Gao, Tianxiang Wu, Jiaqing Fan, Yue Zhang, Jielei Zhang, Huyang Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  More and more end-to-end text spotting methods based on Transformer
architecture have demonstrated superior performance. These methods utilize a
bipartite graph matching algorithm to perform one-to-one optimal matching
between predicted objects and actual objects. However, the instability of
bipartite graph matching can lead to inconsistent optimization targets, thereby
affecting the training performance of the model. Existing literature applies
denoising training to solve the problem of bipartite graph matching instability
in object detection tasks. Unfortunately, this denoising training method cannot
be directly applied to text spotting tasks, as these tasks need to perform
irregular shape detection tasks and more complex text recognition tasks than
classification. To address this issue, we propose a novel denoising training
method (DNTextSpotter) for arbitrary-shaped text spotting. Specifically, we
decompose the queries of the denoising part into noised positional queries and
noised content queries. We use the four Bezier control points of the Bezier
center curve to generate the noised positional queries. For the noised content
queries, considering that the output of the text in a fixed positional order is
not conducive to aligning position with content, we employ a masked character
sliding method to initialize noised content queries, thereby assisting in the
alignment of text content and position. To improve the model's perception of
the background, we further utilize an additional loss function for background
characters classification in the denoising training part.Although DNTextSpotter
is conceptually simple, it outperforms the state-of-the-art methods on four
benchmarks (Total-Text, SCUT-CTW1500, ICDAR15, and Inverse-Text), especially
yielding an improvement of 11.3% against the best approach in Inverse-Text
dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM'MM2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Large Uni- and <span class="highlight-title">Multi-modal</span> Models for Unsupervised Classification of
  Social Media Images: Nature's Contribution to People as a case study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00275v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00275v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohaifa Khaldi, Domingo Alcaraz-Segura, Ignacio Sánchez-Herrera, Javier Martinez-Lopez, Carlos Javier Navarro, Siham Tabik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media images have proven to be a valuable source of information for
understanding human interactions with important subjects such as cultural
heritage, biodiversity, and nature, among others. The task of grouping such
images into a number of semantically meaningful clusters without labels is
challenging due to the high diversity and complex nature of the visual content
in addition to their large volume. On the other hand, recent advances in Large
Visual Models (LVMs), Large Language Models (LLMs), and Large Visual Language
Models (LVLMs) provide an important opportunity to explore new productive and
scalable solutions. This work proposes, analyzes, and compares various
approaches based on one or more state-of-the-art LVM, LLM, and LVLM, for
mapping social media images into a number of predefined classes. As a case
study, we consider the problem of understanding the interactions between humans
and nature, also known as Nature's Contribution to People or Cultural Ecosystem
Services (CES). Our experiments show that the highest-performing approaches,
with accuracy above 95%, still require the creation of a small labeled dataset.
These include the fine-tuned LVM DINOv2 and the LVLM LLaVA-1.5 combined with a
fine-tuned LLM. The top fully unsupervised approaches, achieving accuracy above
84%, are the LVLMs, specifically the proprietary GPT-4 model and the public
LLaVA-1.5 model. Additionally, the LVM DINOv2, when applied in a 10-shot
learning setup, delivered competitive results with an accuracy of 83.99%,
closely matching the performance of the LVLM LLaVA-1.5.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topological reconstruction of sampled surfaces via Morse theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17257v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17257v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Franco Coltraro, Jaume Amorós, Maria Alberich-Carramiñana, Carme Torras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we study the perception problem for sampled surfaces (possibly
with boundary) using tools from computational topology, specifically, how to
identify their underlying topology starting from point-cloud samples in space,
such as those obtained with 3D scanners. We present a reconstruction algorithm
based on a careful topological study of the point sample that allows us to
obtain a cellular decomposition of it using a Morse function. No triangulation
or local implicit equations are used as intermediate steps, avoiding in this
way reconstruction-induced artifices. The algorithm can be run without any
prior knowledge of the surface topology, density or regularity of the
point-sample. The results consist of a piece-wise decomposition of the given
surface as a union of Morse cells (i.e. topological disks), suitable for tasks
such as mesh-independent reparametrization or noise-filtering, and a small-rank
cellular complex determining the topology of the surface. The algorithm, which
we test with several real and synthetic surfaces, can be applied to smooth
surfaces with or without boundary, embedded in an ambient space of any
dimension.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 17 figures, 1 table, 1 algorithm, 1 appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MFC-Bench: Benchmarking <span class="highlight-title">Multimodal</span> Fact-Checking with Large
  <span class="highlight-title">Vision-Language</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11288v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11288v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengkang Wang, Hongzhan Lin, Ziyang Luo, Zhen Ye, Guang Chen, Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) have significantly improved multimodal
reasoning tasks, such as visual question answering and image captioning. These
models embed multimodal facts within their parameters, rather than relying on
external knowledge bases to store factual information explicitly. However, the
content discerned by LVLMs may deviate from actual facts due to inherent bias
or incorrect inference. To address this issue, we introduce MFC-Bench, a
rigorous and comprehensive benchmark designed to evaluate the factual accuracy
of LVLMs across three stages of verdict prediction for MFC: Manipulation,
Out-of-Context, and Veracity Classification. Through our evaluation on
MFC-Bench, we benchmarked a dozen diverse and representative LVLMs, uncovering
that current models still fall short in multimodal fact-checking and
demonstrate insensitivity to various forms of manipulated content. We hope that
MFC-Bench could raise attention to the trustworthy AI potentially assisted by
LVLMs in the future. The MFC-Bench and accompanying resources are publicly
accessible at https://github.com/wskbest/MFC-Bench, contributing to ongoing
research in the multimodal fact-checking field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Free Video-LLM: <span class="highlight-title">Prompt</span>-guided Visual Perception for Efficient
  Training-free Video LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10441v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10441v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Han, Jianyuan Guo, Yehui Tang, Wei He, Enhua Wu, Yunhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language large models have achieved remarkable success in various
multi-modal tasks, yet applying them to video understanding remains challenging
due to the inherent complexity and computational demands of video data. While
training-based video-LLMs deliver high performance, they often require
substantial resources for training and inference. Conversely, training-free
approaches offer a more efficient alternative by adapting pre-trained
image-LLMs models for video tasks without additional training, but they face
inference efficiency bottlenecks due to the large number of visual tokens
generated from video frames. In this work, we present a novel prompt-guided
visual perception framework (abbreviated as Free Video-LLM) for efficient
inference of training-free video LLMs. The proposed framework decouples
spatial-temporal dimension and performs temporal frame sampling and spatial RoI
cropping respectively based on task-specific prompts. Our method effectively
reduces the number of visual tokens while maintaining high performance across
multiple video question-answering benchmarks. Extensive experiments demonstrate
that our approach achieves competitive results with significantly fewer tokens,
offering an optimal trade-off between accuracy and computational efficiency
compared to state-of-the-art video LLMs. The code will be available at
https://github.com/contrastive/FreeVideoLLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Robustness of <span class="highlight-title">Vision-Language</span> Models through Orthogonality
  Learning and Self-Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08374v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08374v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinlong Li, Dong Zhao, Zequn Jie, Elisa Ricci, Lin Ma, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient fine-tuning of vision-language models (VLMs) like CLIP for specific
downstream tasks is gaining significant attention. Previous works primarily
focus on prompt learning to adapt the CLIP into a variety of downstream tasks,
however, suffering from task overfitting when fine-tuned on a small data set.
In this paper, we introduce an orthogonal fine-tuning method for efficiently
fine-tuning pretrained weights and enabling enhanced robustness and
generalization, while a self-regularization strategy is further exploited to
maintain the stability in terms of zero-shot generalization of VLMs, dubbed
OrthSR. Specifically, trainable orthogonal matrices are injected seamlessly
into the transformer architecture and enforced with orthogonality constraint
during the training, benefiting from the norm-preserving property and thus
leading to stable and faster convergence, while keeping the pre-trained weights
frozen. To alleviate deviation from fine-tuning, a self-regularization strategy
is further employed to retain the generalization of the model during the
training within a bypass manner. In addition, to enrich the sample diversity
for downstream tasks under the small dataset scenario, we first explore
attentive CutOut data augmentation to boost the efficient fine-tuning, leading
to better model fitting capacity for specific downstream task. Then we support
the theoretical analysis on how our approach improves the specific downstream
performance and maintains the generalizability. For the first time, we revisit
the CLIP and CoOp with our method to effectively improve the model on few-shot
image classficiation scenario on par with the elaborated prompt learning
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reverse Stable Diffusion: What <span class="highlight-title">prompt</span> was used to generate this image? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.01472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.01472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have recently attracted the interest of many
researchers, and inverting the diffusion process can play an important role in
better understanding the generative process and how to engineer prompts in
order to obtain the desired images. To this end, we study the task of
predicting the prompt embedding given an image generated by a generative
diffusion model. We consider a series of white-box and black-box models (with
and without access to the weights of the diffusion network) to deal with the
proposed task. We propose a novel learning framework comprising a joint prompt
regression and multi-label vocabulary classification objective that generates
improved prompts. To further improve our method, we employ a curriculum
learning procedure that promotes the learning of image-prompt pairs with lower
labeling noise (i.e. that are better aligned). We conduct experiments on the
DiffusionDB data set, predicting text prompts from images generated by Stable
Diffusion. In addition, we make an interesting discovery: training a diffusion
model on the prompt generation task can make the model generate images that are
much better aligned with the input prompts, when the model is directly reused
for text-to-image generation. Our code is publicly available for download at
https://github.com/CroitoruAlin/Reverse-Stable-Diffusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Computer Vision and Image Understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instruction-Guided Visual Masking <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinliang Zheng, Jianxiong Li, Sijie Cheng, Yinan Zheng, Jiaming Li, Jihao Liu, Yu Liu, Jingjing Liu, Xianyuan Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction following is crucial in contemporary LLM. However, when extended
to multimodal setting, it often suffers from misalignment between specific
textual instruction and targeted local region of an image. To achieve more
accurate and nuanced multimodal instruction following, we introduce
Instruction-guided Visual Masking (IVM), a new versatile visual grounding model
that is compatible with diverse multimodal models, such as LMM and robot model.
By constructing visual masks for instruction-irrelevant regions, IVM-enhanced
multimodal models can effectively focus on task-relevant image regions to
better align with complex instructions. Specifically, we design a visual
masking data generation pipeline and create an IVM-Mix-1M dataset with 1
million image-instruction pairs. We further introduce a new learning technique,
Discriminator Weighted Supervised Learning (DWSL) for preferential IVM training
that prioritizes high-quality data samples. Experimental results on generic
multimodal tasks such as VQA and embodied robotic control demonstrate the
versatility of IVM, which as a plug-and-play tool, significantly boosts the
performance of diverse multimodal models, yielding new state-of-the-art results
across challenging multimodal benchmarks. Code, model and data are available at
https://github.com/2toinf/IVM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InterACT: Inter-dependency Aware Action Chunking with Hierarchical
  Attention Transformers for Bimanual Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07914v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07914v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Lee, Ian Chuang, Ling-Yuan Chen, Iman Soltani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bimanual manipulation presents unique challenges compared to unimanual tasks
due to the complexity of coordinating two robotic arms. In this paper, we
introduce InterACT: Inter-dependency aware Action Chunking with Hierarchical
Attention Transformers, a novel imitation learning framework designed
specifically for bimanual manipulation. InterACT leverages hierarchical
attention mechanisms to effectively capture inter-dependencies between dual-arm
joint states and visual inputs. The framework comprises a Hierarchical
Attention Encoder, which processes multi-modal inputs through segment-wise and
cross-segment attention mechanisms, and a Multi-arm Decoder that generates each
arm's action predictions in parallel, while sharing information between the
arms through synchronization blocks by providing the other arm's intermediate
output as context. Our experiments, conducted on various simulated and
real-world bimanual manipulation tasks, demonstrate that InterACT outperforms
existing methods. Detailed ablation studies further validate the significance
of key components, including the impact of CLS tokens, cross-segment encoders,
and synchronization blocks on task performance. We provide supplementary
materials and videos on our project page.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Conference on Robot Learning (CoRL) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No Bells, Just Whistles: Sports Field Registration by Leveraging
  Geometric Properties <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Gutiérrez-Pérez, Antonio Agudo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Broadcast sports field registration is traditionally addressed as a
homography estimation task, mapping the visible image area to a planar field
model, predominantly focusing on the main camera shot. Addressing the
shortcomings of previous approaches, we propose a novel calibration pipeline
enabling camera calibration using a 3D soccer field model and extending the
process to assess the multiple-view nature of broadcast videos. Our approach
begins with a keypoint generation pipeline derived from SoccerNet dataset
annotations, leveraging the geometric properties of the court. Subsequently, we
execute classical camera calibration through DLT algorithm in a minimalist
fashion, without further refinement. Through extensive experimentation on
real-world soccer broadcast datasets such as SoccerNet-Calibration, WorldCup
2014 and TS- WorldCup, our method demonstrates superior performance in both
multiple- and single-view 3D camera calibration while maintaining competitive
results in homography estimation compared to state-of-the-art techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in CVPRW 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Fake: DeepFake Camouflage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03200v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03200v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pu Sun, Honggang Qi, Yuezun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DeepFake technology has gained significant attention due to its ability to
manipulate facial attributes with high realism, raising serious societal
concerns. Face-Swap DeepFake is the most harmful among these techniques, which
fabricates behaviors by swapping original faces with synthesized ones. Existing
forensic methods, primarily based on Deep Neural Networks (DNNs), effectively
expose these manipulations and have become important authenticity indicators.
However, these methods mainly concentrate on capturing the blending
inconsistency in DeepFake faces, raising a new security issue, termed Active
Fake, emerges when individuals intentionally create blending inconsistency in
their authentic videos to evade responsibility. This tactic is called DeepFake
Camouflage. To achieve this, we introduce a new framework for creating DeepFake
camouflage that generates blending inconsistencies while ensuring
imperceptibility, effectiveness, and transferability. This framework, optimized
via an adversarial learning strategy, crafts imperceptible yet effective
inconsistencies to mislead forensic detectors. Extensive experiments
demonstrate the effectiveness and robustness of our method, highlighting the
need for further research in active fake detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PIVOT-R: Primitive-Driven Waypoint-Aware World Model for Robotic
  Manipulation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10394v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10394v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaidong Zhang, Pengzhen Ren, Bingqian Lin, Junfan Lin, Shikui Ma, Hang Xu, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language-guided robotic manipulation is a challenging task that requires an
embodied agent to follow abstract user instructions to accomplish various
complex manipulation tasks. Previous work trivially fitting the data without
revealing the relation between instruction and low-level executable actions,
these models are prone to memorizing the surficial pattern of the data instead
of acquiring the transferable knowledge, and thus are fragile to dynamic
environment changes. To address this issue, we propose a PrIrmitive-driVen
waypOinT-aware world model for Robotic manipulation (PIVOT-R) that focuses
solely on the prediction of task-relevant waypoints. Specifically, PIVOT-R
consists of a Waypoint-aware World Model (WAWM) and a lightweight action
prediction module. The former performs primitive action parsing and
primitive-driven waypoint prediction, while the latter focuses on decoding
low-level actions. Additionally, we also design an asynchronous hierarchical
executor (AHE), which can use different execution frequencies for different
modules of the model, thereby helping the model reduce computational redundancy
and improve model execution efficiency. Our PIVOT-R outperforms
state-of-the-art (SoTA) open-source models on the SeaWave benchmark, achieving
an average relative improvement of 19.45% across four levels of instruction
tasks. Moreover, compared to the synchronously executed PIVOT-R, the execution
efficiency of PIVOT-R with AHE is increased by 28-fold, with only a 2.9% drop
in performance. These results provide compelling evidence that our PIVOT-R can
significantly improve both the performance and efficiency of robotic
manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Progressive Retinal Image Registration via Global and Local Deformable
  Transformations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01068v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01068v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yepeng Liu, Baosheng Yu, Tian Chen, Yuliang Gu, Bo Du, Yongchao Xu, Jun Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retinal image registration plays an important role in the ophthalmological
diagnosis process. Since there exist variances in viewing angles and anatomical
structures across different retinal images, keypoint-based approaches become
the mainstream methods for retinal image registration thanks to their
robustness and low latency. These methods typically assume the retinal surfaces
are planar, and adopt feature matching to obtain the homography matrix that
represents the global transformation between images. Yet, such a planar
hypothesis inevitably introduces registration errors since retinal surface is
approximately curved. This limitation is more prominent when registering image
pairs with significant differences in viewing angles. To address this problem,
we propose a hybrid registration framework called HybridRetina, which
progressively registers retinal images with global and local deformable
transformations. For that, we use a keypoint detector and a deformation network
called GAMorph to estimate the global transformation and local deformable
transformation, respectively. Specifically, we integrate multi-level pixel
relation knowledge to guide the training of GAMorph. Additionally, we utilize
an edge attention module that includes the geometric priors of the images,
ensuring the deformation field focuses more on the vascular regions of clinical
interest. Experiments on two widely-used datasets, FIRE and FLoRI21, show that
our proposed HybridRetina significantly outperforms some state-of-the-art
methods. The code is available at
https://github.com/lyp-deeplearning/awesome-retinal-registration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at BIBM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deciphering Cross-Modal Alignment in Large <span class="highlight-title">Vision-Language</span> Models with
  Modality Integration Rate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qidong Huang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Dahua Lin, Weiming Zhang, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Modality Integration Rate (MIR), an effective, robust, and
generalized metric to indicate the multi-modal pre-training quality of Large
Vision Language Models (LVLMs). Large-scale pre-training plays a critical role
in building capable LVLMs, while evaluating its training quality without the
costly supervised fine-tuning stage is under-explored. Loss, perplexity, and
in-context evaluation results are commonly used pre-training metrics for Large
Language Models (LLMs), while we observed that these metrics are less
indicative when aligning a well-trained LLM with a new modality. Due to the
lack of proper metrics, the research of LVLMs in the critical pre-training
stage is hindered greatly, including the training data choice, efficient module
design, etc. In this paper, we propose evaluating the pre-training quality from
the inter-modal distribution distance perspective and present MIR, the Modality
Integration Rate, which is 1) \textbf{Effective} to represent the pre-training
quality and show a positive relation with the benchmark performance after
supervised fine-tuning. 2) \textbf{Robust} toward different training/evaluation
data. 3) \textbf{Generalize} across training configurations and architecture
choices. We conduct a series of pre-training experiments to explore the
effectiveness of MIR and observe satisfactory results that MIR is indicative
about training data selection, training strategy schedule, and model
architecture design to get better pre-training results. We hope MIR could be a
helpful metric for building capable LVLMs and inspire the following research
about modality alignment in different areas. Our code is at:
https://github.com/shikiw/Modality-Integration-Rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/shikiw/Modality-Integration-Rate</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hyper-YOLO: When Visual Object Detection Meets Hypergraph Computation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04804v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04804v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Feng, Jiangang Huang, Shaoyi Du, Shihui Ying, Jun-Hai Yong, Yipeng Li, Guiguang Ding, Rongrong Ji, Yue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Hyper-YOLO, a new object detection method that integrates
hypergraph computations to capture the complex high-order correlations among
visual features. Traditional YOLO models, while powerful, have limitations in
their neck designs that restrict the integration of cross-level features and
the exploitation of high-order feature interrelationships. To address these
challenges, we propose the Hypergraph Computation Empowered Semantic Collecting
and Scattering (HGC-SCS) framework, which transposes visual feature maps into a
semantic space and constructs a hypergraph for high-order message propagation.
This enables the model to acquire both semantic and structural information,
advancing beyond conventional feature-focused learning. Hyper-YOLO incorporates
the proposed Mixed Aggregation Network (MANet) in its backbone for enhanced
feature extraction and introduces the Hypergraph-Based Cross-Level and
Cross-Position Representation Network (HyperC2Net) in its neck. HyperC2Net
operates across five scales and breaks free from traditional grid structures,
allowing for sophisticated high-order interactions across levels and positions.
This synergy of components positions Hyper-YOLO as a state-of-the-art
architecture in various scale models, as evidenced by its superior performance
on the COCO dataset. Specifically, Hyper-YOLO-N significantly outperforms the
advanced YOLOv8-N and YOLOv9-T with 12\% $\text{AP}^{val}$ and 9\%
$\text{AP}^{val}$ improvements. The source codes are at
ttps://github.com/iMoonLab/Hyper-YOLO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Depth Estimation From Monocular Images With Enhanced Encoder-Decoder
  Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11610v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11610v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dabbrata Das, Argho Deb Das, Farhan Sadaf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating depth from a single 2D image is a challenging task because of the
need for stereo or multi-view data, which normally provides depth information.
This paper deals with this challenge by introducing a novel deep learning-based
approach using an encoder-decoder architecture, where the Inception-ResNet-v2
model is utilized as the encoder. According to the available literature, this
is the first instance of using Inception-ResNet-v2 as an encoder for monocular
depth estimation, illustrating better performance than previous models. The use
of Inception-ResNet-v2 enables our model to capture complex objects and
fine-grained details effectively that are generally difficult to predict.
Besides, our model incorporates multi-scale feature extraction to enhance depth
prediction accuracy across different kinds of object sizes and distances. We
propose a composite loss function consisting of depth loss, gradient edge loss,
and SSIM loss, where the weights are fine-tuned to optimize the weighted sum,
ensuring better balance across different aspects of depth estimation.
Experimental results on the NYU Depth V2 dataset show that our model achieves
state-of-the-art performance, with an ARE of 0.064, RMSE of 0.228, and accuracy
($\delta$ $<1.25$) of 89.3%. These metrics demonstrate that our model
effectively predicts depth, even in challenging circumstances, providing a
scalable solution for real-world applications in robotics, 3D reconstruction,
and augmented reality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Models: What Do They Know? Do They Know Things? Let's Find
  Out! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17137v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17137v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaodan Du, Nicholas Kolkin, Greg Shakhnarovich, Anand Bhattad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models excel at mimicking real scenes, suggesting they might
inherently encode important intrinsic scene properties. In this paper, we aim
to explore the following key questions: (1) What intrinsic knowledge do
generative models like GANs, Autoregressive models, and Diffusion models
encode? (2) Can we establish a general framework to recover intrinsic
representations from these models, regardless of their architecture or model
type? (3) How minimal can the required learnable parameters and labeled data be
to successfully recover this knowledge? (4) Is there a direct link between the
quality of a generative model and the accuracy of the recovered scene
intrinsics?
  Our findings indicate that a small Low-Rank Adaptators (LoRA) can recover
intrinsic images-depth, normals, albedo and shading-across different generators
(Autoregressive, GANs and Diffusion) while using the same decoder head that
generates the image. As LoRA is lightweight, we introduce very few learnable
parameters (as few as 0.04% of Stable Diffusion model weights for a rank of 2),
and we find that as few as 250 labeled images are enough to generate intrinsic
images with these LoRA modules. Finally, we also show a positive correlation
between the generative model's quality and the accuracy of the recovered
intrinsics through control experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://intrinsic-lora.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mini-Splatting: Representing Scenes with a Constrained Number of
  Gaussians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14166v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14166v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangchi Fang, Bing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we explore the challenge of efficiently representing scenes
with a constrained number of Gaussians. Our analysis shifts from traditional
graphics and 2D computer vision to the perspective of point clouds,
highlighting the inefficient spatial distribution of Gaussian representation as
a key limitation in model performance. To address this, we introduce strategies
for densification including blur split and depth reinitialization, and
simplification through intersection preserving and sampling. These techniques
reorganize the spatial positions of the Gaussians, resulting in significant
improvements across various datasets and benchmarks in terms of rendering
quality, resource consumption, and storage compression. Our Mini-Splatting
integrates seamlessly with the original rasterization pipeline, providing a
strong baseline for future research in Gaussian-Splatting-based works.
\href{https://github.com/fatPeter/mini-splatting}{Code is available}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ See Where You Read with Eye Gaze Tracking and Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19454v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19454v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sikai Yang, Gang Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Losing track of reading progress during line switching can be frustrating.
Eye gaze tracking technology offers a potential solution by highlighting read
paragraphs, aiding users in avoiding wrong line switches. However, the gap
between gaze tracking accuracy (2-3 cm) and text line spacing (3-5 mm) makes
direct application impractical. Existing methods leverage the linear reading
pattern but fail during jump reading. This paper presents a reading tracking
and highlighting system that supports both linear and jump reading. Based on
experimental insights from the gaze nature study of 16 users, two gaze error
models are designed to enable both jump reading detection and relocation. The
system further leverages the large language model's contextual perception
capability in aiding reading tracking. A reading tracking domain-specific
line-gaze alignment opportunity is also exploited to enable dynamic and
frequent calibration of the gaze results. Controlled experiments demonstrate
reliable linear reading tracking, as well as 84% accuracy in tracking jump
reading. Furthermore, real field tests with 18 volunteers demonstrated the
system's effectiveness in tracking and highlighting read paragraphs, improving
reading efficiency, and enhancing user experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11548v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11548v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuyan Xiong, Chengyu Shen, Xiaoqi Li, Kaichen Zhou, Jiaming Liu, Ruiping Wang, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to reflect on and correct failures is crucial for robotic systems
to interact stably with real-life objects. Observing the generalization and
reasoning capabilities of Multimodal Large Language Models (MLLMs), previous
approaches have aimed to utilize these models to enhance robotic systems
accordingly. However, these methods typically focus on high-level planning
corrections using an additional MLLM, with limited utilization of failed
samples to correct low-level contact poses which is particularly prone to occur
during articulated object manipulation. To address this gap, we propose an
Autonomous Interactive Correction (AIC) MLLM, which makes use of previous
low-level interaction experiences to correct SE(3) pose predictions for
articulated object. Specifically, AIC MLLM is initially fine-tuned to acquire
both pose prediction and feedback prompt comprehension abilities. We design two
types of prompt instructions for interactions with objects: 1) visual masks to
highlight unmovable parts for position correction, and 2) textual descriptions
to indicate potential directions for rotation correction. During inference, a
Feedback Information Extraction module is introduced to recognize the failure
cause, allowing AIC MLLM to adaptively correct the pose prediction using the
corresponding prompts. To further enhance manipulation stability, we devise a
Test Time Adaptation strategy that enables AIC MLLM to better adapt to the
current scene configuration. Finally, extensive experiments are conducted in
both simulated and real-world environments to evaluate the proposed method. The
results demonstrate that our AIC MLLM can efficiently correct failure samples
by leveraging interaction experience prompts. Our project website is
https://sites.google.com/view/aic-mllm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Key-Grid: Unsupervised 3D Keypoints Detection using Grid Heatmap
  Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengkai Hou, Zhengrong Xue, Bingyang Zhou, Jinghan Ke, Lin Shao, Huazhe Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting 3D keypoints with semantic consistency is widely used in many
scenarios such as pose estimation, shape registration and robotics. Currently,
most unsupervised 3D keypoint detection methods focus on the rigid-body
objects. However, when faced with deformable objects, the keypoints they
identify do not preserve semantic consistency well. In this paper, we introduce
an innovative unsupervised keypoint detector Key-Grid for both the rigid-body
and deformable objects, which is an autoencoder framework. The encoder predicts
keypoints and the decoder utilizes the generated keypoints to reconstruct the
objects. Unlike previous work, we leverage the identified keypoint in formation
to form a 3D grid feature heatmap called grid heatmap, which is used in the
decoder section. Grid heatmap is a novel concept that represents the latent
variables for grid points sampled uniformly in the 3D cubic space, where these
variables are the shortest distance between the grid points and the skeleton
connected by keypoint pairs. Meanwhile, we incorporate the information from
each layer of the encoder into the decoder section. We conduct an extensive
evaluation of Key-Grid on a list of benchmark datasets. Key-Grid achieves the
state-of-the-art performance on the semantic consistency and position accuracy
of keypoints. Moreover, we demonstrate the robustness of Key-Grid to noise and
downsampling. In addition, we achieve SE-(3) invariance of keypoints though
generalizing Key-Grid to a SE(3)-invariant backbone.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MERLIN: <span class="highlight-title">Multimodal</span> Embedding Refinement via LLM-based Iterative
  Navigation for Text-Video Retrieval-Rerank Pipeline <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghoon Han, Eunhwan Park, Gisang Lee, Adam Lee, Nojun Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid expansion of multimedia content has made accurately retrieving
relevant videos from large collections increasingly challenging. Recent
advancements in text-video retrieval have focused on cross-modal interactions,
large-scale foundation model training, and probabilistic modeling, yet often
neglect the crucial user perspective, leading to discrepancies between user
queries and the content retrieved. To address this, we introduce MERLIN
(Multimodal Embedding Refinement via LLM-based Iterative Navigation), a novel,
training-free pipeline that leverages Large Language Models (LLMs) for
iterative feedback learning. MERLIN refines query embeddings from a user
perspective, enhancing alignment between queries and video content through a
dynamic question answering process. Experimental results on datasets like
MSR-VTT, MSVD, and ActivityNet demonstrate that MERLIN substantially improves
Recall@1, outperforming existing systems and confirming the benefits of
integrating LLMs into multimodal retrieval systems for more responsive and
context-aware multimedia retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Industry Track Accepted (Camera-Ready Version)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tri-Cam: Practical Eye Gaze Tracking via Camera Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19554v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19554v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sikai Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As human eyes serve as conduits of rich information, unveiling emotions,
intentions, and even aspects of an individual's health and overall well-being,
gaze tracking also enables various human-computer interaction applications, as
well as insights in psychological and medical research. However, existing gaze
tracking solutions fall short at handling free user movement, and also require
laborious user effort in system calibration. We introduce Tri-Cam, a practical
deep learning-based gaze tracking system using three affordable RGB webcams. It
features a split network structure for efficient training, as well as
designated network designs to handle the separated gaze tracking tasks. Tri-Cam
is also equipped with an implicit calibration module, which makes use of mouse
click opportunities to reduce calibration overhead on the user's end. We
evaluate Tri-Cam against Tobii, the state-of-the-art commercial eye tracker,
achieving comparable accuracy, while supporting a wider free movement area. In
conclusion, Tri-Cam provides a user-friendly, affordable, and robust gaze
tracking solution that could practically enable various applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViLReF: An Expert Knowledge Enabled <span class="highlight-title">Vision-Language</span> Retinal Foundation
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10894v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10894v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengzhu Yang, Jiawei Du, Jia Guo, Weihang Zhang, Hanruo Liu, Huiqi Li, Ningli Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Subtle semantic differences in retinal image and text data present great
challenges for pre-training visual-language models. Moreover, false negative
samples, i.e., image-text pairs having the same semantics but incorrectly
regarded as negatives, disrupt the visual-language pre-training process and
affect the model's learning ability. This work aims to develop a retinal
foundation model, called ViLReF, by pre-training on a paired dataset comprising
451,956 retinal images and corresponding diagnostic text reports. In our
vision-language pre-training strategy, we leverage expert knowledge to
facilitate the extraction of labels and propose a novel constraint, the
Weighted Similarity Coupling Loss, to adjust the speed of pushing sample pairs
further apart dynamically within the feature space. Furthermore, we employ a
batch expansion module with dynamic memory queues, maintained by momentum
encoders, to supply extra samples and compensate for the vacancies caused by
eliminating false negatives. Extensive experiments are conducted on multiple
datasets for downstream classification and segmentation tasks. The experimental
results demonstrate the powerful zero-shot and transfer learning capabilities
of ViLReF, verifying the effectiveness of our pre-training strategy. Our ViLReF
model is available at: https://github.com/T6Yang/ViLReF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Up Personalized Image Aesthetic Assessment via Task Vector
  Customization <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07176v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07176v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jooyeol Yun, Jaegul Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of personalized image aesthetic assessment seeks to tailor aesthetic
score prediction models to match individual preferences with just a few
user-provided inputs. However, the scalability and generalization capabilities
of current approaches are considerably restricted by their reliance on an
expensive curated database. To overcome this long-standing scalability
challenge, we present a unique approach that leverages readily available
databases for general image aesthetic assessment and image quality assessment.
Specifically, we view each database as a distinct image score regression task
that exhibits varying degrees of personalization potential. By determining
optimal combinations of task vectors, known to represent specific traits of
each database, we successfully create personalized models for individuals. This
approach of integrating multiple models allows us to harness a substantial
amount of data. Our extensive experiments demonstrate the effectiveness of our
approach in generalizing to previously unseen domains-a challenge previous
approaches have struggled to achieve-making it highly applicable to real-world
scenarios. Our novel approach significantly advances the field by offering
scalable solutions for personalized aesthetic assessment and establishing high
standards for future research.
https://yeolj00.github.io/personal-projects/personalized-aesthetics/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnimateLCM: Computation-Efficient Personalized Style Video Generation
  without Personalized Video Data <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00769v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00769v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fu-Yun Wang, Zhaoyang Huang, Weikang Bian, Xiaoyu Shi, Keqiang Sun, Guanglu Song, Yu Liu, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an effective method for computation-efficient
personalized style video generation without requiring access to any
personalized video data. It reduces the necessary generation time of similarly
sized video diffusion models from 25 seconds to around 1 second while
maintaining the same level of performance. The method's effectiveness lies in
its dual-level decoupling learning approach: 1) separating the learning of
video style from video generation acceleration, which allows for personalized
style video generation without any personalized style video data, and 2)
separating the acceleration of image generation from the acceleration of video
motion generation, enhancing training efficiency and mitigating the negative
effects of low-quality video data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a Short Paper by SIGGRAPH ASIA 2024 Technical
  Communications. This is a short version of the original work. Project Page:
  https://animatelcm.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-Based Adaptive Robotics for Autonomous Surface Crack Repair 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16874v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16874v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Genova, Eric Cabrera, Vedhus Hoskere
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surface cracks in infrastructure can lead to significant deterioration and
costly maintenance if not efficiently repaired. Manual repair methods are
labor-intensive, time-consuming, and imprecise and thus difficult to scale to
large areas. While advancements in robotic perception and manipulation have
progressed autonomous crack repair, existing methods still face three key
challenges: accurate localization of cracks within the robot's coordinate
frame, (ii) adaptability to varying crack depths and widths, and (iii)
validation of the repair process under realistic conditions. This paper
presents an adaptive, autonomous system for surface crack detection and repair
using robotics with advanced sensing technologies to enhance precision and
safety for humans. The system uses an RGB-D camera for crack detection, a laser
scanner for precise measurement, and an extruder and pump for material
deposition. To address one of the key challenges, the laser scanner is used to
enhance the crack coordinates for accurate localization. Furthermore, our
approach demonstrates that an adaptive crack-filling method is more efficient
and effective than a fixed-speed approach, with experimental results confirming
both precision and consistency. In addition, to ensure real-world applicability
and testing repeatability, we introduce a novel validation procedure using
3D-printed crack specimens that accurately simulate real-world conditions. This
research contributes to the evolving field of human-robot interaction in
construction by demonstrating how adaptive robotic systems can reduce the need
for manual labor, improve safety, and enhance the efficiency of maintenance
operations, ultimately paving the way for more sophisticated and integrated
construction robotics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 14 figures, submitted to Advanced Engineering Informatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample what you cant compress 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02529v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02529v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vighnesh Birodkar, Gabriel Barcik, James Lyon, Sergey Ioffe, David Minnen, Joshua V. Dillon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For learned image representations, basic autoencoders often produce blurry
results. Reconstruction quality can be improved by incorporating additional
penalties such as adversarial (GAN) and perceptual losses. Arguably, these
approaches lack a principled interpretation. Concurrently, in generative
settings diffusion has demonstrated a remarkable ability to create crisp, high
quality results and has solid theoretical underpinnings (from variational
inference to direct study as the Fisher Divergence). Our work combines
autoencoder representation learning with diffusion and is, to our knowledge,
the first to demonstrate the efficacy of jointly learning a continuous encoder
and decoder under a diffusion-based loss. We demonstrate that this approach
yields better reconstruction quality as compared to GAN-based autoencoders
while being easier to tune. We also show that the resulting representation is
easier to model with a latent diffusion model as compared to the representation
obtained from a state-of-the-art GAN-based loss. Since our decoder is
stochastic, it can generate details not encoded in the otherwise deterministic
latent representation; we therefore name our approach "Sample what you can't
compress", or SWYCC for short.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MuseTalk: Real-Time High Quality Lip Synchronization with Latent Space
  Inpainting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10122v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10122v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Zhang, Minhao Liu, Zhaokang Chen, Bin Wu, Yubin Zeng, Chao Zhan, Yingjie He, Junxin Huang, Wenjiang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving high-resolution, identity consistency, and accurate lip-speech
synchronization in face visual dubbing presents significant challenges,
particularly for real-time applications like live video streaming. We propose
MuseTalk, which generates lip-sync targets in a latent space encoded by a
Variational Autoencoder, enabling high-fidelity talking face video generation
with efficient inference. Specifically, we project the occluded lower half of
the face image and itself as an reference into a low-dimensional latent space
and use a multi-scale U-Net to fuse audio and visual features at various
levels. We further propose a novel sampling strategy during training, which
selects reference images with head poses closely matching the target, allowing
the model to focus on precise lip movement by filtering out redundant
information. Additionally, we analyze the mechanism of lip-sync loss and reveal
its relationship with input information volume. Extensive experiments show that
MuseTalk consistently outperforms recent state-of-the-art methods in visual
fidelity and achieves comparable lip-sync accuracy. As MuseTalk supports the
online generation of face at 256x256 at more than 30 FPS with negligible
starting latency, it paves the way for real-time applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Delta-ICM: Entropy Modeling with Delta Function for Learned Image
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07669v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07669v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takahiro Shindo, Taiju Watanabe, Yui Tatsumi, Hiroshi Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Coding for Machines (ICM) is becoming more important as research in
computer vision progresses. ICM is a vital research field that pursues the use
of images for image recognition models, facilitating efficient image
transmission and storage. The demand for recognition models is growing rapidly
among the general public, and their performance continues to improve. To meet
these needs, exchanging image data between consumer devices and cloud AI using
ICM technology could be one possible solution. In ICM, various image
compression methods have adopted Learned Image Compression (LIC). LIC includes
an entropy model for estimating the bitrate of latent features, and the design
of this model significantly affects its performance. Typically, LIC methods
assume that the distribution of latent features follows a normal distribution.
This assumption is effective for compressing images intended for human vision.
However, employing an entropy model based on normal distribution is inefficient
in ICM due to the limitation of image parts that require precise decoding. To
address this, we propose Delta-ICM, which uses a probability distribution based
on a delta function. Assuming the delta distribution as a distribution of
latent features reduces the entropy of image portions unnecessary for machines.
We compress the remaining portions using an entropy model based on normal
distribution, similar to existing methods. Delta-ICM selects between the
entropy model based on the delta distribution and the one based on the normal
distribution for each latent feature. Our method outperforms existing ICM
methods in image compression performance aimed at machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-to-Audio Generation with Hidden Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manjie Xu, Chenxing Li, Xinyi Tu, Yong Ren, Rilin Chen, Yu Gu, Wei Liang, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating semantically and temporally aligned audio content in accordance
with video input has become a focal point for researchers, particularly
following the remarkable breakthrough in text-to-video generation. In this
work, we aim to offer insights into the video-to-audio generation paradigm,
focusing on three crucial aspects: vision encoders, auxiliary embeddings, and
data augmentation techniques. Beginning with a foundational model built on a
simple yet surprisingly effective intuition, we explore various vision encoders
and auxiliary embeddings through ablation studies. Employing a comprehensive
evaluation pipeline that emphasizes generation quality and video-audio
synchronization alignment, we demonstrate that our model exhibits
state-of-the-art video-to-audio generation capabilities. Furthermore, we
provide critical insights into the impact of different data augmentation
methods on enhancing the generation framework's overall capacity. We showcase
possibilities to advance the challenge of generating synchronized audio from
semantic and temporal perspectives. We hope these insights will serve as a
stepping stone toward developing more realistic and accurate audio-visual
generation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://sites.google.com/view/vta-ldm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Circuits in <span class="highlight-title">Pretrain</span>ed Transformers <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable capabilities of modern large language models are rooted in
their vast repositories of knowledge encoded within their parameters, enabling
them to perceive the world and engage in reasoning. The inner workings of how
these models store knowledge have long been a subject of intense interest and
investigation among researchers. To date, most studies have concentrated on
isolated components within these models, such as the Multilayer Perceptrons and
attention head. In this paper, we delve into the computation graph of the
language model to uncover the knowledge circuits that are instrumental in
articulating specific knowledge. The experiments, conducted with GPT2 and
TinyLLAMA, have allowed us to observe how certain information heads, relation
heads, and Multilayer Perceptrons collaboratively encode knowledge within the
model. Moreover, we evaluate the impact of current knowledge editing techniques
on these knowledge circuits, providing deeper insights into the functioning and
constraints of these editing methodologies. Finally, we utilize knowledge
circuits to analyze and interpret language model behaviors such as
hallucinations and in-context learning. We believe the knowledge circuits hold
potential for advancing our understanding of Transformers and guiding the
improved design of knowledge editing. Code and data are available in
https://github.com/zjunlp/KnowledgeCircuits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024, 32 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CVCP-Fusion: On Implicit Depth Estimation for 3D Bounding Box Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11211v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11211v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Gupta, Rishabh Rengarajan, Viren Bankapur, Vedansh Mannem, Lakshit Ahuja, Surya Vijay, Kevin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining LiDAR and Camera-view data has become a common approach for 3D
Object Detection. However, previous approaches combine the two input streams at
a point-level, throwing away semantic information derived from camera features.
In this paper we propose Cross-View Center Point-Fusion, a state-of-the-art
model to perform 3D object detection by combining camera and LiDAR-derived
features in the BEV space to preserve semantic density from the camera stream
while incorporating spacial data from the LiDAR stream. Our architecture
utilizes aspects from previously established algorithms, Cross-View
Transformers and CenterPoint, and runs their backbones in parallel, allowing
efficient computation for real-time processing and application. In this paper
we find that while an implicitly calculated depth-estimate may be sufficiently
accurate in a 2D map-view representation, explicitly calculated geometric and
spacial information is needed for precise bounding box prediction in the 3D
world-view space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures. arXiv admin note: text overlap with
  arXiv:2205.02833 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReLayout: Towards Real-World Document Understanding via Layout-enhanced
  <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10471v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10471v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouqiang Jiang, Bowen Wang, Junhao Chen, Yuta Nakashima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent approaches for visually-rich document understanding (VrDU) uses
manually annotated semantic groups, where a semantic group encompasses all
semantically relevant but not obviously grouped words. As OCR tools are unable
to automatically identify such grouping, we argue that current VrDU approaches
are unrealistic. We thus introduce a new variant of the VrDU task, real-world
visually-rich document understanding (ReVrDU), that does not allow for using
manually annotated semantic groups. We also propose a new method, ReLayout,
compliant with the ReVrDU scenario, which learns to capture semantic grouping
through arranging words and bringing the representations of words that belong
to the potential same semantic group closer together. Our experimental results
demonstrate the performance of existing methods is deteriorated with the ReVrDU
task, while ReLayout shows superiour performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ultra-High-Definition Image Restoration: New Benchmarks and A Dual
  Interaction Prior-Driven Solution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13607v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13607v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liyan Wang, Cong Wang, Jinshan Pan, Xiaofeng Liu, Weixiang Zhou, Xiaoran Sun, Wei Wang, Zhixun Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultra-High-Definition (UHD) image restoration has acquired remarkable
attention due to its practical demand. In this paper, we construct UHD snow and
rain benchmarks, named UHD-Snow and UHD-Rain, to remedy the deficiency in this
field. The UHD-Snow/UHD-Rain is established by simulating the physics process
of rain/snow into consideration and each benchmark contains 3200 degraded/clear
image pairs of 4K resolution. Furthermore, we propose an effective UHD image
restoration solution by considering gradient and normal priors in model design
thanks to these priors' spatial and detail contributions. Specifically, our
method contains two branches: (a) feature fusion and reconstruction branch in
high-resolution space and (b) prior feature interaction branch in
low-resolution space. The former learns high-resolution features and fuses
prior-guided low-resolution features to reconstruct clear images, while the
latter utilizes normal and gradient priors to mine useful spatial features and
detail features to guide high-resolution recovery better. To better utilize
these priors, we introduce single prior feature interaction and dual prior
feature interaction, where the former respectively fuses normal and gradient
priors with high-resolution features to enhance prior ones, while the latter
calculates the similarity between enhanced prior ones and further exploits dual
guided filtering to boost the feature interaction of dual priors. We conduct
experiments on both new and existing public datasets and demonstrate the
state-of-the-art performance of our method on UHD image low-light enhancement,
dehazing, deblurring, desonwing, and deraining. The source codes and benchmarks
are available at \url{https://github.com/wlydlut/UHDDIP}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdaMSS: Adaptive <span class="highlight-title">Multi-Modal</span>ity Segmentation-to-Survival Learning for
  Survival Outcome Prediction from PET/CT Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09946v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09946v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyuan Meng, Bingxin Gu, Michael Fulham, Shaoli Song, Dagan Feng, Lei Bi, Jinman Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Survival prediction is a major concern for cancer management. Deep survival
models based on deep learning have been widely adopted to perform end-to-end
survival prediction from medical images. Recent deep survival models achieved
promising performance by jointly performing tumor segmentation with survival
prediction, where the models were guided to extract tumor-related information
through Multi-Task Learning (MTL). However, these deep survival models have
difficulties in exploring out-of-tumor prognostic information. In addition,
existing deep survival models are unable to effectively leverage multi-modality
images. Empirically-designed fusion strategies were commonly adopted to fuse
multi-modality information via task-specific manually-designed networks, thus
limiting the adaptability to different scenarios. In this study, we propose an
Adaptive Multi-modality Segmentation-to-Survival model (AdaMSS) for survival
prediction from PET/CT images. Instead of adopting MTL, we propose a novel
Segmentation-to-Survival Learning (SSL) strategy, where our AdaMSS is trained
for tumor segmentation and survival prediction sequentially in two stages. This
strategy enables the AdaMSS to focus on tumor regions in the first stage and
gradually expand its focus to include other prognosis-related regions in the
second stage. We also propose a data-driven strategy to fuse multi-modality
information, which realizes adaptive optimization of fusion strategies based on
training data during training. With the SSL and data-driven fusion strategies,
our AdaMSS is designed as an adaptive model that can self-adapt its focus
regions and fusion strategy for different training stages. Extensive
experiments with two large clinical datasets show that our AdaMSS outperforms
state-of-the-art survival prediction methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The extended version of this paper has been published at npj
  Precision Oncology as "Adaptive segmentation-to-survival learning for
  survival prediction from multi-modality medical images"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-shot Generalizable Incremental Learning for <span class="highlight-title">Vision-Language</span> Object
  Detection <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01680v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01680v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieren Deng, Haojian Zhang, Kun Ding, Jianhua Hu, Xingxuan Zhang, Yunkuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Incremental Vision-Language Object Detection (IVLOD), a
novel learning task designed to incrementally adapt pre-trained Vision-Language
Object Detection Models (VLODMs) to various specialized domains, while
simultaneously preserving their zero-shot generalization capabilities for the
generalized domain. To address this new challenge, we present the
Zero-interference Reparameterizable Adaptation (ZiRa), a novel method that
introduces Zero-interference Loss and reparameterization techniques to tackle
IVLOD without incurring additional inference costs or a significant increase in
memory usage. Comprehensive experiments on COCO and ODinW-13 datasets
demonstrate that ZiRa effectively safeguards the zero-shot generalization
ability of VLODMs while continuously adapting to new tasks. Specifically, after
training on ODinW-13 datasets, ZiRa exhibits superior performance compared to
CL-DETR and iDETR, boosting zero-shot generalizability by substantial 13.91 and
8.74 AP, respectively.Our code is available at
https://github.com/JarintotionDin/ZiRaGroundingDINO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In the Eye of Transformer: Global-Local Correlation for Egocentric Gaze
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.04464v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.04464v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolin Lai, Miao Liu, Fiona Ryan, James M. Rehg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present the first transformer-based model to address the
challenging problem of egocentric gaze estimation. We observe that the
connection between the global scene context and local visual information is
vital for localizing the gaze fixation from egocentric video frames. To this
end, we design the transformer encoder to embed the global context as one
additional visual token and further propose a novel Global-Local Correlation
(GLC) module to explicitly model the correlation of the global token and each
local token. We validate our model on two egocentric video datasets - EGTEA
Gaze+ and Ego4D. Our detailed ablation studies demonstrate the benefits of our
method. In addition, our approach exceeds previous state-of-the-arts by a large
margin. We also provide additional visualizations to support our claim that
global-local correlation serves a key representation for predicting gaze
fixation from egocentric videos. More details can be found in our website
(https://bolinlai.github.io/GLC-EgoGazeEst).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lotus: Diffusion-based Visual Foundation Model for High-quality Dense
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18124v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18124v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing He, Haodong Li, Wei Yin, Yixun Liang, Leheng Li, Kaiqiang Zhou, Hongbo Zhang, Bingbing Liu, Ying-Cong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging the visual priors of pre-trained text-to-image diffusion models
offers a promising solution to enhance zero-shot generalization in dense
prediction tasks. However, existing methods often uncritically use the original
diffusion formulation, which may not be optimal due to the fundamental
differences between dense prediction and image generation. In this paper, we
provide a systemic analysis of the diffusion formulation for the dense
prediction, focusing on both quality and efficiency. And we find that the
original parameterization type for image generation, which learns to predict
noise, is harmful for dense prediction; the multi-step noising/denoising
diffusion process is also unnecessary and challenging to optimize. Based on
these insights, we introduce Lotus, a diffusion-based visual foundation model
with a simple yet effective adaptation protocol for dense prediction.
Specifically, Lotus is trained to directly predict annotations instead of
noise, thereby avoiding harmful variance. We also reformulate the diffusion
process into a single-step procedure, simplifying optimization and
significantly boosting inference speed. Additionally, we introduce a novel
tuning strategy called detail preserver, which achieves more accurate and
fine-grained predictions. Without scaling up the training data or model
capacity, Lotus achieves SoTA performance in zero-shot depth and normal
estimation across various datasets. It also enhances efficiency, being
significantly faster than most existing diffusion-based methods. Lotus'
superior quality and efficiency also enable a wide range of practical
applications, such as joint estimation, single/multi-view 3D reconstruction,
etc. Project page: https://lotus3d.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally. Project page:
  https://lotus3d.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly
  Mixed Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02263v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02263v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yatong Bai, Mo Zhou, Vishal M. Patel, Somayeh Sojoudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial robustness often comes at the cost of degraded accuracy, impeding
real-life applications of robust classification models. Training-based
solutions for better trade-offs are limited by incompatibilities with
already-trained high-performance large models, necessitating the exploration of
training-free ensemble approaches. Observing that robust models are more
confident in correct predictions than in incorrect ones on clean and
adversarial data alike, we speculate amplifying this "benign confidence
property" can reconcile accuracy and robustness in an ensemble setting. To
achieve so, we propose "MixedNUTS", a training-free method where the output
logits of a robust classifier and a standard non-robust classifier are
processed by nonlinear transformations with only three parameters, which are
optimized through an efficient algorithm. MixedNUTS then converts the
transformed logits into probabilities and mixes them as the overall output. On
CIFAR-10, CIFAR-100, and ImageNet datasets, experimental results with custom
strong adaptive attacks demonstrate MixedNUTS's vastly improved accuracy and
near-SOTA robustness -- it boosts CIFAR-100 clean accuracy by 7.86 points,
sacrificing merely 0.87 points in robust accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06949v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06949v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, Kfir Aberman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalization has emerged as a prominent aspect within the field of
generative AI, enabling the synthesis of individuals in diverse contexts and
styles, while retaining high-fidelity to their identities. However, the process
of personalization presents inherent challenges in terms of time and memory
requirements. Fine-tuning each personalized model needs considerable GPU time
investment, and storing a personalized model per subject can be demanding in
terms of storage capacity. To overcome these challenges, we propose
HyperDreamBooth - a hypernetwork capable of efficiently generating a small set
of personalized weights from a single image of a person. By composing these
weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth
can generate a person's face in various contexts and styles, with high subject
details while also preserving the model's crucial knowledge of diverse styles
and semantic modifications. Our method achieves personalization on faces in
roughly 20 seconds, 25x faster than DreamBooth and 125x faster than Textual
Inversion, using as few as one reference image, with the same quality and style
diversity as DreamBooth. Also our method yields a model that is 10,000x smaller
than a normal DreamBooth model. Project page: https://hyperdreambooth.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://hyperdreambooth.github.io</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">23</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RosePO: Aligning LLM-based Recommenders with Human Values 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Liao, Xiangnan He, Ruobing Xie, Jiancan Wu, Yancheng Yuan, Xingwu Sun, Zhanhui Kang, Xiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been a growing interest in leveraging Large Language
Models (LLMs) for recommendation systems, which usually adapt a pre-trained LLM
to the recommendation scenario through supervised fine-tuning (SFT). However,
both the pre-training and SFT stages fail to explicitly model the comparative
relationships of a user's preferences on different items. To construct a
"helpful and harmless" LLM-based recommender, we propose a general framework --
Recommendation with smoothing personalized Preference Optimization (RosePO),
which better aligns with customized human values during the post-training
stage. Specifically, in addition to the input and chosen response that
naturally align with SFT data, we design a rejected sampling strategy tailored
for enhancing helpfulness, along with two strategies aimed at mitigating biases
to promote harmlessness. To ensure robustness against uncertain labels present
in automatically constructed preference data, we introduce a personalized
smoothing factor predicted by a preference oracle into the optimization
objective. Evaluation on three real-world datasets demonstrates the
effectiveness of our method, showcasing not only improved recommendation
performance but also mitigation of semantic hallucination and popularity bias.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifying Economic and Language Models for Enhanced Sentiment Analysis of
  the Oil Market 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Himmet Kaplan, Ralf-Peter Mundani, Heiko Rölke, Albert Weichselbraun, Martin Tschudy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crude oil, a critical component of the global economy, has its prices
influenced by various factors such as economic trends, political events, and
natural disasters. Traditional prediction methods based on historical data have
their limits in forecasting, but recent advancements in natural language
processing bring new possibilities for event-based analysis. In particular,
Language Models (LM) and their advancement, the Generative Pre-trained
Transformer (GPT), have shown potential in classifying vast amounts of natural
language. However, these LMs often have difficulty with domain-specific
terminology, limiting their effectiveness in the crude oil sector. Addressing
this gap, we introduce CrudeBERT, a fine-tuned LM specifically for the crude
oil market. The results indicate that CrudeBERT's sentiment scores align more
closely with the WTI Futures curve and significantly enhance price predictions,
underscoring the crucial role of integrating economic principles into LMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Dual Latent Confounding Biases in Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfeng Deng, Qingfeng Chen, Debo Cheng, Jiuyong Li, Lin Liu, Xiaojing Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are extensively utilised across various areas to predict
user preferences for personalised experiences and enhanced user engagement and
satisfaction. Traditional recommender systems, however, are complicated by
confounding bias, particularly in the presence of latent confounders that
affect both item exposure and user feedback. Existing debiasing methods often
fail to capture the complex interactions caused by latent confounders in
interaction data, especially when dual latent confounders affect both the user
and item sides. To address this, we propose a novel debiasing method that
jointly integrates the Instrumental Variables (IV) approach and identifiable
Variational Auto-Encoder (iVAE) for Debiased representation learning in
Recommendation systems, referred to as IViDR. Specifically, IViDR leverages the
embeddings of user features as IVs to address confounding bias caused by latent
confounders between items and user feedback, and reconstructs the embedding of
items to obtain debiased interaction data. Moreover, IViDR employs an
Identifiable Variational Auto-Encoder (iVAE) to infer identifiable
representations of latent confounders between item exposure and user feedback
from both the original and debiased interaction data. Additionally, we provide
theoretical analyses of the soundness of using IV and the identifiability of
the latent representations. Extensive experiments on both synthetic and
real-world datasets demonstrate that IViDR outperforms state-of-the-art models
in reducing bias and providing reliable recommendations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QUIDS: Query Intent Generation via Dual Space Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yumeng Wang, Xiuying Chen, Suzan Verberne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query understanding is a crucial component of Information Retrieval (IR),
aimed at identifying the underlying search intent of textual queries. However,
most existing approaches oversimplify this task into query classification or
clustering, which fails to fully capture the nuanced intent behind the query.
In this paper, we address the task of query intent generation: to automatically
generate detailed and precise intent descriptions for search queries using
relevant and irrelevant documents given a query. These intent descriptions can
help users understand why the search engine considered the top-ranked documents
relevant, and provide more transparency to the retrieval process. We propose a
dual-space model that uses semantic relevance and irrelevance information in
the returned documents to explain the understanding of the query intent.
Specifically, in the encoding process, we project, separate, and distinguish
relevant and irrelevant documents in the representation space. Then, we
introduce a semantic decoupling model in the novel disentangling space, where
the semantics of irrelevant information are removed from the relevant space,
ensuring that only the essential and relevant intent is captured. This process
refines the understanding of the query and provides more accurate explanations
for the search results. Experiments on benchmark data demonstrate that our
methods produce high-quality query intent descriptions, outperforming existing
methods for this task, as well as state-of-the-art query-based summarization
methods. A token-level visualization of attention scores reveals that our model
effectively reduces the focus on irrelevant intent topics. Our findings open up
promising research and application directions for query intent generation,
particularly in exploratory search.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Cause Deconfounding for Recommender Systems with Latent
  Confounders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhirong Huang, Shichao Zhang, Debo Cheng, Jiuyong Li, Lin Liu, Guixian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recommender systems, various latent confounding factors (e.g., user social
environment and item public attractiveness) can affect user behavior, item
exposure, and feedback in distinct ways. These factors may directly or
indirectly impact user feedback and are often shared across items or users,
making them multi-cause latent confounders. However, existing methods typically
fail to account for latent confounders between users and their feedback, as
well as those between items and user feedback simultaneously. To address the
problem of multi-cause latent confounders, we propose a multi-cause
deconfounding method for recommender systems with latent confounders (MCDCF).
MCDCF leverages multi-cause causal effect estimation to learn substitutes for
latent confounders associated with both users and items, using user behaviour
data. Specifically, MCDCF treats the multiple items that users interact with
and the multiple users that interact with items as treatment variables,
enabling it to learn substitutes for the latent confounders that influence the
estimation of causality between users and their feedback, as well as between
items and user feedback. Additionally, we theoretically demonstrate the
soundness of our MCDCF method. Extensive experiments on three real-world
datasets demonstrate that our MCDCF method effectively recovers latent
confounders related to users and items, reducing bias and thereby improving
recommendation accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehending Knowledge Graphs with Large Language Models for
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, Chen Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the introduction of knowledge graphs (KGs) has significantly
advanced recommender systems by facilitating the discovery of potential
associations between items. However, existing methods still face several
limitations. First, most KGs suffer from missing facts or limited scopes. This
can lead to biased knowledge representations, thereby constraining the model's
performance. Second, existing methods typically convert textual information
into IDs, resulting in the loss of natural semantic connections between
different items. Third, existing methods struggle to capture high-order
relationships in global KGs due to their inefficient layer-by-layer information
propagation mechanisms, which are prone to introducing significant noise. To
address these limitations, we propose a novel method called CoLaKG, which
leverages large language models (LLMs) for knowledge-aware recommendation. The
extensive world knowledge and remarkable reasoning capabilities of LLMs enable
them to supplement KGs. Additionally, the strong text comprehension abilities
of LLMs allow for a better understanding of semantic information. Based on
this, we first extract subgraphs centered on each item from the KG and convert
them into textual inputs for the LLM. The LLM then outputs its comprehension of
these item-centered subgraphs, which are subsequently transformed into semantic
embeddings. Furthermore, to utilize the global information of the KG, we
construct an item-item graph using these semantic embeddings, which can
directly capture higher-order associations between items. Both the semantic
embeddings and the structural information from the item-item graph are
effectively integrated into the recommendation model through our designed
representation alignment and neighbor augmentation modules. Extensive
experiments on four real-world datasets demonstrate the superiority of our
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Triple Modality Fusion: Aligning Visual, Textual, and Graph Data with
  Large Language Models for Multi-Behavior Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luyi Ma, Xiaohan Li, Zezhong Fan, Jianpeng Xu, Jason Cho, Praveen Kanumala, Kaushiki Nag, Sushant Kumar, Kannan Achan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating diverse data modalities is crucial for enhancing the performance
of personalized recommendation systems. Traditional models, which often rely on
singular data sources, lack the depth needed to accurately capture the
multifaceted nature of item features and user behaviors. This paper introduces
a novel framework for multi-behavior recommendations, leveraging the fusion of
triple-modality, which is visual, textual, and graph data through alignment
with large language models (LLMs). By incorporating visual information, we
capture contextual and aesthetic item characteristics; textual data provides
insights into user interests and item features in detail; and graph data
elucidates relationships within the item-behavior heterogeneous graphs. Our
proposed model called Triple Modality Fusion (TMF) utilizes the power of LLMs
to align and integrate these three modalities, achieving a comprehensive
representation of user behaviors. The LLM models the user's interactions
including behaviors and item features in natural languages. Initially, the LLM
is warmed up using only natural language-based prompts. We then devise the
modality fusion module based on cross-attention and self-attention mechanisms
to integrate different modalities from other models into the same embedding
space and incorporate them into an LLM. Extensive experiments demonstrate the
effectiveness of our approach in improving recommendation accuracy. Further
ablation studies validate the effectiveness of our model design and benefits of
the TMF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Semantic Chunking Worth the Computational Cost? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renyi Qu, Ruixuan Tu, Forrest Bao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Retrieval-Augmented Generation (RAG) systems have
popularized semantic chunking, which aims to improve retrieval performance by
dividing documents into semantically coherent segments. Despite its growing
adoption, the actual benefits over simpler fixed-size chunking, where documents
are split into consecutive, fixed-size segments, remain unclear. This study
systematically evaluates the effectiveness of semantic chunking using three
common retrieval-related tasks: document retrieval, evidence retrieval, and
retrieval-based answer generation. The results show that the computational
costs associated with semantic chunking are not justified by consistent
performance gains. These findings challenge the previous assumptions about
semantic chunking and highlight the need for more efficient chunking strategies
in RAG systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supply Chain Network Extraction and Entity Classification Leveraging
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Liu, Hadi Meidani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supply chain networks are critical to the operational efficiency of
industries, yet their increasing complexity presents significant challenges in
mapping relationships and identifying the roles of various entities.
Traditional methods for constructing supply chain networks rely heavily on
structured datasets and manual data collection, limiting their scope and
efficiency. In contrast, recent advancements in Natural Language Processing
(NLP) and large language models (LLMs) offer new opportunities for discovering
and analyzing supply chain networks using unstructured text data. This paper
proposes a novel approach that leverages LLMs to extract and process raw
textual information from publicly available sources to construct a
comprehensive supply chain graph. We focus on the civil engineering sector as a
case study, demonstrating how LLMs can uncover hidden relationships among
companies, projects, and other entities. Additionally, we fine-tune an LLM to
classify entities within the supply chain graph, providing detailed insights
into their roles and relationships. The results show that domain-specific
fine-tuning improves classification accuracy, highlighting the potential of
LLMs for industry-specific supply chain analysis. Our contributions include the
development of a supply chain graph for the civil engineering sector, as well
as a fine-tuned LLM model that enhances entity classification and understanding
of supply chain networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Confidence Evaluation Measures in Zero-Shot CSS Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Farr, Iain Cruickshank, Nico Manzonelli, Nicholas Clark, Kate Starbird, Jevin West
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing classification confidence is critical for leveraging large language
models (LLMs) in automated labeling tasks, especially in the sensitive domains
presented by Computational Social Science (CSS) tasks. In this paper, we make
three key contributions: (1) we propose an uncertainty quantification (UQ)
performance measure tailored for data annotation tasks, (2) we compare, for the
first time, five different UQ strategies across three distinct LLMs and CSS
data annotation tasks, (3) we introduce a novel UQ aggregation strategy that
effectively identifies low-confidence LLM annotations and disproportionately
uncovers data incorrectly labeled by the LLMs. Our results demonstrate that our
proposed UQ aggregation strategy improves upon existing methods andcan be used
to significantly improve human-in-the-loop data annotation processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LFOSum: Summarizing Long-form Opinions with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mir Tafseer Nayeem, Davood Rafiei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online reviews play a pivotal role in influencing consumer decisions across
various domains, from purchasing products to selecting hotels or restaurants.
However, the sheer volume of reviews -- often containing repetitive or
irrelevant content -- leads to information overload, making it challenging for
users to extract meaningful insights. Traditional opinion summarization models
face challenges in handling long inputs and large volumes of reviews, while
newer Large Language Model (LLM) approaches often fail to generate accurate and
faithful summaries. To address those challenges, this paper introduces (1) a
new dataset of long-form user reviews, each entity comprising over a thousand
reviews, (2) two training-free LLM-based summarization approaches that scale to
long inputs, and (3) automatic evaluation metrics. Our dataset of user reviews
is paired with in-depth and unbiased critical summaries by domain experts,
serving as a reference for evaluation. Additionally, our novel reference-free
evaluation metrics provide a more granular, context-sensitive assessment of
summary faithfulness. We benchmark several open-source and closed-source LLMs
using our methods. Our evaluation reveals that LLMs still face challenges in
balancing sentiment and format adherence in long-form summaries, though
open-source models can narrow the gap when relevant information is retrieved in
a focused manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Computational Analysis of Pansori Singing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangheon Park, Danbinaerin Han, Dasaem Jeong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pansori is one of the most representative vocal genres of Korean traditional
music, which has an elaborated vocal melody line with strong vibrato. Although
the music is transmitted orally without any music notation, transcribing
pansori music in Western staff notation has been introduced for several
purposes, such as documentation of music, education, or research. In this
paper, we introduce computational analysis of pansori based on both audio and
corresponding transcription, how modern Music Information Retrieval tasks can
be used in analyzing traditional music and how it revealed different audio
characteristics of what pansori contains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Late-Breaking Demo Session of the 25th International Society for
  Music Information Retrieval (ISMIR) Conference, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REFINE on Scarce Data: Retrieval Enhancement through <span class="highlight-title">Fine-Tuning</span> via
  Model Fusion of Embedding Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12890v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12890v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ambuje Gupta, Mrinal Rawat, Andreas Stolcke, Roberto Pieraccini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval augmented generation (RAG) pipelines are commonly used in tasks
such as question-answering (QA), relying on retrieving relevant documents from
a vector store computed using a pretrained embedding model. However, if the
retrieved context is inaccurate, the answers generated using the large language
model (LLM) may contain errors or hallucinations. Although pretrained embedding
models have advanced, adapting them to new domains remains challenging.
Fine-tuning is a potential solution, but industry settings often lack the
necessary fine-tuning data. To address these challenges, we propose REFINE, a
novel technique that generates synthetic data from available documents and then
uses a model fusion approach to fine-tune embeddings for improved retrieval
performance in new domains, while preserving out-of-domain capability. We
conducted experiments on the two public datasets: SQUAD and RAG-12000 and a
proprietary TOURISM dataset. Results demonstrate that even the standard
fine-tuning with the proposed data augmentation technique outperforms the
vanilla pretrained model. Furthermore, when combined with model fusion, the
proposed approach achieves superior performance, with a 5.76% improvement in
recall on the TOURISM dataset, and 6.58 % and 0.32% enhancement on SQUAD and
RAG-12000 respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in AJCAI'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AT-RAG: An Adaptive RAG Model Enhancing Query Efficiency with Topic
  Filtering and Iterative Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Reza Rezaei, Maziar Hafezi, Amit Satpathy, Lovell Hodge, Ebrahim Pourjafari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in QA with LLM, like GPT-4, have shown limitations in
handling complex multi-hop queries. We propose AT-RAG, a novel multistep RAG
incorporating topic modeling for efficient document retrieval and reasoning.
Using BERTopic, our model dynamically assigns topics to queries, improving
retrieval accuracy and efficiency. We evaluated AT-RAG on multihop benchmark
datasets QA and a medical case study QA. Results show significant improvements
in correctness, completeness, and relevance compared to existing methods.
AT-RAG reduces retrieval time while maintaining high precision, making it
suitable for general tasks QA and complex domain-specific challenges such as
medical QA. The integration of topic filtering and iterative reasoning enables
our model to handle intricate queries efficiently, which makes it suitable for
applications that require nuanced information retrieval and decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ P4GCN: Vertical Federated Social Recommendation with Privacy-Preserving
  Two-Party Graph Convolution Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Wang, Wanwan Wang, Yimin Huang, Zhaopeng Peng, Ziqi Yang, Cheng Wang, Xiaoliang Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, graph neural networks (GNNs) have been commonly utilized for
social recommendation systems. However, real-world scenarios often present
challenges related to user privacy and business constraints, inhibiting direct
access to valuable social information from other platforms. While many existing
methods have tackled matrix factorization-based social recommendations without
direct social data access, developing GNN-based federated social recommendation
models under similar conditions remains largely unexplored. To address this
issue, we propose a novel vertical federated social recommendation method
leveraging privacy-preserving two-party graph convolution networks (P4GCN) to
enhance recommendation accuracy without requiring direct access to sensitive
social information. First, we introduce a Sandwich-Encryption module to ensure
comprehensive data privacy during the collaborative computing process. Second,
we provide a thorough theoretical analysis of the privacy guarantees,
considering the participation of both curious and honest parties. Extensive
experiments on four real-world datasets demonstrate that P4GCN outperforms
state-of-the-art methods in terms of recommendation accuracy. The code is
available at https://github.com/WwZzz/P4GCN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>DSI: <span class="highlight-title">Prompt</span>-based Rehearsal-free Instance-wise Incremental
  Learning for Document Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan-Luc Huynh, Thuy-Trang Vu, Weiqing Wang, Yinwei Wei, Trung Le, Dragan Gasevic, Yuan-Fang Li, Thanh-Toan Do
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiable Search Index (DSI) utilizes Pre-trained Language Models (PLMs)
for efficient document retrieval without relying on external indexes. However,
DSI needs full re-training to handle updates in dynamic corpora, causing
significant computational inefficiencies. We introduce PromptDSI, a
prompt-based rehearsal-free approach for instance-wise incremental learning
document retrieval. PromptDSI attaches prompts to the frozen PLM's encoder of
DSI, leveraging its powerful representation to efficiently index new corpora
while maintaining a balance between stability and plasticity. We eliminate the
initial forward pass of prompt-based continual learning methods that doubles
training and inference time. Moreover, we propose a topic-aware prompt pool
that employs neural topic embeddings as fixed keys. This strategy ensures
diverse and effective prompt usage, addressing the challenge of parameter
underutilization caused by the collapse of the query-key matching mechanism.
Our empirical evaluations demonstrate that BERT-based PromptDSI matches IncDSI
in managing forgetting while improving new corpora performance by more than 4%
Hits@10 on NQ320k and upto 3% MRR@10 on MS MARCO 300k.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval
  Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14162v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14162v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Ni, Tobias Schimanski, Meihong Lin, Mrinmaya Sachan, Elliott Ash, Markus Leippold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) is widely employed to ground responses
to queries on domain-specific documents. But do RAG implementations leave out
important information when answering queries that need an integrated analysis
of information (e.g., Tell me good news in the stock market today.)? To address
these concerns, RAG developers need to annotate information retrieval (IR) data
for their domain of interest, which is challenging because (1) domain-specific
queries usually need nuanced definitions of relevance beyond shallow semantic
relevance; and (2) human or GPT-4 annotation is costly and cannot cover all
(query, document) pairs (i.e., annotation selection bias), thus harming the
effectiveness in evaluating IR recall. To address these challenges, we propose
DIRAS (Domain-specific Information Retrieval Annotation with Scalability), a
manual-annotation-free schema that fine-tunes open-sourced LLMs to consider
nuanced relevance definition and annotate (partial) relevance labels with
calibrated relevance scores. Extensive evaluation shows that DIRAS enables
smaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking
unseen (query, document) pairs, and is helpful for real-world RAG development.
All code, LLM generations, and human annotations can be found in
\url{https://github.com/EdisonNi-hku/DIRAS}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19014v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19014v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heegyu Kim, Taeyang Jeon, Seunghwan Choi, Seungtaek Choi, Hyunsouk Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL systems have become crucial for translating natural language into
SQL queries in various industries, enabling non-technical users to perform
complex data operations. The need for accurate evaluation methods has increased
as these systems have grown more sophisticated. However, the Execution Accuracy
(EX), the most prevalent evaluation metric, still shows many false positives
and negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel
approach to evaluating text-to-SQL systems using large language models (LLMs)
to emulate human expert-level evaluation of SQL queries. Our metric improves
agreement with human experts (from 62 to 87.04 in Cohen's kappa) with
comprehensive context and sophisticated criteria. Our extensive experiments
yield several key insights: (1) Models' performance increases by over 2.6
points on average, substantially affecting rankings on Spider and BIRD
benchmarks; (2) The underestimation of models in EX primarily stems from
annotation quality issues; and (3) Model performance on particularly
challenging questions tends to be overestimated. This work contributes to a
more accurate and nuanced evaluation of text-to-SQL systems, potentially
reshaping our understanding of state-of-the-art performance in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Practice-Friendly LLM-Enhanced Paradigm with Preference Parsing for
  Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00333v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00333v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dugang Liu, Shenxian Xian, Xiaolin Lin, Xiaolian Zhang, Hong Zhu, Yuan Fang, Zhen Chen, Zhong Ming
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The training paradigm integrating large language models (LLM) is gradually
reshaping sequential recommender systems (SRS) and has shown promising results.
However, most existing LLM-enhanced methods rely on rich textual information on
the item side and instance-level supervised fine-tuning (SFT) to inject
collaborative information into LLM, which is inefficient and limited in many
applications. To alleviate these problems, this paper proposes a
practice-friendly LLM-enhanced paradigm with preference parsing (P2Rec) for
SRS. Specifically, in the information reconstruction stage, we design a new
user-level SFT task for collaborative information injection with the assistance
of a pre-trained SRS model, which is more efficient and compatible with limited
text information. Our goal is to let LLM learn to reconstruct a corresponding
prior preference distribution from each user's interaction sequence, where LLM
needs to effectively parse the latent category of each item and the
relationship between different items to accomplish this task. In the
information augmentation stage, we feed each item into LLM to obtain a set of
enhanced embeddings that combine collaborative information and LLM inference
capabilities. These embeddings can then be used to help train various future
SRS models. Finally, we verify the effectiveness and efficiency of our TSLRec
on three SRS benchmark datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Circuits in <span class="highlight-title">Pretrain</span>ed Transformers <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable capabilities of modern large language models are rooted in
their vast repositories of knowledge encoded within their parameters, enabling
them to perceive the world and engage in reasoning. The inner workings of how
these models store knowledge have long been a subject of intense interest and
investigation among researchers. To date, most studies have concentrated on
isolated components within these models, such as the Multilayer Perceptrons and
attention head. In this paper, we delve into the computation graph of the
language model to uncover the knowledge circuits that are instrumental in
articulating specific knowledge. The experiments, conducted with GPT2 and
TinyLLAMA, have allowed us to observe how certain information heads, relation
heads, and Multilayer Perceptrons collaboratively encode knowledge within the
model. Moreover, we evaluate the impact of current knowledge editing techniques
on these knowledge circuits, providing deeper insights into the functioning and
constraints of these editing methodologies. Finally, we utilize knowledge
circuits to analyze and interpret language model behaviors such as
hallucinations and in-context learning. We believe the knowledge circuits hold
potential for advancing our understanding of Transformers and guiding the
improved design of knowledge editing. Code and data are available in
https://github.com/zjunlp/KnowledgeCircuits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024, 32 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\textit{lucie}$: An Improved Python Package for Loading Datasets from
  the UCI Machine Learning Repository 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenneth Ge, Phuc Nguyen, Ramy Arnaout
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The University of California--Irvine (UCI) Machine Learning (ML) Repository
(UCIMLR) is consistently cited as one of the most popular dataset repositories,
hosting hundreds of high-impact datasets. However, a significant portion,
including 28.4% of the top 250, cannot be imported via the $\textit{ucimlrepo}$
package that is provided and recommended by the UCIMLR website. Instead, they
are hosted as .zip files, containing nonstandard formats that are difficult to
import without additional ad hoc processing. To address this issue, here we
present $\textit{lucie}$ -- $\underline{l}oad$ $\underline{U}niversity$
$\underline{C}alifornia$ $\underline{I}rvine$ $\underline{e}xamples$ -- a
utility that automatically determines the data format and imports many of these
previously non-importable datasets, while preserving as much of a tabular data
structure as possible. $\textit{lucie}$ was designed using the top 100 most
popular datasets and benchmarked on the next 130, where it resulted in a
success rate of 95.4% vs. 73.1% for $\textit{ucimlrepo}$. $\textit{lucie}$ is
available as a Python package on PyPI with 98% code coverage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Inter-Item Relations: Dynamic Adaption for Enhancing LLM-Based
  Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        CanYi Liu, Wei Li,  Youchen,  Zhang, Hui Li, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommender systems (SRS) predict the next items that users may
prefer based on user historical interaction sequences. Inspired by the rise of
large language models (LLMs) in various AI applications, there is a surge of
work on LLM-based SRS. Despite their attractive performance, existing LLM-based
SRS still exhibit some limitations, including neglecting intra-item relations,
ignoring long-term collaborative knowledge and using inflexible architecture
designs for adaption. To alleviate these issues, we propose an LLM-based
sequential recommendation model named DARec. Built on top of coarse-grained
adaption for capturing inter-item relations, DARec is further enhanced with (1)
context masking that models intra-item relations to help LLM better understand
token and item semantics in the context of SRS, (2) collaborative knowledge
injection that helps LLM incorporate long-term collaborative knowledge, and (3)
a dynamic adaption mechanism that uses Bayesian optimization to flexibly choose
layer-wise adapter architectures in order to better incorporate different
sequential information. Extensive experiments demonstrate that DARec can
effectively handle sequential recommendation in a dynamic and adaptive manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LitSearch: A Retrieval Benchmark for Scientific Literature Search <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18940v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18940v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirudh Ajith, Mengzhou Xia, Alexis Chevalier, Tanya Goyal, Danqi Chen, Tianyu Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Literature search questions, such as "Where can I find research on the
evaluation of consistency in generated summaries?" pose significant challenges
for modern search engines and retrieval systems. These questions often require
a deep understanding of research concepts and the ability to reason across
entire articles. In this work, we introduce LitSearch, a retrieval benchmark
comprising 597 realistic literature search queries about recent ML and NLP
papers. LitSearch is constructed using a combination of (1) questions generated
by GPT-4 based on paragraphs containing inline citations from research papers
and (2) questions manually written by authors about their recently published
papers. All LitSearch questions were manually examined or edited by experts to
ensure high quality. We extensively benchmark state-of-the-art retrieval models
and also evaluate two LLM-based reranking pipelines. We find a significant
performance gap between BM25 and state-of-the-art dense retrievers, with a
24.8% absolute difference in recall@5. The LLM-based reranking strategies
further improve the best-performing dense retriever by 4.4%. Additionally,
commercial search engines and research tools like Google Search perform poorly
on LitSearch, lagging behind the best dense retriever by up to 32 recall
points. Taken together, these results show that LitSearch is an informative new
testbed for retrieval systems while catering to a real-world use case.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024. Dataset and code are available at
  https://github.com/princeton-nlp/LitSearch</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Prototype Evolving for Test-Time Generalization of <span class="highlight-title">Vision-Language</span>
  Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ce Zhang, Simon Stepputtis, Katia Sycara, Yaqi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time adaptation, which enables models to generalize to diverse data with
unlabeled test samples, holds significant value in real-world scenarios.
Recently, researchers have applied this setting to advanced pre-trained
vision-language models (VLMs), developing approaches such as test-time prompt
tuning to further extend their practical applicability. However, these methods
typically focus solely on adapting VLMs from a single modality and fail to
accumulate task-specific knowledge as more samples are processed. To address
this, we introduce Dual Prototype Evolving (DPE), a novel test-time adaptation
approach for VLMs that effectively accumulates task-specific knowledge from
multi-modalities. Specifically, we create and evolve two sets of
prototypes--textual and visual--to progressively capture more accurate
multi-modal representations for target classes during test time. Moreover, to
promote consistent multi-modal representations, we introduce and optimize
learnable residuals for each test sample to align the prototypes from both
modalities. Extensive experimental results on 15 benchmark datasets demonstrate
that our proposed DPE consistently outperforms previous state-of-the-art
methods while also exhibiting competitive computational efficiency. Code is
available at https://github.com/zhangce01/DPE-CLIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. Project page:
  https://zhangce01.github.io/DPE-CLIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metal Price Spike Prediction via a Neurosymbolic Ensemble Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathaniel Lee, Noel Ngu, Harshdeep Singh Sahdev, Pramod Motaganahall, Al Mehdi Saadat Chowdhury, Bowen Xi, Paulo Shakarian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting price spikes in critical metals such as Cobalt, Copper, Magnesium,
and Nickel is crucial for mitigating economic risks associated with global
trends like the energy transition and reshoring of manufacturing. While
traditional models have focused on regression-based approaches, our work
introduces a neurosymbolic ensemble framework that integrates multiple neural
models with symbolic error detection and correction rules. This framework is
designed to enhance predictive accuracy by correcting individual model errors
and offering interpretability through rule-based explanations. We show that our
method provides up to 6.42% improvement in precision, 29.41% increase in recall
at 13.24% increase in F1 over the best performing neural models. Further, our
method, as it is based on logical rules, has the benefit of affording an
explanation as to which combination of neural models directly contribute to a
given prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JudgeBench: A Benchmark for Evaluating LLM-based Judges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Y. Tang, Alejandro Cuadron, Chenguang Wang, Raluca Ada Popa, Ion Stoica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-based judges have emerged as a scalable alternative to human evaluation
and are increasingly used to assess, compare, and improve models. However, the
reliability of LLM-based judges themselves is rarely scrutinized. As LLMs
become more advanced, their responses grow more sophisticated, requiring
stronger judges to evaluate them. Existing benchmarks primarily focus on a
judge's alignment with human preferences, but often fail to account for more
challenging tasks where crowdsourced human preference is a poor indicator of
factual and logical correctness. To address this, we propose a novel evaluation
framework to objectively evaluate LLM-based judges. Based on this framework, we
propose JudgeBench, a benchmark for evaluating LLM-based judges on challenging
response pairs spanning knowledge, reasoning, math, and coding. JudgeBench
leverages a novel pipeline for converting existing difficult datasets into
challenging response pairs with preference labels reflecting objective
correctness. Our comprehensive evaluation on a collection of prompted judges,
fine-tuned judges, multi-agent judges, and reward models shows that JudgeBench
poses a significantly greater challenge than previous benchmarks, with many
strong models (e.g., GPT-4o) performing just slightly better than random
guessing. Overall, JudgeBench offers a reliable platform for assessing
increasingly advanced LLM-based judges. Data and code are available at
https://github.com/ScalerLab/JudgeBench .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-Scaling versus Task-Scaling in In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhesam Abedsoltan, Adityanarayanan Radhakrishnan, Jingfeng Wu, Mikhail Belkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers exhibit In-Context Learning (ICL), where these models solve new
tasks by using examples in the prompt without additional training. In our work,
we identify and analyze two key components of ICL: (1) context-scaling, where
model performance improves as the number of in-context examples increases and
(2) task-scaling, where model performance improves as the number of
pre-training tasks increases. While transformers are capable of both
context-scaling and task-scaling, we empirically show that standard Multi-Layer
Perceptrons (MLPs) with vectorized input are only capable of task-scaling. To
understand how transformers are capable of context-scaling, we first propose a
significantly simplified transformer architecture without key, query, value
weights. We show that it performs ICL comparably to the original GPT-2 model in
various statistical learning tasks including linear regression, teacher-student
settings. Furthermore, a single block of our simplified transformer can be
viewed as data dependent feature map followed by an MLP. This feature map on
its own is a powerful predictor that is capable of context-scaling but is not
capable of task-scaling. We show empirically that concatenating the output of
this feature map with vectorized data as an input to MLPs enables both
context-scaling and task-scaling. This finding provides a simple setting to
study context and task-scaling for ICL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometry-Aware Generative Autoencoders for Warped Riemannian Metric
  Learning and Generative Modeling on Data Manifolds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingzhi Sun, Danqi Liao, Kincaid MacDonald, Yanlei Zhang, Chen Liu, Guillaume Huguet, Guy Wolf, Ian Adelstein, Tim G. J. Rudner, Smita Krishnaswamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid growth of high-dimensional datasets in fields such as single-cell RNA
sequencing and spatial genomics has led to unprecedented opportunities for
scientific discovery, but it also presents unique computational and statistical
challenges. Traditional methods struggle with geometry-aware data generation,
interpolation along meaningful trajectories, and transporting populations via
feasible paths. To address these issues, we introduce Geometry-Aware Generative
Autoencoder (GAGA), a novel framework that combines extensible manifold
learning with generative modeling. GAGA constructs a neural network embedding
space that respects the intrinsic geometries discovered by manifold learning
and learns a novel warped Riemannian metric on the data space. This warped
metric is derived from both the points on the data manifold and negative
samples off the manifold, allowing it to characterize a meaningful geometry
across the entire latent space. Using this metric, GAGA can uniformly sample
points on the manifold, generate points along geodesics, and interpolate
between populations across the learned manifold. GAGA shows competitive
performance in simulated and real world datasets, including a 30% improvement
over the state-of-the-art methods in single-cell population-level trajectory
inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned
  Concepts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongcheng Gao, Tianyu Pang, Chao Du, Taihang Hu, Zhijie Deng, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid progress of diffusion-based content generation, significant
efforts are being made to unlearn harmful or copyrighted concepts from
pretrained diffusion models (DMs) to prevent potential model misuse. However,
it is observed that even when DMs are properly unlearned before release,
malicious finetuning can compromise this process, causing DMs to relearn the
unlearned concepts. This occurs partly because certain benign concepts (e.g.,
"skin") retained in DMs are related to the unlearned ones (e.g., "nudity"),
facilitating their relearning via finetuning. To address this, we propose
meta-unlearning on DMs. Intuitively, a meta-unlearned DM should behave like an
unlearned DM when used as is; moreover, if the meta-unlearned DM undergoes
malicious finetuning on unlearned concepts, the related benign concepts
retained within it will be triggered to self-destruct, hindering the relearning
of unlearned concepts. Our meta-unlearning framework is compatible with most
existing unlearning methods, requiring only the addition of an
easy-to-implement meta objective. We validate our approach through empirical
experiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4
and SDXL), supported by extensive ablation studies. Our code is available at
https://github.com/sail-sg/Meta-Unlearning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Non-Local Model Merging Problem: Permutation Symmetries and Variance
  Collapse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekansh Sharma, Daniel M. Roy, Gintare Karolina Dziugaite
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging aims to efficiently combine the weights of multiple expert
models, each trained on a specific task, into a single multi-task model, with
strong performance across all tasks. When applied to all but the last layer of
weights, existing methods -- such as Task Arithmetic, TIES-merging, and TALL
mask merging -- work well to combine expert models obtained by fine-tuning a
common foundation model, operating within a "local" neighborhood of the
foundation model. This work explores the more challenging scenario of
"non-local" merging, which we find arises when an expert model changes
significantly during pretraining or where the expert models do not even share a
common foundation model.
  We observe that standard merging techniques often fail to generalize
effectively in this non-local setting, even when accounting for permutation
symmetries using standard techniques. We identify that this failure is, in
part, due to "variance collapse", a phenomenon identified also in the setting
of linear mode connectivity by Jordan et al. (2023). To address this, we
propose a multi-task technique to re-scale and shift the output activations of
the merged model for each task, aligning its output statistics with those of
the corresponding task-specific expert models. Our experiments demonstrate that
this correction significantly improves the performance of various model merging
approaches in non-local settings, providing a strong baseline for future
research on this problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And
  Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehong Yoon, Shoubin Yu, Vaidehi Patil, Huaxiu Yao, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in diffusion models have significantly enhanced their ability
to generate high-quality images and videos, but they have also increased the
risk of producing unsafe content. Existing unlearning/editing-based methods for
safe generation remove harmful concepts from models but face several
challenges: (1) They cannot instantly remove harmful concepts without training.
(2) Their safe generation capabilities depend on collected training data. (3)
They alter model weights, risking degradation in quality for content unrelated
to toxic concepts. To address these, we propose SAFREE, a novel, training-free
approach for safe T2I and T2V, that does not alter the model's weights.
Specifically, we detect a subspace corresponding to a set of toxic concepts in
the text embedding space and steer prompt embeddings away from this subspace,
thereby filtering out harmful content while preserving intended semantics. To
balance the trade-off between filtering toxicity and preserving safe concepts,
SAFREE incorporates a novel self-validating filtering mechanism that
dynamically adjusts the denoising steps when applying the filtered embeddings.
Additionally, we incorporate adaptive re-attention mechanisms within the
diffusion latent space to selectively diminish the influence of features
related to toxic concepts at the pixel level. In the end, SAFREE ensures
coherent safety checking, preserving the fidelity, quality, and safety of the
output. SAFREE achieves SOTA performance in suppressing unsafe content in T2I
generation compared to training-free baselines and effectively filters targeted
concepts while maintaining high-quality images. It also shows competitive
results against training-based methods. We extend SAFREE to various T2I
backbones and T2V tasks, showcasing its flexibility and generalization. SAFREE
provides a robust and adaptable safeguard for ensuring safe visual generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally; Project page:
  https://safree-safe-t2i-t2v.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StyleDistance: Stronger Content-Independent Style Embeddings with
  Synthetic Parallel Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajay Patel, Jiacheng Zhu, Justin Qiu, Zachary Horvitz, Marianna Apidianaki, Kathleen McKeown, Chris Callison-Burch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Style representations aim to embed texts with similar writing styles closely
and texts with different styles far apart, regardless of content. However, the
contrastive triplets often used for training these representations may vary in
both style and content, leading to potential content leakage in the
representations. We introduce StyleDistance, a novel approach to training
stronger content-independent style embeddings. We use a large language model to
create a synthetic dataset of near-exact paraphrases with controlled style
variations, and produce positive and negative examples across 40 distinct style
features for precise contrastive learning. We assess the quality of our
synthetic data and embeddings through human and automatic evaluations.
StyleDistance enhances the content-independence of style embeddings, which
generalize to real-world benchmarks and outperform leading style
representations in downstream applications. Our model can be found at
https://huggingface.co/StyleDistance/styledistance .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Initialization Method for Factorization Machine Based on Low-Rank
  Approximation for Constructing a Corrected Approximate Ising Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuya Seki, Hyakka Nakada, Shu Tanaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an initialization method that can approximate a given
approximate Ising model with a high degree of accuracy using the Factorization
Machine (FM), a machine learning model. The construction of Ising models using
FM is applied to the combinatorial optimization problem using the factorization
machine with quantum annealing. It is anticipated that the optimization
performance of FMQA will be enhanced through the implementation of the
warm-start method. Nevertheless, the optimal initialization method for
leveraging the warm-start approach in FMQA remains undetermined. Consequently,
the present study compares a number of initialization methods and identifies
the most appropriate for use with a warm-start in FMQA through numerical
experimentation. Furthermore, the properties of the proposed FM initialization
method are analyzed using random matrix theory, demonstrating that the
approximation accuracy of the proposed method is not significantly influenced
by the specific Ising model under consideration. The findings of this study
will facilitate the advancement of combinatorial optimization problem-solving
through the use of Ising machines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CREAM: Consistency Regularized Self-Rewarding Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Wang, Weilei He, Zhiyuan Liang, Xuchao Zhang, Chetan Bansal, Ying Wei, Weitong Zhang, Huaxiu Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent self-rewarding large language models (LLM) have successfully applied
LLM-as-a-Judge to iteratively improve the alignment performance without the
need of human annotations for preference data. These methods commonly utilize
the same LLM to act as both the policy model (which generates responses) and
the reward model (which scores and ranks those responses). The ranked responses
are then used as preference pairs to train the LLM via direct alignment
technologies (e.g. DPO). However, it is noteworthy that throughout this
process, there is no guarantee of accuracy in the rewarding and ranking, which
is critical for ensuring accurate rewards and high-quality preference data.
Empirical results from relatively small LLMs (e.g., 7B parameters) also
indicate that improvements from self-rewarding may diminish after several
iterations in certain situations, which we hypothesize is due to accumulated
bias in the reward system. This bias can lead to unreliable preference data for
training the LLM. To address this issue, we first formulate and analyze the
generalized iterative preference fine-tuning framework for self-rewarding
language model. We then introduce the regularization to this generalized
framework to mitigate the overconfident preference labeling in the
self-rewarding process. Based on this theoretical insight, we propose a
Consistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages
the rewarding consistency across different iterations to regularize the
self-rewarding training, helping the model to learn from more reliable
preference data. With this explicit regularization, our empirical results
demonstrate the superiority of CREAM in improving both reward consistency and
alignment performance. The code is publicly available at
https://github.com/Raibows/CREAM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual Generative Modeling with Variational Causal Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulun Wu, Louie McConnell, Claudia Iriondo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating an individual's potential outcomes under counterfactual treatments
is a challenging task for traditional causal inference and supervised learning
approaches when the outcome is high-dimensional (e.g. gene expressions, facial
images) and covariates are relatively limited. In this case, to predict one's
outcomes under counterfactual treatments, it is crucial to leverage individual
information contained in its high-dimensional observed outcome in addition to
the covariates. Prior works using variational inference in counterfactual
generative modeling have been focusing on neural adaptations and model variants
within the conditional variational autoencoder formulation, which we argue is
fundamentally ill-suited to the notion of counterfactual in causal inference.
In this work, we present a novel variational Bayesian causal inference
framework and its theoretical backings to properly handle counterfactual
generative modeling tasks, through which we are able to conduct counterfactual
supervision end-to-end during training without any counterfactual samples, and
encourage latent disentanglement that aids the correct identification of causal
effect in counterfactual generations. In experiments, we demonstrate the
advantage of our framework compared to state-of-the-art models in
counterfactual generative modeling on multiple benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transformer based super-resolution downscaling for regional reanalysis:
  Full domain vs tiling approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Pérez, Mario Santa Cruz, Daniel San Martín, José Manuel Gutiérrez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Super-resolution (SR) is a promising cost-effective downscaling methodology
for producing high-resolution climate information from coarser counterparts. A
particular application is downscaling regional reanalysis outputs (predictand)
from the driving global counterparts (predictor). This study conducts an
intercomparison of various SR downscaling methods focusing on temperature and
using the CERRA reanalysis (5.5 km resolution, produced with a regional
atmospheric model driven by ERA5) as example. The method proposed in this work
is the Swin transformer and two alternative methods are used as benchmark
(fully convolutional U-Net and convolutional and dense DeepESD) as well as the
simple bicubic interpolation. We compare two approaches, the standard one using
the full domain as input and a more scalable tiling approach, dividing the full
domain into tiles that are used as input. The methods are trained to downscale
CERRA surface temperature, based on temperature information from the driving
ERA5; in addition, the tiling approach includes static orographic information.
We show that the tiling approach, which requires spatial transferability, comes
at the cost of a lower performance (although it outperforms some full-domain
benchmarks), but provides an efficient scalable solution that allows SR
reduction on a pan-European scale and is valuable for real-time applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing 3D Geometry Reconstruction from Implicit Neural
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shen Fan, Przemyslaw Musialski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit neural representations have emerged as a powerful tool in learning
3D geometry, offering unparalleled advantages over conventional representations
like mesh-based methods. A common type of INR implicitly encodes a shape's
boundary as the zero-level set of the learned continuous function and learns a
mapping from a low-dimensional latent space to the space of all possible shapes
represented by its signed distance function. However, most INRs struggle to
retain high-frequency details, which are crucial for accurate geometric
depiction, and they are computationally expensive. To address these
limitations, we present a novel approach that both reduces computational
expenses and enhances the capture of fine details. Our method integrates
periodic activation functions, positional encodings, and normals into the
neural network architecture. This integration significantly enhances the
model's ability to learn the entire space of 3D shapes while preserving
intricate details and sharp features, areas where conventional representations
often fall short.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Does Variance Shape the Regret in Contextual Bandits? <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Jia, Jian Qian, Alexander Rakhlin, Chen-Yu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider realizable contextual bandits with general function
approximation, investigating how small reward variance can lead to
better-than-minimax regret bounds. Unlike in minimax bounds, we show that the
eluder dimension $d_\text{elu}$$-$a complexity measure of the function
class$-$plays a crucial role in variance-dependent bounds. We consider two
types of adversary:
  (1) Weak adversary: The adversary sets the reward variance before observing
the learner's action. In this setting, we prove that a regret of
$\Omega(\sqrt{\min\{A,d_\text{elu}\}\Lambda}+d_\text{elu})$ is unavoidable when
$d_{\text{elu}}\leq\sqrt{AT}$, where $A$ is the number of actions, $T$ is the
total number of rounds, and $\Lambda$ is the total variance over $T$ rounds.
For the $A\leq d_\text{elu}$ regime, we derive a nearly matching upper bound
$\tilde{O}(\sqrt{A\Lambda}+d_\text{elu})$ for the special case where the
variance is revealed at the beginning of each round.
  (2) Strong adversary: The adversary sets the reward variance after observing
the learner's action. We show that a regret of
$\Omega(\sqrt{d_\text{elu}\Lambda}+d_\text{elu})$ is unavoidable when
$\sqrt{d_\text{elu}\Lambda}+d_\text{elu}\leq\sqrt{AT}$. In this setting, we
provide an upper bound of order
$\tilde{O}(d_\text{elu}\sqrt{\Lambda}+d_\text{elu})$.
  Furthermore, we examine the setting where the function class additionally
provides distributional information of the reward, as studied by Wang et al.
(2024). We demonstrate that the regret bound
$\tilde{O}(\sqrt{d_\text{elu}\Lambda}+d_\text{elu})$ established in their work
is unimprovable when $\sqrt{d_{\text{elu}}\Lambda}+d_\text{elu}\leq\sqrt{AT}$.
However, with a slightly different definition of the total variance and with
the assumption that the reward follows a Gaussian distribution, one can achieve
a regret of $\tilde{O}(\sqrt{A\Lambda}+d_\text{elu})$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the sample complexity of purity and inner product estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiyuan Gong, Jonas Haferkamp, Qi Ye, Zhihan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the sample complexity of the prototypical tasks quantum purity
estimation and quantum inner product estimation. In purity estimation, we are
to estimate $tr(\rho^2)$ of an unknown quantum state $\rho$ to additive error
$\epsilon$. Meanwhile, for quantum inner product estimation, Alice and Bob are
to estimate $tr(\rho\sigma)$ to additive error $\epsilon$ given copies of
unknown quantum state $\rho$ and $\sigma$ using classical communication and
restricted quantum communication.
  In this paper, we show a strong connection between the sample complexity of
purity estimation with bounded quantum memory and inner product estimation with
bounded quantum communication and unentangled measurements. We propose a
protocol that solves quantum inner product estimation with $k$-qubit one-way
quantum communication and unentangled local measurements using
$O(median\{1/\epsilon^2,2^{n/2}/\epsilon,2^{n-k}/\epsilon^2\})$ copies of
$\rho$ and $\sigma$. Our protocol can be modified to estimate the purity of an
unknown quantum state $\rho$ using $k$-qubit quantum memory with the same
complexity. We prove that arbitrary protocols with $k$-qubit quantum memory
that estimate purity to error $\epsilon$ require
$\Omega(median\{1/\epsilon^2,2^{n/2}/\sqrt{\epsilon},2^{n-k}/\epsilon^2\})$
copies of $\rho$. This indicates the same lower bound for quantum inner product
estimation with one-way $k$-qubit quantum communication and classical
communication, and unentangled local measurements. For purity estimation, we
further improve the lower bound to
$\Omega(\max\{1/\epsilon^2,2^{n/2}/\epsilon\})$ for any protocols using an
identical single-copy projection-valued measurement.
  Additionally, we investigate a decisional variant of quantum distributed
inner product estimation without quantum communication for mixed state and
provide a lower bound on the sample complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FusionLLM: A Decentralized LLM Training System on Geo-distributed GPUs
  with Adaptive Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenheng Tang, Xueze Kang, Yiming Yin, Xinglin Pan, Yuxin Wang, Xin He, Qiang Wang, Rongfei Zeng, Kaiyong Zhao, Shaohuai Shi, Amelie Chi Zhou, Bo Li, Bingsheng He, Xiaowen Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To alleviate hardware scarcity in training large deep neural networks (DNNs),
particularly large language models (LLMs), we present FusionLLM, a
decentralized training system designed and implemented for training DNNs using
geo-distributed GPUs across different computing clusters or individual devices.
Decentralized training faces significant challenges regarding system design and
efficiency, including: 1) the need for remote automatic differentiation (RAD),
2) support for flexible model definitions and heterogeneous software, 3)
heterogeneous hardware leading to low resource utilization or the straggler
problem, and 4) slow network communication. To address these challenges, in the
system design, we represent the model as a directed acyclic graph of operators
(OP-DAG). Each node in the DAG represents the operator in the DNNs, while the
edge represents the data dependency between operators. Based on this design, 1)
users are allowed to customize any DNN without caring low-level operator
implementation; 2) we enable the task scheduling with the more fine-grained
sub-tasks, offering more optimization space; 3) a DAG runtime executor can
implement RAD withour requiring the consistent low-level ML framework versions.
  To enhance system efficiency, we implement a workload estimator and design an
OP-Fence scheduler to cluster devices with similar bandwidths together and
partition the DAG to increase throughput. Additionally, we propose an AdaTopK
compressor to adaptively compress intermediate activations and gradients at the
slowest communication links. To evaluate the convergence and efficiency of our
system and algorithms, we train ResNet-101 and GPT-2 on three real-world
testbeds using 48 GPUs connected with 8 Mbps~10 Gbps networks. Experimental
results demonstrate that our system and method can achieve 1.45 - 9.39x speedup
compared to baseline methods while ensuring convergence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sarcasm Detection in a Less-Resourced Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lazar Đoković, Marko Robnik-Šikonja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sarcasm detection task in natural language processing tries to classify
whether an utterance is sarcastic or not. It is related to sentiment analysis
since it often inverts surface sentiment. Because sarcastic sentences are
highly dependent on context, and they are often accompanied by various
non-verbal cues, the task is challenging. Most of related work focuses on
high-resourced languages like English. To build a sarcasm detection dataset for
a less-resourced language, such as Slovenian, we leverage two modern
techniques: a machine translation specific medium-size transformer model, and a
very large generative language model. We explore the viability of translated
datasets and how the size of a pretrained transformer affects its ability to
detect sarcasm. We train ensembles of detection models and evaluate models'
performance. The results show that larger models generally outperform smaller
ones and that ensembling can slightly improve sarcasm detection performance.
Our best ensemble approach achieves an $\text{F}_1$-score of 0.765 which is
close to annotators' agreement in the source language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, published in the Slovenian Conference on Artificial
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural-based Control for CubeSat Docking Maneuvers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Stoisa, Federica Paganelli Azza, Luca Romanelli, Mattia Varile
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous Rendezvous and Docking (RVD) have been extensively studied in
recent years, addressing the stringent requirements of spacecraft dynamics
variations and the limitations of GNC systems. This paper presents an
innovative approach employing Artificial Neural Networks (ANN) trained through
Reinforcement Learning (RL) for autonomous spacecraft guidance and control
during the final phase of the rendezvous maneuver. The proposed strategy is
easily implementable onboard and offers fast adaptability and robustness to
disturbances by learning control policies from experience rather than relying
on predefined models. Extensive Monte Carlo simulations within a relevant
environment are conducted in 6DoF settings to validate our approach, along with
hardware tests that demonstrate deployment feasibility. Our findings highlight
the efficacy of RL in assuring the adaptability and efficiency of spacecraft
RVD, offering insights into future mission expectations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via
  Lightweight Value Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingqi Wang, Xiaoyuan Yi, Xing Xie, Jia Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion models trained on large-scale data have
enabled the generation of indistinguishable human-level images, yet they often
produce harmful content misaligned with human values, e.g., social bias, and
offensive content. Despite extensive research on Large Language Models (LLMs),
the challenge of Text-to-Image (T2I) model alignment remains largely
unexplored. Addressing this problem, we propose LiVO (Lightweight Value
Optimization), a novel lightweight method for aligning T2I models with human
values. LiVO only optimizes a plug-and-play value encoder to integrate a
specified value principle with the input prompt, allowing the control of
generated images over both semantics and values. Specifically, we design a
diffusion model-tailored preference optimization loss, which theoretically
approximates the Bradley-Terry model used in LLM alignment but provides a more
flexible trade-off between image quality and value conformity. To optimize the
value encoder, we also develop a framework to automatically construct a
text-image preference dataset of 86k (prompt, aligned image, violating image,
value principle) samples. Without updating most model parameters and through
adaptive value selection from the input prompt, LiVO significantly reduces
harmful outputs and achieves faster convergence, surpassing several strong
baselines and taking an initial step towards ethically aligned T2I models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Multimedia 2024. The dataset and code can be found at
  https://github.com/achernarwang/LiVO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning Approach to Brain Tumor Detection and Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alice Oh, Inyoung Noh, Jian Choo, Jihoo Lee, Justin Park, Kate Hwang, Sanghyeon Kim, Soo Min Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain tumor detection and classification are critical tasks in medical image
analysis, particularly in early-stage diagnosis, where accurate and timely
detection can significantly improve treatment outcomes. In this study, we apply
various statistical and machine learning models to detect and classify brain
tumors using brain MRI images. We explore a variety of statistical models
including linear, logistic, and Bayesian regressions, and the machine learning
models including decision tree, random forest, single-layer perceptron,
multi-layer perceptron, convolutional neural network (CNN), recurrent neural
network, and long short-term memory. Our findings show that CNN outperforms
other models, achieving the best performance. Additionally, we confirm that the
CNN model can also work for multi-class classification, distinguishing between
four categories of brain MRI images such as normal, glioma, meningioma, and
pituitary tumor images. This study demonstrates that machine learning
approaches are suitable for brain tumor detection and classification,
facilitating real-world medical applications in assisting radiologists with
early and accurate diagnosis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local transfer learning Gaussian process modeling, with applications to
  surrogate modeling of expensive computer simulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinming Wang, Simon Mak, John Miller, Jianguo Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A critical bottleneck for scientific progress is the costly nature of
computer simulations for complex systems. Surrogate models provide an appealing
solution: such models are trained on simulator evaluations, then used to
emulate and quantify uncertainty on the expensive simulator at unexplored
inputs. In many applications, one often has available data on related systems.
For example, in designing a new jet turbine, there may be existing studies on
turbines with similar configurations. A key question is how information from
such "source" systems can be transferred for effective surrogate training on
the "target" system of interest. We thus propose a new LOcal transfer Learning
Gaussian Process (LOL-GP) model, which leverages a carefully-designed Gaussian
process to transfer such information for surrogate modeling. The key novelty of
the LOL-GP is a latent regularization model, which identifies regions where
transfer should be performed and regions where it should be avoided. This
"local transfer" property is desirable in scientific systems: at certain
parameters, such systems may behave similarly and thus transfer is beneficial;
at other parameters, they may behave differently and thus transfer is
detrimental. By accounting for local transfer, the LOL-GP can rectify a
critical limitation of "negative transfer" in existing transfer learning
models, where the transfer of information worsens predictive performance. We
derive a Gibbs sampling algorithm for efficient posterior predictive sampling
on the LOL-GP, for both the multi-source and multi-fidelity transfer settings.
We then show, via a suite of numerical experiments and an application for jet
turbine design, the improved surrogate performance of the LOL-GP over existing
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A distance function for stochastic matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antony Lee, Peter Tino, Iain Bruce Styles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by information geometry, a distance function on the space of
stochastic matrices is advocated. Starting with sequences of Markov chains the
Bhattacharyya angle is advocated as the natural tool for comparing both short
and long term Markov chain runs. Bounds on the convergence of the distance and
mixing times are derived. Guided by the desire to compare different Markov
chain models, especially in the setting of healthcare processes, a new distance
function on the space of stochastic matrices is presented. It is a true
distance measure which has a closed form and is efficient to implement for
numerical evaluation. In the case of ergodic Markov chains, it is shown that
considering either the Bhattacharyya angle on Markov sequences or the new
stochastic matrix distance leads to the same distance between models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Mapping of Anatomical Landmarks from Free-Text Using Large
  Language Models: Insights from Llama-2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamad Abdi, Gerardo Hemosillo Valadez, Halid Ziya Yerebakan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anatomical landmarks are vital in medical imaging for navigation and anomaly
detection. Modern large language models (LLMs), like Llama-2, offer promise for
automating the mapping of these landmarks in free-text radiology reports to
corresponding positions in image data. Recent studies propose LLMs may develop
coherent representations of generative processes. Motivated by these insights,
we investigated whether LLMs accurately represent the spatial positions of
anatomical landmarks. Through experiments with Llama-2 models, we found that
they can linearly represent anatomical landmarks in space with considerable
robustness to different prompts. These results underscore the potential of LLMs
to enhance the efficiency and accuracy of medical imaging workflows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Neural Reparameterization for Differentiable PDE-constrained
  Optimization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Archis S. Joglekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial-differential-equation (PDE)-constrained optimization is a well-worn
technique for acquiring optimal parameters of systems governed by PDEs.
However, this approach is limited to providing a single set of optimal
parameters per optimization. Given a differentiable PDE solver, if the free
parameters are reparameterized as the output of a neural network, that neural
network can be trained to learn a map from a probability distribution to the
distribution of optimal parameters. This proves useful in the case where there
are many well performing local minima for the PDE. We apply this technique to
train a neural network that generates optimal parameters that minimize
laser-plasma instabilities relevant to laser fusion and show that the neural
network generates many well performing and diverse minima.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to D3S3: Data-driven and Differentiable Simulations,
  Surrogates, and Solvers - Workshop @ NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Multi-Task Learning for Accurate Spacecraft Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Evangelisti, Francesco Rossi, Tobia Giani, Ilaria Bloise, Mattia Varile
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate satellite pose estimation is crucial for autonomous guidance,
navigation, and control (GNC) systems in in-orbit servicing (IOS) missions.
This paper explores the impact of different tasks within a multi-task learning
(MTL) framework for satellite pose estimation using monocular images. By
integrating tasks such as direct pose estimation, keypoint prediction, object
localization, and segmentation into a single network, the study aims to
evaluate the reciprocal influence between tasks by testing different multi-task
configurations thanks to the modularity of the convolutional neural network
(CNN) used in this work. The trends of mutual bias between the analyzed tasks
are found by employing different weighting strategies to further test the
robustness of the findings. A synthetic dataset was developed to train and test
the MTL network. Results indicate that direct pose estimation and heatmap-based
pose estimation positively influence each other in general, while both the
bounding box and segmentation tasks do not provide significant contributions
and tend to degrade the overall estimation accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Optimization Algorithms for Linear Adversarial Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antônio H. RIbeiro, Thomas B. Schön, Dave Zahariah, Francis Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial training can be used to learn models that are robust against
perturbations. For linear models, it can be formulated as a convex optimization
problem. Compared to methods proposed in the context of deep learning,
leveraging the optimization structure allows significantly faster convergence
rates. Still, the use of generic convex solvers can be inefficient for
large-scale problems. Here, we propose tailored optimization algorithms for the
adversarial training of linear models, which render large-scale regression and
classification problems more tractable. For regression problems, we propose a
family of solvers based on iterative ridge regression and, for classification,
a family of solvers based on projected gradient descent. The methods are based
on extended variable reformulations of the original problem. We illustrate
their efficiency in numerical examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context Matters: Leveraging Contextual Features for Time Series
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sameep Chattopadhyay, Pulkit Paliwal, Sai Shankar Narasimhan, Shubhankar Agarwal, Sandeep P. Chinchali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series forecasts are often influenced by exogenous contextual features
in addition to their corresponding history. For example, in financial settings,
it is hard to accurately predict a stock price without considering public
sentiments and policy decisions in the form of news articles, tweets, etc.
Though this is common knowledge, the current state-of-the-art (SOTA)
forecasting models fail to incorporate such contextual information, owing to
its heterogeneity and multimodal nature. To address this, we introduce
ContextFormer, a novel plug-and-play method to surgically integrate multimodal
contextual information into existing pre-trained forecasting models.
ContextFormer effectively distills forecast-specific information from rich
multimodal contexts, including categorical, continuous, time-varying, and even
textual information, to significantly enhance the performance of existing base
forecasters. ContextFormer outperforms SOTA forecasting models by up to 30% on
a range of real-world datasets spanning energy, traffic, environmental, and
financial domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ New Paradigm of Adversarial Training: Breaking Inherent Trade-Off
  between Accuracy and Robustness via Dummy Classes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanyun Wang, Li Liu, Zi Liang, Qingqing Ye, Haibo Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial Training (AT) is one of the most effective methods to enhance the
robustness of DNNs. However, existing AT methods suffer from an inherent
trade-off between adversarial robustness and clean accuracy, which seriously
hinders their real-world deployment. While this problem has been widely studied
within the current AT paradigm, existing AT methods still typically experience
a reduction in clean accuracy by over 10% to date, without significant
improvements in robustness compared with simple baselines like PGD-AT. This
inherent trade-off raises a question: whether the current AT paradigm, which
assumes to learn the corresponding benign and adversarial samples as the same
class, inappropriately combines clean and robust objectives that may be
essentially inconsistent. In this work, we surprisingly reveal that up to 40%
of CIFAR-10 adversarial samples always fail to satisfy such an assumption
across various AT methods and robust models, explicitly indicating the
improvement room for the current AT paradigm. Accordingly, to relax the tension
between clean and robust learning derived from this overstrict assumption, we
propose a new AT paradigm by introducing an additional dummy class for each
original class, aiming to accommodate the hard adversarial samples with shifted
distribution after perturbation. The robustness w.r.t. these adversarial
samples can be achieved by runtime recovery from the predicted dummy classes to
their corresponding original ones, eliminating the compromise with clean
learning. Building on this new paradigm, we propose a novel plug-and-play AT
technology named DUmmy Classes-based Adversarial Training (DUCAT). Extensive
experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that the
DUCAT concurrently improves clean accuracy and adversarial robustness compared
with state-of-the-art benchmarks, effectively breaking the existing inherent
trade-off.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Work in progress. The code is available at
  https://github.com/FlaAI/DUCAT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explanation-Preserving Augmentation for Semi-Supervised Graph
  Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuomin Chen, Jingchao Ni, Hojat Allah Salehi, Xu Zheng, Esteban Schafir, Farhad Shirani, Dongsheng Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph representation learning (GRL), enhanced by graph augmentation methods,
has emerged as an effective technique achieving performance improvements in
wide tasks such as node classification and graph classification. In
self-supervised GRL, paired graph augmentations are generated from each graph.
Its objective is to infer similar representations for augmentations of the same
graph, but maximally distinguishable representations for augmentations of
different graphs. Analogous to image and language domains, the desiderata of an
ideal augmentation method include both (1) semantics-preservation; and (2)
data-perturbation; i.e., an augmented graph should preserve the semantics of
its original graph while carrying sufficient variance. However, most existing
(un-)/self-supervised GRL methods focus on data perturbation but largely
neglect semantics preservation. To address this challenge, in this paper, we
propose a novel method, Explanation-Preserving Augmentation (EPA), that
leverages graph explanation techniques for generating augmented graphs that can
bridge the gap between semantics-preservation and data-perturbation. EPA first
uses a small number of labels to train a graph explainer to infer the
sub-structures (explanations) that are most relevant to a graph's semantics.
These explanations are then used to generate semantics-preserving augmentations
for self-supervised GRL, namely EPA-GRL. We demonstrate theoretically, using an
analytical example, and through extensive experiments on a variety of benchmark
datasets that EPA-GRL outperforms the state-of-the-art (SOTA) GRL methods,
which are built upon semantics-agnostic data augmentations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Position Specific Scoring Is All You Need? Revisiting Protein Sequence
  Classification Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarwan Ali, Taslim Murad, Prakash Chourasia, Haris Mansoor, Imdad Ullah Khan, Pin-Yu Chen, Murray Patterson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the structural and functional characteristics of proteins are
crucial for developing preventative and curative strategies that impact fields
from drug discovery to policy development. An important and popular technique
for examining how amino acids make up these characteristics of the protein
sequences with position-specific scoring (PSS). While the string kernel is
crucial in natural language processing (NLP), it is unclear if string kernels
can extract biologically meaningful information from protein sequences, despite
the fact that they have been shown to be effective in the general sequence
analysis tasks. In this work, we propose a weighted PSS kernel matrix (or
W-PSSKM), that combines a PSS representation of protein sequences, which
encodes the frequency information of each amino acid in a sequence, with the
notion of the string kernel. This results in a novel kernel function that
outperforms many other approaches for protein sequence classification. We
perform extensive experimentation to evaluate the proposed method. Our findings
demonstrate that the W-PSSKM significantly outperforms existing baselines and
state-of-the-art methods and achieves up to 45.1\% improvement in
classification accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constrained Posterior Sampling: Time Series Generation with Hard
  Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Shankar Narasimhan, Shubhankar Agarwal, Litu Rout, Sanjay Shakkottai, Sandeep P. Chinchali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating realistic time series samples is crucial for stress-testing models
and protecting user privacy by using synthetic data. In engineering and
safety-critical applications, these samples must meet certain hard constraints
that are domain-specific or naturally imposed by physics or nature. Consider,
for example, generating electricity demand patterns with constraints on peak
demand times. This can be used to stress-test the functioning of power grids
during adverse weather conditions. Existing approaches for generating
constrained time series are either not scalable or degrade sample quality. To
address these challenges, we introduce Constrained Posterior Sampling (CPS), a
diffusion-based sampling algorithm that aims to project the posterior mean
estimate into the constraint set after each denoising update. Notably, CPS
scales to a large number of constraints (~100) without requiring additional
training. We provide theoretical justifications highlighting the impact of our
projection step on sampling. Empirically, CPS outperforms state-of-the-art
methods in sample quality and similarity to real time series by around 10% and
42%, respectively, on real-world stocks, traffic, and air quality datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimization and Application of Cloud-based Deep Learning Architecture
  for Multi-Source Data Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhang, Fa Wang, Xin Huang, Xintao Li, Sibei Liu, Hansong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study develops a cloud-based deep learning system for early prediction
of diabetes, leveraging the distributed computing capabilities of the AWS cloud
platform and deep learning technologies to achieve efficient and accurate risk
assessment. The system utilizes EC2 p3.8xlarge GPU instances to accelerate
model training, reducing training time by 93.2% while maintaining a prediction
accuracy of 94.2%. With an automated data processing and model training
pipeline built using Apache Airflow, the system can complete end-to-end updates
within 18.7 hours. In clinical applications, the system demonstrates a
prediction accuracy of 89.8%, sensitivity of 92.3%, and specificity of 95.1%.
Early interventions based on predictions lead to a 37.5% reduction in diabetes
incidence among the target population. The system's high performance and
scalability provide strong support for large-scale diabetes prevention and
management, showcasing significant public health value.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 Pages, 5 Figures, 3 Tables. The final version will be published in
  the proceedings of the IEEE conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Arbitrary QUBO Optimization: Analysis of Classical and
  Quantum-Activated Feedforward Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Tso Lai, Carsten Blank, Peter Schmelcher, Rick Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quadratic Unconstrained Binary Optimization (QUBO) sits at the heart of many
industries and academic fields such as logistics, supply chain, finance,
pharmaceutical science, chemistry, IT, and energy sectors, among others. These
problems typically involve optimizing a large number of binary variables, which
makes finding exact solutions exponentially more difficult. Consequently, most
QUBO problems are classified as NP-hard. To address this challenge, we
developed a powerful feedforward neural network (FNN) optimizer for arbitrary
QUBO problems. In this work, we demonstrate that the FNN optimizer can provide
high-quality approximate solutions for large problems, including dense
80-variable weighted MaxCut and random QUBOs, achieving an average accuracy of
over 99% in less than 1.1 seconds on an 8-core CPU. Additionally, the FNN
optimizer outperformed the Gurobi optimizer by 72% on 200-variable random QUBO
problems within a 100-second computation time limit, exhibiting strong
potential for real-time optimization tasks. Building on this model, we explored
the novel approach of integrating FNNs with a quantum annealer-based activation
function to create a quantum-classical encoder-decoder (QCED) optimizer, aiming
to further enhance the performance of FNNs in QUBO optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Exact Finite-dimensional Explicit Feature Map for Kernel Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamaledin Ghiasi-Shirazi, Mohammadreza Qaraei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kernel methods in machine learning use a kernel function that takes two data
points as input and returns their inner product after mapping them to a Hilbert
space, implicitly and without actually computing the mapping. For many kernel
functions, such as Gaussian and Laplacian kernels, the feature space is known
to be infinite-dimensional, making operations in this space possible only
implicitly. This implicit nature necessitates algorithms to be expressed using
dual representations and the kernel trick. In this paper, given an arbitrary
kernel function, we introduce an explicit, finite-dimensional feature map for
any arbitrary kernel function that ensures the inner product of data points in
the feature space equals the kernel function value, during both training and
testing. The existence of this explicit mapping allows for kernelized
algorithms to be formulated in their primal form, without the need for the
kernel trick or the dual representation. As a first application, we demonstrate
how to derive kernelized machine learning algorithms directly, without
resorting to the dual representation, and apply this method specifically to
PCA. As another application, without any changes to the t-SNE algorithm and its
implementation, we use it for visualizing the feature space of kernel
functions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Moral Values: a neuro-symbolic approach to value
  classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Lazzari, Stefano De Giorgis, Aldo Gangemi, Valentina Presutti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work explores the integration of ontology-based reasoning and Machine
Learning techniques for explainable value classification. By relying on an
ontological formalization of moral values as in the Moral Foundations Theory,
relying on the DnS Ontology Design Pattern, the \textit{sandra} neuro-symbolic
reasoner is used to infer values (fomalized as descriptions) that are
\emph{satisfied by} a certain sentence. Sentences, alongside their structured
representation, are automatically generated using an open-source Large Language
Model. The inferred descriptions are used to automatically detect the value
associated with a sentence. We show that only relying on the reasoner's
inference results in explainable classification comparable to other more
complex approaches. We show that combining the reasoner's inferences with
distributional semantics methods largely outperforms all the baselines,
including complex models based on neural network architectures. Finally, we
build a visualization tool to explore the potential of theory-based values
classification, which is publicly available at http://xmv.geomeaning.com/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ESWC24 Satellite Event</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weak-to-Strong Generalization beyond Accuracy: a Pilot Study in Safety,
  Toxicity, and Legal Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruimeng Ye, Yang Xiao, Bo Hui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) continue to advance, ensuring their alignment
with human values becomes increasingly critical. Traditional alignment methods
heavily rely on human feedback to fine-tune models. With the emergence of
superhuman models whose outputs may surpass human understanding, evaluating and
aligning these models using human judgments poses significant challenges. To
address the challenges, recent works use weak supervisors to elicit knowledge
from much stronger models. However, there are important disanalogies between
the empirical setup in the existing works and the genuine goal of alignment. We
remark that existing works investigate the phenomenon of weak-to-strong
generation in analogous setup (i.e., binary classification), rather than
practical alignment-relevant tasks (e.g., safety). In this paper, we bridge
this gap by extending weak-to-strong generation to the context of practical
alignment. We empirically demonstrate the widespread phenomenon of
weak-to-strong generation in three complicated alignment tasks: safety,
toxicity, and legal reasoning}. Furthermore, we explore efficient strategies
for improving alignment performance to enhance the quality of model outcomes.
Lastly, we summarize and analyze the challenges and potential solutions in
regard to specific alignment tasks, which we hope to catalyze the research
progress on the topic of weak-to-strong generalization. Our code is released at
https://github.com/yeruimeng/WTS.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Model Kinship for Merging Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yedi Hu, Yunzhi Yao, Ningyu Zhang, Shumin Deng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging has become one of the key technologies for enhancing the
capabilities and efficiency of Large Language Models (LLMs). However, our
understanding of the expected performance gains and principles when merging any
two models remains limited. In this work, we introduce model kinship, the
degree of similarity or relatedness between LLMs, analogous to biological
evolution. With comprehensive empirical analysis, we find that there is a
certain relationship between model kinship and the performance gains after
model merging, which can help guide our selection of candidate models. Inspired
by this, we propose a new model merging strategy: Top-k Greedy Merging with
Model Kinship, which can yield better performance on benchmark datasets.
Specifically, we discover that using model kinship as a criterion can assist us
in continuously performing model merging, alleviating the degradation (local
optima) in model evolution, whereas model kinship can serve as a guide to
escape these traps. Code is available at
https://github.com/zjunlp/ModelKinship.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Graph Foundation Models: The Perspective of Zero-shot Reasoning
  on Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Wang, Siqiang Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the success of artificial general intelligence, there is a trend
towards developing Graph Foundation Models that excel in generalization across
various graph tasks and domains. However, current models often require
extensive training or fine-tuning to capture structural and semantic insights
on new graphs, which limits their versatility. In this work, we explore graph
foundation models from the perspective of zero-shot reasoning on Knowledge
Graphs (KGs). Our focus is on utilizing KGs as a unified topological structure
to tackle diverse tasks, while addressing semantic isolation challenges in KG
reasoning to effectively integrate diverse semantic and structural features.
This brings us new methodological insights into KG reasoning, as well as high
generalizability towards foundation models in practice. Methodologically, we
introduce SCORE, a unified graph reasoning framework that effectively
generalizes diverse graph tasks using zero-shot learning. At the core of SCORE
is semantic conditional message passing, a technique designed to capture both
structural and semantic invariances in graphs, with theoretical backing for its
expressive power. Practically, we evaluate the zero-shot reasoning capability
of SCORE using 38 diverse graph datasets, covering node-level, link-level, and
graph-level tasks across multiple domains. Our experiments reveal a substantial
performance improvement over prior foundation models and supervised baselines,
highlighting the efficacy and adaptability of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 Pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Rank Adversarial PGD Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dayana Savostianova, Emanuele Zangrando, Francesco Tudisco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks on deep neural network models have seen rapid development
and are extensively used to study the stability of these networks. Among
various adversarial strategies, Projected Gradient Descent (PGD) is a widely
adopted method in computer vision due to its effectiveness and quick
implementation, making it suitable for adversarial training. In this work, we
observe that in many cases, the perturbations computed using PGD predominantly
affect only a portion of the singular value spectrum of the original image,
suggesting that these perturbations are approximately low-rank. Motivated by
this observation, we propose a variation of PGD that efficiently computes a
low-rank attack. We extensively validate our method on a range of standard
models as well as robust models that have undergone adversarial training. Our
analysis indicates that the proposed low-rank PGD can be effectively used in
adversarial training due to its straightforward and fast implementation coupled
with competitive performance. Notably, we find that low-rank PGD often performs
comparably to, and sometimes even outperforms, the traditional full-rank PGD
attack, while using significantly less memory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Supervised Learning of Disentangled Representations for
  Multivariate Time-Series <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching Chang, Chiao-Tung Chan, Wei-Yao Wang, Wen-Chih Peng, Tien-Fu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate time-series data in fields like healthcare and industry are
informative but challenging due to high dimensionality and lack of labels.
Recent self-supervised learning methods excel in learning rich representations
without labels but struggle with disentangled embeddings and inductive bias
issues like transformation-invariance. To address these challenges, we
introduce TimeDRL, a framework for multivariate time-series representation
learning with dual-level disentangled embeddings. TimeDRL features: (i)
disentangled timestamp-level and instance-level embeddings using a [CLS] token
strategy; (ii) timestamp-predictive and instance-contrastive tasks for
representation learning; and (iii) avoidance of augmentation methods to
eliminate inductive biases. Experiments on forecasting and classification
datasets show TimeDRL outperforms existing methods, with further validation in
semi-supervised settings with limited labeled data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Workshop: Self-Supervised Learning - Theory and Practice</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Bayesian Confidence (BACON) Estimator for Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick D. Kee, Max J. Brown, Jonathan C. Rice, Christian A. Howell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the Bayesian Confidence Estimator (BACON) for deep
neural networks. Current practice of interpreting Softmax values in the output
layer as probabilities of outcomes is prone to extreme predictions of class
probability. In this work we extend Waagen's method of representing the
terminal layers with a geometric model, where the probability associated with
an output vector is estimated with Bayes' Rule using validation data to provide
likelihood and normalization values. This estimator provides superior ECE and
ACE calibration error compared to Softmax for ResNet-18 at 85% network
accuracy, and EfficientNet-B0 at 95% network accuracy, on the CIFAR-10 dataset
with an imbalanced test set, except for very high accuracy edge cases. In
addition, when using the ACE metric, BACON demonstrated improved calibration
error when estimating probabilities for the imbalanced test set when using
actual class distribution fractions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 15 figures (10 of which include sub-figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henrique Donâncio, Antoine Barrier, Leah F. South, Florence Forbes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Deep Reinforcement Learning models trained using gradient-based
techniques, the choice of optimizer and its learning rate are crucial to
achieving good performance: higher learning rates can prevent the model from
learning effectively, while lower ones might slow convergence. Additionally,
due to the non-stationarity of the objective function, the best-performing
learning rate can change over the training steps. To adapt the learning rate, a
standard technique consists of using decay schedulers. However, these
schedulers assume that the model is progressively approaching convergence,
which may not always be true, leading to delayed or premature adjustments. In
this work, we propose dynamic Learning Rate for deep Reinforcement Learning
(LRRL), a meta-learning approach that selects the learning rate based on the
agent's performance during training. LRRL is based on a multi-armed bandit
algorithm, where each arm represents a different learning rate, and the bandit
feedback is provided by the cumulative returns of the RL policy to update the
arms' probability distribution. Our empirical results demonstrate that LRRL can
substantially improve the performance of deep RL algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Prediction Models for Changes in Knee Pain among Patients
  with Osteoarthritis Participating in Supervised Exercise and Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Rafiei, S. Das, M. Bakhtiari, E. M. Roos, S. T. Skou, D. T. Grønne, J. Baumbach, L. Baumbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knee osteoarthritis (OA) is a widespread chronic condition that impairs
mobility and diminishes quality of life. Despite the proven benefits of
exercise therapy and patient education in managing the OA symptoms pain and
functional limitations, these strategies are often underutilized. Personalized
outcome prediction models can help motivate and engage patients, but the
accuracy of existing models in predicting changes in knee pain remains
insufficiently examined. To validate existing models and introduce a concise
personalized model predicting changes in knee pain before to after
participating in a supervised education and exercise therapy program (GLA:D)
for knee OA patients. Our models use self-reported patient information and
functional measures. To refine the number of variables, we evaluated the
variable importance and applied clinical reasoning. We trained random forest
regression models and compared the rate of true predictions of our models with
those utilizing average values. We evaluated the performance of a full,
continuous, and concise model including all 34, all 11 continuous, and the six
most predictive variables respectively. All three models performed similarly
and were comparable to the existing model, with R-squares of 0.31-0.32 and
RMSEs of 18.65-18.85 - despite our increased sample size. Allowing a deviation
of 15 VAS points from the true change in pain, our concise model and utilizing
the average values estimated the change in pain at 58% and 51% correctly,
respectively. Our supplementary analysis led to similar outcomes. Our concise
personalized prediction model more accurately predicts changes in knee pain
following the GLA:D program compared to average pain improvement values.
Neither the increase in sample size nor the inclusion of additional variables
improved previous models. To improve predictions, new variables beyond those in
the GLA:D are required.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expand and Compress: Exploring Tuning Principles for Continual
  Spatio-Temporal Graph Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Chen, Yuxuan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread deployment of sensing devices leads to a surge in data for
spatio-temporal forecasting applications such as traffic flow, air quality, and
wind energy. Although spatio-temporal graph neural networks have achieved
success in modeling various static spatio-temporal forecasting scenarios,
real-world spatio-temporal data are typically received in a streaming manner,
and the network continuously expands with the installation of new sensors.
Thus, spatio-temporal forecasting in streaming scenarios faces dual challenges:
the inefficiency of retraining models over newly arrived data and the
detrimental effects of catastrophic forgetting over long-term history. To
address these challenges, we propose a novel prompt tuning-based continuous
forecasting method, following two fundamental tuning principles guided by
empirical and theoretical analysis: expand and compress, which effectively
resolve the aforementioned problems with lightweight tuning parameters.
Specifically, we integrate the base spatio-temporal graph neural network with a
continuous prompt pool, utilizing stored prompts (i.e., few learnable
parameters) in memory, and jointly optimize them with the base spatio-temporal
graph neural network. This method ensures that the model sequentially learns
from the spatio-temporal data stream to accomplish tasks for corresponding
periods. Extensive experimental results on multiple real-world datasets
demonstrate the multi-faceted superiority of our method over the
state-of-the-art baselines, including effectiveness, efficiency, universality,
etc.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cocoon: Robust <span class="highlight-title">Multi-Modal</span> Perception with Uncertainty-Aware Sensor
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minkyoung Cho, Yulong Cao, Jiachen Sun, Qingzhao Zhang, Marco Pavone, Jeong Joon Park, Heng Yang, Z. Morley Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An important paradigm in 3D object detection is the use of multiple
modalities to enhance accuracy in both normal and challenging conditions,
particularly for long-tail scenarios. To address this, recent studies have
explored two directions of adaptive approaches: MoE-based adaptive fusion,
which struggles with uncertainties arising from distinct object configurations,
and late fusion for output-level adaptive fusion, which relies on separate
detection pipelines and limits comprehensive understanding. In this work, we
introduce Cocoon, an object- and feature-level uncertainty-aware fusion
framework. The key innovation lies in uncertainty quantification for
heterogeneous representations, enabling fair comparison across modalities
through the introduction of a feature aligner and a learnable surrogate ground
truth, termed feature impression. We also define a training objective to ensure
that their relationship provides a valid metric for uncertainty quantification.
Cocoon consistently outperforms existing static and adaptive methods in both
normal and challenging conditions, including those with natural and artificial
corruptions. Furthermore, we show the validity and efficacy of our uncertainty
metric across diverse datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Lab to Pocket: A Novel Continual Learning-based Mobile Application
  for Screening COVID-19 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danny Falero, Muhammad Ashad Kabir, Nusrat Homaira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) has emerged as a promising tool for predicting
COVID-19 from medical images. In this paper, we propose a novel continual
learning-based approach and present the design and implementation of a mobile
application for screening COVID-19. Our approach demonstrates the ability to
adapt to evolving datasets, including data collected from different locations
or hospitals, varying virus strains, and diverse clinical presentations,
without retraining from scratch. We have evaluated state-of-the-art continual
learning methods for detecting COVID-19 from chest X-rays and selected the
best-performing model for our mobile app. We evaluated various deep learning
architectures to select the best-performing one as a foundation model for
continual learning. Both regularization and memory-based methods for continual
learning were tested, using different memory sizes to develop the optimal
continual learning model for our app. DenseNet161 emerged as the best
foundation model with 96.87\% accuracy, and Learning without Forgetting (LwF)
was the top continual learning method with an overall performance of 71.99\%.
The mobile app design considers both patient and doctor perspectives. It
incorporates the continual learning DenseNet161 LwF model on a cloud server,
enabling the model to learn from new instances of chest X-rays and their
classifications as they are submitted. The app is designed, implemented, and
evaluated to ensure it provides an efficient tool for COVID-19 screening. The
app is available to download from
https://github.com/DannyFGitHub/COVID-19PneumoCheckApp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-DenseMobileNet: A Robust Framework for Lung Nodule Classification
  using Self-ONN and Stacking-based Meta-Classifier 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Sohanur Rahman, Muhammad E. H. Chowdhury, Hasib Ryan Rahman, Mosabber Uddin Ahmed, Muhammad Ashad Kabir, Sanjiban Sekhar Roy, Rusab Sarmun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose a novel and robust framework, Self-DenseMobileNet,
designed to enhance the classification of nodules and non-nodules in chest
radiographs (CXRs). Our approach integrates advanced image standardization and
enhancement techniques to optimize the input quality, thereby improving
classification accuracy. To enhance predictive accuracy and leverage the
strengths of multiple models, the prediction probabilities from
Self-DenseMobileNet were transformed into tabular data and used to train eight
classical machine learning (ML) models; the top three performers were then
combined via a stacking algorithm, creating a robust meta-classifier that
integrates their collective insights for superior classification performance.
To enhance the interpretability of our results, we employed class activation
mapping (CAM) to visualize the decision-making process of the best-performing
model. Our proposed framework demonstrated remarkable performance on internal
validation data, achieving an accuracy of 99.28\% using a Meta-Random Forest
Classifier. When tested on an external dataset, the framework maintained strong
generalizability with an accuracy of 89.40\%. These results highlight a
significant improvement in the classification of CXRs with lung nodules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Role of Activation Functions in EEG-To-Text Decoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zenon Lamprou, Iakovos Tenedios, Yashar Moshfeghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, much interdisciplinary research has been conducted exploring
potential use cases of neuroscience to advance the field of information
retrieval. Initial research concentrated on the use of fMRI data, but fMRI was
deemed to be not suitable for real-world applications, and soon, research
shifted towards using EEG data. In this paper, we try to improve the original
performance of a first attempt at generating text using EEG by focusing on the
less explored area of optimising neural network performance. We test a set of
different activation functions and compare their performance. Our results show
that introducing a higher degree polynomial activation function can enhance
model performance without changing the model architecture. We also show that
the learnable 3rd-degree activation function performs better on the 1-gram
evaluation compared to a 3rd-degree non-learnable function. However, when
evaluating the model on 2-grams and above, the polynomial function lacks in
performance, whilst the leaky ReLU activation function outperforms the
baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One Step Diffusion via Shortcut Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Frans, Danijar Hafner, Sergey Levine, Pieter Abbeel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models and flow-matching models have enabled generating diverse and
realistic images by learning to transfer noise to data. However, sampling from
these models involves iterative denoising over many neural network passes,
making generation slow and expensive. Previous approaches for speeding up
sampling require complex training regimes, such as multiple training phases,
multiple networks, or fragile scheduling. We introduce shortcut models, a
family of generative models that use a single network and training phase to
produce high-quality samples in a single or multiple sampling steps. Shortcut
models condition the network not only on the current noise level but also on
the desired step size, allowing the model to skip ahead in the generation
process. Across a wide range of sampling step budgets, shortcut models
consistently produce higher quality samples than previous approaches, such as
consistency models and reflow. Compared to distillation, shortcut models reduce
complexity to a single network and training phase and additionally allow
varying step budgets at inference time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Sensitive Directions in GPT-2: An Improved Baseline and
  Comparative Analysis of SAEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel J. Lee, Stefan Heimersheim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sensitive directions experiments attempt to understand the computational
features of Language Models (LMs) by measuring how much the next token
prediction probabilities change by perturbing activations along specific
directions. We extend the sensitive directions work by introducing an improved
baseline for perturbation directions. We demonstrate that KL divergence for
Sparse Autoencoder (SAE) reconstruction errors are no longer pathologically
high compared to the improved baseline. We also show that feature directions
uncovered by SAEs have varying impacts on model outputs depending on the SAE's
sparsity, with lower L0 SAE feature directions exerting a greater influence.
Additionally, we find that end-to-end SAE features do not exhibit stronger
effects on model outputs compared to traditional SAEs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Utility of Memory Efficient Medical Image Generation: A Study
  on Lung Nodule Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kathrin Khadra, Utku Türkbey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scarcity of publicly available medical imaging data limits the
development of effective AI models. This work proposes a memory-efficient
patch-wise denoising diffusion probabilistic model (DDPM) for generating
synthetic medical images, focusing on CT scans with lung nodules. Our approach
generates high-utility synthetic images with nodule segmentation while
efficiently managing memory constraints, enabling the creation of training
datasets. We evaluate the method in two scenarios: training a segmentation
model exclusively on synthetic data, and augmenting real-world training data
with synthetic images. In the first case, models trained solely on synthetic
data achieve Dice scores comparable to those trained on real-world data
benchmarks. In the second case, augmenting real-world data with synthetic
images significantly improves segmentation performance. The generated images
demonstrate their potential to enhance medical image datasets in scenarios with
limited real-world data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Complex Query Answering Really Complex? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cosimo Gregucci, Bo Xiong, Daniel Hernandez, Lorenzo Loconte, Pasquale Minervini, Steffen Staab, Antonio Vergari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex query answering (CQA) on knowledge graphs (KGs) is gaining momentum
as a challenging reasoning task. In this paper, we show that the current
benchmarks for CQA are not really complex, and the way they are built distorts
our perception of progress in this field. For example, we find that in these
benchmarks, most queries (up to 98% for some query types) can be reduced to
simpler problems, e.g., link prediction, where only one link needs to be
predicted. The performance of state-of-the-art CQA models drops significantly
when such models are evaluated on queries that cannot be reduced to easier
types. Thus, we propose a set of more challenging benchmarks, composed of
queries that require models to reason over multiple hops and better reflect the
construction of real-world KGs. In a systematic empirical investigation, the
new benchmarks show that current methods leave much to be desired from current
CQA methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SiFiSinger: A High-Fidelity End-to-End Singing Voice Synthesizer based
  on Source-filter Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianwei Cui, Yu Gu, Chao Weng, Jie Zhang, Liping Chen, Lirong Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an advanced end-to-end singing voice synthesis (SVS)
system based on the source-filter mechanism that directly translates lyrical
and melodic cues into expressive and high-fidelity human-like singing.
Similarly to VISinger 2, the proposed system also utilizes training paradigms
evolved from VITS and incorporates elements like the fundamental pitch (F0)
predictor and waveform generation decoder. To address the issue that the
coupling of mel-spectrogram features with F0 information may introduce errors
during F0 prediction, we consider two strategies. Firstly, we leverage
mel-cepstrum (mcep) features to decouple the intertwined mel-spectrogram and F0
characteristics. Secondly, inspired by the neural source-filter models, we
introduce source excitation signals as the representation of F0 in the SVS
system, aiming to capture pitch nuances more accurately. Meanwhile,
differentiable mcep and F0 losses are employed as the waveform decoder
supervision to fortify the prediction accuracy of speech envelope and pitch in
the generated speech. Experiments on the Opencpop dataset demonstrate efficacy
of the proposed model in synthesis quality and intonation accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024, Synthesized audio samples are available at:
  https://sounddemos.github.io/sifisinger</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling data distribution for Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Zhao, Hanlin Gu, Lixin Fan, Qiang Yang, Yuxing Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) facilitates collaborative training of a global model
whose performance is boosted by private data owned by distributed clients,
without compromising data privacy. Yet the wide applicability of FL is hindered
by entanglement of data distributions across different clients. This paper
demonstrates for the first time that by disentangling data distributions FL can
in principle achieve efficiencies comparable to those of distributed systems,
requiring only one round of communication. To this end, we propose a novel
FedDistr algorithm, which employs stable diffusion models to decouple and
recover data distributions. Empirical results on the CIFAR100 and DomainNet
datasets show that FedDistr significantly enhances model utility and efficiency
in both disentangled and near-disentangled scenarios while ensuring privacy,
outperforming traditional federated learning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MING: A Functional Approach to Learning Molecular Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Van Khoa Nguyen, Maciej Falkiewicz, Giangiacomo Mercatali, Alexandros Kalousis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional molecule generation methods often rely on sequence or graph-based
representations, which can limit their expressive power or require complex
permutation-equivariant architectures. This paper introduces a novel paradigm
for learning molecule generative models based on functional representations.
Specifically, we propose Molecular Implicit Neural Generation (MING), a
diffusion-based model that learns molecular distributions in function space.
Unlike standard diffusion processes in data space, MING employs a novel
functional denoising probabilistic process, which jointly denoises the
information in both the function's input and output spaces by leveraging an
expectation-maximization procedure for latent implicit neural representations
of data. This approach allows for a simple yet effective model design that
accurately captures underlying function distributions. Experimental results on
molecule-related datasets demonstrate MING's superior performance and ability
to generate plausible molecular samples, surpassing state-of-the-art data-space
methods while offering a more streamlined architecture and significantly faster
generation times.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-end Planner Training for Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Cornille, Florian Mai, Jingyuan Sun, Marie-Francine Moens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Through end-to-end training to predict the next token, LLMs have become
valuable tools for various tasks. Enhancing their core training in language
modeling can improve numerous downstream applications. A successful approach to
enhance language modeling uses a separate planning module to predict abstract
labels of future sentences and conditions the LM on these predictions. However,
this method is non-differentiable, preventing joint end-to-end tuning of the
planner with the LM. We propose an effective method to improve this approach by
enabling joint fine-tuning of the planner and the LM. We show that a naive way
of approximating the gradient of selecting a label via the straight-through
estimator is not effective. Instead, we propose to use the predicted label
probabilities as mixing weights to condition the LM on a weighted average of
label embeddings in a differentiable manner. This not only enables joint
fine-tuning of the planner and the LM, but also allows the LM to draw on the
full label distribution predicted by the planner, retaining more information.
Our experimental results show consistent improvements in perplexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Driven Gyroscope Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeev Yampolsky, Itzik Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gyroscopes are inertial sensors that measure the angular velocity of the
platforms to which they are attached. To estimate the gyroscope deterministic
error terms prior mission start, a calibration procedure is performed. When
considering low-cost gyroscopes, the calibration requires a turntable as the
gyros are incapable of sensing the Earth turn rate. In this paper, we propose a
data-driven framework to estimate the scale factor and bias of a gyroscope. To
train and validate our approach, a dataset of 56 minutes was recorded using a
turntable. We demonstrated that our proposed approach outperforms the
model-based approach, in terms of accuracy and convergence time. Specifically,
we improved the scale factor and bias estimation by an average of 72% during
six seconds of calibration time, demonstrating an average of 75% calibration
time improvement. That is, instead of minutes, our approach requires only
several seconds for the calibration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 Pages, 5 Figures, 3 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and
  Hindsight Relabeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Loris Gaven, Clement Romac, Thomas Carta, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The past years have seen Large Language Models (LLMs) strive not only as
generative models but also as agents solving textual sequential decision-making
tasks. When facing complex environments where their zero-shot abilities are
insufficient, recent work showed online Reinforcement Learning (RL) could be
used for the LLM agent to discover and learn efficient strategies
interactively. However, most prior work sticks to on-policy algorithms, which
greatly reduces the scope of methods such agents could use for both exploration
and exploitation, such as experience replay and hindsight relabeling. Yet, such
methods may be key for LLM learning agents, and in particular when designing
autonomous intrinsically motivated agents sampling and pursuing their own goals
(i.e. autotelic agents). This paper presents and studies an adaptation of Soft
Actor-Critic and hindsight relabeling to LLM agents. Our method not only paves
the path towards autotelic LLM agents that learn online but can also outperform
on-policy methods in more classic multi-goal RL environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KcMF: A Knowledge-compliant Framework for Schema and Entity Matching
  with <span class="highlight-title">Fine-tuning</span>-free LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqin Xu, Huan Li, Ke Chen, Lidan Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Schema and entity matching tasks are crucial for data integration and
management. While large language models (LLMs) have shown promising results in
these tasks, they suffer from hallucinations and confusion about task
instructions. In this paper, we present the Knowledge-Compliant Matching
Framework (KcMF), an LLM-based approach that addresses these issues without the
need for domain-specific fine-tuning. KcMF employs a pseudo-code-based task
decomposition strategy to adopt task-specific natural language statements that
guide LLM reasoning and reduce confusion. We also propose two mechanisms,
Dataset as Knowledge (DaK) and Example as Knowledge (EaK), to build domain
knowledge sets when unstructured domain knowledge is lacking. Additionally, we
introduce a result-ensembling strategy to leverage multiple knowledge sources
and suppress poorly formatted outputs. Comprehensive evaluations on schema and
entity matching tasks demonstrate that KcMF outperforms previous non-LLM
state-of-the-art (SOTA) methods by an average F1 score of 22.9% and competes
effectively with SOTA fine-tuned LLMs. Moreover, KcMF generalizes well across
different LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zerui Xu, Fang Wu, Tianfan Fu, Yue Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) exhibits promise in the clinical domain. However, it is
constrained by data scarcity and ethical considerations, as the generation of
clinical trials presents significant challenges due to stringent privacy
regulations, high costs, and the extended duration required for conducting
studies with human participants. Despite the advancements of large language
models (LLMs) in general generation tasks, their potential in facilitating the
generation of synthetic clinical trials is under-explored. To address this gap,
we introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs
to generate artificial yet realistic and diverse clinical trials with binary
success/failure labels. Experiments conducted on real clinical trials from the
\url{ClinicalTrials.gov} database demonstrate that our synthetic data can
effectively augment real datasets. Furthermore, by fine-tuning a pre-trained
model as a binary classifier on synthetic clinical trial datasets, we
demonstrate that this augmentation enhances model training for downstream tasks
such as trial outcome prediction. Our findings suggest that LLMs for synthetic
clinical trial generation hold promise for accelerating clinical research and
upholding ethical standards for patient privacy. The code is publicly available
at
https://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mind the Gap Between Prototypes and Images in Cross-domain Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongduan Tian, Feng Liu, Zhanke Zhou, Tongliang Liu, Chengqi Zhang, Bo Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In cross-domain few-shot classification (CFC), recent works mainly focus on
adapting a simple transformation head on top of a frozen pre-trained backbone
with few labeled data to project embeddings into a task-specific metric space
where classification can be performed by measuring similarities between image
instance and prototype representations. Technically, an assumption implicitly
adopted in such a framework is that the prototype and image instance embeddings
share the same representation transformation. However, in this paper, we find
that there naturally exists a gap, which resembles the modality gap, between
the prototype and image instance embeddings extracted from the frozen
pre-trained backbone, and simply applying the same transformation during the
adaptation phase constrains exploring the optimal representations and shrinks
the gap between prototype and image representations. To solve this problem, we
propose a simple yet effective method, contrastive prototype-image adaptation
(CoPA), to adapt different transformations respectively for prototypes and
images similarly to CLIP by treating prototypes as text prompts. Extensive
experiments on Meta-Dataset demonstrate that CoPA achieves the state-of-the-art
performance more efficiently. Meanwhile, further analyses also indicate that
CoPA can learn better representation clusters, enlarge the gap, and achieve
minimal validation loss at the enlarged gap.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Challenges, Methods, Data -- a Survey of Machine Learning in Water
  Distribution Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valerie Vaquet, Fabian Hinder, André Artelt, Inaam Ashraf, Janine Strotherm, Jonas Vaquet, Johannes Brinkrolf, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on methods for planning and controlling water distribution networks
gains increasing relevance as the availability of drinking water will decrease
as a consequence of climate change. So far, the majority of approaches is based
on hydraulics and engineering expertise. However, with the increasing
availability of sensors, machine learning techniques constitute a promising
tool. This work presents the main tasks in water distribution networks,
discusses how they relate to machine learning and analyses how the
particularities of the domain pose challenges to and can be leveraged by
machine learning approaches. Besides, it provides a technical toolkit by
presenting evaluation benchmarks and a structured survey of the exemplary task
of leakage detection and localization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not undergone any post-submission improvements or
  corrections. The Version of Record of this contribution is published in
  Artificial Neural Networks and Machine Learning -- ICANN 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HELM: Hierarchical Encoding for mRNA Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Yazdani-Jahromi, Mangal Prakash, Tommaso Mansi, Artem Moskalev, Rui Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Messenger RNA (mRNA) plays a crucial role in protein synthesis, with its
codon structure directly impacting biological properties. While Language Models
(LMs) have shown promise in analyzing biological sequences, existing approaches
fail to account for the hierarchical nature of mRNA's codon structure. We
introduce Hierarchical Encoding for mRNA Language Modeling (HELM), a novel
pre-training strategy that incorporates codon-level hierarchical structure into
language model training. HELM modulates the loss function based on codon
synonymity, aligning the model's learning process with the biological reality
of mRNA sequences. We evaluate HELM on diverse mRNA datasets and tasks,
demonstrating that HELM outperforms standard language model pre-training as
well as existing foundation model baselines on six diverse downstream property
prediction tasks and an antibody region annotation tasks on average by around
8\%. Additionally, HELM enhances the generative capabilities of language model,
producing diverse mRNA sequences that better align with the underlying true
data distribution compared to non-hierarchical baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sharpness-Aware Black-Box Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feiyang Ye, Yueming Lyu, Xuehao Wang, Masashi Sugiyama, Yu Zhang, Ivor Tsang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Black-box optimization algorithms have been widely used in various machine
learning problems, including reinforcement learning and prompt fine-tuning.
However, directly optimizing the training loss value, as commonly done in
existing black-box optimization methods, could lead to suboptimal model quality
and generalization performance. To address those problems in black-box
optimization, we propose a novel Sharpness-Aware Black-box Optimization (SABO)
algorithm, which applies a sharpness-aware minimization strategy to improve the
model generalization. Specifically, the proposed SABO method first
reparameterizes the objective function by its expectation over a Gaussian
distribution. Then it iteratively updates the parameterized distribution by
approximated stochastic gradients of the maximum objective value within a small
neighborhood around the current solution in the Gaussian distribution space.
Theoretically, we prove the convergence rate and generalization bound of the
proposed SABO algorithm. Empirically, extensive experiments on the black-box
prompt fine-tuning tasks demonstrate the effectiveness of the proposed SABO
method in improving model generalization performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Neural Samplers with Reverse Diffusive KL Divergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun He, Wenlin Chen, Mingtian Zhang, David Barber, José Miguel Hernández-Lobato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training generative models to sample from unnormalized density functions is
an important and challenging task in machine learning. Traditional training
methods often rely on the reverse Kullback-Leibler (KL) divergence due to its
tractability. However, the mode-seeking behavior of reverse KL hinders
effective approximation of multi-modal target distributions. To address this,
we propose to minimize the reverse KL along diffusion trajectories of both
model and target densities. We refer to this objective as the reverse diffusive
KL divergence, which allows the model to capture multiple modes. Leveraging
this objective, we train neural samplers that can efficiently generate samples
from the target distribution in one step. We demonstrate that our method
enhances sampling performance across various Boltzmann distributions, including
both synthetic multi-modal densities and n-body particle systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 6 figures, 3 tables, 1 algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Loss Landscape Characterization of Neural Networks without
  Over-Parametrziation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rustem Islamov, Niccolò Ajroldi, Antonio Orvieto, Aurelien Lucchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimization methods play a crucial role in modern machine learning, powering
the remarkable empirical achievements of deep learning models. These successes
are even more remarkable given the complex non-convex nature of the loss
landscape of these models. Yet, ensuring the convergence of optimization
methods requires specific structural conditions on the objective function that
are rarely satisfied in practice. One prominent example is the widely
recognized Polyak-Lojasiewicz (PL) inequality, which has gained considerable
attention in recent years. However, validating such assumptions for deep neural
networks entails substantial and often impractical levels of
over-parametrization. In order to address this limitation, we propose a novel
class of functions that can characterize the loss landscape of modern deep
models without requiring extensive over-parametrization and can also include
saddle points. Crucially, we prove that gradient-based optimizers possess
theoretical guarantees of convergence under this assumption. Finally, we
validate the soundness of our new function class through both theoretical
analysis and empirical experimentation across a diverse range of deep learning
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FairGLVQ: Fairness in Partition-Based Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Störck, Fabian Hinder, Johannes Brinkrolf, Benjamin Paassen, Valerie Vaquet, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness is an important objective throughout society. From the distribution
of limited goods such as education, over hiring and payment, to taxes,
legislation, and jurisprudence. Due to the increasing importance of machine
learning approaches in all areas of daily life including those related to
health, security, and equity, an increasing amount of research focuses on fair
machine learning. In this work, we focus on the fairness of partition- and
prototype-based models. The contribution of this work is twofold: 1) we develop
a general framework for fair machine learning of partition-based models that
does not depend on a specific fairness definition, and 2) we derive a fair
version of learning vector quantization (LVQ) as a specific instantiation. We
compare the resulting algorithm against other algorithms from the literature on
theoretical and real-world data showing its practical relevance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not undergone any post-submission improvements or
  corrections. The Version of Record of this contribution is published in
  Advances in Self-Organizing Maps, Learning Vector Quantization, Interpretable
  Machine Learning, and Beyond</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reconstruction of Differentially Private Text Sanitization via Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuchao Pang, Zhigang Lu, Haichen Wang, Peng Fu, Yongbin Zhou, Minhui Xue, Bo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differential privacy (DP) is the de facto privacy standard against privacy
leakage attacks, including many recently discovered ones against large language
models (LLMs). However, we discovered that LLMs could reconstruct the
altered/removed privacy from given DP-sanitized prompts. We propose two attacks
(black-box and white-box) based on the accessibility to LLMs and show that LLMs
could connect the pair of DP-sanitized text and the corresponding private
training data of LLMs by giving sample text pairs as instructions (in the
black-box attacks) or fine-tuning data (in the white-box attacks). To
illustrate our findings, we conduct comprehensive experiments on modern LLMs
(e.g., LLaMA-2, LLaMA-3, ChatGPT-3.5, ChatGPT-4, ChatGPT-4o, Claude-3,
Claude-3.5, OPT, GPT-Neo, GPT-J, Gemma-2, and Pythia) using commonly used
datasets (such as WikiMIA, Pile-CC, and Pile-Wiki) against both word-level and
sentence-level DP. The experimental results show promising recovery rates,
e.g., the black-box attacks against the word-level DP over WikiMIA dataset gave
72.18% on LLaMA-2 (70B), 82.39% on LLaMA-3 (70B), 75.35% on Gemma-2, 91.2% on
ChatGPT-4o, and 94.01% on Claude-3.5 (Sonnet). More urgently, this study
indicates that these well-known LLMs have emerged as a new security risk for
existing DP text sanitization approaches in the current environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConLUX: Concept-Based Local Unified Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Liu, Haonan Yu, Xin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancements of various machine learning models, there is a
significant demand for model-agnostic explanation techniques, which can explain
these models across different architectures. Mainstream model-agnostic
explanation techniques generate local explanations based on basic features
(e.g., words for text models and (super-)pixels for image models). However,
these explanations often do not align with the decision-making processes of the
target models and end-users, resulting in explanations that are unfaithful and
difficult for users to understand. On the other hand, concept-based techniques
provide explanations based on high-level features (e.g., topics for text models
and objects for image models), but most are model-specific or require
additional pre-defined external concept knowledge. To address this limitation,
we propose \toolname, a general framework to provide concept-based local
explanations for any machine learning models. Our key insight is that we can
automatically extract high-level concepts from large pre-trained models, and
uniformly extend existing local model-agnostic techniques to provide unified
concept-based explanations. We have instantiated \toolname on four different
types of explanation techniques: LIME, Kernel SHAP, Anchor, and LORE, and
applied these techniques to text and image models. Our evaluation results
demonstrate that 1) compared to the vanilla versions, \toolname offers more
faithful explanations and makes them more understandable to users, and 2) by
offering multiple forms of explanations, \toolname outperforms state-of-the-art
concept-based explanation techniques specifically designed for text and image
models, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approaching Metaheuristic Deep Learning Combos for Automated Data Mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustavo Assunção, Paulo Menezes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lack of data on which to perform experimentation is a recurring issue in many
areas of research, particularly in machine learning. The inability of most
automated data mining techniques to be generalized to all types of data is
inherently related with their dependency on those types which deems them
ineffective against anything slightly different. Meta-heuristics are algorithms
which attempt to optimize some solution independently of the type of data used,
whilst classifiers or neural networks focus on feature extrapolation and
dimensionality reduction to fit some model onto data arranged in a particular
way. These two algorithmic fields encompass a group of characteristics which
when combined are seemingly capable of achieving data mining regardless of how
it is arranged. To this end, this work proposes a means of combining
meta-heuristic methods with conventional classifiers and neural networks in
order to perform automated data mining. Experiments on the MNIST dataset for
handwritten digit recognition were performed and it was empirically observed
that using a ground truth labeled dataset's validation accuracy is inadequate
for correcting labels of other previously unseen data instances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tentative submission for data mining and knowledge discovery</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perseus: Leveraging Common Data Patterns with Curriculum Learning for
  More Robust Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiwen Xia, Huijun Wu, Duanyu Li, Min Xie, Ruibo Wang, Wenzhe Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) excel at handling graph data but remain
vulnerable to adversarial attacks. Existing defense methods typically rely on
assumptions like graph sparsity and homophily to either preprocess the graph or
guide structure learning. However, preprocessing methods often struggle to
accurately distinguish between normal edges and adversarial perturbations,
leading to suboptimal results due to the loss of valuable edge information.
Robust graph neural network models train directly on graph data affected by
adversarial perturbations, without preprocessing. This can cause the model to
get stuck in poor local optima, negatively affecting its performance. To
address these challenges, we propose Perseus, a novel adversarial defense
method based on curriculum learning. Perseus assesses edge difficulty using
global homophily and applies a curriculum learning strategy to adjust the
learning order, guiding the model to learn the full graph structure while
adaptively focusing on common data patterns. This approach mitigates the impact
of adversarial perturbations. Experiments show that models trained with Perseus
achieve superior performance and are significantly more robust to adversarial
attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonlinear bayesian tomography of ion temperature and velocity for
  Doppler coherence imaging spectroscopy in RT-1 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenji Ueda, Masaki. Nishiura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel Bayesian tomography approach for Coherence Imaging
Spectroscopy (CIS) that simultaneously reconstructs ion temperature and
velocity distributions in plasmas. Utilizing nonlinear Gaussian Process
Tomography (GPT) with the Laplace approximation, we model prior distributions
of log-emissivity, temperature, and velocity as Gaussian processes. This
framework rigorously incorporates nonlinear effects and temperature
dependencies often neglected in conventional CIS tomography, enabling robust
reconstruction even in the region of high temperature and velocity. By applying
a log-Gaussian process, we also address issues like velocity divergence in
low-emissivity regions. Validated with phantom simulations and experimental
data from the RT-1 device, our method reveals detailed spatial structures of
ion temperature and toroidal ion flow characteristic of magnetospheric plasma.
This work significantly broadens the scope of CIS tomography, offering a robust
tool for plasma diagnostics and facilitating integration with complementary
measurement techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 page, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tracking Universal Features Through <span class="highlight-title">Fine-Tuning</span> and Model Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niels Horn, Desmond Elliott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study how features emerge, disappear, and persist across models fine-tuned
on different domains of text. More specifically, we start from a base one-layer
Transformer language model that is trained on a combination of the BabyLM
corpus, and a collection of Python code from The Stack. This base model is
adapted to two new domains of text: TinyStories, and the Lua programming
language, respectively; and then these two models are merged using these two
models using spherical linear interpolation. Our exploration aims to provide
deeper insights into the stability and transformation of features across
typical transfer-learning scenarios using small-scale models and sparse
auto-encoders.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive and Stratified Subsampling Techniques for High Dimensional
  Non-Standard Data Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prateek Mittal, Jai Dalmotra, Joohi Chauhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of estimating high-dimensional parameters
in non-standard data environments, where traditional methods often falter due
to issues such as heavy-tailed distributions, data contamination, and dependent
observations. We propose robust subsampling techniques, specifically Adaptive
Importance Sampling (AIS) and Stratified Subsampling, designed to enhance the
reliability and efficiency of parameter estimation. Under some clearly outlined
conditions, we establish consistency and asymptotic normality for the proposed
estimators, providing non-asymptotic error bounds that quantify their
performance. Our theoretical foundations are complemented by controlled
experiments demonstrating the superiority of our methods over conventional
approaches. By bridging the gap between theory and practice, this work offers
significant contributions to robust statistical estimation, paving the way for
advancements in various applied domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Neural Scaling Laws for Time Series Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingren Yao, Chao-Han Huck Yang, Renhe Jiang, Yuxuan Liang, Ming Jin, Shirui Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling laws offer valuable insights into the design of time series
foundation models (TSFMs). However, previous research has largely focused on
the scaling laws of TSFMs for in-distribution (ID) data, leaving their
out-of-distribution (OOD) scaling behavior and the influence of model
architectures less explored. In this work, we examine two common TSFM
architectures, encoder-only and decoder-only Transformers, and investigate
their scaling behavior on both ID and OOD data. These models are trained and
evaluated across varying parameter counts, compute budgets, and dataset sizes.
Our experiments reveal that the log-likelihood loss of TSFMs exhibits similar
scaling behavior in both OOD and ID settings. We further compare the scaling
properties across different architectures, incorporating two state-of-the-art
TSFMs as case studies, showing that model architecture plays a significant role
in scaling. The encoder-only Transformers demonstrate better scalability than
the decoder-only Transformers, while the architectural enhancements in the two
advanced TSFMs primarily improve ID performance but reduce OOD scalability.
While scaling up TSFMs is expected to drive performance breakthroughs, the lack
of a comprehensive understanding of TSFM scaling laws has hindered the
development of a robust framework to guide model scaling. We fill this gap in
this work by synthesizing our findings and providing practical guidelines for
designing and scaling larger TSFMs with enhanced model capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Temporal Graph Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Liu, Zihao Zhou, Xianghong Xu, Qian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal graph clustering is a complex task that involves discovering
meaningful structures in dynamic graphs where relationships and entities change
over time. Existing methods typically require centralized data collection,
which poses significant privacy and communication challenges. In this work, we
introduce a novel Federated Temporal Graph Clustering (FTGC) framework that
enables decentralized training of graph neural networks (GNNs) across multiple
clients, ensuring data privacy throughout the process. Our approach
incorporates a temporal aggregation mechanism to effectively capture the
evolution of graph structures over time and a federated optimization strategy
to collaboratively learn high-quality clustering representations. By preserving
data privacy and reducing communication overhead, our framework achieves
competitive performance on temporal graph datasets, making it a promising
solution for privacy-sensitive, real-world applications involving dynamic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAX: Masked Autoencoder for X-ray Fluorescence in Geological
  Investigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        An-Sheng Lee, Yu-Wen Pao, Hsuan-Tien Lin, Sofia Ya Hsuan Liou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training foundation models has become the de-facto procedure for deep
learning approaches, yet its application remains limited in the geological
studies, where in needs of the model transferability to break the shackle of
data scarcity. Here we target on the X-ray fluorescence (XRF) scanning data, a
standard high-resolution measurement in extensive scientific drilling projects.
We propose a scalable self-supervised learner, masked autoencoders on XRF
spectra (MAX), to pre-train a foundation model covering geological records from
multiple regions of the Pacific and Southern Ocean. In pre-training, we find
that masking a high proportion of the input spectrum (50\%) yields a nontrivial
and meaningful self-supervisory task. For downstream tasks, we select the
quantification of XRF spectra into two costly geochemical measurements,
CaCO$_3$ and total organic carbon, due to their importance in understanding the
paleo-oceanic carbon system. Our results show that MAX, requiring only
one-third of the data, outperforms models without pre-training in terms of
quantification accuracy. Additionally, the model's generalizability improves by
more than 60\% in zero-shot tests on new materials, with explainability further
ensuring its robustness. Thus, our approach offers a promising pathway to
overcome data scarcity in geological discovery by leveraging the
self-supervised foundation model and fast-acquired XRF scanning data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Anomaly Detection through Conditional Latent Space VAE
  Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oskar Åström, Alexandros Sopasakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel Conditional Latent space Variational Autoencoder (CL-VAE)
to perform improved pre-processing for anomaly detection on data with known
inlier classes and unknown outlier classes. This proposed variational
autoencoder (VAE) improves latent space separation by conditioning on
information within the data. The method fits a unique prior distribution to
each class in the dataset, effectively expanding the classic prior distribution
for VAEs to include a Gaussian mixture model. An ensemble of these VAEs are
merged in the latent spaces to form a group consensus that greatly improves the
accuracy of anomaly detection across data sets. Our approach is compared
against the capabilities of a typical VAE, a CNN, and a PCA, with regards AUC
for anomaly detection. The proposed model shows increased accuracy in anomaly
detection, achieving an AUC of 97.4% on the MNIST dataset compared to 95.7% for
the second best model. In addition, the CL-VAE shows increased benefits from
ensembling, a more interpretable latent space, and an increased ability to
learn patterns in complex data with limited model sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages of main article, 19 pages including references and appendix,
  4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisited Large Language Model for Time Series Analysis through Modality
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangwei Nathan Zheng, Chang George Dong, Wei Emma Zhang, Lin Yue, Miao Xu, Olaf Maennel, Weitong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have demonstrated impressive performance in many
pivotal web applications such as sensor data analysis. However, since LLMs are
not designed for time series tasks, simpler models like linear regressions can
often achieve comparable performance with far less complexity. In this study,
we perform extensive experiments to assess the effectiveness of applying LLMs
to key time series tasks, including forecasting, classification, imputation,
and anomaly detection. We compare the performance of LLMs against simpler
baseline models, such as single-layer linear models and randomly initialized
LLMs. Our results reveal that LLMs offer minimal advantages for these core time
series tasks and may even distort the temporal structure of the data. In
contrast, simpler models consistently outperform LLMs while requiring far fewer
parameters. Furthermore, we analyze existing reprogramming techniques and show,
through data manifold analysis, that these methods fail to effectively align
time series data with language and display pseudo-alignment behaviour in
embedding space. Our findings suggest that the performance of LLM-based methods
in time series tasks arises from the intrinsic characteristics and structure of
time series data, rather than any meaningful alignment with the language model
architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TPFL: A Trustworthy Personalized Federated Learning Framework via
  Subjective Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinqian Chen, Jihua Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) enables collaborative model training across
distributed clients while preserving data privacy. Despite its widespread
adoption, most FL approaches focusing solely on privacy protection fall short
in scenarios where trustworthiness is crucial, necessitating advancements in
secure training, dependable decision-making mechanisms, robustness on
corruptions, and enhanced performance with Non-IID data. To bridge this gap, we
introduce Trustworthy Personalized Federated Learning (TPFL) framework designed
for classification tasks via subjective logic in this paper. Specifically, TPFL
adopts a unique approach by employing subjective logic to construct federated
models, providing probabilistic decisions coupled with an assessment of
uncertainty rather than mere probability assignments. By incorporating a
trainable heterogeneity prior to the local training phase, TPFL effectively
mitigates the adverse effects of data heterogeneity. Model uncertainty and
instance uncertainty are further utilized to ensure the safety and reliability
of the training and inference stages. Through extensive experiments on widely
recognized federated learning benchmarks, we demonstrate that TPFL not only
achieves competitive performance compared with advanced methods but also
exhibits resilience against prevalent malicious attacks, robustness on domain
shifts, and reliability in high-stake scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 Pages with Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAT: Improving Adversarial Robustness via Generative Amplitude Mix-up in
  Frequency Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengpeng Li, Kemou Li, Haiwei Wu, Jinyu Tian, Jiantao Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To protect deep neural networks (DNNs) from adversarial attacks, adversarial
training (AT) is developed by incorporating adversarial examples (AEs) into
model training. Recent studies show that adversarial attacks disproportionately
impact the patterns within the phase of the sample's frequency spectrum --
typically containing crucial semantic information -- more than those in the
amplitude, resulting in the model's erroneous categorization of AEs. We find
that, by mixing the amplitude of training samples' frequency spectrum with
those of distractor images for AT, the model can be guided to focus on phase
patterns unaffected by adversarial perturbations. As a result, the model's
robustness can be improved. Unfortunately, it is still challenging to select
appropriate distractor images, which should mix the amplitude without affecting
the phase patterns. To this end, in this paper, we propose an optimized
Adversarial Amplitude Generator (AAG) to achieve a better tradeoff between
improving the model's robustness and retaining phase patterns. Based on this
generator, together with an efficient AE production procedure, we design a new
Dual Adversarial Training (DAT) strategy. Experiments on various datasets show
that our proposed DAT leads to significantly improved robustness against
diverse adversarial attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous Pupillography: A Case for Visual Health Ecosystem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usama Younus, Nirupam Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article aims to cover pupillography, and its potential use in a number
of ophthalmological diagnostic applications in biomedical space. With the
ever-increasing incorporation of technology within our daily lives and an
ever-growing active research into smart devices and technologies, we try to
make a case for a health ecosystem that revolves around continuous eye
monitoring. We tend to summarize the design constraints & requirements for an
IoT-based continuous pupil detection system, with an attempt at developing a
pipeline for wearable pupillographic device, while comparing two compact
mini-camera modules currently available in the market. We use a light algorithm
that can be directly adopted to current micro-controllers, and share our
results for different lighting conditions, and scenarios. Lastly, we present
our findings, along with an analysis on the challenges faced and a way ahead
towards successfully building this ecosystem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two Birds with One Stone: Multi-Task Semantic Communications Systems
  over Relay Channel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujie Cao, Tong Wu, Zhiyong Chen, Yin Xu, Meixia Tao, Wenjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel multi-task, multi-link relay semantic
communications (MTML-RSC) scheme that enables the destination node to
simultaneously perform image reconstruction and classification with one
transmission from the source node. In the MTML-RSC scheme, the source node
broadcasts a signal using semantic communications, and the relay node forwards
the signal to the destination. We analyze the coupling relationship between the
two tasks and the two links (source-to-relay and source-to-destination) and
design a semantic-focused forward method for the relay node, where it
selectively forwards only the semantics of the relevant class while ignoring
others. At the destination, the node combines signals from both the source node
and the relay node to perform classification, and then uses the classification
result to assist in decoding the signal from the relay node for image
reconstructing. Experimental results demonstrate that the proposed MTML-RSC
scheme achieves significant performance gains, e.g., $1.73$ dB improvement in
peak-signal-to-noise ratio (PSNR) for image reconstruction and increasing the
accuracy from $64.89\%$ to $70.31\%$ for classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE WCNC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conjunction Subspaces Test for Conformal and Selective Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zengyou He, Zerun Li, Junjie Dong, Xinying Liu, Mudi Jiang, Lianyu Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a new classifier, which integrates significance
testing results over different random subspaces to yield consensus p-values for
quantifying the uncertainty of classification decision. The null hypothesis is
that the test sample has no association with the target class on a randomly
chosen subspace, and hence the classification problem can be formulated as a
problem of testing for the conjunction of hypotheses. The proposed classifier
can be easily deployed for the purpose of conformal prediction and selective
classification with reject and refine options by simply thresholding the
consensus p-values. The theoretical analysis on the generalization error bound
of the proposed classifier is provided and empirical studies on real data sets
are conducted as well to demonstrate its effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consistency Calibration: Improving Uncertainty Calibration via
  Consistency among Perturbed Neighbors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linwei Tao, Haolan Guo, Minjing Dong, Chang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Calibration is crucial in deep learning applications, especially in fields
like healthcare and autonomous driving, where accurate confidence estimates are
vital for decision-making. However, deep neural networks often suffer from
miscalibration, with reliability diagrams and Expected Calibration Error (ECE)
being the only standard perspective for evaluating calibration performance. In
this paper, we introduce the concept of consistency as an alternative
perspective on model calibration, inspired by uncertainty estimation literature
in large language models (LLMs). We highlight its advantages over the
traditional reliability-based view. Building on this concept, we propose a
post-hoc calibration method called Consistency Calibration (CC), which adjusts
confidence based on the model's consistency across perturbed inputs. CC is
particularly effective in locally uncertainty estimation, as it requires no
additional data samples or label information, instead generating input
perturbations directly from the source data. Moreover, we show that performing
perturbations at the logit level significantly improves computational
efficiency. We validate the effectiveness of CC through extensive comparisons
with various post-hoc and training-time calibration methods, demonstrating
state-of-the-art performance on standard datasets such as CIFAR-10, CIFAR-100,
and ImageNet, as well as on long-tailed datasets like ImageNet-LT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Scalable Exact Machine Unlearning Using Parameter-Efficient
  <span class="highlight-title">Fine-Tuning</span> <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16257v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16257v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somnath Basu Roy Chowdhury, Krzysztof Choromanski, Arijit Sehanobish, Avinava Dubey, Snigdha Chaturvedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning is the process of efficiently removing the influence of a
training data instance from a trained machine learning model without retraining
it from scratch. A popular subclass of unlearning approaches is exact machine
unlearning, which focuses on techniques that explicitly guarantee the removal
of the influence of a data instance from a model. Exact unlearning approaches
use a machine learning model in which individual components are trained on
disjoint subsets of the data. During deletion, exact unlearning approaches only
retrain the affected components rather than the entire model. While existing
approaches reduce retraining costs, it can still be expensive for an
organization to retrain a model component as it requires halting a system in
production, which leads to service failure and adversely impacts customers. To
address these challenges, we introduce an exact unlearning framework --
Sequence-aware Sharded Sliced Training (S3T), which is designed to enhance the
deletion capabilities of an exact unlearning system while minimizing the impact
on model's performance. At the core of S3T, we utilize a lightweight
parameter-efficient fine-tuning approach that enables parameter isolation by
sequentially training layers with disjoint data slices. This enables efficient
unlearning by simply deactivating the layers affected by data deletion.
Furthermore, to reduce the retraining cost and improve model performance, we
train the model on multiple data sequences, which allows S3T to handle an
increased number of deletion requests. Both theoretically and empirically, we
demonstrate that S3T attains superior deletion capabilities and enhanced
performance compared to baselines across a wide range of settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preliminary version accepted at the SafeGenAi Workshop, NeurIPS, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Algorithmic Reasoning with Multiple Correct Solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeno Kujawa, John Poole, Dobrik Georgiev, Danilo Numeroso, Pietro Liò
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Algorithmic Reasoning (NAR) aims to optimize classical algorithms.
However, canonical implementations of NAR train neural networks to return only
a single solution, even when there are multiple correct solutions to a problem,
such as single-source shortest paths. For some applications, it is desirable to
recover more than one correct solution. To that end, we give the first method
for NAR with multiple solutions. We demonstrate our method on two classical
algorithms: Bellman-Ford (BF) and Depth-First Search (DFS), favouring deeper
insight into two algorithms over a broader survey of algorithms. This method
involves generating appropriate training data as well as sampling and
validating solutions from model output. Each step of our method, which can
serve as a framework for neural algorithmic reasoning beyond the tasks
presented in this paper, might be of independent interest to the field and our
results represent the first attempt at this task in the NAR literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ cedar: Optimized and Unified Machine Learning Input Data Pipelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08895v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08895v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Zhao, Emanuel Adamiak, Christos Kozyrakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The input data pipeline is an essential component of each machine learning
(ML) training job. It is responsible for reading massive amounts of training
data, processing batches of samples using complex transformations, and loading
them onto training nodes at low latency and high throughput. Performant input
data systems are becoming increasingly critical, driven by skyrocketing data
volumes and training throughput demands. Unfortunately, current input data
systems cannot fully leverage key performance optimizations, resulting in
hugely inefficient infrastructures that require significant resources - or
worse - underutilize expensive accelerators.
  To address these demands, we present cedar, an optimized and unified
programming framework for ML input data pipelines. cedar allows users to define
input data pipelines using composable operators that support arbitrary ML
frameworks and libraries. cedar introduces an extensible optimizer that
systematically applies a complex combination of optimizations (e.g.,
offloading, caching, prefetching, fusion, and reordering). It orchestrates
processing across a customizable set of local and distributed compute resources
in order to improve processing performance and efficiency, all without user
input. Across eight pipelines, cedar improves performance by up to 1.87x to
10.65x compared to state-of-the-art input data systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to PVLDB Volume 18</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Training of Two-Layer Polynomial and ReLU Activation
  Networks via Convex Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14033v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14033v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Kuelbs, Sanjay Lall, Mert Pilanci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training neural networks which are robust to adversarial attacks remains an
important problem in deep learning, especially as heavily overparameterized
models are adopted in safety-critical settings. Drawing from recent work which
reformulates the training problems for two-layer ReLU and polynomial activation
networks as convex programs, we devise a convex semidefinite program (SDP) for
adversarial training of two-layer polynomial activation networks and prove that
the convex SDP achieves the same globally optimal solution as its nonconvex
counterpart. The convex SDP is observed to improve robust test accuracy against
$\ell_\infty$ attacks relative to the original convex training formulation on
multiple datasets. Additionally, we present scalable implementations of
adversarial training for two-layer polynomial and ReLU networks which are
compatible with standard machine learning libraries and GPU acceleration.
Leveraging these implementations, we retrain the final two fully connected
layers of a Pre-Activation ResNet-18 model on the CIFAR-10 dataset with both
polynomial and ReLU activations. The two `robustified' models achieve
significantly higher robust test accuracies against $\ell_\infty$ attacks than
a Pre-Activation ResNet-18 model trained with sharpness-aware minimization,
demonstrating the practical utility of convex adversarial training on
large-scale problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 2 figures. Added a proof of the main theorem in the
  appendix. Expanded numerical results section. Added references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An exactly solvable model for emergence and scaling laws in the
  multitask sparse parity problem <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17563v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17563v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoonsoo Nam, Nayara Fonseca, Seok Hyeong Lee, Chris Mingard, Ard A. Louis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models can exhibit what appears to be a sudden ability to solve
a new problem as training time, training data, or model size increases, a
phenomenon known as emergence. In this paper, we present a framework where each
new ability (a skill) is represented as a basis function. We solve a simple
multi-linear model in this skill-basis, finding analytic expressions for the
emergence of new skills, as well as for scaling laws of the loss with training
time, data size, model size, and optimal compute. We compare our detailed
calculations to direct simulations of a two-layer neural network trained on
multitask sparse parity, where the tasks in the dataset are distributed
according to a power-law. Our simple model captures, using a single fit
parameter, the sigmoidal emergence of multiple new skills as training time,
data size or model size increases in the neural network.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Energy and Carbon Considerations of <span class="highlight-title">Fine-Tuning</span> BERT <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10267v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10267v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaorong Wang, Clara Na, Emma Strubell, Sorelle Friedler, Sasha Luccioni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP
community, existing work quantifying energy costs and associated carbon
emissions has largely focused on language model pre-training. Although a single
pre-training run draws substantially more energy than fine-tuning, fine-tuning
is performed more frequently by many more individual actors, and thus must be
accounted for when considering the energy and carbon footprint of NLP. In order
to better characterize the role of fine-tuning in the landscape of energy and
carbon emissions in NLP, we perform a careful empirical study of the
computational costs of fine-tuning across tasks, datasets, hardware
infrastructure and measurement modalities. Our experimental results allow us to
place fine-tuning energy and carbon costs into perspective with respect to
pre-training and inference, and outline recommendations to NLP researchers and
practitioners who wish to improve their fine-tuning energy efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Findings; First two authors contributed equally; 12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preferential Normalizing Flows <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petrus Mikkola, Luigi Acerbi, Arto Klami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Eliciting a high-dimensional probability distribution from an expert via
noisy judgments is notoriously challenging, yet useful for many applications,
such as prior elicitation and reward modeling. We introduce a method for
eliciting the expert's belief density as a normalizing flow based solely on
preferential questions such as comparing or ranking alternatives. This allows
eliciting in principle arbitrarily flexible densities, but flow estimation is
susceptible to the challenge of collapsing or diverging probability mass that
makes it difficult in practice. We tackle this problem by introducing a novel
functional prior for the flow, motivated by a decision-theoretic argument, and
show empirically that the belief density can be inferred as the function-space
maximum a posteriori estimate. We demonstrate our method by eliciting
multivariate belief densities of simulated experts, including the prior belief
of a general-purpose large language model over a real-world dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 18 figures, Accepted at NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Effective Horizon of Inverse Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06541v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06541v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqing Xu, Finale Doshi-Velez, David Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse reinforcement learning (IRL) algorithms often rely on (forward)
reinforcement learning or planning over a given time horizon to compute an
approximately optimal policy for a hypothesized reward function and then match
this policy with expert demonstrations. The time horizon plays a critical role
in determining both the accuracy of reward estimates and the computational
efficiency of IRL algorithms. Interestingly, an \emph{effective time horizon}
shorter than the ground-truth value often produces better results faster. This
work formally analyzes this phenomenon and provides an explanation: the time
horizon controls the complexity of an induced policy class and mitigates
overfitting with limited data. This analysis serves as a guide for the
principled choice of the effective horizon for IRL. It also prompts us to
re-examine the classic IRL formulation: it is more natural to learn jointly the
reward and the effective horizon rather than the reward alone with a given
horizon. To validate our findings, we implement a cross-validation extension
and the experimental results confirm the theoretical analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Optimal Experimental Design for Parameter Estimation Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14003v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14003v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Shahriar Rahim Siddiqui, Arman Rahmim, Eldad Haber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal experimental design is a well studied field in applied science and
engineering. Techniques for estimating such a design are commonly used within
the framework of parameter estimation. Nonetheless, in recent years parameter
estimation techniques are changing rapidly with the introduction of deep
learning techniques to replace traditional estimation methods. This in turn
requires the adaptation of optimal experimental design that is associated with
these new techniques. In this paper we investigate a new experimental design
methodology that uses deep learning. We show that the training of a network as
a Likelihood Free Estimator can be used to significantly simplify the design
process and circumvent the need for the computationally expensive bi-level
optimization problem that is inherent in optimal experimental design for
non-linear systems. Furthermore, deep design improves the quality of the
recovery process for parameter estimation problems. As proof of concept we
apply our methodology to two different systems of Ordinary Differential
Equations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Likelihood-based Differentiable Structure Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Deng, Kevin Bello, Pradeep Ravikumar, Bryon Aragam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing approaches to differentiable structure learning of directed acyclic
graphs (DAGs) rely on strong identifiability assumptions in order to guarantee
that global minimizers of the acyclicity-constrained optimization problem
identifies the true DAG. Moreover, it has been observed empirically that the
optimizer may exploit undesirable artifacts in the loss function. We explain
and remedy these issues by studying the behavior of differentiable
acyclicity-constrained programs under general likelihoods with multiple global
minimizers. By carefully regularizing the likelihood, it is possible to
identify the sparsest model in the Markov equivalence class, even in the
absence of an identifiable parametrization. We first study the Gaussian case in
detail, showing how proper regularization of the likelihood defines a score
that identifies the sparsest model. Assuming faithfulness, it also recovers the
Markov equivalence class. These results are then generalized to general models
and likelihoods, where the same claims hold. These theoretical results are
validated empirically, showing how this can be done using standard
gradient-based optimizers, thus paving the way for differentiable structure
learning under general models and losses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 14 figures, to appear at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SplitLLM: Collaborative Inference of LLMs for Model Placement and
  Throughput Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10759v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10759v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akrit Mudvari, Yuang Jiang, Leandros Tassiulas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been a disruptive innovation in recent
years, and they play a crucial role in our daily lives due to their ability to
understand and generate human-like text. Their capabilities include natural
language understanding, information retrieval and search, translation,
chatbots, virtual assistance, and many more. However, it is well known that
LLMs are massive in terms of the number of parameters. Additionally, the
self-attention mechanism in the underlying architecture of LLMs, Transformers,
has quadratic complexity in terms of both computation and memory with respect
to the input sequence length. For these reasons, LLM inference is
resource-intensive, and thus, the throughput of LLM inference is limited,
especially for the longer sequences. In this report, we design a collaborative
inference architecture between a server and its clients to alleviate the
throughput limit. In this design, we consider the available resources on both
sides, i.e., the computation and communication costs. We develop a dynamic
programming-based algorithm to optimally allocate computation between the
server and the client device to increase the server throughput, while not
violating the service level agreement (SLA). We show in the experiments that we
are able to efficiently distribute the workload allowing for roughly 1/3
reduction in the server workload, while achieving 19 percent improvement over a
greedy method. As a result, we are able to demonstrate that, in an environment
with different types of LLM inference requests, the throughput of the server is
improved.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Language Models Are Versatile Protein Learners <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18567v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18567v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, Quanquan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces diffusion protein language model (DPLM), a versatile
protein language model that demonstrates strong generative and predictive
capabilities for protein sequences. We first pre-train scalable DPLMs from
evolutionary-scale protein sequences within a generative self-supervised
discrete diffusion probabilistic framework, which generalizes language modeling
for proteins in a principled way. After pre-training, DPLM exhibits the ability
to generate structurally plausible, novel, and diverse protein sequences for
unconditional generation. We further demonstrate the proposed diffusion
generative pre-training makes DPLM possess a better understanding of proteins,
making it a superior representation learner, which can be fine-tuned for
various predictive tasks, comparing favorably to ESM2 (Lin et al., 2022).
Moreover, DPLM can be tailored for various needs, which showcases its prowess
of conditional generation in several ways: (1) conditioning on partial peptide
sequences, e.g., generating scaffolds for functional motifs with high success
rate; (2) incorporating other modalities as conditioner, e.g.,
structure-conditioned generation for inverse folding; and (3) steering sequence
generation towards desired properties, e.g., satisfying specified secondary
structures, through a plug-and-play classifier guidance. Code is released at
\url{https://github.com/bytedance/dplm}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024 camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extreme time extrapolation capabilities and thermodynamic consistency of
  physics-inspired Neural Networks for the 3D microstructure evolution of
  materials via Cahn-Hilliard flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20126v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20126v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele Lanzoni, Andrea Fantasia, Roberto Bergamaschini, Olivier Pierre-Louis, Francesco Montalenti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A Convolutional Recurrent Neural Network (CRNN) is trained to reproduce the
evolution of the spinodal decomposition process in three dimensions as
described by the Cahn-Hilliard equation. A specialized, physics-inspired
architecture is proven to provide close accordance between the predicted
evolutions and the ground truth ones obtained via conventional integration
schemes. The method can accurately reproduce the evolution of microstructures
not represented in the training set at a fraction of the computational costs.
Extremely long-time extrapolation capabilities are achieved, up to reaching the
theoretically expected equilibrium state of the system, consisting of a
layered, phase-separated morphology, despite the training set containing only
relatively-short, initial phases of the evolution. Quantitative accordance with
the decay rate of the Free energy is also demonstrated up to the late
coarsening stages, proving that this class of Machine Learning approaches can
become a new and powerful tool for the long timescale and high throughput
simulation of materials, while retaining thermodynamic consistency and
high-accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 main text figures, 2 appendix figures, 1 supplementary
  material figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open-Source Conversational AI with SpeechBrain 1.0 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00463v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00463v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mirco Ravanelli, Titouan Parcollet, Adel Moumen, Sylvain de Langen, Cem Subakan, Peter Plantinga, Yingzhi Wang, Pooneh Mousavi, Luca Della Libera, Artem Ploujnikov, Francesco Paissan, Davide Borra, Salah Zaiem, Zeyu Zhao, Shucong Zhang, Georgios Karakasidis, Sung-Lin Yeh, Pierre Champion, Aku Rouhe, Rudolf Braun, Florian Mai, Juan Zuluaga-Gomez, Seyed Mahed Mousavi, Andreas Nautsch, Xuechen Liu, Sangeet Sagar, Jarod Duret, Salima Mdhaffar, Gaelle Laperriere, Mickael Rouvier, Renato De Mori, Yannick Esteve
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SpeechBrain is an open-source Conversational AI toolkit based on PyTorch,
focused particularly on speech processing tasks such as speech recognition,
speech enhancement, speaker recognition, text-to-speech, and much more. It
promotes transparency and replicability by releasing both the pre-trained
models and the complete "recipes" of code and algorithms required for training
them. This paper presents SpeechBrain 1.0, a significant milestone in the
evolution of the toolkit, which now has over 200 recipes for speech, audio, and
language processing tasks, and more than 100 models available on Hugging Face.
SpeechBrain 1.0 introduces new technologies to support diverse learning
modalities, Large Language Model (LLM) integration, and advanced decoding
strategies, along with novel models, tasks, and modalities. It also includes a
new benchmark repository, offering researchers a unified platform for
evaluating models across diverse tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Journal of Machine Learning research (JMLR), Machine
  Learning Open Source Software</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nearly Tight Black-Box Auditing of Differentially Private Machine
  Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14106v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14106v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meenatchi Sundaram Muthu Selva Annamalai, Emiliano De Cristofaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an auditing procedure for the Differentially Private
Stochastic Gradient Descent (DP-SGD) algorithm in the black-box threat model
that is substantially tighter than prior work. The main intuition is to craft
worst-case initial model parameters, as DP-SGD's privacy analysis is agnostic
to the choice of the initial model parameters. For models trained on MNIST and
CIFAR-10 at theoretical $\varepsilon=10.0$, our auditing procedure yields
empirical estimates of $\varepsilon_{emp} = 7.21$ and $6.95$, respectively, on
a 1,000-record sample and $\varepsilon_{emp}= 6.48$ and $4.96$ on the full
datasets. By contrast, previous audits were only (relatively) tight in stronger
white-box models, where the adversary can access the model's inner parameters
and insert arbitrary gradients. Overall, our auditing procedure can offer
valuable insight into how the privacy analysis of DP-SGD could be improved and
detect bugs and DP violations in real-world implementations. The source code
needed to reproduce our experiments is available at
https://github.com/spalabucr/bb-audit-dpsgd.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Proceedings of the Thirty-eighth Annual Conference
  on Neural Information Processing Systems (NeurIPS 2024). Please cite
  accordingly</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task Aware Modulation using Representation Learning: An Approach for Few
  Shot Learning in Environmental Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arvind Renganathan, Rahul Ghosh, Ankush Khandelwal, Vipin Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce TAM-RL (Task Aware Modulation using Representation Learning), a
novel multimodal meta-learning framework for few-shot learning in heterogeneous
systems, designed for science and engineering problems where entities share a
common underlying forward model but exhibit heterogeneity due to
entity-specific characteristics. TAM-RL leverages an amortized training process
with a modulation network and a base network to learn task-specific modulation
parameters, enabling efficient adaptation to new tasks with limited data. We
evaluate TAM-RL on two real-world environmental datasets: Gross Primary Product
(GPP) prediction and streamflow forecasting, demonstrating significant
improvements over existing meta-learning methods. On the FLUXNET dataset,
TAM-RL improves RMSE by 18.9\% over MMAML with just one month of few-shot data,
while for streamflow prediction, it achieves an 8.21\% improvement with one
year of data. Synthetic data experiments further validate TAM-RL's superior
performance in heterogeneous task distributions, outperforming the baselines in
the most heterogeneous setting. Notably, TAM-RL offers substantial
computational efficiency, with at least 3x faster training times compared to
gradient-based meta-learning approaches while being much simpler to train due
to reduced complexity. Ablation studies highlight the importance of pretraining
and adaptation mechanisms in TAM-RL's performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncovering, Explaining, and Mitigating the Superficial Safety of
  Backdoor Defense <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Min, Zeyu Qin, Nevin L. Zhang, Li Shen, Minhao Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backdoor attacks pose a significant threat to Deep Neural Networks (DNNs) as
they allow attackers to manipulate model predictions with backdoor triggers. To
address these security vulnerabilities, various backdoor purification methods
have been proposed to purify compromised models. Typically, these purified
models exhibit low Attack Success Rates (ASR), rendering them resistant to
backdoored inputs. However, Does achieving a low ASR through current safety
purification methods truly eliminate learned backdoor features from the
pretraining phase? In this paper, we provide an affirmative answer to this
question by thoroughly investigating the Post-Purification Robustness of
current backdoor purification methods. We find that current safety purification
methods are vulnerable to the rapid re-learning of backdoor behavior, even when
further fine-tuning of purified models is performed using a very small number
of poisoned samples. Based on this, we further propose the practical
Query-based Reactivation Attack (QRA) which could effectively reactivate the
backdoor by merely querying purified models. We find the failure to achieve
satisfactory post-purification robustness stems from the insufficient deviation
of purified models from the backdoored model along the backdoor-connected path.
To improve the post-purification robustness, we propose a straightforward
tuning defense, Path-Aware Minimization (PAM), which promotes deviation along
backdoor-connected paths with extra model updates. Extensive experiments
demonstrate that PAM significantly improves post-purification robustness while
maintaining a good clean accuracy and low ASR. Our work provides a new
perspective on understanding the effectiveness of backdoor safety tuning and
highlights the importance of faithfully assessing the model's safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Spotlight paper. The first two authors contributed
  equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pessimistic Backward Policy for GFlowNets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyosoon Jang, Yunhui Jang, Minsu Kim, Jinkyoo Park, Sungsoo Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies Generative Flow Networks (GFlowNets), which learn to
sample objects proportionally to a given reward function through the trajectory
of state transitions. In this work, we observe that GFlowNets tend to
under-exploit the high-reward objects due to training on insufficient number of
trajectories, which may lead to a large gap between the estimated flow and the
(known) reward value. In response to this challenge, we propose a pessimistic
backward policy for GFlowNets (PBP-GFN), which maximizes the observed flow to
align closely with the true reward for the object. We extensively evaluate
PBP-GFN across eight benchmarks, including hyper-grid environment, bag
generation, structured set generation, molecular generation, and four RNA
sequence generation tasks. In particular, PBP-GFN enhances the discovery of
high-reward objects, maintains the diversity of the objects, and consistently
outperforms existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ITINERA: Integrating Spatial Optimization with Large Language Models for
  Open-domain Urban Itinerary Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07204v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07204v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Tang, Zhaokai Wang, Ao Qu, Yihao Yan, Zhaofeng Wu, Dingyi Zhuang, Jushi Kai, Kebing Hou, Xiaotong Guo, Jinhua Zhao, Zhan Zhao, Wei Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Citywalk, a recently popular form of urban travel, requires genuine
personalization and understanding of fine-grained requests compared to
traditional itinerary planning. In this paper, we introduce the novel task of
Open-domain Urban Itinerary Planning (OUIP), which generates personalized urban
itineraries from user requests in natural language. We then present ITINERA, an
OUIP system that integrates spatial optimization with large language models to
provide customized urban itineraries based on user needs. This involves
decomposing user requests, selecting candidate points of interest (POIs),
ordering the POIs based on cluster-aware spatial optimization, and generating
the itinerary. Experiments on real-world datasets and the performance of the
deployed system demonstrate our system's capacity to deliver personalized and
spatially coherent itineraries compared to current solutions. Source codes of
ITINERA are available at https://github.com/YihongT/ITINERA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CELL your Model: Contrastive Explanations for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11785v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11785v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronny Luss, Erik Miehling, Amit Dhurandhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of black-box deep neural network classification models has sparked
the need to explain their decisions. However, in the case of generative AI,
such as large language models (LLMs), there is no class prediction to explain.
Rather, one can ask why an LLM output a particular response to a given prompt.
In this paper, we answer this question by proposing, to the best of our
knowledge, the first contrastive explanation methods requiring simply
black-box/query access. Our explanations suggest that an LLM outputs a reply to
a given prompt because if the prompt was slightly modified, the LLM would have
given a different response that is either less preferable or contradicts the
original response. The key insight is that contrastive explanations simply
require a scoring function that has meaning to the user and not necessarily a
specific real valued quantity (viz. class label). We offer two algorithms for
finding contrastive explanations: i) A myopic algorithm, which although
effective in creating contrasts, requires many model calls and ii) A budgeted
algorithm, our main algorithmic contribution, which intelligently creates
contrasts adhering to a query budget, necessary for longer contexts. We show
the efficacy of these methods on diverse natural language tasks such as
open-text generation, automated red teaming, and explaining conversational
degradation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Light-Weight Fault Tolerant Attention for Large Language Model Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11720v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11720v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Liang, Xinyi Li, Jie Ren, Ang Li, Bo Fang, Jieyang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable performance in
various natural language processing tasks. However, the training of these
models is computationally intensive and susceptible to faults, particularly in
the attention mechanism, which is a critical component of transformer-based
LLMs. In this paper, we investigate the impact of faults on LLM training,
focusing on INF, NaN, and near-INF values in the computation results with
systematic fault injection experiments. We observe the propagation patterns of
these errors, which can trigger non-trainable states in the model and disrupt
training, forcing the procedure to load from checkpoints. To mitigate the
impact of these faults, we propose ATTNChecker, the first Algorithm-Based Fault
Tolerance (ABFT) technique tailored for the attention mechanism in LLMs.
ATTNChecker is designed based on fault propagation patterns of LLM and
incorporates performance optimization to adapt to both system reliability and
model vulnerability while providing lightweight protection for fast LLM
training. Evaluations on four LLMs show that ATTNChecker on average incurs on
average 7% overhead on training while detecting and correcting all extreme
errors. Compared with the state-of-the-art checkpoint/restore approach,
ATTNChecker reduces recovery overhead by up to 49x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CECILIA: Comprehensive Secure Machine Learning Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.03023v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.03023v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Burak Ünal, Nico Pfeifer, Mete Akgün
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since ML algorithms have proven their success in many different applications,
there is also a big interest in privacy preserving (PP) ML methods for building
models on sensitive data. Moreover, the increase in the number of data sources
and the high computational power required by those algorithms force individuals
to outsource the training and/or the inference of a ML model to the clouds
providing such services. To address this, we propose a secure 3-party
computation framework, CECILIA, offering PP building blocks to enable complex
operations privately. In addition to the adapted and common operations like
addition and multiplication, it offers multiplexer, most significant bit and
modulus conversion. The first two are novel in terms of methodology and the
last one is novel in terms of both functionality and methodology. CECILIA also
has two complex novel methods, which are the exact exponential of a public base
raised to the power of a secret value and the inverse square root of a secret
Gram matrix. We use CECILIA to realize the private inference on pre-trained
RKNs, which require more complex operations than most other DNNs, on the
structural classification of proteins as the first study ever accomplishing the
PP inference on RKNs. In addition to the successful private computation of
basic building blocks, the results demonstrate that we perform the exact and
fully private exponential computation, which is done by approximation in the
literature so far. Moreover, they also show that we compute the exact inverse
square root of a secret Gram matrix up to a certain privacy level, which has
not been addressed in the literature at all. We also analyze the scalability of
CECILIA to various settings on a synthetic dataset. The framework shows a great
promise to make other ML algorithms as well as further computations privately
computable by the building blocks of the framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint version of "A privacy-preserving approach for cloud-based
  protein fold recognition" paper published in Patterns, ~8 pages of the main
  paper, ~5 pages of Supplement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards aerodynamic surrogate modeling based on $β$-variational
  autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Víctor Francés-Belda, Alberto Solera-Rico, Javier Nieto-Centenero, Esther Andrés, Carlos Sanmiguel Vila, Rodrigo Castellanos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surrogate models that combine dimensionality reduction and regression
techniques are essential to reduce the need for costly high-fidelity
computational fluid dynamics data. New approaches using $\beta$-Variational
Autoencoder ($\beta$-VAE) architectures have shown promise in obtaining
high-quality low-dimensional representations of high-dimensional flow data
while enabling physical interpretation of their latent spaces. We propose a
surrogate model based on latent space regression to predict pressure
distributions on a transonic wing given the flight conditions: Mach number and
angle of attack. The $\beta$-VAE model, enhanced with Principal Component
Analysis (PCA), maps high-dimensional data to a low-dimensional latent space,
showing a direct correlation with flight conditions. Regularization through
$\beta$ requires careful tuning to improve overall performance, while PCA
preprocessing helps to construct an effective latent space, improving
autoencoder training and performance. Gaussian Process Regression is used to
predict latent space variables from flight conditions, showing robust behavior
independent of $\beta$, and the decoder reconstructs the high-dimensional
pressure field data. This pipeline provides insight into unexplored flight
conditions. Furthermore, a fine-tuning process of the decoder further refines
the model, reducing the dependence on $\beta$ and enhancing accuracy.
Structured latent space, robust regression performance, and significant
improvements in fine-tuning collectively create a highly accurate and efficient
surrogate model. Our methodology demonstrates the effectiveness of $\beta$-VAEs
for aerodynamic surrogate modeling, offering a rapid, cost-effective, and
reliable alternative for aerodynamic data prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reward-Robust RLHF in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15360v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15360v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzi Yan, Xingzhou Lou, Jialian Li, Yiping Zhang, Jian Xie, Chao Yu, Yu Wang, Dong Yan, Yuan Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) continue to progress toward more advanced
forms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is
increasingly seen as a key pathway toward achieving Artificial General
Intelligence (AGI). However, the reliance on reward-model-based (RM-based)
alignment methods introduces significant challenges due to the inherent
instability and imperfections of Reward Models (RMs), which can lead to
critical issues such as reward hacking and misalignment with human intentions.
In this paper, we introduce a reward-robust RLHF framework aimed at addressing
these fundamental challenges, paving the way for more reliable and resilient
learning in LLMs. Our approach introduces a novel optimization objective that
carefully balances performance and robustness by incorporating Bayesian Reward
Model Ensembles (BRME) to model the uncertainty set of reward functions. This
allows the framework to integrate both nominal performance and minimum reward
signals, ensuring more stable learning even with imperfect RMs. Empirical
results demonstrate that our framework consistently outperforms baselines
across diverse benchmarks, showing improved accuracy and long-term stability.
We also provide a theoretical analysis, demonstrating that reward-robust RLHF
approaches the stability of constant reward settings, which proves to be
acceptable even in a stochastic-case analysis. Together, these contributions
highlight the framework potential to enhance both the performance and stability
of LLM alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-Varying Gaussian Process Bandits with Unknown Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01632v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01632v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juliusz Ziomek, Masaki Adachi, Michael A. Osborne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian optimisation requires fitting a Gaussian process model, which in
turn requires specifying prior on the unknown black-box function -- most of the
theoretical literature assumes this prior is known. However, it is common to
have more than one possible prior for a given black-box function, for example
suggested by domain experts with differing opinions. In some cases, the type-II
maximum likelihood estimator for selecting prior enjoys the consistency
guarantee, but it does not universally apply to all types of priors. If the
problem is stationary, one could rely on the Regret Balancing scheme to conduct
the optimisation, but in the case of time-varying problems, such a scheme
cannot be used. To address this gap in existing research, we propose a novel
algorithm, PE-GP-UCB, which is capable of solving time-varying Bayesian
optimisation problems even without the exact knowledge of the function's prior.
The algorithm relies on the fact that either the observed function values are
consistent with some of the priors, in which case it is easy to reject the
wrong priors, or the observations are consistent with all candidate priors, in
which case it does not matter which prior our model relies on. We provide a
regret bound on the proposed algorithm. Finally, we empirically evaluate our
algorithm on toy and real-world time-varying problems and show that it
outperforms the maximum likelihood estimator, fully Bayesian treatment of
unknown prior and Regret Balancing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Energy-Efficient Computation with DVFS using Deep Reinforcement Learning
  for Multi-Task Systems in Edge Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19434v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19434v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Li, Ti Zhou, Haoyu Wang, Man Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Periodic soft real-time systems have broad applications in many areas, such
as IoT. Finding an optimal energy-efficient policy that is adaptable to
underlying edge devices while meeting deadlines for tasks has always been
challenging. This research studies generalized systems with multi-task,
multi-deadline scenarios with reinforcement learning-based DVFS for energy
saving. This work addresses the limitation of previous work that models a
periodic system as a single task and single-deadline scenario, which is too
simplified to cope with complex situations. The method encodes time series
information in the Linux kernel into information that is easy to use for
reinforcement learning, allowing the system to generate DVFS policies to adapt
system patterns based on the general workload. For encoding, we present two
different methods for comparison. Both methods use only one performance
counter: system utilization and the kernel only needs minimal information from
the userspace. Our method is implemented on Jetson Nano Board (2GB) and is
tested with three fixed multitask workloads, which are three, five, and eight
tasks in the workload, respectively. For randomness and generalization, we also
designed a random workload generator to build different multitask workloads to
test. Based on the test results, our method could save 3%-10% power compared to
Linux built-in governors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain
  Readability Assessment <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14463v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14463v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tarek Naous, Michael J. Ryan, Anton Lavrouk, Mohit Chandra, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a comprehensive evaluation of large language models for
multilingual readability assessment. Existing evaluation resources lack domain
and language diversity, limiting the ability for cross-domain and cross-lingual
analyses. This paper introduces ReadMe++, a multilingual multi-domain dataset
with human annotations of 9757 sentences in Arabic, English, French, Hindi, and
Russian, collected from 112 different data sources. This benchmark will
encourage research on developing robust multilingual readability assessment
methods. Using ReadMe++, we benchmark multilingual and monolingual language
models in the supervised, unsupervised, and few-shot prompting settings. The
domain and language diversity in ReadMe++ enable us to test more effective
few-shot prompting, and identify shortcomings in state-of-the-art unsupervised
methods. Our experiments also reveal exciting results of superior domain
generalization and enhanced cross-lingual transfer capabilities by models
trained on ReadMe++. We will make our data publicly available and release a
python package tool for multilingual sentence readability prediction using our
trained models at: https://github.com/tareknaous/readme
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Increasing Both Batch Size and Learning Rate Accelerates Stochastic
  Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08770v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08770v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hikaru Umeda, Hideaki Iiduka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of mini-batch stochastic gradient descent (SGD) strongly
depends on setting the batch size and learning rate to minimize the empirical
loss in training the deep neural network. In this paper, we present theoretical
analyses of mini-batch SGD with four schedulers: (i) constant batch size and
decaying learning rate scheduler, (ii) increasing batch size and decaying
learning rate scheduler, (iii) increasing batch size and increasing learning
rate scheduler, and (iv) increasing batch size and warm-up decaying learning
rate scheduler. We show that mini-batch SGD using scheduler (i) does not always
minimize the expectation of the full gradient norm of the empirical loss,
whereas it does using any of schedulers (ii), (iii), and (iv). Furthermore,
schedulers (iii) and (iv) accelerate mini-batch SGD. The paper also provides
numerical results of supporting analyses showing that using scheduler (iii) or
(iv) minimizes the full gradient norm of the empirical loss faster than using
scheduler (i) or (ii).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Explainable to Interpretable Deep Learning for Natural Language
  Processing in Healthcare: How Far from Reality? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11894v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11894v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangming Huang, Yingya Li, Shoaib Jameel, Yunfei Long, Giorgos Papanastasiou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) has substantially enhanced natural language processing
(NLP) in healthcare research. However, the increasing complexity of DL-based
NLP necessitates transparent model interpretability, or at least
explainability, for reliable decision-making. This work presents a thorough
scoping review of explainable and interpretable DL in healthcare NLP. The term
"eXplainable and Interpretable Artificial Intelligence" (XIAI) is introduced to
distinguish XAI from IAI. Different models are further categorized based on
their functionality (model-, input-, output-based) and scope (local, global).
Our analysis shows that attention mechanisms are the most prevalent emerging
IAI technique. The use of IAI is growing, distinguishing it from XAI. The major
challenges identified are that most XIAI does not explore "global" modelling
processes, the lack of best practices, and the lack of systematic evaluation
and benchmarks. One important opportunity is to use attention mechanisms to
enhance multi-modal XIAI for personalized medicine. Additionally, combining DL
with causal logic holds promise. Our discussion encourages the integration of
XIAI in Large Language Models (LLMs) and domain-specific smaller models. In
conclusion, XIAI adoption in healthcare requires dedicated in-house expertise.
Collaboration with domain experts, end-users, and policymakers can lead to
ready-to-use XIAI methods across NLP and medical tasks. While challenges exist,
XIAI techniques offer a valuable foundation for interpretable NLP algorithms in
healthcare.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by Computational and Structural
  Biotechnology Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Five Years of COVID-19 Discourse on Instagram: A Labeled Instagram
  Dataset of Over Half a Million Posts for Multilingual Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03293v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03293v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nirmalya Thakur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The work presented in this paper makes three scientific contributions with a
specific focus on mining and analysis of COVID-19-related posts on Instagram.
First, it presents a multilingual dataset of 500,153 Instagram posts about
COVID-19 published between January 2020 and September 2024. This dataset,
available at https://dx.doi.org/10.21227/d46p-v480, contains Instagram posts in
161 different languages as well as 535,021 distinct hashtags. After the
development of this dataset, multilingual sentiment analysis was performed,
which involved classifying each post as positive, negative, or neutral. The
results of sentiment analysis are presented as a separate attribute in this
dataset. Second, it presents the results of performing sentiment analysis per
year from 2020 to 2024. The findings revealed the trends in sentiment related
to COVID-19 on Instagram since the beginning of the pandemic. For instance,
between 2020 and 2024, the sentiment trends show a notable shift, with positive
sentiment decreasing from 38.35% to 28.69%, while neutral sentiment rising from
44.19% to 58.34%. Finally, the paper also presents findings of
language-specific sentiment analysis. This analysis highlighted similar and
contrasting trends of sentiment across posts published in different languages
on Instagram. For instance, out of all English posts, 49.68% were positive,
14.84% were negative, and 35.48% were neutral. In contrast, among Hindi posts,
4.40% were positive, 57.04% were negative, and 38.56% were neutral, reflecting
distinct differences in the sentiment distribution between these two languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic Token Reweighting for Interpretable and Controllable Text
  Embeddings in <span class="highlight-title">CLIP</span> <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eunji Kim, Kyuhong Shim, Simyung Chang, Sungroh Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial
role in translating textual input into an embedding space shared with images,
thereby facilitating the interpretative analysis of vision tasks through
natural language. Despite the varying significance of different textual
elements within a sentence depending on the context, efforts to account for
variation of importance in constructing text embeddings have been lacking. We
propose a framework of Semantic Token Reweighting to build Interpretable text
embeddings (SToRI), which incorporates controllability as well. SToRI refines
the text encoding process in CLIP by differentially weighting semantic elements
based on contextual importance, enabling finer control over emphasis responsive
to data-driven insights and user preferences. The efficacy of SToRI is
demonstrated through comprehensive experiments on few-shot image classification
and image retrieval tailored to user preferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NAR-*ICP: Neural Execution of Classical ICP-based Pointcloud
  Registration Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Efimia Panagiotaki, Daniele De Martini, Lars Kunze, Petar Veličković
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores the intersection of neural networks and classical
robotics algorithms through the Neural Algorithmic Reasoning (NAR) framework,
allowing to train neural networks to effectively reason like classical robotics
algorithms by learning to execute them. Algorithms are integral to robotics and
safety-critical applications due to their predictable and consistent
performance through logical and mathematical principles. In contrast, while
neural networks are highly adaptable, handling complex, high-dimensional data
and generalising across tasks, they often lack interpretability and
transparency in their internal computations. We propose a Graph Neural Network
(GNN)-based learning framework, NAR-*ICP, which learns the intermediate
algorithmic steps of classical ICP-based pointcloud registration algorithms,
and extend the CLRS Algorithmic Reasoning Benchmark with classical robotics
perception algorithms. We evaluate our approach across diverse datasets, from
real-world to synthetic, demonstrating its flexibility in handling complex and
noisy inputs, along with its potential to be used as part of a larger learning
system. Our results indicate that our method achieves superior performance
across all benchmarks and datasets, consistently surpassing even the algorithms
it has been trained on, further demonstrating its ability to generalise beyond
the capabilities of traditional algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TorchQL: A Programming Framework for Integrity Constraints in Machine
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06686v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06686v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaditya Naik, Adam Stein, Yinjun Wu, Mayur Naik, Eric Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding errors in machine learning applications requires a thorough
exploration of their behavior over data. Existing approaches used by
practitioners are often ad-hoc and lack the abstractions needed to scale this
process. We present TorchQL, a programming framework to evaluate and improve
the correctness of machine learning applications. TorchQL allows users to write
queries to specify and check integrity constraints over machine learning models
and datasets. It seamlessly integrates relational algebra with functional
programming to allow for highly expressive queries using only eight intuitive
operators. We evaluate TorchQL on diverse use-cases including finding critical
temporal inconsistencies in objects detected across video frames in autonomous
driving, finding data imputation errors in time-series medical records, finding
data labeling errors in real-world images, and evaluating biases and
constraining outputs of language models. Our experiments show that TorchQL
enables up to 13x faster query executions than baselines like Pandas and
MongoDB, and up to 40% shorter queries than native Python. We also conduct a
user study and find that TorchQL is natural enough for developers familiar with
Python to specify complex integrity constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>DSI: <span class="highlight-title">Prompt</span>-based Rehearsal-free Instance-wise Incremental
  Learning for Document Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan-Luc Huynh, Thuy-Trang Vu, Weiqing Wang, Yinwei Wei, Trung Le, Dragan Gasevic, Yuan-Fang Li, Thanh-Toan Do
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiable Search Index (DSI) utilizes Pre-trained Language Models (PLMs)
for efficient document retrieval without relying on external indexes. However,
DSI needs full re-training to handle updates in dynamic corpora, causing
significant computational inefficiencies. We introduce PromptDSI, a
prompt-based rehearsal-free approach for instance-wise incremental learning
document retrieval. PromptDSI attaches prompts to the frozen PLM's encoder of
DSI, leveraging its powerful representation to efficiently index new corpora
while maintaining a balance between stability and plasticity. We eliminate the
initial forward pass of prompt-based continual learning methods that doubles
training and inference time. Moreover, we propose a topic-aware prompt pool
that employs neural topic embeddings as fixed keys. This strategy ensures
diverse and effective prompt usage, addressing the challenge of parameter
underutilization caused by the collapse of the query-key matching mechanism.
Our empirical evaluations demonstrate that BERT-based PromptDSI matches IncDSI
in managing forgetting while improving new corpora performance by more than 4%
Hits@10 on NQ320k and upto 3% MRR@10 on MS MARCO 300k.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Augmentation for Continual RL via Adversarial Gradient Episodic
  Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13452v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13452v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihao Wu, Xingyu Zhao, Xiaowei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data efficiency of learning, which plays a key role in the Reinforcement
Learning (RL) training process, becomes even more important in continual RL
with sequential environments. In continual RL, the learner interacts with
non-stationary, sequential tasks and is required to learn new tasks without
forgetting previous knowledge. However, there is little work on implementing
data augmentation for continual RL. In this paper, we investigate the efficacy
of data augmentation for continual RL. Specifically, we provide benchmarking
data augmentations for continual RL, by (1) summarising existing data
augmentation methods and (2) including a new augmentation method for continual
RL: Adversarial Augmentation with Gradient Episodic Memory (Adv-GEM). Extensive
experiments show that data augmentations, such as random amplitude scaling,
state-switch, mixup, adversarial augmentation, and Adv-GEM, can improve
existing continual RL algorithms in terms of their average performance,
catastrophic forgetting, and forward transfer, on robot control tasks. All data
augmentation methods are implemented as plug-in modules for trivial integration
into continual RL methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Structure Learning for Sparse Context-Specific Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07762v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07762v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Leopoldo Rios, Alex Markham, Liam Solus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several approaches to graphically representing context-specific relations
among jointly distributed categorical variables have been proposed, along with
structure learning algorithms. While existing optimization-based methods have
limited scalability due to the large number of context-specific models, the
constraint-based methods are more prone to error than even constraint-based
directed acyclic graph learning algorithms since more relations must be tested.
We present an algorithm for learning context-specific models that scales to
hundreds of variables. Scalable learning is achieved through a combination of
an order-based Markov chain Monte-Carlo search and a novel, context-specific
sparsity assumption that is analogous to those typically invoked for directed
acyclic graphical models. Unlike previous Markov chain Monte-Carlo search
methods, our Markov chain is guaranteed to have the true posterior of the
variable orderings as the stationary distribution. To implement the method, we
solve a first case of an open problem recently posed by Alon and Balogh. Future
work solving increasingly general instances of this problem would allow our
methods to learn increasingly dense models. The method is shown to perform well
on synthetic data and real world examples, in terms of both accuracy and
scalability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 6 figures; for associated code, see
  https://cstrees.readthedocs.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19014v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19014v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heegyu Kim, Taeyang Jeon, Seunghwan Choi, Seungtaek Choi, Hyunsouk Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL systems have become crucial for translating natural language into
SQL queries in various industries, enabling non-technical users to perform
complex data operations. The need for accurate evaluation methods has increased
as these systems have grown more sophisticated. However, the Execution Accuracy
(EX), the most prevalent evaluation metric, still shows many false positives
and negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel
approach to evaluating text-to-SQL systems using large language models (LLMs)
to emulate human expert-level evaluation of SQL queries. Our metric improves
agreement with human experts (from 62 to 87.04 in Cohen's kappa) with
comprehensive context and sophisticated criteria. Our extensive experiments
yield several key insights: (1) Models' performance increases by over 2.6
points on average, substantially affecting rankings on Spider and BIRD
benchmarks; (2) The underestimation of models in EX primarily stems from
annotation quality issues; and (3) Model performance on particularly
challenging questions tends to be overestimated. This work contributes to a
more accurate and nuanced evaluation of text-to-SQL systems, potentially
reshaping our understanding of state-of-the-art performance in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixture of Experts Made Personalized: Federated <span class="highlight-title">Prompt</span> Learning for
  <span class="highlight-title">Vision-Language</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Luo, Chen Chen, Shandong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt learning for pre-trained Vision-Language Models (VLMs) like CLIP has
demonstrated potent applicability across diverse downstream tasks. This
lightweight approach has quickly gained traction from federated learning (FL)
researchers who seek to efficiently adapt VLMs to heterogeneous scenarios.
However, current federated prompt learning methods are habitually restricted to
the traditional FL paradigm, where the participating clients are generally only
allowed to download a single globally aggregated model from the server. While
justifiable for training full-sized models under federated settings, in this
work, we argue that this paradigm is ill-suited for lightweight prompts. By
facilitating the clients to download multiple pre-aggregated prompts as fixed
non-local experts, we propose Personalized Federated Mixture of Adaptive
Prompts (pFedMoAP), a novel FL framework that personalizes the prompt learning
process through the lens of Mixture of Experts (MoE). pFedMoAP implements a
local attention-based gating network that learns to generate enhanced text
features for better alignment with local image data on the client, benefiting
from both local and downloaded non-local adaptive prompt experts. The non-local
experts are sparsely selected from a server-maintained pool, fostering
collaborative learning across clients. To evaluate the proposed algorithm, we
conduct extensive experiments across 9 datasets under various heterogeneous
federated settings. The results show that pFedMoAP consistently outperforms the
state-of-the-art alternatives, underscoring its efficacy in personalizing
prompt learning for CLIP within the federated learning paradigm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Discovery under Latent Class Confounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07454v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07454v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bijan Mazaheri, Spencer Gordon, Yuval Rabani, Leonard Schulman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An acyclic causal structure can be described with directed acyclic graph
(DAG), where arrows indicate the possibility of direct causation. The task of
learning this structure from data is known as "causal discovery." Diverse
populations or changing environments can sometimes give rise to data that is
heterogeneous in the following sense: each population/environment is a "source"
which idiosyncratically determines the forms of those direct causal effects.
From this perspective, the source is a latent common cause for every observed
variable. While some methods for causal discovery are able to work around
latent confounding in special cases, especially when only few observables are
confounded, a global confounder is a difficult challenge. The only known ways
to deal with latent global confounding involve assumptions that limit the
structural equations and/or noise functions. We demonstrate that globally
confounded causal structures can still be identifiable with arbitrary
structural equations and noise functions, so long as the number of latent
classes remains small relative to the size and sparsity of the underlying DAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parsimony or Capability? Decomposition Delivers Both in Long-term Time
  Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11929v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11929v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinliang Deng, Feiyang Ye, Du Yin, Xuan Song, Ivor W. Tsang, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-term time series forecasting (LTSF) represents a critical frontier in
time series analysis, characterized by extensive input sequences, as opposed to
the shorter spans typical of traditional approaches. While longer sequences
inherently offer richer information for enhanced predictive precision,
prevailing studies often respond by escalating model complexity. These
intricate models can inflate into millions of parameters, resulting in
prohibitive parameter scales. Our study demonstrates, through both analytical
and empirical evidence, that decomposition is key to containing excessive model
inflation while achieving uniformly superior and robust results across various
datasets. Remarkably, by tailoring decomposition to the intrinsic dynamics of
time series data, our proposed model outperforms existing benchmarks, using
over 99 \% fewer parameters than the majority of competing methods. Through
this work, we aim to unleash the power of a restricted set of parameters by
capitalizing on domain characteristics--a timely reminder that in the realm of
LTSF, bigger is not invariably better.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpret Your Decision: Logical Reasoning Regularization for
  Generalization in Visual Classification <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04492v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04492v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaorui Tan, Xi Yang, Qiufeng Wang, Anh Nguyen, Kaizhu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision models excel in image classification but struggle to generalize to
unseen data, such as classifying images from unseen domains or discovering
novel categories. In this paper, we explore the relationship between logical
reasoning and deep learning generalization in visual classification. A logical
regularization termed L-Reg is derived which bridges a logical analysis
framework to image classification. Our work reveals that L-Reg reduces the
complexity of the model in terms of the feature distribution and classifier
weights. Specifically, we unveil the interpretability brought by L-Reg, as it
enables the model to extract the salient features, such as faces to persons,
for classification. Theoretical analysis and experiments demonstrate that L-Reg
enhances generalization across various scenarios, including multi-domain
generalization and generalized category discovery. In complex real-world
scenarios where images span unknown classes and unseen domains, L-Reg
consistently improves generalization, highlighting its practical efficacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024 as Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Token-based Decision Criteria Are Suboptimal in In-context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16535v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16535v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hakaze Cho, Yoshihiro Sakai, Mariko Kato, Kenshiro Tanaka, Akira Ishii, Naoya Inoue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-Context Learning (ICL) typically utilizes classification criteria from
output probabilities of manually selected label tokens. However, we argue that
such token-based classification criteria lead to suboptimal decision
boundaries, despite delicate calibrations through translation and constrained
rotation applied. To address this problem, we propose Hidden Calibration, which
renounces token probabilities and uses the nearest centroid classifier on the
LM's last hidden states. In detail, we assign the label of the nearest centroid
previously estimated from a calibration set to the test sample as the predicted
label. Our experiments on 6 models and 10 classification datasets indicate that
Hidden Calibration consistently outperforms current token-based baselines by
about 20%~50%, achieving a strong state-of-the-art in ICL. Our further analysis
demonstrates that Hidden Calibration finds better classification criteria with
less inter-class overlap, and LMs provide linearly separable intra-class
clusters with the help of demonstrations, which supports Hidden Calibration and
gives new insights into the principle of ICL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 15 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeltaDock: A Unified Framework for Accurate, Efficient, and Physically
  Reliable Molecular Docking <span class="chip">NeurIPS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11224v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11224v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxian Yan, Zaixi Zhang, Jintao Zhu, Kai Zhang, Jianfeng Pei, Qi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Molecular docking, a technique for predicting ligand binding poses, is
crucial in structure-based drug design for understanding protein-ligand
interactions. Recent advancements in docking methods, particularly those
leveraging geometric deep learning (GDL), have demonstrated significant
efficiency and accuracy advantages over traditional sampling methods. Despite
these advancements, current methods are often tailored for specific docking
settings, and limitations such as the neglect of protein side-chain structures,
difficulties in handling large binding pockets, and challenges in predicting
physically valid structures exist. To accommodate various docking settings and
achieve accurate, efficient, and physically reliable docking, we propose a
novel two-stage docking framework, DeltaDock, consisting of pocket prediction
and site-specific docking. We innovatively reframe the pocket prediction task
as a pocket-ligand alignment problem rather than direct prediction in the first
stage. Then we follow a bi-level coarse-to-fine iterative refinement process to
perform site-specific docking. Comprehensive experiments demonstrate the
superior performance of DeltaDock. Notably, in the blind docking setting,
DeltaDock achieves a 31\% relative improvement over the docking success rate
compared with the previous state-of-the-art GDL model. With the consideration
of physical validity, this improvement increases to about 300\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sharing to learn and learning to share; Fitting together Meta-Learning,
  Multi-Task Learning, and Transfer Learning: A meta review 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.12146v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.12146v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richa Upadhyay, Ronald Phlypo, Rajkumar Saini, Marcus Liwicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating knowledge across different domains is an essential feature of
human learning. Learning paradigms such as transfer learning, meta-learning,
and multi-task learning reflect the human learning process by exploiting the
prior knowledge for new tasks, encouraging faster learning and good
generalization for new tasks. This article gives a detailed view of these
learning paradigms and their comparative analysis. The weakness of one learning
algorithm turns out to be a strength of another, and thus, merging them is a
prevalent trait in the literature. Numerous research papers focus on each of
these learning paradigms separately and provide a comprehensive overview of
them. However, this article reviews research studies that combine (two of)
these learning algorithms. This survey describes how these techniques are
combined to solve problems in many different fields of research, including
computer vision, natural language processing, hyper-spectral imaging, and many
more, in a supervised setting only. Based on the knowledge accumulated from the
literature, we hypothesize a generic task-agnostic and model-agnostic learning
network - an ensemble of meta-learning, transfer learning, and multi-task
learning, termed Multi-modal Multi-task Meta Transfer Learning. We also present
some open research questions, limitations, and future research directions for
this proposed network. The aim of this article is to spark interest among
scholars in effectively merging existing learning algorithms with the intention
of advancing research in this field. Instead of presenting experimental
results, we invite readers to explore and contemplate techniques for merging
algorithms while navigating through their limitations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article has been accepted for publication in IEEE Access. This
  is the author's version which has not been fully edited and content may
  slightly change prior to final publication. Citation information: DOI
  10.1109/ACCESS.2024.3478805</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex
  Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11190v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11190v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifei Xie, Changqiao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GPT-4o, an all-encompassing model, represents a milestone in the development
of large multi-modal language models. It can understand visual, auditory, and
textual modalities, directly output audio, and support flexible duplex
interaction. Models from the open-source community often achieve some
functionalities of GPT-4o, such as visual understanding and voice chat.
Nevertheless, training a unified model that incorporates all modalities is
challenging due to the complexities of multi-modal data, intricate model
architectures, and training processes. In this paper, we introduce Mini-Omni2,
a visual-audio assistant capable of providing real-time, end-to-end voice
responses to visoin and audio queries. By integrating pretrained visual and
auditory encoders, Mini-Omni2 maintains performance in individual modalities.
We propose a three-stage training process to align modalities, allowing the
language model to handle multi-modal inputs and outputs after training on a
limited dataset. For interaction, we introduce a command-based interruption
mechanism, enabling more flexible interaction with users. To the best of our
knowledge, Mini-Omni2 is one of the closest reproductions of GPT-4o, which have
similar form of functionality, and we hope it can offer valuable insights for
subsequent research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedCCRL: Federated Domain Generalization with Cross-Client
  Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11267v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11267v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinpeng Wang, Xiaoying Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain Generalization (DG) aims to train models that can effectively
generalize to unseen domains. However, in the context of Federated Learning
(FL), where clients collaboratively train a model without directly sharing
their data, most existing DG algorithms are not directly applicable to the FL
setting due to privacy constraints, as well as the limited data quantity and
domain diversity at each client. To tackle these challenges, we propose
FedCCRL, a novel federated domain generalization method that significantly
improves the model's ability to generalize to unseen domains without
compromising privacy or incurring excessive computational and communication
costs. Specifically, we adapt MixStyle to the federated setting to transfer
domain-specific features while AugMix is employed to perturb domain-invariant
features. Furthermore, we leverage supervised contrastive loss for
representation alignment and utilize Jensen-Shannon divergence to ensure
consistent predictions between original and augmented samples. Extensive
experimental results demonstrate that FedCCRL achieves the state-of-the-art
performances on the PACS, OfficeHome and miniDomainNet datasets across varying
numbers of clients. Code is available at
https://github.com/SanphouWang/FedCCRL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixed-Precision Federated Learning via Multi-Precision Over-The-Air
  Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03402v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03402v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinsheng Yuan, Zhuangkun Wei, Weisi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over-the-Air Federated Learning (OTA-FL) is a privacy-preserving distributed
learning mechanism, by aggregating updates in the electromagnetic channel
rather than at the server. A critical research gap in existing OTA-FL research
is the assumption of homogeneous client computational bit precision. While in
real world application, clients with varying hardware resources may exploit
approximate computing (AxC) to operate at different bit precisions optimized
for energy and computational efficiency. And model updates of various
precisions amongst clients poses an open challenge for OTA-FL, as it is
incompatible in the wireless modulation superposition. Here, we propose an
mixed-precision OTA-FL framework of clients with multiple bit precisions,
demonstrating the following innovations: (i) the superior trade-off for both
server and clients within the constraints of varying edge computing
capabilities, energy efficiency, and learning accuracy requirements comparing
to homogeneous client bit precision, and (ii) a multi-precision gradient
modulation scheme to ensure compatibility with OTA aggregation and eliminate
the overheads of precision conversion. Through case study with real world data,
we validate our modulation scheme that enables AxC based mixed-precision
OTA-FL. In comparison to homogeneous standard precision of 32-bit and 16-bit,
our framework presents more than 10% in 4-bit ultra low precision client
performance and over 65%and 13% of energy savings respectively. This
demonstrates the great potential of our mixed-precision OTA-FL approach in
heterogeneous edge computing environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to WCNC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parallel Momentum Methods Under Biased Gradient Estimations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00853v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00853v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Beikmohammadi, Sarit Khirirat, Sindri Magnússon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parallel stochastic gradient methods are gaining prominence in solving
large-scale machine learning problems that involve data distributed across
multiple nodes. However, obtaining unbiased stochastic gradients, which have
been the focus of most theoretical research, is challenging in many distributed
machine learning applications. The gradient estimations easily become biased,
for example, when gradients are compressed or clipped, when data is shuffled,
and in meta-learning and reinforcement learning. In this work, we establish
worst-case bounds on parallel momentum methods under biased gradient estimation
on both general non-convex and $\mu$-PL problems. Our analysis covers general
distributed optimization problems, and we work out the implications for special
cases where gradient estimates are biased, i.e. in meta-learning and when the
gradients are compressed or clipped. Our numerical experiments verify our
theoretical findings and show faster convergence performance of momentum
methods than traditional biased gradient descent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reverse Stable Diffusion: What <span class="highlight-title">prompt</span> was used to generate this image? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.01472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.01472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have recently attracted the interest of many
researchers, and inverting the diffusion process can play an important role in
better understanding the generative process and how to engineer prompts in
order to obtain the desired images. To this end, we study the task of
predicting the prompt embedding given an image generated by a generative
diffusion model. We consider a series of white-box and black-box models (with
and without access to the weights of the diffusion network) to deal with the
proposed task. We propose a novel learning framework comprising a joint prompt
regression and multi-label vocabulary classification objective that generates
improved prompts. To further improve our method, we employ a curriculum
learning procedure that promotes the learning of image-prompt pairs with lower
labeling noise (i.e. that are better aligned). We conduct experiments on the
DiffusionDB data set, predicting text prompts from images generated by Stable
Diffusion. In addition, we make an interesting discovery: training a diffusion
model on the prompt generation task can make the model generate images that are
much better aligned with the input prompts, when the model is directly reused
for text-to-image generation. Our code is publicly available for download at
https://github.com/CroitoruAlin/Reverse-Stable-Diffusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Computer Vision and Image Understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instruction-Guided Visual Masking <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinliang Zheng, Jianxiong Li, Sijie Cheng, Yinan Zheng, Jiaming Li, Jihao Liu, Yu Liu, Jingjing Liu, Xianyuan Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction following is crucial in contemporary LLM. However, when extended
to multimodal setting, it often suffers from misalignment between specific
textual instruction and targeted local region of an image. To achieve more
accurate and nuanced multimodal instruction following, we introduce
Instruction-guided Visual Masking (IVM), a new versatile visual grounding model
that is compatible with diverse multimodal models, such as LMM and robot model.
By constructing visual masks for instruction-irrelevant regions, IVM-enhanced
multimodal models can effectively focus on task-relevant image regions to
better align with complex instructions. Specifically, we design a visual
masking data generation pipeline and create an IVM-Mix-1M dataset with 1
million image-instruction pairs. We further introduce a new learning technique,
Discriminator Weighted Supervised Learning (DWSL) for preferential IVM training
that prioritizes high-quality data samples. Experimental results on generic
multimodal tasks such as VQA and embodied robotic control demonstrate the
versatility of IVM, which as a plug-and-play tool, significantly boosts the
performance of diverse multimodal models, yielding new state-of-the-art results
across challenging multimodal benchmarks. Code, model and data are available at
https://github.com/2toinf/IVM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collocation-based Robust Variational Physics-Informed Neural Networks
  (CRVPINN) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02300v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02300v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcin Łoś, Tomasz Służalec, Paweł Maczuga, Askold Vilkha, Carlos Uriarte, Maciej Paszyński
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-Informed Neural Networks (PINNs) have been successfully applied to
solve Partial Differential Equations (PDEs). Their loss function is founded on
a strong residual minimization scheme. Variational Physics-Informed Neural
Networks (VPINNs) are their natural extension to weak variational settings. In
this context, the recent work of Robust Variational Physics-Informed Neural
Networks (RVPINNs) highlights the importance of conveniently translating the
norms of the underlying continuum-level spaces to the discrete level.
Otherwise, VPINNs might become unrobust, implying that residual minimization
might be highly uncorrelated with a desired minimization of the error in the
energy norm. However, applying this robustness to VPINNs typically entails
dealing with the inverse of a Gram matrix, usually producing slow convergence
speeds during training. In this work, we accelerate the implementation of
RVPINN, establishing a LU factorization of sparse Gram matrix in a kind of
point-collocation scheme with the same spirit as original PINNs. We call out
method the Collocation-based Robust Variational Physics Informed Neural
Networks (CRVPINN). We test our efficient CRVPINN algorithm on Laplace,
advection-diffusion, and Stokes problems in two spatial dimensions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Manifolds, Random Matrices and Spectral Gaps: The geometric phases of
  generative diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05898v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05898v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrico Ventura, Beatrice Achilli, Gianluigi Silvestri, Carlo Lucibello, Luca Ambrogioni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the latent geometry of generative diffusion
models under the manifold hypothesis. To this purpose, we analyze the spectrum
of eigenvalues (and singular values) of the Jacobian of the score function,
whose discontinuities (gaps) reveal the presence and dimensionality of distinct
sub-manifolds. Using a statistical physics approach, we derive the spectral
distributions and formulas for the spectral gaps under several distributional
assumptions and we compare these theoretical predictions with the spectra
estimated from trained networks. Our analysis reveals the existence of three
distinct qualitative phases during the generative process: a trivial phase; a
manifold coverage phase where the diffusion process fits the distribution
internal to the manifold; a consolidation phase where the score becomes
orthogonal to the manifold and all particles are projected on the support of
the data. This `division of labor' between different timescales provides an
elegant explanation on why generative diffusion models are not affected by the
manifold overfitting phenomenon that plagues likelihood-based models, since the
internal distribution and the manifold geometry are produced at different time
points during generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Search-Based Testing with Pareto Optimization Effectively Cover
  Failure-Revealing Test Inputs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lev Sorokin, Damir Safin, Shiva Nejati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Search-based software testing (SBST) is a widely adopted technique for
testing complex systems with large input spaces, such as Deep Learning-enabled
(DL-enabled) systems. Many SBST techniques focus on Pareto-based optimization,
where multiple objectives are optimized in parallel to reveal failures.
However, it is important to ensure that identified failures are spread
throughout the entire failure-inducing area of a search domain and not
clustered in a sub-region. This ensures that identified failures are
semantically diverse and reveal a wide range of underlying causes. In this
paper, we present a theoretical argument explaining why testing based on Pareto
optimization is inadequate for covering failure-inducing areas within a search
domain. We support our argument with empirical results obtained by applying two
widely used types of Pareto-based optimization techniques, namely NSGA-II (an
evolutionary algorithm) and OMOPSO (a swarm-based Pareto-optimization
algorithm), to two DL-enabled systems: an industrial Automated Valet Parking
(AVP) system and a system for classifying handwritten digits. We measure the
coverage of failure-revealing test inputs in the input space using a metric
that we refer to as the Coverage Inverted Distance quality indicator. Our
results show that NSGA-II-based search and OMOPSO are not more effective than a
na\"ive random search baseline in covering test inputs that reveal failures.
The replication package for this study is available in a GitHub repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication by Empirical Software Engineering Journal
  (EMSE) (in October 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Survey of Out-of-distribution Generalization for Graph Machine
  Learning from a Causal View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09858v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09858v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph machine learning (GML) has been successfully applied across a wide
range of tasks. Nonetheless, GML faces significant challenges in generalizing
over out-of-distribution (OOD) data, which raises concerns about its wider
applicability. Recent advancements have underscored the crucial role of
causality-driven approaches in overcoming these generalization challenges.
Distinct from traditional GML methods that primarily rely on statistical
dependencies, causality-focused strategies delve into the underlying causal
mechanisms of data generation and model prediction, thus significantly
improving the generalization of GML across different environments. This paper
offers a thorough review of recent progress in causality-involved GML
generalization. We elucidate the fundamental concepts of employing causality to
enhance graph model generalization and categorize the various approaches,
providing detailed descriptions of their methodologies and the connections
among them. Furthermore, we explore the incorporation of causality in other
related important areas of trustworthy GML, such as explanation, fairness, and
robustness. Concluding with a discussion on potential future research
directions, this review seeks to articulate the continuing development and
future potential of causality in enhancing the trustworthiness of graph machine
learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PIVOT-R: Primitive-Driven Waypoint-Aware World Model for Robotic
  Manipulation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10394v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10394v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaidong Zhang, Pengzhen Ren, Bingqian Lin, Junfan Lin, Shikui Ma, Hang Xu, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language-guided robotic manipulation is a challenging task that requires an
embodied agent to follow abstract user instructions to accomplish various
complex manipulation tasks. Previous work trivially fitting the data without
revealing the relation between instruction and low-level executable actions,
these models are prone to memorizing the surficial pattern of the data instead
of acquiring the transferable knowledge, and thus are fragile to dynamic
environment changes. To address this issue, we propose a PrIrmitive-driVen
waypOinT-aware world model for Robotic manipulation (PIVOT-R) that focuses
solely on the prediction of task-relevant waypoints. Specifically, PIVOT-R
consists of a Waypoint-aware World Model (WAWM) and a lightweight action
prediction module. The former performs primitive action parsing and
primitive-driven waypoint prediction, while the latter focuses on decoding
low-level actions. Additionally, we also design an asynchronous hierarchical
executor (AHE), which can use different execution frequencies for different
modules of the model, thereby helping the model reduce computational redundancy
and improve model execution efficiency. Our PIVOT-R outperforms
state-of-the-art (SoTA) open-source models on the SeaWave benchmark, achieving
an average relative improvement of 19.45% across four levels of instruction
tasks. Moreover, compared to the synchronously executed PIVOT-R, the execution
efficiency of PIVOT-R with AHE is increased by 28-fold, with only a 2.9% drop
in performance. These results provide compelling evidence that our PIVOT-R can
significantly improve both the performance and efficiency of robotic
manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding What Affects the Generalization Gap in Visual
  Reinforcement Learning: Theory and Empirical Evidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02701v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02701v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiafei Lyu, Le Wan, Xiu Li, Zongqing Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there are many efforts attempting to learn useful policies for
continuous control in visual reinforcement learning (RL). In this scenario, it
is important to learn a generalizable policy, as the testing environment may
differ from the training environment, e.g., there exist distractors during
deployment. Many practical algorithms are proposed to handle this problem.
However, to the best of our knowledge, none of them provide a theoretical
understanding of what affects the generalization gap and why their proposed
methods work. In this paper, we bridge this issue by theoretically answering
the key factors that contribute to the generalization gap when the testing
environment has distractors. Our theories indicate that minimizing the
representation distance between training and testing environments, which aligns
with human intuition, is the most critical for the benefit of reducing the
generalization gap. Our theoretical results are supported by the empirical
evidence in the DMControl Generalization Benchmark (DMC-GB).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Journal of Artificial Intelligence Research (JAIR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nonconvex Stochastic Bregman Proximal Gradient Method with Application
  to Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.14522v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.14522v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuangyu Ding, Jingyang Li, Kim-Chuan Toh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic gradient methods for minimizing nonconvex composite objective
functions typically rely on the Lipschitz smoothness of the differentiable
part, but this assumption fails in many important problem classes like
quadratic inverse problems and neural network training, leading to instability
of the algorithms in both theory and practice. To address this, we propose a
family of stochastic Bregman proximal gradient (SBPG) methods that only require
smooth adaptivity. SBPG replaces the quadratic approximation in SGD with a
Bregman proximity measure, offering a better approximation model that handles
non-Lipschitz gradients in nonconvex objectives. We establish the convergence
properties of vanilla SBPG and show it achieves optimal sample complexity in
the nonconvex setting. Experimental results on quadratic inverse problems
demonstrate SBPG's robustness in terms of stepsize selection and sensitivity to
the initial point. Furthermore, we introduce a momentum-based variant, MSBPG,
which enhances convergence by relaxing the mini-batch size requirement while
preserving the optimal oracle complexity. We apply MSBPG to the training of
deep neural networks, utilizing a polynomial kernel function to ensure smooth
adaptivity of the loss function. Experimental results on benchmark datasets
confirm the effectiveness and robustness of MSBPG in training neural networks.
Given its negligible additional computational cost compared to SGD in
large-scale optimization, MSBPG shows promise as a universal open-source
optimizer for future applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A SARS-CoV-2 Interaction Dataset and VHH Sequence Corpus for Antibody
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18749v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18749v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hirofumi Tsuruta, Hiroyuki Yamazaki, Ryota Maeda, Ryotaro Tamura, Akihiro Imura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Antibodies are crucial proteins produced by the immune system to eliminate
harmful foreign substances and have become pivotal therapeutic agents for
treating human diseases. To accelerate the discovery of antibody therapeutics,
there is growing interest in constructing language models using antibody
sequences. However, the applicability of pre-trained language models for
antibody discovery has not been thoroughly evaluated due to the scarcity of
labeled datasets. To overcome these limitations, we introduce AVIDa-SARS-CoV-2,
a dataset featuring the antigen-variable domain of heavy chain of heavy chain
antibody (VHH) interactions obtained from two alpacas immunized with severe
acute respiratory syndrome coronavirus 2 (SARS-CoV-2) spike proteins.
AVIDa-SARS-CoV-2 includes binary labels indicating the binding or non-binding
of diverse VHH sequences to 12 SARS-CoV-2 mutants, such as the Delta and
Omicron variants. Furthermore, we release VHHCorpus-2M, a pre-training dataset
for antibody language models, containing over two million VHH sequences. We
report benchmark results for predicting SARS-CoV-2-VHH binding using VHHBERT
pre-trained on VHHCorpus-2M and existing general protein and antibody-specific
pre-trained language models. These results confirm that AVIDa-SARS-CoV-2
provides valuable benchmarks for evaluating the representation capabilities of
antibody language models for binding prediction, thereby facilitating the
development of AI-driven antibody discovery. The datasets are available at
https://datasets.cognanous.com.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benign Overfitting under Learning Rate Conditions for $α$
  Sub-exponential Input 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00733v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00733v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kota Okudo, Kei Kobayashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the phenomenon of benign overfitting in binary
classification problems with heavy-tailed input distributions, extending the
analysis of maximum margin classifiers to $\alpha$ sub-exponential
distributions ($\alpha \in (0, 2]$). This generalizes previous work focused on
sub-gaussian inputs. We provide generalization error bounds for linear
classifiers trained using gradient descent on unregularized logistic loss in
this heavy-tailed setting. Our results show that, under certain conditions on
the dimensionality $p$ and the distance between the centers of the
distributions, the misclassification error of the maximum margin classifier
asymptotically approaches the noise level, the theoretical optimal value.
Moreover, we derive an upper bound on the learning rate $\beta$ for benign
overfitting to occur and show that as the tail heaviness of the input
distribution $\alpha$ increases, the upper bound on the learning rate
decreases. These results demonstrate that benign overfitting persists even in
settings with heavier-tailed inputs than previously studied, contributing to a
deeper understanding of the phenomenon in more realistic data environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Changes in Nation Perception with Nationality-Assigned
  Personas in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13993v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13993v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahammed Kamruzzaman, Gene Louis Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Persona assignment has become a common strategy for customizing LLM use to
particular tasks and contexts. In this study, we explore how evaluation of
different nations change when LLMs are assigned specific nationality personas.
We assign 193 different nationality personas (e.g., an American person) to four
LLMs and examine how the LLM evaluations (or ''perceptions'')of countries
change. We find that all LLM-persona combinations tend to favor Western
European nations, though nation-personas push LLM behaviors to focus more on
and treat the nation-persona's own region more favorably. Eastern European,
Latin American, and African nations are treated more negatively by different
nationality personas. We additionally find that evaluations by nation-persona
LLMs of other nations correlate with human survey responses but fail to match
the values closely. Our study provides insight into how biases and stereotypes
are realized within LLMs when adopting different national personas. In line
with the ''Blueprint for an AI Bill of Rights'', our findings underscore the
critical need for developing mechanisms to ensure that LLM outputs promote
fairness and avoid over-generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print, Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Models: What Do They Know? Do They Know Things? Let's Find
  Out! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17137v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17137v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaodan Du, Nicholas Kolkin, Greg Shakhnarovich, Anand Bhattad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models excel at mimicking real scenes, suggesting they might
inherently encode important intrinsic scene properties. In this paper, we aim
to explore the following key questions: (1) What intrinsic knowledge do
generative models like GANs, Autoregressive models, and Diffusion models
encode? (2) Can we establish a general framework to recover intrinsic
representations from these models, regardless of their architecture or model
type? (3) How minimal can the required learnable parameters and labeled data be
to successfully recover this knowledge? (4) Is there a direct link between the
quality of a generative model and the accuracy of the recovered scene
intrinsics?
  Our findings indicate that a small Low-Rank Adaptators (LoRA) can recover
intrinsic images-depth, normals, albedo and shading-across different generators
(Autoregressive, GANs and Diffusion) while using the same decoder head that
generates the image. As LoRA is lightweight, we introduce very few learnable
parameters (as few as 0.04% of Stable Diffusion model weights for a rank of 2),
and we find that as few as 250 labeled images are enough to generate intrinsic
images with these LoRA modules. Finally, we also show a positive correlation
between the generative model's quality and the accuracy of the recovered
intrinsics through control experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://intrinsic-lora.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptation Odyssey in LLMs: Why Does Additional <span class="highlight-title">Pretrain</span>ing Sometimes
  Fail to Improve? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fırat Öncel, Matthias Bethge, Beyza Ermis, Mirco Ravanelli, Cem Subakan, Çağatay Yıldız
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last decade, the generalization and adaptation abilities of deep
learning models were typically evaluated on fixed training and test
distributions. Contrary to traditional deep learning, large language models
(LLMs) are (i) even more overparameterized, (ii) trained on unlabeled text
corpora curated from the Internet with minimal human intervention, and (iii)
trained in an online fashion. These stark contrasts prevent researchers from
transferring lessons learned on model generalization and adaptation in deep
learning contexts to LLMs. To this end, our short paper introduces empirical
observations that aim to shed light on further training of already pretrained
language models. Specifically, we demonstrate that training a model on a text
domain could degrade its perplexity on the test portion of the same domain. We
observe with our subsequent analysis that the performance degradation is
positively correlated with the similarity between the additional and the
original pretraining dataset of the LLM. Our further token-level perplexity
observations reveals that the perplexity degradation is due to a handful of
tokens that are not informative about the domain. We hope these findings will
guide us in determining when to adapt a model vs when to rely on its
foundational capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conv-Basis: A New Paradigm for Efficient Attention Inference and
  Gradient Computation in Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingyu Liang, Heshan Liu, Zhenmei Shi, Zhao Song, Zhuoyan Xu, Junze Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The self-attention mechanism is the key to the success of transformers in
recent Large Language Models (LLMs). However, the quadratic computational cost
$O(n^2)$ in the input sequence length $n$ is a notorious obstacle for further
improvement and scalability in longer contexts. In this work, we leverage the
convolution-like structure of attention matrices to develop an efficient
approximation method for attention computation using convolution matrices. We
propose a $\mathsf{conv}$ basis system, analogous to the rank basis, and show
that any lower triangular matrix can always be decomposed as a sum of
structured convolution matrices in this basis. We then design a fast algorithm
to approximate the attention matrix via a sum of such $k$ convolution matrices.
This allows us to compute the attention {\it inference} via Fast Fourier
Transforms (FFT) in $O(knd \log n)$ time, where $d$ is the hidden dimension,
and thus achieve almost linear time $n^{1+o(1)}$ in the practical scenario
where $kd = n^{o(1)}$. Furthermore, the attention {\it training forward} and
{\it backward gradient} can be computed in $n^{1+o(1)}$ as well. We provide
theoretical guarantees on the run time and approximation error and conduct
preliminary experiments to evaluate its effectiveness. We hope our new paradigm
for accelerating attention computation in transformer models can help their
application to longer contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Physically Consistent Deep Learning For Climate Model
  Parameterizations <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03920v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03920v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Birgit Kühbacher, Fernando Iglesias-Suarez, Niki Kilbertus, Veronika Eyring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Climate models play a critical role in understanding and projecting climate
change. Due to their complexity, their horizontal resolution of about 40-100 km
remains too coarse to resolve processes such as clouds and convection, which
need to be approximated via parameterizations. These parameterizations are a
major source of systematic errors and large uncertainties in climate
projections. Deep learning (DL)-based parameterizations, trained on data from
computationally expensive short, high-resolution simulations, have shown great
promise for improving climate models in that regard. However, their lack of
interpretability and tendency to learn spurious non-physical correlations
result in reduced trust in the climate simulation. We propose an efficient
supervised learning framework for DL-based parameterizations that leads to
physically consistent models with improved interpretability and negligible
computational overhead compared to standard supervised training. First, key
features determining the target physical processes are uncovered. Subsequently,
the neural network is fine-tuned using only those relevant features. We show
empirically that our method robustly identifies a small subset of the inputs as
actual physical drivers, therefore removing spurious non-physical
relationships. This results in by design physically consistent and
interpretable neural networks while maintaining the predictive performance of
unconstrained black-box DL-based parameterizations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICMLA 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via
  Lightweight Value Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingqi Wang, Xiaoyuan Yi, Xing Xie, Jia Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion models trained on large-scale data have
enabled the generation of indistinguishable human-level images, yet they often
produce harmful content misaligned with human values, e.g., social bias, and
offensive content. Despite extensive research on Large Language Models (LLMs),
the challenge of Text-to-Image (T2I) model alignment remains largely
unexplored. Addressing this problem, we propose LiVO (Lightweight Value
Optimization), a novel lightweight method for aligning T2I models with human
values. LiVO only optimizes a plug-and-play value encoder to integrate a
specified value principle with the input prompt, allowing the control of
generated images over both semantics and values. Specifically, we design a
diffusion model-tailored preference optimization loss, which theoretically
approximates the Bradley-Terry model used in LLM alignment but provides a more
flexible trade-off between image quality and value conformity. To optimize the
value encoder, we also develop a framework to automatically construct a
text-image preference dataset of 86k (prompt, aligned image, violating image,
value principle) samples. Without updating most model parameters and through
adaptive value selection from the input prompt, LiVO significantly reduces
harmful outputs and achieves faster convergence, surpassing several strong
baselines and taking an initial step towards ethically aligned T2I models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Multimedia 2024. The dataset and code can be found at
  https://github.com/achernarwang/LiVO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Coarse-Grained Matching in Video-Text Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aozhu Chen, Hazel Doughty, Xirong Li, Cees G. M. Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-text retrieval has seen significant advancements, yet the ability of
models to discern subtle differences in captions still requires verification.
In this paper, we introduce a new approach for fine-grained evaluation. Our
approach can be applied to existing datasets by automatically generating hard
negative test captions with subtle single-word variations across nouns, verbs,
adjectives, adverbs, and prepositions. We perform comprehensive experiments
using four state-of-the-art models across two standard benchmarks (MSR-VTT and
VATEX) and two specially curated datasets enriched with detailed descriptions
(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our
analyses show that the current evaluation benchmarks fall short in detecting a
model's ability to perceive subtle single-word differences, 2) our fine-grained
evaluation highlights the difficulty models face in distinguishing such subtle
variations. To enhance fine-grained understanding, we propose a new baseline
that can be easily combined with current methods. Experiments on our
fine-grained evaluations demonstrate that this approach enhances a model's
ability to understand fine-grained differences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Bjøntegaard Delta for Compression Efficiency Evaluation:
  Are We Calculating It Precisely and Reliably? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Hang, Shenpeng Song, Zhimeng Huang, Chuanmin Jia, Siwei Ma, Wen Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For decades, the Bj{\o}ntegaard Delta (BD) has been the metric for evaluating
codec Rate-Distortion (R-D) performance. Yet, in most studies, BD is determined
using just 4-5 R-D data points, could this be sufficient? As codecs and quality
metrics advance, does the conventional BD estimation still hold up? Crucially,
are the performance improvements of new codecs and tools genuine, or merely
artifacts of estimation flaws? This paper addresses these concerns by
reevaluating BD estimation. We present a novel approach employing a
parameterized deep neural network to model R-D curves with high precision
across various metrics, accompanied by a comprehensive R-D dataset. This
approach both assesses the reliability of BD calculations and serves as a
precise BD estimator. Our findings advocate for the adoption of rigorous R-D
sampling and reliability metrics in future compression research to ensure the
validity and reliability of results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmnixR: Evaluating Omni-modality Language Models on Reasoning across
  Modalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lichang Chen, Hexiang Hu, Mingda Zhang, Yiwen Chen, Zifeng Wang, Yandong Li, Pranav Shyam, Tianyi Zhou, Heng Huang, Ming-Hsuan Yang, Boqing Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce OmnixR, an evaluation suite designed to benchmark SoTA
Omni-modality Language Models, such as GPT-4o and Gemini. Evaluating OLMs,
which integrate multiple modalities such as text, vision, and audio, presents
unique challenges. Particularly, the user message might often consist of
multiple modalities, such that OLMs have to establish holistic understanding
and reasoning across modalities to accomplish the task. Existing benchmarks are
limited to single modality or dual-modality tasks, overlooking comprehensive
multi-modal assessments of model reasoning. To address this, OmnixR offers two
evaluation variants: (1)synthetic subset: a synthetic dataset generated
automatically by translating text into multiple modalities--audio, images,
video, and hybrids (Omnify). (2)realistic subset: a real-world dataset,
manually curated and annotated by experts, for evaluating cross-modal reasoning
in natural settings. OmnixR presents a unique evaluation towards assessing OLMs
over a diverse mix of modalities, such as a question that involves video,
audio, and text, providing a rigorous cross-modal reasoning testbed unlike any
existing benchmarks. Our experiments find that all state-of-the-art OLMs
struggle with OmnixR questions that require integrating information from
multiple modalities to answer. Further analysis highlights differences in
reasoning behavior, underscoring the challenges of omni-modal AI alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test-time adaptation for image compression with distribution
  regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kecheng Chen, Pingping Zhang, Tiexin Qin, Shiqi Wang, Hong Yan, Haoliang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current test- or compression-time adaptation image compression (TTA-IC)
approaches, which leverage both latent and decoder refinements as a two-step
adaptation scheme, have potentially enhanced the rate-distortion (R-D)
performance of learned image compression models on cross-domain compression
tasks, \textit{e.g.,} from natural to screen content images. However, compared
with the emergence of various decoder refinement variants, the latent
refinement, as an inseparable ingredient, is barely tailored to cross-domain
scenarios. To this end, we aim to develop an advanced latent refinement method
by extending the effective hybrid latent refinement (HLR) method, which is
designed for \textit{in-domain} inference improvement but shows noticeable
degradation of the rate cost in \textit{cross-domain} tasks. Specifically, we
first provide theoretical analyses, in a cue of marginalization approximation
from in- to cross-domain scenarios, to uncover that the vanilla HLR suffers
from an underlying mismatch between refined Gaussian conditional and hyperprior
distributions, leading to deteriorated joint probability approximation of
marginal distribution with increased rate consumption. To remedy this issue, we
introduce a simple Bayesian approximation-endowed \textit{distribution
regularization} to encourage learning a better joint probability approximation
in a plug-and-play manner. Extensive experiments on six in- and cross-domain
datasets demonstrate that our proposed method not only improves the R-D
performance compared with other latent refinement counterparts, but also can be
flexibly integrated into existing TTA-IC methods with incremental benefits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Comparison for Dataset-Level Membership Inference in Large
  (Vision-)Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Ren, Kangrui Chen, Chen Chen, Vikash Sehwag, Yue Xing, Jiliang Tang, Lingjuan Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) and Vision-Language Models (VLMs) have made
significant advancements in a wide range of natural language processing and
vision-language tasks. Access to large web-scale datasets has been a key factor
in their success. However, concerns have been raised about the unauthorized use
of copyrighted materials and potential copyright infringement. Existing
methods, such as sample-level Membership Inference Attacks (MIA) and
distribution-based dataset inference, distinguish member data (data used for
training) and non-member data by leveraging the common observation that models
tend to memorize and show greater confidence in member data. Nevertheless,
these methods face challenges when applied to LLMs and VLMs, such as the
requirement for ground-truth member data or non-member data that shares the
same distribution as the test data. In this paper, we propose a novel
dataset-level membership inference method based on Self-Comparison. We find
that a member prefix followed by a non-member suffix (paraphrased from a member
suffix) can further trigger the model's memorization on training data. Instead
of directly comparing member and non-member data, we introduce paraphrasing to
the second half of the sequence and evaluate how the likelihood changes before
and after paraphrasing. Unlike prior approaches, our method does not require
access to ground-truth member data or non-member data in identical
distribution, making it more practical. Extensive experiments demonstrate that
our proposed method outperforms traditional MIA and dataset inference
techniques across various datasets and models, including including public
models, fine-tuned models, and API-based commercial models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MuVi: Video-to-Music Generation with Semantic Alignment and Rhythmic
  Synchronization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiqi Li, Siqi Zheng, Xize Cheng, Ziang Zhang, Shengpeng Ji, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating music that aligns with the visual content of a video has been a
challenging task, as it requires a deep understanding of visual semantics and
involves generating music whose melody, rhythm, and dynamics harmonize with the
visual narratives. This paper presents MuVi, a novel framework that effectively
addresses these challenges to enhance the cohesion and immersive experience of
audio-visual content. MuVi analyzes video content through a specially designed
visual adaptor to extract contextually and temporally relevant features. These
features are used to generate music that not only matches the video's mood and
theme but also its rhythm and pacing. We also introduce a contrastive
music-visual pre-training scheme to ensure synchronization, based on the
periodicity nature of music phrases. In addition, we demonstrate that our
flow-matching-based music generator has in-context learning ability, allowing
us to control the style and genre of the generated music. Experimental results
show that MuVi demonstrates superior performance in both audio quality and
temporal synchronization. The generated music video samples are available at
https://muvi-v2m.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-to-Audio Generation with Hidden Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manjie Xu, Chenxing Li, Xinyi Tu, Yong Ren, Rilin Chen, Yu Gu, Wei Liang, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating semantically and temporally aligned audio content in accordance
with video input has become a focal point for researchers, particularly
following the remarkable breakthrough in text-to-video generation. In this
work, we aim to offer insights into the video-to-audio generation paradigm,
focusing on three crucial aspects: vision encoders, auxiliary embeddings, and
data augmentation techniques. Beginning with a foundational model built on a
simple yet surprisingly effective intuition, we explore various vision encoders
and auxiliary embeddings through ablation studies. Employing a comprehensive
evaluation pipeline that emphasizes generation quality and video-audio
synchronization alignment, we demonstrate that our model exhibits
state-of-the-art video-to-audio generation capabilities. Furthermore, we
provide critical insights into the impact of different data augmentation
methods on enhancing the generation framework's overall capacity. We showcase
possibilities to advance the challenge of generating synchronized audio from
semantic and temporal perspectives. We hope these insights will serve as a
stepping stone toward developing more realistic and accurate audio-visual
generation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://sites.google.com/view/vta-ldm</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-15T00:00:00Z">2024-10-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OMCAT: Omni Context Aware Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arushi Goel, Karan Sapra, Matthieu Le, Rafael Valle, Andrew Tao, Bryan Catanzaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have made significant strides in text generation
and comprehension, with recent advancements extending into multimodal LLMs that
integrate visual and audio inputs. However, these models continue to struggle
with fine-grained, cross-modal temporal understanding, particularly when
correlating events across audio and video streams. We address these challenges
with two key contributions: a new dataset and model, called OCTAV and OMCAT
respectively. OCTAV (Omni Context and Temporal Audio Video) is a novel dataset
designed to capture event transitions across audio and video. Second, OMCAT
(Omni Context Aware Transformer) is a powerful model that leverages RoTE
(Rotary Time Embeddings), an innovative extension of RoPE, to enhance temporal
grounding and computational efficiency in time-anchored tasks. Through a robust
three-stage training pipeline-feature alignment, instruction tuning, and
OCTAV-specific training-OMCAT excels in cross-modal temporal understanding. Our
model demonstrates state-of-the-art performance on Audio-Visual Question
Answering (AVQA) tasks and the OCTAV benchmark, showcasing significant gains in
temporal reasoning and cross-modal alignment, as validated through
comprehensive experiments and ablation studies. Our dataset and code will be
made publicly available. The link to our demo page is https://om-cat.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Demo page: https://om-cat.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SplatPose+: Real-time Image-Based Pose-Agnostic 3D Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhe Liu, Yan Song Hu, Yuhao Chen, John Zelek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-based Pose-Agnostic 3D Anomaly Detection is an important task that has
emerged in industrial quality control. This task seeks to find anomalies from
query images of a tested object given a set of reference images of an
anomaly-free object. The challenge is that the query views (a.k.a poses) are
unknown and can be different from the reference views. Currently, new methods
such as OmniposeAD and SplatPose have emerged to bridge the gap by synthesizing
pseudo reference images at the query views for pixel-to-pixel comparison.
However, none of these methods can infer in real-time, which is critical in
industrial quality control for massive production. For this reason, we propose
SplatPose+, which employs a hybrid representation consisting of a Structure
from Motion (SfM) model for localization and a 3D Gaussian Splatting (3DGS)
model for Novel View Synthesis. Although our proposed pipeline requires the
computation of an additional SfM model, it offers real-time inference speeds
and faster training compared to SplatPose. Quality-wise, we achieved a new SOTA
on the Pose-agnostic Anomaly Detection benchmark with the Multi-Pose Anomaly
Detection (MAD-SIM) dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WeatherDG: LLM-assisted Procedural Weather Generation for
  Domain-Generalized Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenghao Qian, Yuhu Guo, Yuhong Mo, Wenjing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a novel approach, namely WeatherDG, that can
generate realistic, weather-diverse, and driving-screen images based on the
cooperation of two foundation models, i.e, Stable Diffusion (SD) and Large
Language Model (LLM). Specifically, we first fine-tune the SD with source data,
aligning the content and layout of generated samples with real-world driving
scenarios. Then, we propose a procedural prompt generation method based on LLM,
which can enrich scenario descriptions and help SD automatically generate more
diverse, detailed images. In addition, we introduce a balanced generation
strategy, which encourages the SD to generate high-quality objects of tailed
classes under various weather conditions, such as riders and motorcycles. This
segmentation-model-agnostic method can improve the generalization ability of
existing models by additionally adapting them with the generated synthetic
data. Experiments on three challenging datasets show that our method can
significantly improve the segmentation performance of different
state-of-the-art models on target domains. Notably, in the setting of
''Cityscapes to ACDC'', our method improves the baseline HRDA by 13.9% in mIoU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ nvTorchCam: An Open-source Library for Camera-Agnostic Differentiable
  Geometric Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Lichy, Hang Su, Abhishek Badki, Jan Kautz, Orazio Gallo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce nvTorchCam, an open-source library under the Apache 2.0 license,
designed to make deep learning algorithms camera model-independent. nvTorchCam
abstracts critical camera operations such as projection and unprojection,
allowing developers to implement algorithms once and apply them across diverse
camera models--including pinhole, fisheye, and 360 equirectangular panoramas,
which are commonly used in automotive and real estate capture applications.
Built on PyTorch, nvTorchCam is fully differentiable and supports GPU
acceleration and batching for efficient computation. Furthermore, deep learning
models trained for one camera type can be directly transferred to other camera
types without requiring additional modification. In this paper, we provide an
overview of nvTorchCam, its functionality, and present various code examples
and diagrams to demonstrate its usage. Source code and installation
instructions can be found on the nvTorchCam GitHub page at
https://github.com/NVlabs/nvTorchCam.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Source code and installation instructions are available at
  https://github.com/NVlabs/nvTorchCam</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ V3D-SLAM: Robust RGB-D SLAM in Dynamic Environments with 3D Semantic
  Geometry Voting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan Dang, Khang Nguyen, Mandfred Huber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous localization and mapping (SLAM) in highly dynamic environments
is challenging due to the correlation complexity between moving objects and the
camera pose. Many methods have been proposed to deal with this problem;
however, the moving properties of dynamic objects with a moving camera remain
unclear. Therefore, to improve SLAM's performance, minimizing disruptive events
of moving objects with a physical understanding of 3D shapes and dynamics of
objects is needed. In this paper, we propose a robust method, V3D-SLAM, to
remove moving objects via two lightweight re-evaluation stages, including
identifying potentially moving and static objects using a spatial-reasoned
Hough voting mechanism and refining static objects by detecting dynamic noise
caused by intra-object motions using Chamfer distances as similarity
measurements. Our experiment on the TUM RGB-D benchmark on dynamic sequences
with ground-truth camera trajectories showed that our methods outperform the
most recent state-of-the-art SLAM methods. Our source code is available at
https://github.com/tuantdang/v3d-slam.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SOE: SO(3)-Equivariant 3D MRI Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shizhe He, Magdalini Paschali, Jiahong Ouyang, Adnan Masood, Akshay Chaudhari, Ehsan Adeli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation learning has become increasingly important, especially as
powerful models have shifted towards learning latent representations before
fine-tuning for downstream tasks. This approach is particularly valuable in
leveraging the structural information within brain anatomy. However, a common
limitation of recent models developed for MRIs is their tendency to ignore or
remove geometric information, such as translation and rotation, thereby
creating invariance with respect to geometric operations. We contend that
incorporating knowledge about these geometric transformations into the model
can significantly enhance its ability to learn more detailed anatomical
information within brain structures. As a result, we propose a novel method for
encoding 3D MRIs that enforces equivariance with respect to all rotations in 3D
space, in other words, SO(3)-equivariance (SOE). By explicitly modeling this
geometric equivariance in the representation space, we ensure that any
rotational operation applied to the input image space is also reflected in the
embedding representation space. This approach requires moving beyond
traditional representation learning methods, as we need a representation vector
space that allows for the application of the same SO(3) operation in that
space. To facilitate this, we leverage the concept of vector neurons. The
representation space formed by our method captures the brain's structural and
anatomical information more effectively. We evaluate SOE pretrained on the
structural MRIs of two public data sets with respect to the downstream task of
predicting age and diagnosing Alzheimer's Disease from T1-weighted brain scans
of the ADNI data set. We demonstrate that our approach not only outperforms
other methods but is also robust against various degrees of rotation along
different axes. The code is available at
https://github.com/shizhehe/SOE-representation-learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learned Neural Physics Simulation for Articulated 3D Human Pose
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mykhaylo Andriluka, Baruch Tabanpour, C. Daniel Freeman, Cristian Sminchisescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel neural network approach, LARP (Learned Articulated Rigid
body Physics), to model the dynamics of articulated human motion with contact.
Our goal is to develop a faster and more convenient methodological alternative
to traditional physics simulators for use in computer vision tasks such as
human motion reconstruction from video. To that end we introduce a training
procedure and model components that support the construction of a recurrent
neural architecture to accurately simulate articulated rigid body dynamics. Our
neural architecture supports features typically found in traditional physics
simulators, such as modeling of joint motors, variable dimensions of body
parts, contact between body parts and objects, and is an order of magnitude
faster than traditional systems when multiple simulations are run in parallel.
To demonstrate the value of LARP we use it as a drop-in replacement for a state
of the art classical non-differentiable simulator in an existing video-based
reconstruction framework and show comparative or better 3D human pose
reconstruction accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LocoMotion: Learning Motion-Focused Video-Language Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hazel Doughty, Fida Mohammad Thoker, Cees G. M. Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper strives for motion-focused video-language representations.
Existing methods to learn video-language representations use spatial-focused
data, where identifying the objects and scene is often enough to distinguish
the relevant caption. We instead propose LocoMotion to learn from
motion-focused captions that describe the movement and temporal progression of
local object motions. We achieve this by adding synthetic motions to videos and
using the parameters of these motions to generate corresponding captions.
Furthermore, we propose verb-variation paraphrasing to increase the caption
variety and learn the link between primitive motions and high-level verbs. With
this, we are able to learn a motion-focused video-language representation.
Experiments demonstrate our approach is effective for a variety of downstream
tasks, particularly when limited data is available for fine-tuning. Code is
available: https://hazeldoughty.github.io/Papers/LocoMotion/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Labels: A Self-Supervised Framework with Masked Autoencoders and
  Random Cropping for Breast Cancer Subtype Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annalisa Chiocchetti, Marco Dossena, Christopher Irwin, Luigi Portinale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work contributes to breast cancer sub-type classification using
histopathological images. We utilize masked autoencoders (MAEs) to learn a
self-supervised embedding tailored for computer vision tasks in this domain.
This embedding captures informative representations of histopathological data,
facilitating feature learning without extensive labeled datasets. During
pre-training, we investigate employing a random crop technique to generate a
large dataset from WSIs automatically. Additionally, we assess the performance
of linear probes for multi-class classification tasks of cancer sub-types using
the representations learnt by the MAE. Our approach aims to achieve strong
performance on downstream tasks by leveraging the complementary strengths of
ViTs and autoencoders. We evaluate our model's performance on the BRACS dataset
and compare it with existing benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Invariance in Images through One-way Wave Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12976v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12976v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinpeng Chen, Dongdong Chen, Xiyang Dai, Mengchen Liu, Yinan Feng, Youzuo Lin, Lu Yuan, Zicheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we empirically reveal an invariance over images-images share a
set of one-way wave equations with latent speeds. Each image is uniquely
associated with a solution to these wave equations, allowing for its
reconstruction with high fidelity from an initial condition. We demonstrate it
using an intuitive encoder-decoder framework where each image is encoded into
its corresponding initial condition (a single vector). Subsequently, the
initial condition undergoes a specialized decoder, transforming the one-way
wave equations into a first-order norm + linear autoregressive process. This
process propagates the initial condition along the x and y directions,
generating a high-resolution feature map (up to the image resolution), followed
by a few convolutional layers to reconstruct image pixels. The revealed
invariance, rooted in the shared wave equations, offers a fresh perspective for
comprehending images, establishing a promising avenue for further exploration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an improvement version, which fuses some parts from the
  preliminary work arXiv:2305.16319</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Rationality in Language and <span class="highlight-title">Multimodal</span> Agents: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00252v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00252v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Jiang, Yangxinyu Xie, Xiaomeng Wang, Yuan Yuan, Zhuoqun Hao, Xinyi Bai, Weijie J. Su, Camillo J. Taylor, Tanwi Mallick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rationality is the quality of being guided by reason, characterized by
decision-making that aligns with evidence and logical principles. It plays a
crucial role in reliable problem-solving by ensuring well-grounded and
consistent solutions. While large language models (LLMs) have made significant
progress in generating human-like text, they still exhibit limitations such as
bounded knowledge space and inconsistent outputs. In response, recent efforts
have shifted toward developing multimodal and multi-agent systems, as well as
integrating modules like external tools, programming codes, symbolic reasoners,
utility function, and conformal risk controls rather than relying solely on a
single LLM for decision-making. This paper surveys the state-of-the-art
advancements in language and multimodal agents, evaluates how they contribute
to make intelligent agents more rational, and identifies open challenges and
future research directions. We maintain an open repository at
https://github.com/bowen-upenn/Agent_Rationality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We maintain an open repository at
  https://github.com/bowen-upenn/Agent_Rationality</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the power of data augmentation for head pose estimation <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05357v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05357v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Welter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has been impressively successful in the last decade in
predicting human head poses from monocular images. However, for in-the-wild
inputs the research community relies predominantly on a single training set,
300W-LP, of semisynthetic nature without many alternatives. This paper focuses
on gradual extension and improvement of the data to explore the performance
achievable with augmentation and synthesis strategies further. Modeling-wise a
novel multitask head/loss design which includes uncertainty estimation is
proposed. Overall, the thus obtained models are small, efficient, suitable for
full 6 DoF pose estimation, and exhibit very competitive accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR version. Added evaluation on BIWI. Plenty of writing changes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision transformers in domain adaptation and domain generalization: a
  study of robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shadi Alijani, Jamil Fayyad, Homayoun Najjaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models are often evaluated in scenarios where the data
distribution is different from those used in the training and validation
phases. The discrepancy presents a challenge for accurately predicting the
performance of models once deployed on the target distribution. Domain
adaptation and generalization are widely recognized as effective strategies for
addressing such shifts, thereby ensuring reliable performance. The recent
promising results in applying vision transformers in computer vision tasks,
coupled with advancements in self-attention mechanisms, have demonstrated their
significant potential for robustness and generalization in handling
distribution shifts. Motivated by the increased interest from the research
community, our paper investigates the deployment of vision transformers in
domain adaptation and domain generalization scenarios. For domain adaptation
methods, we categorize research into feature-level, instance-level, model-level
adaptations, and hybrid approaches, along with other categorizations with
respect to diverse strategies for enhancing domain adaptation. Similarly, for
domain generalization, we categorize research into multi-domain learning,
meta-learning, regularization techniques, and data augmentation strategies. We
further classify diverse strategies in research, underscoring the various
approaches researchers have taken to address distribution shifts by integrating
vision transformers. The inclusion of comprehensive tables summarizing these
categories is a distinct feature of our work, offering valuable insights for
researchers. These findings highlight the versatility of vision transformers in
managing distribution shifts, crucial for real-world applications, especially
in critical safety and decision-making scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Moral Case for Using Language Model Agents for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seth Lazar, Luke Thorburn, Tian Jin, Luca Belli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our information and communication environment has fallen short of the ideals
that networked global communication might have served. Identifying all the
causes of its pathologies is difficult, but existing recommender systems very
likely play a contributing role. In this paper, which draws on the normative
tools of philosophy of computing, informed by empirical and technical insights
from natural language processing and recommender systems, we make the moral
case for an alternative approach. We argue that existing recommenders
incentivise mass surveillance, concentrate power, fall prey to narrow
behaviourism, and compromise user agency. Rather than just trying to avoid
algorithms entirely, or to make incremental improvements to the current
paradigm, researchers and engineers should explore an alternative paradigm: the
use of language model (LM) agents to source and curate content that matches
users' preferences and values, expressed in natural language. The use of LM
agents for recommendation poses its own challenges, including those related to
candidate generation, computational efficiency, preference modelling, and
prompt injection. Nonetheless, if implemented successfully LM agents could:
guide us through the digital public sphere without relying on mass
surveillance; shift power away from platforms towards users; optimise for what
matters instead of just for behavioural proxies; and scaffold our agency
instead of undermining it.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GaVaMoE: Gaussian-Variational Gated Mixture of Experts for Explainable
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Tang, Yongliang Shen, Hang Zhang, Zeqi Tan, Wenqi Zhang, Guiyang Hou, Kaitao Song, Weiming Lu, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model-based explainable recommendation (LLM-based ER) systems
show promise in generating human-like explanations for recommendations.
However, they face challenges in modeling user-item collaborative preferences,
personalizing explanations, and handling sparse user-item interactions. To
address these issues, we propose GaVaMoE, a novel Gaussian-Variational Gated
Mixture of Experts framework for explainable recommendation. GaVaMoE introduces
two key components: (1) a rating reconstruction module that employs Variational
Autoencoder (VAE) with a Gaussian Mixture Model (GMM) to capture complex
user-item collaborative preferences, serving as a pre-trained multi-gating
mechanism; and (2) a set of fine-grained expert models coupled with the
multi-gating mechanism for generating highly personalized explanations. The VAE
component models latent factors in user-item interactions, while the GMM
clusters users with similar behaviors. Each cluster corresponds to a gate in
the multi-gating mechanism, routing user-item pairs to appropriate expert
models. This architecture enables GaVaMoE to generate tailored explanations for
specific user types and preferences, mitigating data sparsity by leveraging
user similarities. Extensive experiments on three real-world datasets
demonstrate that GaVaMoE significantly outperforms existing methods in
explanation quality, personalization, and consistency. Notably, GaVaMoE
exhibits robust performance in scenarios with sparse user-item interactions,
maintaining high-quality explanations even for users with limited historical
data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Coordinators and <span class="highlight-title">Prompt</span>s on Heterogeneous Graphs for
  Cross-Domain Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengyu Zhang, Chunxu Shen, Xiangguo Sun, Jie Tan, Yu Rong, Chengzhi Piao, Hong Cheng, Lingling Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the online digital world, users frequently engage with diverse items
across multiple domains (e.g., e-commerce platforms, streaming services, and
social media networks), forming complex heterogeneous interaction graphs.
Leveraging this multi-domain information can undoubtedly enhance the
performance of recommendation systems by providing more comprehensive user
insights and alleviating data sparsity in individual domains. However,
integrating multi-domain knowledge for the cross-domain recommendation is very
hard due to inherent disparities in user behavior and item characteristics and
the risk of negative transfer, where irrelevant or conflicting information from
the source domains adversely impacts the target domain's performance. To
address these challenges, we offer HAGO, a novel framework with
$\textbf{H}$eterogeneous $\textbf{A}$daptive $\textbf{G}$raph
co$\textbf{O}$rdinators, which dynamically integrate multi-domain graphs into a
cohesive structure by adaptively adjusting the connections between coordinators
and multi-domain graph nodes, thereby enhancing beneficial inter-domain
interactions while mitigating negative transfer effects. Additionally, we
develop a universal multi-domain graph pre-training strategy alongside HAGO to
collaboratively learn high-quality node representations across domains. To
effectively transfer the learned multi-domain knowledge to the target domain,
we design an effective graph prompting method, which incorporates pre-trained
embeddings with learnable prompts for the recommendation task. Our framework is
compatible with various graph-based models and pre-training techniques,
demonstrating broad applicability and effectiveness. Further experimental
results show that our solutions outperform state-of-the-art methods in
multi-domain recommendation scenarios and highlight their potential for
real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoActionGraphRec: Sequential Multi-Interest Recommendations Using
  Co-Action Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Sun, Yuri M. Brovman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are unique challenges to developing item recommender systems for
e-commerce platforms like eBay due to sparse data and diverse user interests.
While rich user-item interactions are important, eBay's data sparsity exceeds
other e-commerce sites by an order of magnitude. To address this challenge, we
propose CoActionGraphRec (CAGR), a text based two-tower deep learning model
(Item Tower and User Tower) utilizing co-action graph layers. In order to
enhance user and item representations, a graph-based solution tailored to
eBay's environment is utilized. For the Item Tower, we represent each item
using its co-action items to capture collaborative signals in a co-action graph
that is fully leveraged by the graph neural network component. For the User
Tower, we build a fully connected graph of each user's behavior sequence, with
edges encoding pairwise relationships. Furthermore, an explicit interaction
module learns representations capturing behavior interactions. Extensive
offline and online A/B test experiments demonstrate the effectiveness of our
proposed approach and results show improved performance over state-of-the-art
methods on key metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LR-SQL: A Supervised <span class="highlight-title">Fine-Tuning</span> Method for Text2SQL Tasks under
  Low-Resource Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Wuzhenghong, Zhang Yongpan, Pan Su, Sun Yuwei, Lu Pengwei, Ding Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models revolutionize Text2SQL through supervised fine-tuning,
yet a crucial limitation is overlooked: the complexity of databases leads to an
increased context length, consequently resulting in higher GPU memory demands
for model fine-tuning. To address this issue, we propose LR-SQL. LR-SQL
comprises two supervised fine-tuning models: the schema\_link model and the
SQL\_generation model, with the schema\_link model serving as the focal point
for streamlining the overall process. During the fine-tuning of the
schema\_link model, LR-SQL breaks down the complete database into flexible
combinations of tables with adjustable quantities, enabling the model to learn
the relationships within the entire database from these dispersed slices.
Furthermore, to enhance the model's ability to perceive the relationships among
various discrete slices during inference, LR-SQL trains the model's
Chain-of-Thought capability for this task. Experimental results demonstrate
that LR-SQL can reduce the total GPU memory usage by 40\% compared to existing
fine-tuning methods, while only losing 2\% of table prediction accuracy in
schema\_link task. For the overall Text2SQL task, the Execution Accuracy
decrease by 0.6\%.Our project is now available on
https://github.com/hongWin/LR-SQL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12pages, 4 figures,submitting to a journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhance Graph Alignment for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitong Luo, Xuying Meng, Suhang Wang, Tianxiang Zhao, Fali Wang, Hanyun Cao, Yujun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-structured data is prevalent in the real world. Recently, due to the
powerful emergent capabilities, Large Language Models (LLMs) have shown
promising performance in modeling graphs. The key to effectively applying LLMs
on graphs is converting graph data into a format LLMs can comprehend.
Graph-to-token approaches are popular in enabling LLMs to process graph
information. They transform graphs into sequences of tokens and align them with
text tokens through instruction tuning, where self-supervised instruction
tuning helps LLMs acquire general knowledge about graphs, and supervised
fine-tuning specializes LLMs for the downstream tasks on graphs. Despite their
initial success, we find that existing methods have a misalignment between
self-supervised tasks and supervised downstream tasks, resulting in negative
transfer from self-supervised fine-tuning to downstream tasks. To address these
issues, we propose Graph Alignment Large Language Models (GALLM) to benefit
from aligned task templates. In the self-supervised tuning stage, we introduce
a novel text matching task using templates aligned with downstream tasks. In
the task-specific tuning stage, we propose two category prompt methods that
learn supervision information from additional explanation with further aligned
templates. Experimental evaluations on four datasets demonstrate substantial
improvements in supervised learning, multi-dataset generalizability, and
particularly in zero-shot capability, highlighting the model's potential as a
graph foundation model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reducing Labeling Costs in Sentiment Analysis via Semi-Supervised
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minoo Jafarlou, Mario M. Kubek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Labeling datasets is a noteworthy challenge in machine learning, both in
terms of cost and time. This research, however, leverages an efficient answer.
By exploring label propagation in semi-supervised learning, we can
significantly reduce the number of labels required compared to traditional
methods. We employ a transductive label propagation method based on the
manifold assumption for text classification. Our approach utilizes a
graph-based method to generate pseudo-labels for unlabeled data for the text
classification task, which are then used to train deep neural networks. By
extending labels based on cosine proximity within a nearest neighbor graph from
network embeddings, we combine unlabeled data into supervised learning, thereby
reducing labeling costs. Based on previous successes in other domains, this
study builds and evaluates this approach's effectiveness in sentiment analysis,
presenting insights into semi-supervised learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures, accepted at the 2024 8th International
  Conference on Natural Language Processing and Information Retrieval (NLPIR
  2024), Okayama, Japan, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequential LLM Framework for Fashion Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Liu, Xianfeng Tang, Tianlang Chen, Jiapeng Liu, Indu Indu, Henry Peng Zou, Peng Dai, Roberto Fernandez Galan, Michael D Porter, Dongmei Jia, Ning Zhang, Lian Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fashion industry is one of the leading domains in the global e-commerce
sector, prompting major online retailers to employ recommendation systems for
product suggestions and customer convenience. While recommendation systems have
been widely studied, most are designed for general e-commerce problems and
struggle with the unique challenges of the fashion domain. To address these
issues, we propose a sequential fashion recommendation framework that leverages
a pre-trained large language model (LLM) enhanced with recommendation-specific
prompts. Our framework employs parameter-efficient fine-tuning with extensive
fashion data and introduces a novel mix-up-based retrieval technique for
translating text into relevant product suggestions. Extensive experiments show
our proposed framework significantly enhances fashion recommendation
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Capacity of Citation Generation by Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haosheng Qian, Yixing Fan, Ruqing Zhang, Jiafeng Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) appears as a promising method to
alleviate the "hallucination" problem in large language models (LLMs), since it
can incorporate external traceable resources for response generation. The
essence of RAG in combating the hallucination issue lies in accurately
attributing claims in responses to the corresponding retrieved documents.
However, most of existing works focus on improving the quality of generated
responses from the LLM, while largely overlooked its ability to attribute
sources accurately. In this study, we conduct a systematic analysis about the
capabilities of LLMs in generating citations within response generation, and
further introduce a novel method to enhance their citation generation
abilities. Specifically, we evaluate both the correctness and citation quality
for seven widely-used LLMs on two benchmark datasets. Meanwhile, we introduce
new citation evaluation metrics to eliminate the over-penalization of
unnecessary and excessive citations in existing metrics. Furthermore, we
propose a Generate-then-Refine method that completes relevant citations and
removes irrelevant ones without altering the response text. The results on
WebGLM-QA, ASQA and ELI5 datasets show that our method substantially improves
the quality of citations in responses generated by LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CCIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Encoder-Only Transformers for Session-Based Recommendation
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anis Redjdal, Luis Pinto, Michel Desmarais
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Session-based recommendation is the task of predicting the next item a user
will interact with, often without access to historical user data. In this work,
we introduce Sequential Masked Modeling, a novel approach for encoder-only
transformer architectures to tackle the challenges of single-session
recommendation. Our method combines data augmentation through window sliding
with a unique penultimate token masking strategy to capture sequential
dependencies more effectively. By enhancing how transformers handle session
data, Sequential Masked Modeling significantly improves next-item prediction
performance.
  We evaluate our approach on three widely-used datasets, Yoochoose 1/64,
Diginetica, and Tmall, comparing it to state-of-the-art single-session,
cross-session, and multi-relation approaches. The results demonstrate that our
Transformer-SMM models consistently outperform all models that rely on the same
amount of information, while even rivaling methods that have access to more
extensive user history. This study highlights the potential of encoder-only
transformers in session-based recommendation and opens the door for further
improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07722v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07722v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thong Nguyen, Shubham Chatterjee, Sean MacAvaney, Iain Mackie, Jeff Dalton, Andrew Yates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned Sparse Retrieval (LSR) models use vocabularies from pre-trained
transformers, which often split entities into nonsensical fragments. Splitting
entities can reduce retrieval accuracy and limits the model's ability to
incorporate up-to-date world knowledge not included in the training data. In
this work, we enhance the LSR vocabulary with Wikipedia concepts and entities,
enabling the model to resolve ambiguities more effectively and stay current
with evolving knowledge. Central to our approach is a Dynamic Vocabulary (DyVo)
head, which leverages existing entity embeddings and an entity retrieval
component that identifies entities relevant to a query or document. We use the
DyVo head to generate entity weights, which are then merged with word piece
weights to create joint representations for efficient indexing and retrieval
using an inverted index. In experiments across three entity-rich document
ranking datasets, the resulting DyVo model substantially outperforms
state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/thongnt99/DyVo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GOVERN: Gradient Orientation Vote Ensemble for Multi-Teacher Reinforced
  Distillation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Zhou, Zhenxin Ding, Xiaodong Zhang, Haibo Shi, Junfeng Wang, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models have become an integral component of
question-answering systems, achieving remarkable performance. However, for
practical deployment, it is crucial to perform knowledge distillation to
maintain high performance while operating under computational constraints. In
this paper, we address a key question: given the importance of unsupervised
distillation for student model performance, how can knowledge from multiple
teacher models be effectively ensemble during this stage without the guidance
of labels? We propose a novel algorithm, GOVERN, to tackle this issue. GOVERN
has demonstrated significant improvements in both offline and online
experiments, enabling the student model to achieve results comparable to that
of teacher ensembles. Our experiments show that GOVERN remarkably requires a
mere 1\% of the ensemble method's inference budget to achieve 99.5\% of
performance. The proposed algorithm has been successfully deployed in a
real-world commercial question-answering system, demonstrating its real-world
applicability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DISCO: A Hierarchical Disentangled Cognitive Diagnosis Framework for
  Interpretable Job Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07671v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07671v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoshan Yu, Chuan Qin, Qi Zhang, Chen Zhu, Haiping Ma, Xingyi Zhang, Hengshu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of online recruitment platforms has created
unprecedented opportunities for job seekers while concurrently posing the
significant challenge of quickly and accurately pinpointing positions that
align with their skills and preferences. Job recommendation systems have
significantly alleviated the extensive search burden for job seekers by
optimizing user engagement metrics, such as clicks and applications, thus
achieving notable success. In recent years, a substantial amount of research
has been devoted to developing effective job recommendation models, primarily
focusing on text-matching based and behavior modeling based methods. While
these approaches have realized impressive outcomes, it is imperative to note
that research on the explainability of recruitment recommendations remains
profoundly unexplored. To this end, in this paper, we propose DISCO, a
hierarchical Disentanglement based Cognitive diagnosis framework, aimed at
flexibly accommodating the underlying representation learning model for
effective and interpretable job recommendations. Specifically, we first design
a hierarchical representation disentangling module to explicitly mine the
hierarchical skill-related factors implied in hidden representations of job
seekers and jobs. Subsequently, we propose level-aware association modeling to
enhance information communication and robust representation learning both
inter- and intra-level, which consists of the interlevel knowledge influence
module and the level-wise contrastive learning. Finally, we devise an
interaction diagnosis module incorporating a neural diagnosis function for
effectively modeling the multi-level recruitment interaction process between
job seekers and jobs, which introduces the cognitive measurement theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICDM 2024. 10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Curriculum-scheduled Knowledge Distillation from Multiple <span class="highlight-title">Pre-train</span>ed
  Teachers for Multi-domain Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Sun, Ruobing Xie, Junjie Zhang, Wayne Xin Zhao, Leyu Lin, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained recommendation models (PRMs) have received increasing interest
recently. However, their intrinsically heterogeneous model structure, huge
model size and computation cost hinder their adoptions in practical recommender
systems. Hence, it is highly essential to explore how to use different
pre-trained recommendation models efficiently in real-world systems. In this
paper, we propose a novel curriculum-scheduled knowledge distillation from
multiple pre-trained teachers for multi-domain sequential recommendation,
called CKD-MDSR, which takes full advantages of different PRMs as multiple
teacher models to boost a small student recommendation model, integrating the
knowledge across multiple domains from PRMs. Specifically, CKD-MDSR first
adopts curriculum-scheduled user behavior sequence sampling and distills
informative knowledge jointly from the representative PRMs such as UniSRec and
Recformer. Then, the knowledge from the above PRMs are selectively integrated
into the student model in consideration of their confidence and consistency.
Finally, we verify the proposed method on multi-domain sequential
recommendation and further demonstrate its universality with multiple types of
student models, including feature interaction and graph based recommendation
models. Extensive experiments on five real-world datasets demonstrate the
effectiveness and efficiency of CKD-MDSR, which can be viewed as an efficient
shortcut using PRMs in real-world systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoRA: Collaborative Information Perception by Large Language Model's
  Weights for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10645v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10645v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuting Liu, Jinghao Zhang, Yizhou Dang, Yuliang Liang, Qiang Liu, Guibing Guo, Jianzhe Zhao, Xingwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Involving collaborative information in Large Language Models (LLMs) is a
promising technique for adapting LLMs for recommendation. Existing methods
achieve this by concatenating collaborative features with text tokens into a
unified sequence input and then fine-tuning to align these features with LLM's
input space. Although effective, in this work, we identify two limitations when
adapting LLMs to recommendation tasks, which hinder the integration of general
knowledge and collaborative information, resulting in sub-optimal
recommendation performance. (1) Fine-tuning LLM with recommendation data can
undermine its inherent world knowledge and fundamental competencies, which are
crucial for interpreting and inferring recommendation text. (2) Incorporating
collaborative features into textual prompts disrupts the semantics of the
original prompts, preventing LLM from generating appropriate outputs. In this
paper, we propose a new paradigm, CoRA (an acronym for Collaborative LoRA),
with a collaborative weights generator. Rather than input space alignment, this
method aligns collaborative information with LLM's parameter space,
representing them as incremental weights to update LLM's output. This way, LLM
perceives collaborative information without altering its general knowledge and
text inference capabilities. Specifically, we employ a collaborative filtering
model to extract user and item embeddings, converting them into collaborative
weights with low-rank properties through the collaborative weights generator.
We then merge the collaborative weights into LLM's weights, enabling LLM to
perceive the collaborative signals and generate personalized recommendations
without fine-tuning or extra collaborative tokens in prompts. Extensive
experiments confirm that CoRA effectively integrates collaborative information
into LLM, enhancing recommendation performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recommenadation aided Caching using Combinatorial Multi-armed Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00080v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00080v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pavamana K J, Chandramani Kishore Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study content caching with recommendations in a wireless network where the
users are connected through a base station equipped with a finite-capacity
cache. We assume a fixed set of contents with unknown user preferences and
content popularities. The base station can cache a subset of the contents and
can also recommend subsets of the contents to different users in order to
encourage them to request the recommended contents. Recommendations, depending
on their acceptability, can thus be used to increase cache hits. We first
assume that the users' recommendation acceptabilities are known and formulate
the cache hit optimization problem as a combinatorial multi-armed bandit
(CMAB). We propose a UCB-based algorithm to decide which contents to cache and
recommend and provide an upper bound on the regret of this algorithm.
Subsequently, we consider a more general scenario where the users'
recommendation acceptabilities are also unknown and propose another UCB-based
algorithm that learns these as well. We numerically demonstrate the performance
of our algorithms and compare these to state-of-the-art algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectral-Based Graph Neural Networks for Complementary Item
  Recommendation <span class="chip">AAAI-24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02130v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02130v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitong Luo, Xuying Meng, Suhang Wang, Hanyun Cao, Weiyao Zhang, Yequan Wang, Yujun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling complementary relationships greatly helps recommender systems to
accurately and promptly recommend the subsequent items when one item is
purchased. Unlike traditional similar relationships, items with complementary
relationships may be purchased successively (such as iPhone and Airpods Pro),
and they not only share relevance but also exhibit dissimilarity. Since the two
attributes are opposites, modeling complementary relationships is challenging.
Previous attempts to exploit these relationships have either ignored or
oversimplified the dissimilarity attribute, resulting in ineffective modeling
and an inability to balance the two attributes. Since Graph Neural Networks
(GNNs) can capture the relevance and dissimilarity between nodes in the
spectral domain, we can leverage spectral-based GNNs to effectively understand
and model complementary relationships. In this study, we present a novel
approach called Spectral-based Complementary Graph Neural Networks (SComGNN)
that utilizes the spectral properties of complementary item graphs. We make the
first observation that complementary relationships consist of low-frequency and
mid-frequency components, corresponding to the relevance and dissimilarity
attributes, respectively. Based on this spectral observation, we design
spectral graph convolutional networks with low-pass and mid-pass filters to
capture the low-frequency and mid-frequency components. Additionally, we
propose a two-stage attention mechanism to adaptively integrate and balance the
two attributes. Experimental results on four e-commerce datasets demonstrate
the effectiveness of our model, with SComGNN significantly outperforming
existing baseline models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-24</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling Data-Driven and Empathetic Interactions: A Context-Aware 3D
  Virtual Agent in Mixed Reality for Enhanced Financial Customer Experience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cindy Xu, Mengyu Chen, Pranav Deshpande, Elvir Azanli, Runqing Yang, Joseph Ligman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel system designed to enhance customer
service in the financial and retail sectors through a context-aware 3D virtual
agent, utilizing Mixed Reality (MR) and Vision Language Models (VLMs). Our
approach focuses on enabling data-driven and empathetic interactions that
ensure customer satisfaction by introducing situational awareness of the
physical location, personalized interactions based on customer profiles, and
rigorous privacy and security standards. We discuss our design considerations
critical for deployment in real-world customer service environments, addressing
challenges in user data management and sensitive information handling. We also
outline the system architecture and key features unique to banking and retail
environments. Our work demonstrates the potential of integrating MR and VLMs in
service industries, offering practical insights in customer service delivery
while maintaining high standards of security and personalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear at 1st Workshop on Intelligent XR: Harnessing AI for
  Next-Generation XR User Experiences at International Symposium on Mixed and
  Augmented Reality (ISMAR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LocoMotion: Learning Motion-Focused Video-Language Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hazel Doughty, Fida Mohammad Thoker, Cees G. M. Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper strives for motion-focused video-language representations.
Existing methods to learn video-language representations use spatial-focused
data, where identifying the objects and scene is often enough to distinguish
the relevant caption. We instead propose LocoMotion to learn from
motion-focused captions that describe the movement and temporal progression of
local object motions. We achieve this by adding synthetic motions to videos and
using the parameters of these motions to generate corresponding captions.
Furthermore, we propose verb-variation paraphrasing to increase the caption
variety and learn the link between primitive motions and high-level verbs. With
this, we are able to learn a motion-focused video-language representation.
Experiments demonstrate our approach is effective for a variety of downstream
tasks, particularly when limited data is available for fine-tuning. Code is
available: https://hazeldoughty.github.io/Papers/LocoMotion/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Long-Text Alignment for Text-to-Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luping Liu, Chao Du, Tianyu Pang, Zehan Wang, Chongxuan Li, Dong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of text-to-image (T2I) diffusion models has enabled
them to generate unprecedented results from given texts. However, as text
inputs become longer, existing encoding methods like CLIP face limitations, and
aligning the generated images with long texts becomes challenging. To tackle
these issues, we propose LongAlign, which includes a segment-level encoding
method for processing long texts and a decomposed preference optimization
method for effective alignment training. For segment-level encoding, long texts
are divided into multiple segments and processed separately. This method
overcomes the maximum input length limits of pretrained encoding models. For
preference optimization, we provide decomposed CLIP-based preference models to
fine-tune diffusion models. Specifically, to utilize CLIP-based preference
models for T2I alignment, we delve into their scoring mechanisms and find that
the preference scores can be decomposed into two components: a text-relevant
part that measures T2I alignment and a text-irrelevant part that assesses other
visual aspects of human preference. Additionally, we find that the
text-irrelevant part contributes to a common overfitting problem during
fine-tuning. To address this, we propose a reweighting strategy that assigns
different weights to these two components, thereby reducing overfitting and
enhancing alignment. After fine-tuning $512 \times 512$ Stable Diffusion (SD)
v1.5 for about 20 hours using our method, the fine-tuned SD outperforms
stronger foundation models in T2I alignment, such as PixArt-$\alpha$ and
Kandinsky v2.2. The code is available at
https://github.com/luping-liu/LongAlign.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxi Wang, Xiang Chen, Ningyu Zhang, Bozhong Tian, Haoming Xu, Shumin Deng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) frequently exhibit hallucination
phenomena, but the underlying reasons remain poorly understood. In this paper,
we present an empirical analysis and find that, although MLLMs incorrectly
generate the objects in the final output, they are actually able to recognize
visual objects in the preceding layers. We speculate that this may be due to
the strong knowledge priors of the language model suppressing the visual
information, leading to hallucinations. Motivated by this, we propose a novel
dynamic correction decoding method for MLLMs (DeCo), which adaptively selects
the appropriate preceding layers and proportionally integrates knowledge into
the final layer to adjust the output logits. Note that DeCo is model agnostic
and can be seamlessly incorporated with various classic decoding strategies and
applied to different MLLMs. We evaluate DeCo on widely-used benchmarks,
demonstrating that it can reduce hallucination rates by a large margin compared
to baselines, highlighting its potential to mitigate hallucinations. Code is
available at https://github.com/zjunlp/DeCo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Magnifier <span class="highlight-title">Prompt</span>: Tackling <span class="highlight-title">Multimodal</span> Hallucination via Extremely Simple
  Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Fu, Ruobing Xie, Jiazhen Liu, Bangxiang Lan, Xingwu Sun, Zhanhui Kang, Xirong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations in multimodal large language models (MLLMs) hinder their
practical applications. To address this, we propose a Magnifier Prompt
(MagPrompt), a simple yet effective method to tackle hallucinations in MLLMs
via extremely simple instructions. MagPrompt is based on the following two key
principles, which guide the design of various effective prompts, demonstrating
robustness: (1) MLLMs should focus more on the image. (2) When there are
conflicts between the image and the model's inner knowledge, MLLMs should
prioritize the image. MagPrompt is training-free and can be applied to
open-source and closed-source models, such as GPT-4o and Gemini-pro. It
performs well across many datasets and its effectiveness is comparable or even
better than more complex methods like VCD. Furthermore, our prompt design
principles and experimental analyses provide valuable insights into multimodal
hallucination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 13 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On-the-fly Modulation for Balanced <span class="highlight-title">Multimodal</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yake Wei, Di Hu, Henghui Du, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning is expected to boost model performance by integrating
information from different modalities. However, its potential is not fully
exploited because the widely-used joint training strategy, which has a uniform
objective for all modalities, leads to imbalanced and under-optimized uni-modal
representations. Specifically, we point out that there often exists modality
with more discriminative information, e.g., vision of playing football and
sound of blowing wind. They could dominate the joint training process,
resulting in other modalities being significantly under-optimized. To alleviate
this problem, we first analyze the under-optimized phenomenon from both the
feed-forward and the back-propagation stages during optimization. Then,
On-the-fly Prediction Modulation (OPM) and On-the-fly Gradient Modulation (OGM)
strategies are proposed to modulate the optimization of each modality, by
monitoring the discriminative discrepancy between modalities during training.
Concretely, OPM weakens the influence of the dominant modality by dropping its
feature with dynamical probability in the feed-forward stage, while OGM
mitigates its gradient in the back-propagation stage. In experiments, our
methods demonstrate considerable improvement across a variety of multimodal
tasks. These simple yet effective strategies not only enhance performance in
vanilla and task-oriented multimodal models, but also in more complex
multimodal tasks, showcasing their effectiveness and flexibility. The source
code is available at \url{https://github.com/GeWu-Lab/BML_TPAMI2024}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by T-PAMI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging LLM Embeddings for Cross Dataset Label Alignment and Zero
  Shot Music Emotion Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renhang Liu, Abhinaba Roy, Dorien Herremans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a novel method for music emotion recognition that
leverages Large Language Model (LLM) embeddings for label alignment across
multiple datasets and zero-shot prediction on novel categories. First, we
compute LLM embeddings for emotion labels and apply non-parametric clustering
to group similar labels, across multiple datasets containing disjoint labels.
We use these cluster centers to map music features (MERT) to the LLM embedding
space. To further enhance the model, we introduce an alignment regularization
that enables dissociation of MERT embeddings from different clusters. This
further enhances the model's ability to better adaptation to unseen datasets.
We demonstrate the effectiveness of our approach by performing zero-shot
inference on a new dataset, showcasing its ability to generalize to unseen
labels without additional training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VidCompress: Memory-Enhanced Temporal Compression for Video
  Understanding in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Lan, Yitian Yuan, Zequn Jie, Lin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-based multimodal large language models (Video-LLMs) possess significant
potential for video understanding tasks. However, most Video-LLMs treat videos
as a sequential set of individual frames, which results in insufficient
temporal-spatial interaction that hinders fine-grained comprehension and
difficulty in processing longer videos due to limited visual token capacity. To
address these challenges, we propose VidCompress, a novel Video-LLM featuring
memory-enhanced temporal compression. VidCompress employs a dual-compressor
approach: a memory-enhanced compressor captures both short-term and long-term
temporal relationships in videos and compresses the visual tokens using a
multiscale transformer with a memory-cache mechanism, while a text-perceived
compressor generates condensed visual tokens by utilizing Q-Former and
integrating temporal contexts into query embeddings with cross attention.
Experiments on several VideoQA datasets and comprehensive benchmarks
demonstrate that VidCompress efficiently models complex temporal-spatial
relations and significantly outperforms existing Video-LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIA: Unified Spatiotemporal Video Adaptation Framework for Global and
  Local Video Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12831v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12831v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Gu, Yuwei Fang, Ivan Skorokhodov, Peter Wonka, Xinya Du, Sergey Tulyakov, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video editing is a cornerstone of digital media, from entertainment and
education to professional communication. However, previous methods often
overlook the necessity of comprehensively understanding both global and local
contexts, leading to inaccurate and inconsistent edits in the spatiotemporal
dimension, especially for long videos. In this paper, we introduce VIA, a
unified spatiotemporal Video Adaptation framework for global and local video
editing, pushing the limits of consistently editing minute-long videos. First,
to ensure local consistency within individual frames, we designed test-time
editing adaptation to adapt a pre-trained image editing model for improving
consistency between potential editing directions and the text instruction, and
adapt masked latent variables for precise local control. Furthermore, to
maintain global consistency over the video sequence, we introduce
spatiotemporal adaptation that recursively gather consistent attention
variables in key frames and strategically applies them across the whole
sequence to realize the editing effects. Extensive experiments demonstrate
that, compared to baseline methods, our VIA approach produces edits that are
more faithful to the source videos, more coherent in the spatiotemporal
context, and more precise in local control. More importantly, we show that VIA
can achieve consistent long video editing in minutes, unlocking the potential
for advanced video editing tasks over long video sequences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain
  Fake News Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01336v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01336v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Zhou, Xiaoming Zhang, Litian Zhang, Jiacheng Liu, Senzhang Wang, Zheng Liu, Xi Zhang, Chaozhuo Li, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing benchmarks for fake news detection have significantly contributed to
the advancement of models in assessing the authenticity of news content.
However, these benchmarks typically focus solely on news pertaining to a single
semantic topic or originating from a single platform, thereby failing to
capture the diversity of multi-domain news in real scenarios. In order to
understand fake news across various domains, the external knowledge and
fine-grained annotations are indispensable to provide precise evidence and
uncover the diverse underlying strategies for fabrication, which are also
ignored by existing benchmarks. To address this gap, we introduce a novel
multi-domain knowledge-enhanced benchmark with fine-grained annotations, named
\textbf{FineFake}. FineFake encompasses 16,909 data samples spanning six
semantic topics and eight platforms. Each news item is enriched with
multi-modal content, potential social context, semi-manually verified common
knowledge, and fine-grained annotations that surpass conventional binary
labels. Furthermore, we formulate three challenging tasks based on FineFake and
propose a knowledge-enhanced domain adaptation network. Extensive experiments
are conducted on FineFake under various scenarios, providing accurate and
reliable benchmarks for future endeavors. The entire FineFake project is
publicly accessible as an open-source repository at
\url{https://github.com/Accuser907/FineFake}.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-14T00:00:00Z">2024-10-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">23</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SGUQ: Staged Graph Convolution Neural Network for Alzheimer's Disease
  Diagnosis using Multi-Omics Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Tao, Yixin Xie, Jeffrey D Deng, Hui Shen, Hong-Wen Deng, Weihua Zhou, Chen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's disease (AD) is a chronic neurodegenerative disorder and the
leading cause of dementia, significantly impacting cost, mortality, and burden
worldwide. The advent of high-throughput omics technologies, such as genomics,
transcriptomics, proteomics, and epigenomics, has revolutionized the molecular
understanding of AD. Conventional AI approaches typically require the
completion of all omics data at the outset to achieve optimal AD diagnosis,
which are inefficient and may be unnecessary. To reduce the clinical cost and
improve the accuracy of AD diagnosis using multi-omics data, we propose a novel
staged graph convolutional network with uncertainty quantification (SGUQ). SGUQ
begins with mRNA and progressively incorporates DNA methylation and miRNA data
only when necessary, reducing overall costs and exposure to harmful tests.
Experimental results indicate that 46.23% of the samples can be reliably
predicted using only single-modal omics data (mRNA), while an additional 16.04%
of the samples can achieve reliable predictions when combining two omics data
types (mRNA + DNA methylation). In addition, the proposed staged SGUQ achieved
an accuracy of 0.858 on ROSMAP dataset, which outperformed existing methods
significantly. The proposed SGUQ can not only be applied to AD diagnosis using
multi-omics data but also has the potential for clinical decision-making using
multi-viewed data. Our implementation is publicly available at
https://github.com/chenzhao2023/multiomicsuncertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GraFPrint: A GNN-Based Approach for Audio Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Bhattacharjee, Shubhr Singh, Emmanouil Benetos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces GraFPrint, an audio identification framework that
leverages the structural learning capabilities of Graph Neural Networks (GNNs)
to create robust audio fingerprints. Our method constructs a k-nearest neighbor
(k-NN) graph from time-frequency representations and applies max-relative graph
convolutions to encode local and global information. The network is trained
using a self-supervised contrastive approach, which enhances resilience to
ambient distortions by optimizing feature representation. GraFPrint
demonstrates superior performance on large-scale datasets at various levels of
granularity, proving to be both lightweight and scalable, making it suitable
for real-world applications with extensive reference databases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Model Parameters for Controlling: Parameter Diffusion for
  Controllable Multi-Task Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenglei Shen, Jiahao Zhao, Xiao Zhang, Weijie Yu, Ming He, Jianping Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commercial recommender systems face the challenge that task requirements from
platforms or users often change dynamically (e.g., varying preferences for
accuracy or diversity). Ideally, the model should be re-trained after resetting
a new objective function, adapting to these changes in task requirements.
However, in practice, the high computational costs associated with retraining
make this process impractical for models already deployed to online
environments. This raises a new challenging problem: how to efficiently adapt
the learning model to different task requirements by controlling model
parameters after deployment, without the need for retraining. To address this
issue, we propose a novel controllable learning approach via Parameter
Diffusion for controllable multi-task Recommendation (PaDiRec), which allows
the customization and adaptation of recommendation model parameters to new task
requirements without retraining. Specifically, we first obtain the optimized
model parameters through adapter tunning based on the feasible task
requirements. Then, we utilize the diffusion model as a parameter generator,
employing classifier-free guidance in conditional training to learn the
distribution of optimized model parameters under various task requirements.
Finally, the diffusion model is applied to effectively generate model
parameters in a test-time adaptation manner given task requirements. As a
model-agnostic approach, PaDiRec can leverage existing recommendation models as
backbones to enhance their controllability. Extensive experiments on public
datasets and a dataset from a commercial app, indicate that PaDiRec can
effectively enhance controllability through efficient model parameter
generation. The code is released at
https://anonymous.4open.science/r/PaDiRec-DD13.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VisRAG: Vision-based Retrieval-augmented Generation on <span class="highlight-title">Multi-modal</span>ity
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) is an effective technique that enables
large language models (LLMs) to utilize external knowledge sources for
generation. However, current RAG systems are solely based on text, rendering it
impossible to utilize vision information like layout and images that play
crucial roles in real-world multi-modality documents. In this paper, we
introduce VisRAG, which tackles this issue by establishing a vision-language
model (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the
document to obtain text, the document is directly embedded using a VLM as an
image and then retrieved to enhance the generation of a VLM. Compared to
traditional text-based RAG, VisRAG maximizes the retention and utilization of
the data information in the original documents, eliminating the information
loss introduced during the parsing process. We collect both open-source and
synthetic data to train the retriever in VisRAG and explore a variety of
generation methods. Experiments demonstrate that VisRAG outperforms traditional
RAG in both the retrieval and generation stages, achieving a 25--39\%
end-to-end performance gain over traditional text-based RAG pipeline. Further
analysis reveals that VisRAG is effective in utilizing training data and
demonstrates strong generalization capability, positioning it as a promising
solution for RAG on multi-modality documents. Our code and data are available
at https://github.com/openbmb/visrag .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Legal Judgement Prediction in a Realistic Scenario in the Era
  of Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Kumar Nigam, Aniket Deroy, Subhankar Maity, Arnab Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates judgment prediction in a realistic scenario within
the context of Indian judgments, utilizing a range of transformer-based models,
including InLegalBERT, BERT, and XLNet, alongside LLMs such as Llama-2 and
GPT-3.5 Turbo. In this realistic scenario, we simulate how judgments are
predicted at the point when a case is presented for a decision in court, using
only the information available at that time, such as the facts of the case,
statutes, precedents, and arguments. This approach mimics real-world
conditions, where decisions must be made without the benefit of hindsight,
unlike retrospective analyses often found in previous studies. For transformer
models, we experiment with hierarchical transformers and the summarization of
judgment facts to optimize input for these models. Our experiments with LLMs
reveal that GPT-3.5 Turbo excels in realistic scenarios, demonstrating robust
performance in judgment prediction. Furthermore, incorporating additional legal
information, such as statutes and precedents, significantly improves the
outcome of the prediction task. The LLMs also provide explanations for their
predictions. To evaluate the quality of these predictions and explanations, we
introduce two human evaluation metrics: Clarity and Linking. Our findings from
both automatic and human evaluations indicate that, despite advancements in
LLMs, they are yet to achieve expert-level performance in judgment prediction
and explanation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted on NLLP at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Academic Knowledge Retrieval via LLM-enhanced Representation
  Similarity Fusion <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Dai, Peng Fu, Chunjing Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an era marked by robust technological growth and swift information
renewal, furnishing researchers and the populace with top-tier, avant-garde
academic insights spanning various domains has become an urgent necessity. The
KDD Cup 2024 AQA Challenge is geared towards advancing retrieval models to
identify pertinent academic terminologies from suitable papers for scientific
inquiries. This paper introduces the LLM-KnowSimFuser proposed by Robo Space,
which wins the 2nd place in the competition. With inspirations drawed from the
superior performance of LLMs on multiple tasks, after careful analysis of the
provided datasets, we firstly perform fine-tuning and inference using
LLM-enhanced pre-trained retrieval models to introduce the tremendous language
understanding and open-domain knowledge of LLMs into this task, followed by a
weighted fusion based on the similarity matrix derived from the inference
results. Finally, experiments conducted on the competition datasets show the
superiority of our proposal, which achieved a score of 0.20726 on the final
leaderboard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 2nd Place of KDD Cup 2024 OAG-Challenge AQA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Medico: Towards Hallucination Detection and Correction with Multi-source
  Evidence Fusion <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinping Zhao, Jindi Yu, Zhenyu Liu, Jifang Wang, Dongfang Li, Yibin Chen, Baotian Hu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As we all know, hallucinations prevail in Large Language Models (LLMs), where
the generated content is coherent but factually incorrect, which inflicts a
heavy blow on the widespread application of LLMs. Previous studies have shown
that LLMs could confidently state non-existent facts rather than answering ``I
don't know''. Therefore, it is necessary to resort to external knowledge to
detect and correct the hallucinated content. Since manual detection and
correction of factual errors is labor-intensive, developing an automatic
end-to-end hallucination-checking approach is indeed a needful thing. To this
end, we present Medico, a Multi-source evidence fusion enhanced hallucination
detection and correction framework. It fuses diverse evidence from multiple
sources, detects whether the generated content contains factual errors,
provides the rationale behind the judgment, and iteratively revises the
hallucinated content. Experimental results on evidence retrieval (0.964 HR@5,
0.908 MRR@5), hallucination detection (0.927-0.951 F1), and hallucination
correction (0.973-0.979 approval rate) manifest the great potential of Medico.
A video demo of Medico can be found at https://youtu.be/RtsO6CSesBI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures, 6 tables. Accepted by EMNLP 2024's demo track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative filtering based on nonnegative/binary matrix factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukino Terui, Yuka Inoue, Yohei Hamakawa, Kosuke Tatsumura, Kazue Kudo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative filtering generates recommendations based on user-item
similarities through rating data, which may involve numerous unrated items. To
predict scores for unrated items, matrix factorization techniques, such as
nonnegative matrix factorization (NMF), are often employed to predict scores
for unrated items. Nonnegative/binary matrix factorization (NBMF), which is an
extension of NMF, approximates a nonnegative matrix as the product of
nonnegative and binary matrices. Previous studies have employed NBMF for image
analysis where the data were dense. In this paper, we propose a modified NBMF
algorithm that can be applied to collaborative filtering where data are sparse.
In the modified method, unrated elements in a rating matrix are masked, which
improves the collaborative filtering performance. Utilizing a low-latency Ising
machine in NBMF is advantageous in terms of the computation time, making the
proposed method beneficial.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BookWorm: A Dataset for Character Description and Analysis <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Argyrios Papoudakis, Mirella Lapata, Frank Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Characters are at the heart of every story, driving the plot and engaging
readers. In this study, we explore the understanding of characters in
full-length books, which contain complex narratives and numerous interacting
characters. We define two tasks: character description, which generates a brief
factual profile, and character analysis, which offers an in-depth
interpretation, including character development, personality, and social
context. We introduce the BookWorm dataset, pairing books from the Gutenberg
Project with human-written descriptions and analyses. Using this dataset, we
evaluate state-of-the-art long-context models in zero-shot and fine-tuning
settings, utilizing both retrieval-based and hierarchical processing for
book-length inputs. Our findings show that retrieval-based approaches
outperform hierarchical ones in both tasks. Additionally, fine-tuned models
using coreference-based retrieval produce the most factual descriptions, as
measured by fact- and entailment-based metrics. We hope our dataset,
experiments, and analysis will inspire further research in character-based
narrative understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 2 figures, EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hybrid Filtering for Micro-video Hashtag Recommendation using
  Graph-based Deep Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubhi Bansal, Kushaan Gowda, Mohammad Zia Ur Rehman, Chandravardhan Singh Raghaw, Nagendra Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the growing volume of user generated content, hashtags are employed as
topic indicators to manage content efficiently on social media platforms.
However, finding these vital topics is challenging in microvideos since they
contain substantial information in a short duration. Existing methods that
recommend hashtags for microvideos primarily focus on content and
personalization while disregarding relatedness among users. Moreover, the cold
start user issue prevails in hashtag recommendation systems. Considering the
above, we propose a hybrid filtering based MIcro-video haSHtag recommendatiON
MISHON technique to recommend hashtags for micro-videos. Besides content based
filtering, we employ user-based collaborative filtering to enhance
recommendations. Since hashtags reflect users topical interests, we find
similar users based on historical tagging behavior to model user relatedness.
We employ a graph-based deep neural network to model user to user, modality to
modality, and user to modality interactions. We then use refined modality
specific and user representations to recommend pertinent hashtags for
microvideos. The empirical results on three real world datasets demonstrate
that MISHON attains a comparative enhancement of 3.6, 2.8, and 6.5 reported in
percentage concerning the F1 score, respectively. Since cold start users exist
whose historical tagging information is unavailable, we also propose a content
and social influence based technique to model the relatedness of cold start
users with influential users. The proposed solution shows a relative
improvement of 15.8 percent in the F1 score over its content only counterpart.
These results show that the proposed framework mitigates the cold start user
problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parenting: Optimizing Knowledge Selection of Retrieval-Augmented
  Language Models with Parameter Decoupling and Tailored Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxin Xu, Ruizhe Zhang, Xinke Jiang, Yujie Feng, Yuzhen Xiao, Xinyu Ma, Runchuan Zhu, Xu Chu, Junfeng Zhao, Yasha Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) offers an effective solution to the
issues faced by Large Language Models (LLMs) in hallucination generation and
knowledge obsolescence by incorporating externally retrieved knowledge.
However, due to potential conflicts between internal and external knowledge, as
well as retrieval noise, LLMs often struggle to effectively integrate external
evidence, leading to a decline in performance. Although existing methods
attempt to tackle these challenges, they often struggle to strike a balance
between model adherence and robustness, resulting in significant learning
variance. Inspired by human cognitive processes, we propose Parenting, a novel
framework that decouples adherence and robustness within the parameter space of
LLMs. Specifically, Parenting utilizes a key parameter mining method based on
forward activation gain to identify and isolate the crucial parameter units
that are strongly linked to adherence and robustness. Then, Parenting employs a
type-guided tailored tuning strategy, applying specific and appropriate
fine-tuning methods to parameter units representing different capabilities,
aiming to achieve a balanced enhancement of adherence and robustness. Extensive
experiments on various datasets and models validate the effectiveness and
generalizability of our methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Attributed Graph Networks with Alignment and Uniformity
  Constraints for Session-based Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinping Zhao, Chaochao Chen, Jiajie Su, Yizhao Zhang, Baotian Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Session-based Recommendation (SBR), seeking to predict a user's next action
based on an anonymous session, has drawn increasing attention for its
practicability. Most SBR models only rely on the contextual transitions within
a short session to learn item representations while neglecting additional
valuable knowledge. As such, their model capacity is largely limited by the
data sparsity issue caused by short sessions. A few studies have exploited the
Modeling of Item Attributes (MIA) to enrich item representations. However, they
usually involve specific model designs that can hardly transfer to existing
attribute-agnostic SBR models and thus lack universality. In this paper, we
propose a model-agnostic framework, named AttrGAU (Attributed Graph Networks
with Alignment and Uniformity Constraints), to bring the MIA's superiority into
existing attribute-agnostic models, to improve their accuracy and robustness
for recommendation. Specifically, we first build a bipartite attributed graph
and design an attribute-aware graph convolution to exploit the rich attribute
semantics hidden in the heterogeneous item-attribute relationship. We then
decouple existing attribute-agnostic SBR models into the graph neural network
and attention readout sub-modules to satisfy the non-intrusive requirement.
Lastly, we design two representation constraints, i.e., alignment and
uniformity, to optimize distribution discrepancy in representation between the
attribute semantics and collaborative semantics. Extensive experiments on three
public benchmark datasets demonstrate that the proposed AttrGAU framework can
significantly enhance backbone models' recommendation performance and
robustness against data sparsity and data noise issues. Our implementation
codes will be available at https://github.com/ItsukiFujii/AttrGAU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures, 5 tables. Accepted by ICWS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FunnelRAG: A Coarse-to-Fine Progressive Retrieval Paradigm for RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinping Zhao, Yan Zhong, Zetian Sun, Xinshuo Hu, Zhenyu Liu, Dongfang Li, Baotian Hu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) prevails in Large Language Models. It
mainly consists of retrieval and generation. The retrieval modules (a.k.a.
retrievers) aim to find useful information used to facilitate generation
modules (a.k.a. generators). As such, generators' performance largely depends
on the effectiveness and efficiency of retrievers. However, the retrieval
paradigm that we design and use remains flat, which treats the retrieval
procedures as a one-off deal with constant granularity. Despite effectiveness,
we argue that they suffer from two limitations: (1) flat retrieval exerts a
significant burden on one retriever; (2) constant granularity limits the
ceiling of retrieval performance. In this work, we propose a progressive
retrieval paradigm with coarse-to-fine granularity for RAG, termed FunnelRAG,
so as to balance effectiveness and efficiency. Specifically, FunnelRAG
establishes a progressive retrieval pipeline by collaborating coarse-to-fine
granularity, large-to-small quantity, and low-to-high capacity, which can
relieve the burden on one retriever and also promote the ceiling of retrieval
performance. Extensive experiments manifest that FunnelRAG achieves comparable
retrieval performance while the time overhead is reduced by nearly 40 percent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Back-of-the-Book Index Automation for Arabic Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nawal Haidar, Fadi A. Zaraket
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Back-of-the-book indexes are crucial for book readability. Their manual
creation is laborious and error prone. In this paper, we consider automating
back-of-the-book index extraction for Arabic books to help simplify both the
creation and review tasks. Given a back-of-the-book index, we aim to check and
identify the accurate occurrences of index terms relative to the associated
pages. To achieve this, we first define a pool of candidates for each term by
extracting all possible noun phrases from paragraphs appearing on the relevant
index pages. These noun phrases, identified through part-of-speech analysis,
are stored in a vector database for efficient retrieval. We use several
metrics, including exact matches, lexical similarity, and semantic similarity,
to determine the most appropriate occurrence. The candidate with the highest
score based on these metrics is chosen as the occurrence of the term. We
fine-tuned a heuristic method, that considers the above metrics and that
achieves an F1-score of .966 (precision=.966, recall=.966). These excellent
results open the door for future work related to automation of back-of-the-book
index generation and checking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DecKG: Decentralized Collaborative Learning with Knowledge Graph
  Enhancement for POI Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiqi Zheng, Liang Qu, Guanhua Ye, Tong Chen, Yuhui Shi, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized collaborative learning for Point-of-Interest (POI)
recommendation has gained research interest due to its advantages in privacy
preservation and efficiency, as it keeps data locally and leverages
collaborative learning among clients to train models in a decentralized manner.
However, since local data is often limited and insufficient for training
accurate models, a common solution is integrating external knowledge as
auxiliary information to enhance model performance. Nevertheless, this solution
poses challenges for decentralized collaborative learning. Due to private
nature of local data, identifying relevant auxiliary information specific to
each user is non-trivial. Furthermore, resource-constrained local devices
struggle to accommodate all auxiliary information, which places heavy burden on
local storage. To fill the gap, we propose a novel decentralized collaborative
learning with knowledge graph enhancement framework for POI recommendation
(DecKG). Instead of directly uploading interacted items, users generate
desensitized check-in data by uploading general categories of interacted items
and sampling similar items from same category. The server then pretrains KG
without sensitive user-item interactions and deploys relevant partitioned
sub-KGs to individual users. Entities are further refined on the device,
allowing client to client communication to exchange knowledge learned from
local data and sub-KGs. Evaluations across two real-world datasets demonstrate
DecKG's effectiveness recommendation performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAIR: A Massive Benchmark for Evaluating Instructed Retrieval <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiwei Sun, Zhengliang Shi, Jiulong Wu, Lingyong Yan, Xinyu Ma, Yiding Liu, Min Cao, Dawei Yin, Zhaochun Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent information retrieval (IR) models are pre-trained and
instruction-tuned on massive datasets and tasks, enabling them to perform well
on a wide range of tasks and potentially generalize to unseen tasks with
instructions. However, existing IR benchmarks focus on a limited scope of
tasks, making them insufficient for evaluating the latest IR models. In this
paper, we propose MAIR (Massive Instructed Retrieval Benchmark), a
heterogeneous IR benchmark that includes 126 distinct IR tasks across 6
domains, collected from existing datasets. We benchmark state-of-the-art
instruction-tuned text embedding models and re-ranking models. Our experiments
reveal that instruction-tuned models generally achieve superior performance
compared to non-instruction-tuned models on MAIR. Additionally, our results
suggest that current instruction-tuned text embedding models and re-ranking
models still lack effectiveness in specific long-tail tasks. MAIR is publicly
available at https://github.com/sunnweiwei/Mair.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Feature Decorrelation in Cloth-Changing Person Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05536v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05536v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjun Wang, Jiyuan Chen, Renhe Jiang, Xuan Song, Yinqiang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloth-changing person re-identification (CC-ReID) poses a significant
challenge in computer vision. A prevailing approach is to prompt models to
concentrate on causal attributes, like facial features and hairstyles, rather
than confounding elements such as clothing appearance. Traditional methods to
achieve this involve integrating multi-modality data or employing manually
annotated clothing labels, which tend to complicate the model and require
extensive human effort. In our study, we demonstrate that simply reducing
feature correlations during training can significantly enhance the baseline
model's performance. We theoretically elucidate this effect and introduce a
novel regularization technique based on density ratio estimation. This
technique aims to minimize feature correlation in the training process of
cloth-changing ReID baselines. Our approach is model-independent, offering
broad enhancements without needing additional data or labels. We validate our
method through comprehensive experiments on prevalent CC-ReID datasets, showing
its effectiveness in improving baseline models' generalization capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pure Message Passing Can Estimate Common Neighbor for Link Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00976v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00976v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiwen Dong, Zhichun Guo, Nitesh V. Chawla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Message Passing Neural Networks (MPNNs) have emerged as the {\em de facto}
standard in graph representation learning. However, when it comes to link
prediction, they often struggle, surpassed by simple heuristics such as Common
Neighbor (CN). This discrepancy stems from a fundamental limitation: while
MPNNs excel in node-level representation, they stumble with encoding the joint
structural features essential to link prediction, like CN. To bridge this gap,
we posit that, by harnessing the orthogonality of input vectors, pure
message-passing can indeed capture joint structural features. Specifically, we
study the proficiency of MPNNs in approximating CN heuristics. Based on our
findings, we introduce the Message Passing Link Predictor (MPLP), a novel link
prediction model. MPLP taps into quasi-orthogonal vectors to estimate
link-level structural features, all while preserving the node-level
complexities. Moreover, our approach demonstrates that leveraging
message-passing to capture structural features could offset MPNNs'
expressiveness limitations at the expense of estimation variance. We conduct
experiments on benchmark datasets from various domains, where our method
consistently outperforms the baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Neurips'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ mGTE: Generalized Long-Context Text Representation and Reranking Models
  for Multilingual Text Retrieval <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19669v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19669v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan Lin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present systematic efforts in building long-context multilingual text
representation model (TRM) and reranker from scratch for text retrieval. We
first introduce a text encoder (base size) enhanced with RoPE and unpadding,
pre-trained in a native 8192-token context (longer than 512 of previous
multilingual encoders). Then we construct a hybrid TRM and a cross-encoder
reranker by contrastive learning. Evaluations show that our text encoder
outperforms the same-sized previous state-of-the-art XLM-R. Meanwhile, our TRM
and reranker match the performance of large-sized state-of-the-art BGE-M3
models and achieve better results on long-context retrieval benchmarks. Further
analysis demonstrate that our proposed models exhibit higher efficiency during
both training and inference. We believe their efficiency and effectiveness
could benefit various researches and industrial applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version of EMNLP 2024: Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personalized Item Representations in Federated <span class="highlight-title">Multimodal</span> Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Li, Guodong Long, Jing Jiang, Chengqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated recommendation systems are essential for providing personalized
recommendations while protecting user privacy. However, current methods mainly
rely on ID-based item embeddings, neglecting the rich multimodal information of
items. To address this, we propose a Federated Multimodal Recommendation
System, called FedMR. FedMR uses a foundation model on the server to encode
multimodal item data, such as images and text. To handle data heterogeneity
caused by user preference differences, FedMR introduces a Mixing Feature Fusion
Module on each client, which adjusts fusion strategy weights based on user
interaction history to generate personalized item representations that capture
users' fine-grained preferences. FedMR is compatible with existing ID-based
federated recommendation systems, improving performance without modifying the
original framework. Experiments on four real-world multimodal datasets
demonstrate FedMR's effectiveness. The code is available at
https://anonymous.4open.science/r/FedMR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures, 5 tables, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LARA: Linguistic-Adaptive Retrieval-Augmentation for Multi-Turn Intent
  Classification <span class="chip">EMNLP'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16504v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16504v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhua Liu, Yong Keat Tan, Bin Fu, Kwan Hui Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-turn intent classification is notably challenging due to the complexity
and evolving nature of conversational contexts. This paper introduces LARA, a
Linguistic-Adaptive Retrieval-Augmentation framework to enhance accuracy in
multi-turn classification tasks across six languages, accommodating a large
number of intents in chatbot interactions. LARA combines a fine-tuned smaller
model with a retrieval-augmented mechanism, integrated within the architecture
of LLMs. The integration allows LARA to dynamically utilize past dialogues and
relevant intents, thereby improving the understanding of the context.
Furthermore, our adaptive retrieval techniques bolster the cross-lingual
capabilities of LLMs without extensive retraining and fine-tuning.
Comprehensive experiments demonstrate that LARA achieves state-of-the-art
performance on multi-turn intent classification tasks, enhancing the average
accuracy by 3.67\% from state-of-the-art single-turn intent classifiers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Model Powered Digital Biology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02864v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02864v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Pickard, Marc Andrew Choi, Natalie Oliven, Cooper Stansbury, Jillian Cwycyshyn, Nicholas Galioto, Alex Gorodetsky, Alvaro Velasquez, Indika Rajapakse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) are transforming biology,
computer science, and many other research fields, as well as impacting everyday
life. While transformer-based technologies are currently being deployed in
biology, no available agentic system has been developed to tackle
bioinformatics workflows. We present a prototype Bioinformatics Retrieval
Augmented Data (BRAD) digital assistant. BRAD is a chatbot and agentic system
that integrates a suite of tools to handle bioinformatics tasks, from code
execution to online search. We demonstrate its capabilities through (1)
improved question-and-answering with retrieval augmented generation (RAG), (2)
the ability to run complex software pipelines, and (3) the ability to organize
and distribute tasks in agentic workflows. We use BRAD for automation,
performing tasks ranging from gene enrichment and searching the archive to
automatic code generation for running biomarker identification pipelines. BRAD
is a step toward autonomous, self-driving labs for digital biology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 pages, 3 tables, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Dense Retrievers' Robustness with Group-level Reweighting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16605v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16605v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The anchor-document data derived from web graphs offers a wealth of paired
information for training dense retrieval models in an unsupervised manner.
However, unsupervised data contains diverse patterns across the web graph and
often exhibits significant imbalance, leading to suboptimal performance in
underrepresented or difficult groups. In this paper, we introduce WebDRO, an
efficient approach for clustering the web graph data and optimizing group
weights to enhance the robustness of dense retrieval models. Initially, we
build an embedding model for clustering anchor-document pairs. Specifically, we
contrastively train the embedding model for link prediction, which guides the
embedding model in capturing the document features behind the web graph links.
Subsequently, we employ the group distributional robust optimization to
recalibrate the weights across different clusters of anchor-document pairs
during training retrieval models. During training, we direct the model to
assign higher weights to clusters with higher loss and focus more on worst-case
scenarios. This approach ensures that the model has strong generalization
ability on all data patterns. Our experiments on MS MARCO and BEIR demonstrate
that our method can effectively improve retrieval performance in unsupervised
training and finetuning settings. Further analysis confirms the stability and
validity of group weights learned by WebDRO. The code of this paper can be
obtained from https://github.com/Hanpx20/GroupDRO_Dense_Retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatial-Aware Efficient Projector for MLLMs via Multi-Layer Feature
  Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shun Qian, Bingquan Liu, Chengjie Sun, Zhen Xu, Baoxun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The projector plays a crucial role in multi-modal language models (MLLMs).
The number of visual tokens it outputs affects the efficiency of the MLLM,
while the quality of the visual tokens influences the visual understanding
capabilities of the MLLM. Current explorations on the projector focus on
reducing the number of visual tokens to improve efficiency, often overlooking
the inherent spatial discrepancy between the serialized 2-dimensional visual
token sequences and natural language token sequences. A Spatial-Aware Efficient
Projector (SAEP) is proposed to address this issue. In detail, our SAEP method
employs an modified separable depthwise convolution module on multi-layer
visual features to enhance the spatial information of visual tokens. As a
result, our SAEP method can not only largely reduce the number of visual tokens
by 75\%, but also significantly improve the multimodal spatial understanding
capability of MLLMs. Moreover, compared to existing projectors, our SAEP gets
best performances on massive multimodal evaluation benchmarks, which denotes
its effectiveness on bridging the modality gap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangru Zhu, Penglei Sun, Yaoxian Song, Yanghua Xiao, Zhixu Li, Chengyu Wang, Jun Huang, Bei Yang, Xiaoxiao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate interpretation and visualization of human instructions are crucial
for text-to-image (T2I) synthesis. However, current models struggle to capture
semantic variations from word order changes, and existing evaluations, relying
on indirect metrics like text-image similarity, fail to reliably assess these
challenges. This often obscures poor performance on complex or uncommon
linguistic patterns by the focus on frequent word combinations. To address
these deficiencies, we propose a novel metric called SemVarEffect and a
benchmark named SemVarBench, designed to evaluate the causality between
semantic variations in inputs and outputs in T2I synthesis. Semantic variations
are achieved through two types of linguistic permutations, while avoiding
easily predictable literal variations. Experiments reveal that the
CogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.
Semantic variations in object relations are less understood than attributes,
scoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in
UNet or Transformers plays a crucial role in handling semantic variations, a
factor previously overlooked by a focus on textual encoders. Our work
establishes an effective evaluation framework that advances the T2I synthesis
community's exploration of human instruction understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our benchmark and code are available at
  https://github.com/zhuxiangru/SemVarBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GUISE: Graph GaUssIan Shading watErmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the expanding field of generative artificial intelligence, integrating
robust watermarking technologies is essential to protect intellectual property
and maintain content authenticity. Traditionally, watermarking techniques have
been developed primarily for rich information media such as images and audio.
However, these methods have not been adequately adapted for graph-based data,
particularly molecular graphs. Latent 3D graph diffusion(LDM-3DG) is an
ascendant approach in the molecular graph generation field. This model
effectively manages the complexities of molecular structures, preserving
essential symmetries and topological features. We adapt the Gaussian Shading, a
proven performance lossless watermarking technique, to the latent graph
diffusion domain to protect this sophisticated new technology. Our adaptation
simplifies the watermark diffusion process through duplication and padding,
making it adaptable and suitable for various message types. We conduct several
experiments using the LDM-3DG model on publicly available datasets QM9 and
Drugs, to assess the robustness and effectiveness of our technique. Our results
demonstrate that the watermarked molecules maintain statistical parity in 9 out
of 10 performance metrics compared to the original. Moreover, they exhibit a
100% detection rate and a 99% extraction rate in a 2D decoded pipeline, while
also showing robustness against post-editing attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Audio-Visual Deepfakes with Fine-Grained Inconsistencies <span class="chip">BMVC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06753v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06753v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcella Astrid, Enjie Ghorbel, Djamila Aouada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing methods on audio-visual deepfake detection mainly focus on
high-level features for modeling inconsistencies between audio and visual data.
As a result, these approaches usually overlook finer audio-visual artifacts,
which are inherent to deepfakes. Herein, we propose the introduction of
fine-grained mechanisms for detecting subtle artifacts in both spatial and
temporal domains. First, we introduce a local audio-visual model capable of
capturing small spatial regions that are prone to inconsistencies with audio.
For that purpose, a fine-grained mechanism based on a spatially-local distance
coupled with an attention module is adopted. Second, we introduce a
temporally-local pseudo-fake augmentation to include samples incorporating
subtle temporal inconsistencies in our training set. Experiments on the DFDC
and the FakeAVCeleb datasets demonstrate the superiority of the proposed method
in terms of generalization as compared to the state-of-the-art under both
in-dataset and cross-dataset settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in BMVC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Track MusicLDM: Towards Versatile Music Generation with Latent
  Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tornike Karchkhadze, Mohammad Rasool Izadi, Ke Chen, Gerard Assayag, Shlomo Dubnov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have shown promising results in cross-modal generation tasks
involving audio and music, such as text-to-sound and text-to-music generation.
These text-controlled music generation models typically focus on generating
music by capturing global musical attributes like genre and mood. However,
music composition is a complex, multilayered task that often involves musical
arrangement as an integral part of the process. This process involves composing
each instrument to align with existing ones in terms of beat, dynamics,
harmony, and melody, requiring greater precision and control over tracks than
text prompts usually provide. In this work, we address these challenges by
extending the MusicLDM, a latent diffusion model for music, into a multi-track
generative model. By learning the joint probability of tracks sharing a
context, our model is capable of generating music across several tracks that
correspond well to each other, either conditionally or unconditionally.
Additionally, our model is capable of arrangement generation, where the model
can generate any subset of tracks given the others (e.g., generating a piano
track complementing given bass and drum tracks). We compared our model with an
existing multi-track generative model and demonstrated that our model achieves
considerable improvements across objective metrics for both total and
arrangement generation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Proceedings of The second international workshop on eXplainable AI for
  the Arts (XAIxArts) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14485v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14485v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Bryan-Kinns, Corey Ford, Shuoyang Zheng, Helen Kennedy, Alan Chamberlain, Makayla Lewis, Drew Hemment, Zijin Li, Qiong Wu, Lanxi Xiao, Gus Xia, Jeba Rezwana, Michael Clemens, Gabriel Vigliensoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This second international workshop on explainable AI for the Arts (XAIxArts)
brought together a community of researchers in HCI, Interaction Design, AI,
explainable AI (XAI), and digital arts to explore the role of XAI for the Arts.
Workshop held at the 16th ACM Conference on Creativity and Cognition (C&C
2024), Chicago, USA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of The second international workshop on eXplainable AI
  for the Arts (XAIxArts)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving <span class="highlight-title">Multimodal</span> Learning with Multi-Loss Gradient Modulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Kontras, Christos Chatzichristos, Matthew Blaschko, Maarten De Vos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from multiple modalities, such as audio and video, offers
opportunities for leveraging complementary information, enhancing robustness,
and improving contextual understanding and performance. However, combining such
modalities presents challenges, especially when modalities differ in data
structure, predictive contribution, and the complexity of their learning
processes. It has been observed that one modality can potentially dominate the
learning process, hindering the effective utilization of information from other
modalities and leading to sub-optimal model performance. To address this issue
the vast majority of previous works suggest to assess the unimodal
contributions and dynamically adjust the training to equalize them. We improve
upon previous work by introducing a multi-loss objective and further refining
the balancing process, allowing it to dynamically adjust the learning pace of
each modality in both directions, acceleration and deceleration, with the
ability to phase out balancing effects upon convergence. We achieve superior
results across three audio-video datasets: on CREMA-D, models with ResNet
backbone encoders surpass the previous best by 1.9% to 12.4%, and Conformer
backbone models deliver improvements ranging from 2.8% to 14.1% across
different fusion methods. On AVE, improvements range from 2.7% to 7.7%, while
on UCF101, gains reach up to 6.1%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SceneDreamer360: Text-Driven 3D-Consistent Scene Generation with
  Panoramic Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenrui Li, Fucheng Cai, Yapeng Mi, Zhe Yang, Wangmeng Zuo, Xingtao Wang, Xiaopeng Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-driven 3D scene generation has seen significant advancements recently.
However, most existing methods generate single-view images using generative
models and then stitch them together in 3D space. This independent generation
for each view often results in spatial inconsistency and implausibility in the
3D scenes. To address this challenge, we proposed a novel text-driven
3D-consistent scene generation model: SceneDreamer360. Our proposed method
leverages a text-driven panoramic image generation model as a prior for 3D
scene generation and employs 3D Gaussian Splatting (3DGS) to ensure consistency
across multi-view panoramic images. Specifically, SceneDreamer360 enhances the
fine-tuned Panfusion generator with a three-stage panoramic enhancement,
enabling the generation of high-resolution, detail-rich panoramic images.
During the 3D scene construction, a novel point cloud fusion initialization
method is used, producing higher quality and spatially consistent point clouds.
Our extensive experiments demonstrate that compared to other methods,
SceneDreamer360 with its panoramic image generation and 3DGS can produce higher
quality, spatially consistent, and visually appealing 3D scenes from any text
prompt. Our codes are available at
\url{https://github.com/liwrui/SceneDreamer360}.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-13T00:00:00Z">2024-10-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Customer Feedback for <span class="highlight-title">Multi-modal</span> Insight Extraction <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandeep Sricharan Mukku, Abinesh Kanagarajan, Pushpendu Ghosh, Chetan Aggarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Businesses can benefit from customer feedback in different modalities, such
as text and images, to enhance their products and services. However, it is
difficult to extract actionable and relevant pairs of text segments and images
from customer feedback in a single pass. In this paper, we propose a novel
multi-modal method that fuses image and text information in a latent space and
decodes it to extract the relevant feedback segments using an image-text
grounded text decoder. We also introduce a weakly-supervised data generation
technique that produces training data for this task. We evaluate our model on
unseen data and demonstrate that it can effectively mine actionable insights
from multi-modal customer feedback, outperforming the existing baselines by
$14$ points in F1 score.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Rank for Multiple Retrieval-Augmented Models through
  Iterative Utility Maximization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Salemi, Hamed Zamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the design of a unified search engine to serve
multiple retrieval-augmented generation (RAG) agents, each with a distinct
task, backbone large language model (LLM), and retrieval-augmentation strategy.
We introduce an iterative approach where the search engine generates retrieval
results for these RAG agents and gathers feedback on the quality of the
retrieved documents during an offline phase. This feedback is then used to
iteratively optimize the search engine using a novel expectation-maximization
algorithm, with the goal of maximizing each agent's utility function.
Additionally, we adapt this approach to an online setting, allowing the search
engine to refine its behavior based on real-time individual agents feedback to
better serve the results for each of them. Experiments on diverse datasets from
the Knowledge-Intensive Language Tasks (KILT) benchmark demonstrates that our
approach significantly on average outperforms competitive baselines across 18
RAG models. We also demonstrate that our method effectively ``personalizes''
the retrieval process for each RAG agent based on the collected feedback.
Finally, we provide a comprehensive ablation study to explore various aspects
of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Role of Fake Users in Sequential Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippo Betello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential Recommender Systems (SRSs) are widely used to model user behavior
over time, yet their robustness remains an under-explored area of research. In
this paper, we conduct an empirical study to assess how the presence of fake
users, who engage in random interactions, follow popular or unpopular items, or
focus on a single genre, impacts the performance of SRSs in real-world
scenarios. We evaluate two SRS models across multiple datasets, using
established metrics such as Normalized Discounted Cumulative Gain (NDCG) and
Rank Sensitivity List (RLS) to measure performance. While traditional metrics
like NDCG remain relatively stable, our findings reveal that the presence of
fake users severely degrades RLS metrics, often reducing them to near-zero
values. These results highlight the need for further investigation into the
effects of fake users on training data and emphasize the importance of
developing more resilient SRSs that can withstand different types of
adversarial attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis and Design of a Personalized Recommendation System Based on a
  Dynamic User Interest Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunyan Mao, Shuaishuai Huang, Mingxiu Sui, Haowei Yang, Xueshe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of the internet and the explosion of information,
providing users with accurate personalized recommendations has become an
important research topic. This paper designs and analyzes a personalized
recommendation system based on a dynamic user interest model. The system
captures user behavior data, constructs a dynamic user interest model, and
combines multiple recommendation algorithms to provide personalized content to
users. The research results show that this system significantly improves
recommendation accuracy and user satisfaction. This paper discusses the
system's architecture design, algorithm implementation, and experimental
results in detail and explores future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViFi-ReID: A Two-Stream Vision-WiFi <span class="highlight-title">Multimodal</span> Approach for Person
  Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Mao, Chong Tan, Jingqi Hu, Min Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person re-identification(ReID), as a crucial technology in the field of
security, plays a vital role in safety inspections, personnel counting, and
more. Most current ReID approaches primarily extract features from images,
which are easily affected by objective conditions such as clothing changes and
occlusions. In addition to cameras, we leverage widely available routers as
sensing devices by capturing gait information from pedestrians through the
Channel State Information (CSI) in WiFi signals and contribute a multimodal
dataset. We employ a two-stream network to separately process video
understanding and signal analysis tasks, and conduct multi-modal fusion and
contrastive learning on pedestrian video and WiFi data. Extensive experiments
in real-world scenarios demonstrate that our method effectively uncovers the
correlations between heterogeneous data, bridges the gap between visual and
signal modalities, significantly expands the sensing range, and improves ReID
accuracy across multiple sensors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Study of PDF Parsing Tools Across Diverse Document
  Categories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Narayan S. Adhikari, Shradha Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PDF is one of the most prominent data formats, making PDF parsing crucial for
information extraction and retrieval, particularly with the rise of RAG
systems. While various PDF parsing tools exist, their effectiveness across
different document types remains understudied, especially beyond academic
papers. Our research aims to address this gap by comparing 10 popular PDF
parsing tools across 6 document categories using the DocLayNet dataset. These
tools include PyPDF, pdfminer.six, PyMuPDF, pdfplumber, pypdfium2,
Unstructured, Tabula, Camelot, as well as the deep learning-based tools Nougat
and Table Transformer(TATR). We evaluated both text extraction and table
detection capabilities. For text extraction, PyMuPDF and pypdfium generally
outperformed others, but all parsers struggled with Scientific and Patent
documents. For these challenging categories, learning-based tools like Nougat
demonstrated superior performance. In table detection, TATR excelled in the
Financial, Patent, Law & Regulations, and Scientific categories. Table
detection tool Camelot performed best for tender documents, while PyMuPDF
performed superior in the Manual category. Our findings highlight the
importance of selecting appropriate parsing tools based on document type and
specific tasks, providing valuable insights for researchers and practitioners
working with diverse document sources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages,11 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Driving Simulations via Conversation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rimvydas Rubavicius, Antonio Valerio Miceli-Barone, Alex Lascarides, Subramanian Ramamoorthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cyber-physical systems like autonomous vehicles are tested in simulation
before deployment, using domain-specific programs for scenario specification.
To aid the testing of autonomous vehicles in simulation, we design a natural
language interface, using an instruction-following large language model, to
assist a non-coding domain expert in synthesising the desired scenarios and
vehicle behaviours. We show that using it to convert utterances to the symbolic
program is feasible, despite the very small training dataset. Human experiments
show that dialogue is critical to successful simulation generation, leading to
a 4.5 times higher success rate than a generation without engaging in extended
conversation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ContextWIN: Whittle Index Based Mixture-of-Experts Neural Model For
  Restless Bandits Via Deep RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanqiu Guo, Wayne Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces ContextWIN, a novel architecture that extends the
Neural Whittle Index Network (NeurWIN) model to address Restless Multi-Armed
Bandit (RMAB) problems with a context-aware approach. By integrating a mixture
of experts within a reinforcement learning framework, ContextWIN adeptly
utilizes contextual information to inform decision-making in dynamic
environments, particularly in recommendation systems. A key innovation is the
model's ability to assign context-specific weights to a subset of NeurWIN
networks, thus enhancing the efficiency and accuracy of the Whittle index
computation for each arm. The paper presents a thorough exploration of
ContextWIN, from its conceptual foundation to its implementation and potential
applications. We delve into the complexities of RMABs and the significance of
incorporating context, highlighting how ContextWIN effectively harnesses these
elements. The convergence of both the NeurWIN and ContextWIN models is
rigorously proven, ensuring theoretical robustness. This work lays the
groundwork for future advancements in applying contextual information to
complex decision-making scenarios, recognizing the need for comprehensive
dataset exploration and environment development for full potential realization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChartKG: A Knowledge-Graph-Based Representation for Chart Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiguang Zhou, Haoxuan Wang, Zhengqing Zhao, Fengling Zheng, Yongheng Wang, Wei Chen, Yong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chart images, such as bar charts, pie charts, and line charts, are
explosively produced due to the wide usage of data visualizations. Accordingly,
knowledge mining from chart images is becoming increasingly important, which
can benefit downstream tasks like chart retrieval and knowledge graph
completion. However, existing methods for chart knowledge mining mainly focus
on converting chart images into raw data and often ignore their visual
encodings and semantic meanings, which can result in information loss for many
downstream tasks. In this paper, we propose ChartKG, a novel knowledge graph
(KG) based representation for chart images, which can model the visual elements
in a chart image and semantic relations among them including visual encodings
and visual insights in a unified manner. Further, we develop a general
framework to convert chart images to the proposed KG-based representation. It
integrates a series of image processing techniques to identify visual elements
and relations, e.g., CNNs to classify charts, yolov5 and optical character
recognition to parse charts, and rule-based methods to construct graphs. We
present four cases to illustrate how our knowledge-graph-based representation
can model the detailed visual elements and semantic relations in charts, and
further demonstrate how our approach can benefit downstream applications such
as semantic-aware chart retrieval and chart question answering. We also conduct
quantitative evaluations to assess the two fundamental building blocks of our
chart-to-KG framework, i.e., object recognition and optical character
recognition. The results provide support for the usefulness and effectiveness
of ChartKG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Digital Investigative Journalism using SociaLens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11890v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11890v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hasan M. Jamil, Sajratul Y. Rubaiat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Media companies witnessed a significant transformation with the rise of the
internet, bigdata, machine learning (ML) and AI. Recent emergence of large
language models (LLM) have added another aspect to this transformation.
Researchers believe that with the help of these technologies, investigative
digital journalism will enter a new era. Using a smart set of data gathering
and analysis tools, journalists will be able to create data driven contents and
insights in unprecedented ways. In this paper, we introduce a versatile and
autonomous investigative journalism tool, called {\em SociaLens}, for
identifying and extracting query specific data from online sources, responding
to probing queries and drawing conclusions entailed by large volumes of data
using ML analytics fully autonomously. We envision its use in investigative
journalism, law enforcement and social policy planning. The proposed system
capitalizes on the integration of ML technology with LLMs and advanced bigdata
search techniques. We illustrate the functionality of SociaLens using a focused
case study on rape incidents in a developing country and demonstrate that
journalists can gain nuanced insights without requiring coding expertise they
might lack. SociaLens is designed as a ChatBot that is capable of contextual
conversation, find and collect data relevant to queries, initiate ML tasks to
respond to queries, generate textual and visual reports, all fully autonomously
within the ChatBot environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agentic Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weinan Zhang, Junwei Liao, Ning Li, Kounianhua Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What will information entry look like in the next generation of digital
products? Since the 1970s, user access to relevant information has relied on
domain-specific architectures of information retrieval (IR). Over the past two
decades, the advent of modern IR systems, including web search engines and
personalized recommender systems, has greatly improved the efficiency of
retrieving relevant information from vast data corpora. However, the core
paradigm of these IR systems remains largely unchanged, relying on filtering a
predefined set of candidate items. Since 2022, breakthroughs in large language
models (LLMs) have begun transforming how information is accessed, establishing
a new technical paradigm. In this position paper, we introduce Agentic
Information Retrieval (Agentic IR), a novel IR paradigm shaped by the
capabilities of LLM agents. Agentic IR expands the scope of accessible tasks
and leverages a suite of new techniques to redefine information retrieval. We
discuss three types of cutting-edge applications of agentic IR and the
challenges faced. We propose that agentic IR holds promise for generating
innovative applications, potentially becoming a central information entry point
in future digital ecosystems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, position paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ColBERT Retrieval and Ensemble Response Scoring for Language Model
  Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10808v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10808v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Gichamba, Tewodros Kederalah Idris, Brian Ebiyau, Eric Nyberg, Teruko Mitamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain-specific question answering remains challenging for language models,
given the deep technical knowledge required to answer questions correctly. This
difficulty is amplified for smaller language models that cannot encode as much
information in their parameters as larger models. The "Specializing Large
Language Models for Telecom Networks" challenge aimed to enhance the
performance of two small language models, Phi-2 and Falcon-7B in
telecommunication question answering. In this paper, we present our question
answering systems for this challenge. Our solutions achieved leading marks of
81.9% accuracy for Phi-2 and 57.3% for Falcon-7B. We have publicly released our
code and fine-tuned models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, and 8 tables. This paper has been accepted at the
  2024 IEEE Global Communications (GLOBECOM) Workshops</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating D-MERIT of Partial-annotation on Information Retrieval <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16048v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16048v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Royi Rassin, Yaron Fairstein, Oren Kalinsky, Guy Kushilevitz, Nachshon Cohen, Alexander Libov, Yoav Goldberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval models are often evaluated on partially-annotated datasets. Each
query is mapped to a few relevant texts and the remaining corpus is assumed to
be irrelevant. As a result, models that successfully retrieve false negatives
are punished in evaluation. Unfortunately, completely annotating all texts for
every query is not resource efficient. In this work, we show that using
partially-annotated datasets in evaluation can paint a distorted picture. We
curate D-MERIT, a passage retrieval evaluation set from Wikipedia, aspiring to
contain all relevant passages for each query. Queries describe a group (e.g.,
"journals about linguistics") and relevant passages are evidence that entities
belong to the group (e.g., a passage indicating that "Language" is a journal
about linguistics). We show that evaluating on a dataset containing annotations
for only a subset of the relevant passages might result in misleading ranking
of the retrieval systems and that as more relevant texts are included in the
evaluation set, the rankings converge. We propose our dataset as a resource for
evaluation and our study as a recommendation for balance between
resource-efficiency and reliable evaluation when annotating evaluation sets for
text retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 main track. Our dataset can be downloaded from
  https://D-MERIT.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Grained Embedding Dimension Optimization During Training for
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04408v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04408v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinyi Luo, Penghan Wang, Wei Zhang, Fan Lai, Jiachen Mao, Xiaohan Wei, Jun Song, Wei-Yu Tsai, Shuai Yang, Yuxi Hu, Xuehai Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Huge embedding tables in modern deep learning recommender models (DLRM)
require prohibitively large memory during training and inference. This paper
proposes FIITED, a system to automatically reduce the memory footprint via
FIne-grained In-Training Embedding Dimension pruning. By leveraging the key
insight that embedding vectors are not equally important, FIITED adaptively
adjusts the dimension of each individual embedding vector during model
training, assigning larger dimensions to more important embeddings while
adapting to dynamic changes in data. We prioritize embedding dimensions with
higher frequencies and gradients as more important. To enable efficient pruning
of embeddings and their dimensions during model training, we propose an
embedding storage system based on virtually-hashed physically-indexed hash
tables. Experiments on two industry models and months of realistic datasets
show that FIITED can reduce DLRM embedding size by more than 65% while
preserving model quality, outperforming state-of-the-art in-training embedding
pruning methods. On public datasets, FIITED can reduce the size of embedding
tables by 2.1x to 800x with negligible accuracy drop, while improving model
throughput.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EHI: End-to-end Learning of Hierarchical Index for Efficient Dense
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08891v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08891v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramnath Kumar, Anshul Mittal, Nilesh Gupta, Aditya Kusupati, Inderjit Dhillon, Prateek Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense embedding-based retrieval is widely used for semantic search and
ranking. However, conventional two-stage approaches, involving contrastive
embedding learning followed by approximate nearest neighbor search (ANNS), can
suffer from misalignment between these stages. This mismatch degrades retrieval
performance. We propose End-to-end Hierarchical Indexing (EHI), a novel method
that directly addresses this issue by jointly optimizing embedding generation
and ANNS structure. EHI leverages a dual encoder for embedding queries and
documents while simultaneously learning an inverted file index (IVF)-style tree
structure. To facilitate the effective learning of this discrete structure, EHI
introduces dense path embeddings that encodes the path traversed by queries and
documents within the tree. Extensive evaluations on standard benchmarks,
including MS MARCO (Dev set) and TREC DL19, demonstrate EHI's superiority over
traditional ANNS index. Under the same computational constraints, EHI
outperforms existing state-of-the-art methods by +1.45% in MRR@10 on MS MARCO
(Dev) and +8.2% in nDCG@10 on TREC DL19, highlighting the benefits of our
end-to-end approach.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Reproducible Learning-based Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Pang, Muhammad Asad Lodhi, Junghyun Ahn, Yuning Huang, Dong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A deep learning system typically suffers from a lack of reproducibility that
is partially rooted in hardware or software implementation details. The
irreproducibility leads to skepticism in deep learning technologies and it can
hinder them from being deployed in many applications. In this work, the
irreproducibility issue is analyzed where deep learning is employed in
compression systems while the encoding and decoding may be run on devices from
different manufacturers. The decoding process can even crash due to a single
bit difference, e.g., in a learning-based entropy coder. For a given deep
learning-based module with limited resources for protection, we first suggest
that reproducibility can only be assured when the mismatches are bounded. Then
a safeguarding mechanism is proposed to tackle the challenges. The proposed
method may be applied for different levels of protection either at the
reconstruction level or at a selected decoding level. Furthermore, the overhead
introduced for the protection can be scaled down accordingly when the error
bound is being suppressed. Experiments demonstrate the effectiveness of the
proposed approach for learning-based compression systems, e.g., in image
compression and point cloud compression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MMSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VidMuse: A Simple Video-to-Music Generation Framework with
  Long-Short-Term Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04321v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04321v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyue Tian, Zhaoyang Liu, Ruibin Yuan, Jiahao Pan, Qifeng Liu, Xu Tan, Qifeng Chen, Wei Xue, Yike Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we systematically study music generation conditioned solely on
the video. First, we present a large-scale dataset comprising 360K video-music
pairs, including various genres such as movie trailers, advertisements, and
documentaries. Furthermore, we propose VidMuse, a simple framework for
generating music aligned with video inputs. VidMuse stands out by producing
high-fidelity music that is both acoustically and semantically aligned with the
video. By incorporating local and global visual cues, VidMuse enables the
creation of musically coherent audio tracks that consistently match the video
content through Long-Short-Term modeling. Through extensive experiments,
VidMuse outperforms existing models in terms of audio quality, diversity, and
audio-visual alignment. The code and datasets will be available at
https://github.com/ZeyueT/VidMuse/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code and datasets will be available at
  https://github.com/ZeyueT/VidMuse/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Less for More: Enhanced Feedback-aligned Mixed LLMs for Molecule Caption
  Generation and Fine-Grained NLI Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13984v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13984v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitris Gkoumas, Maria Liakata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific language models drive research innovation but require extensive
fine-tuning on large datasets. This work enhances such models by improving
their inference and evaluation capabilities with minimal or no additional
training. Focusing on molecule caption generation, we explore synergies between
alignment fine-tuning and model merging in a cross-modal setup. We reveal
intriguing insights into the behaviour and suitability of such methods while
significantly surpassing state-of-the-art models. Moreover, we propose a novel
atomic-level evaluation method leveraging off-the-shelf Natural Language
Inference (NLI) models for use in the unseen chemical domain. Our experiments
demonstrate that our evaluation operates at the right level of granularity,
effectively handling multiple content units and subsentence reasoning, while
widely adopted NLI methods consistently misalign with assessment criteria.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-10-21T05:28:45.994875951Z">
            2024-10-21 05:28:45 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
