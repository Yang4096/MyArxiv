{"2024-10-16T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.12791v1","updated":"2024-10-16T17:59:52Z","published":"2024-10-16T17:59:52Z","title":"Context is Key(NMF): Modelling Topical Information Dynamics in Chinese\n  Diaspora Media","summary":"  Does the People's Republic of China (PRC) interfere with European elections\nthrough ethnic Chinese diaspora media? This question forms the basis of an\nongoing research project exploring how PRC narratives about European elections\nare represented in Chinese diaspora media, and thus the objectives of PRC news\nmedia manipulation. In order to study diaspora media efficiently and at scale,\nit is necessary to use techniques derived from quantitative text analysis, such\nas topic modelling. In this paper, we present a pipeline for studying\ninformation dynamics in Chinese media. Firstly, we present KeyNMF, a new\napproach to static and dynamic topic modelling using transformer-based\ncontextual embedding models. We provide benchmark evaluations to demonstrate\nthat our approach is competitive on a number of Chinese datasets and metrics.\nSecondly, we integrate KeyNMF with existing methods for describing information\ndynamics in complex systems. We apply this pipeline to data from five news\nsites, focusing on the period of time leading up to the 2024 European\nparliamentary elections. Our methods and results demonstrate the effectiveness\nof KeyNMF for studying information dynamics in Chinese media and lay groundwork\nfor further work addressing the broader research questions.\n","authors":["Ross Deans Kristensen-McLachlan","Rebecca M. M. Hicke","Márton Kardos","Mette Thunø"],"pdf_url":"https://arxiv.org/pdf/2410.12791v1.pdf","comment":"Accepted to the 2024 Computational Humanities Research Conference\n  (CHR)"},{"id":"http://arxiv.org/abs/2410.12788v1","updated":"2024-10-16T17:59:32Z","published":"2024-10-16T17:59:32Z","title":"Meta-Chunking: Learning Efficient Text Segmentation via Logical\n  Perception","summary":"  Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline, which impacts the quality of knowledge-intensive\ntasks. This paper introduces the concept of Meta-Chunking, which refers to a\ngranularity between sentences and paragraphs, consisting of a collection of\nsentences within a paragraph that have deep linguistic logical connections. To\nimplement Meta-Chunking, we designed two strategies based on LLMs: Margin\nSampling Chunking and Perplexity Chunking. The former employs LLMs to perform\nbinary classification on whether consecutive sentences need to be segmented,\nmaking decisions based on the probability difference obtained from margin\nsampling. The latter precisely identifies text chunk boundaries by analyzing\nthe characteristics of perplexity distribution. Additionally, considering the\ninherent complexity of different texts, we propose a strategy that combines\nMeta-Chunking with dynamic merging to achieve a balance between fine-grained\nand coarse-grained text chunking. Experiments conducted on eleven datasets\ndemonstrate that Meta-Chunking can more efficiently improve the performance of\nsingle-hop and multi-hop question answering based on RAG. For instance, on the\n2WikiMultihopQA dataset, it outperforms similarity chunking by 1.32 while only\nconsuming 45.8% of the time. Our code is available at\nhttps://github.com/IAAR-Shanghai/Meta-Chunking.\n","authors":["Jihao Zhao","Zhiyuan Ji","Pengnian Qi","Simin Niu","Bo Tang","Feiyu Xiong","Zhiyu Li"],"pdf_url":"https://arxiv.org/pdf/2410.12788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12784v1","updated":"2024-10-16T17:58:19Z","published":"2024-10-16T17:58:19Z","title":"JudgeBench: A Benchmark for Evaluating LLM-based Judges","summary":"  LLM-based judges have emerged as a scalable alternative to human evaluation\nand are increasingly used to assess, compare, and improve models. However, the\nreliability of LLM-based judges themselves is rarely scrutinized. As LLMs\nbecome more advanced, their responses grow more sophisticated, requiring\nstronger judges to evaluate them. Existing benchmarks primarily focus on a\njudge's alignment with human preferences, but often fail to account for more\nchallenging tasks where crowdsourced human preference is a poor indicator of\nfactual and logical correctness. To address this, we propose a novel evaluation\nframework to objectively evaluate LLM-based judges. Based on this framework, we\npropose JudgeBench, a benchmark for evaluating LLM-based judges on challenging\nresponse pairs spanning knowledge, reasoning, math, and coding. JudgeBench\nleverages a novel pipeline for converting existing difficult datasets into\nchallenging response pairs with preference labels reflecting objective\ncorrectness. Our comprehensive evaluation on a collection of prompted judges,\nfine-tuned judges, multi-agent judges, and reward models shows that JudgeBench\nposes a significantly greater challenge than previous benchmarks, with many\nstrong models (e.g., GPT-4o) performing just slightly better than random\nguessing. Overall, JudgeBench offers a reliable platform for assessing\nincreasingly advanced LLM-based judges. Data and code are available at\nhttps://github.com/ScalerLab/JudgeBench .\n","authors":["Sijun Tan","Siyuan Zhuang","Kyle Montgomery","William Y. Tang","Alejandro Cuadron","Chenguang Wang","Raluca Ada Popa","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2410.12784v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.12782v1","updated":"2024-10-16T17:56:49Z","published":"2024-10-16T17:56:49Z","title":"In-Context Learning Enables Robot Action Prediction in LLMs","summary":"  Recently, Large Language Models (LLMs) have achieved remarkable success using\nin-context learning (ICL) in the language domain. However, leveraging the ICL\ncapabilities within LLMs to directly predict robot actions remains largely\nunexplored. In this paper, we introduce RoboPrompt, a framework that enables\noff-the-shelf text-only LLMs to directly predict robot actions through ICL\nwithout training. Our approach first heuristically identifies keyframes that\ncapture important moments from an episode. Next, we extract end-effector\nactions from these keyframes as well as the estimated initial object poses, and\nboth are converted into textual descriptions. Finally, we construct a\nstructured template to form ICL demonstrations from these textual descriptions\nand a task instruction. This enables an LLM to directly predict robot actions\nat test time. Through extensive experiments and analysis, RoboPrompt shows\nstronger performance over zero-shot and ICL baselines in simulated and\nreal-world settings.\n","authors":["Yida Yin","Zekai Wang","Yuvan Sharma","Dantong Niu","Trevor Darrell","Roei Herzig"],"pdf_url":"https://arxiv.org/pdf/2410.12782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12777v1","updated":"2024-10-16T17:51:25Z","published":"2024-10-16T17:51:25Z","title":"Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned\n  Concepts","summary":"  With the rapid progress of diffusion-based content generation, significant\nefforts are being made to unlearn harmful or copyrighted concepts from\npretrained diffusion models (DMs) to prevent potential model misuse. However,\nit is observed that even when DMs are properly unlearned before release,\nmalicious finetuning can compromise this process, causing DMs to relearn the\nunlearned concepts. This occurs partly because certain benign concepts (e.g.,\n\"skin\") retained in DMs are related to the unlearned ones (e.g., \"nudity\"),\nfacilitating their relearning via finetuning. To address this, we propose\nmeta-unlearning on DMs. Intuitively, a meta-unlearned DM should behave like an\nunlearned DM when used as is; moreover, if the meta-unlearned DM undergoes\nmalicious finetuning on unlearned concepts, the related benign concepts\nretained within it will be triggered to self-destruct, hindering the relearning\nof unlearned concepts. Our meta-unlearning framework is compatible with most\nexisting unlearning methods, requiring only the addition of an\neasy-to-implement meta objective. We validate our approach through empirical\nexperiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4\nand SDXL), supported by extensive ablation studies. Our code is available at\nhttps://github.com/sail-sg/Meta-Unlearning.\n","authors":["Hongcheng Gao","Tianyu Pang","Chao Du","Taihang Hu","Zhijie Deng","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.12777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12774v1","updated":"2024-10-16T17:49:45Z","published":"2024-10-16T17:49:45Z","title":"Identifying Task Groupings for Multi-Task Learning Using Pointwise\n  V-Usable Information","summary":"  The success of multi-task learning can depend heavily on which tasks are\ngrouped together. Naively grouping all tasks or a random set of tasks can\nresult in negative transfer, with the multi-task models performing worse than\nsingle-task models. Though many efforts have been made to identify task\ngroupings and to measure the relatedness among different tasks, it remains a\nchallenging research topic to define a metric to identify the best task\ngrouping out of a pool of many potential task combinations. We propose a metric\nof task relatedness based on task difficulty measured by pointwise V-usable\ninformation (PVI). PVI is a recently proposed metric to estimate how much\nusable information a dataset contains given a model. We hypothesize that tasks\nwith not statistically different PVI estimates are similar enough to benefit\nfrom the joint learning process. We conduct comprehensive experiments to\nevaluate the feasibility of this metric for task grouping on 15 NLP datasets in\nthe general, biomedical, and clinical domains. We compare the results of the\njoint learners against single learners, existing baseline methods, and recent\nlarge language models, including Llama 2 and GPT-4. The results show that by\ngrouping tasks with similar PVI estimates, the joint learners yielded\ncompetitive results with fewer total parameters, with consistent performance\nacross domains.\n","authors":["Yingya Li","Timothy Miller","Steven Bethard","Guergana Savova"],"pdf_url":"https://arxiv.org/pdf/2410.12774v1.pdf","comment":"main paper 12 pages, Appendix 7 pages, 1 figure, 18 tables"},{"id":"http://arxiv.org/abs/2404.12494v2","updated":"2024-10-16T17:45:10Z","published":"2024-04-18T20:17:23Z","title":"BIRD: A Trustworthy Bayesian Inference Framework for Large Language\n  Models","summary":"  Predictive models often need to work with incomplete information in\nreal-world tasks. Consequently, they must provide reliable probability or\nconfidence estimation, especially in large-scale decision making and planning\ntasks. Current large language models (LLM) are insufficient for such accurate\nestimations, but they can generate relevant factors that may affect the\nprobabilities, produce coarse-grained probabilities when the information is\nmore complete, and help determine which factors are relevant to specific\ndownstream contexts. In this paper, we make use of these capabilities of LLMs\nto provide a significantly more accurate probabilistic estimation. We propose\nBIRD, a novel probabilistic inference framework that aligns a Bayesian network\nwith LLM abductions and then estimates more accurate probabilities in a\ndeduction step. We show BIRD provides reliable probability estimations that are\n30\\% better than those provided directly by LLM baselines. These estimates can\nfurther contribute to better and more trustworthy decision-making.\n","authors":["Yu Feng","Ben Zhou","Weidong Lin","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2404.12494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12759v1","updated":"2024-10-16T17:30:58Z","published":"2024-10-16T17:30:58Z","title":"Unitary Multi-Margin BERT for Robust Natural Language Processing","summary":"  Recent developments in adversarial attacks on deep learning leave many\nmission-critical natural language processing (NLP) systems at risk of\nexploitation. To address the lack of computationally efficient adversarial\ndefense methods, this paper reports a novel, universal technique that\ndrastically improves the robustness of Bidirectional Encoder Representations\nfrom Transformers (BERT) by combining the unitary weights with the multi-margin\nloss. We discover that the marriage of these two simple ideas amplifies the\nprotection against malicious interference. Our model, the unitary multi-margin\nBERT (UniBERT), boosts post-attack classification accuracies significantly by\n5.3% to 73.8% while maintaining competitive pre-attack accuracies. Furthermore,\nthe pre-attack and post-attack accuracy tradeoff can be adjusted via a single\nscalar parameter to best fit the design requirements for the target\napplications.\n","authors":["Hao-Yuan Chang","Kang L. Wang"],"pdf_url":"https://arxiv.org/pdf/2410.12759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12757v1","updated":"2024-10-16T17:25:25Z","published":"2024-10-16T17:25:25Z","title":"StyleDistance: Stronger Content-Independent Style Embeddings with\n  Synthetic Parallel Examples","summary":"  Style representations aim to embed texts with similar writing styles closely\nand texts with different styles far apart, regardless of content. However, the\ncontrastive triplets often used for training these representations may vary in\nboth style and content, leading to potential content leakage in the\nrepresentations. We introduce StyleDistance, a novel approach to training\nstronger content-independent style embeddings. We use a large language model to\ncreate a synthetic dataset of near-exact paraphrases with controlled style\nvariations, and produce positive and negative examples across 40 distinct style\nfeatures for precise contrastive learning. We assess the quality of our\nsynthetic data and embeddings through human and automatic evaluations.\nStyleDistance enhances the content-independence of style embeddings, which\ngeneralize to real-world benchmarks and outperform leading style\nrepresentations in downstream applications. Our model can be found at\nhttps://huggingface.co/StyleDistance/styledistance .\n","authors":["Ajay Patel","Jiacheng Zhu","Justin Qiu","Zachary Horvitz","Marianna Apidianaki","Kathleen McKeown","Chris Callison-Burch"],"pdf_url":"https://arxiv.org/pdf/2410.12757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10267v2","updated":"2024-10-16T17:22:54Z","published":"2023-11-17T01:27:01Z","title":"Energy and Carbon Considerations of Fine-Tuning BERT","summary":"  Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP\ncommunity, existing work quantifying energy costs and associated carbon\nemissions has largely focused on language model pre-training. Although a single\npre-training run draws substantially more energy than fine-tuning, fine-tuning\nis performed more frequently by many more individual actors, and thus must be\naccounted for when considering the energy and carbon footprint of NLP. In order\nto better characterize the role of fine-tuning in the landscape of energy and\ncarbon emissions in NLP, we perform a careful empirical study of the\ncomputational costs of fine-tuning across tasks, datasets, hardware\ninfrastructure and measurement modalities. Our experimental results allow us to\nplace fine-tuning energy and carbon costs into perspective with respect to\npre-training and inference, and outline recommendations to NLP researchers and\npractitioners who wish to improve their fine-tuning energy efficiency.\n","authors":["Xiaorong Wang","Clara Na","Emma Strubell","Sorelle Friedler","Sasha Luccioni"],"pdf_url":"https://arxiv.org/pdf/2311.10267v2.pdf","comment":"EMNLP 2023 Findings; First two authors contributed equally; 12 pages"},{"id":"http://arxiv.org/abs/2410.12750v1","updated":"2024-10-16T17:12:06Z","published":"2024-10-16T17:12:06Z","title":"Comparative Analysis of Extrinsic Factors for NER in French","summary":"  Named entity recognition (NER) is a crucial task that aims to identify\nstructured information, which is often replete with complex, technical terms\nand a high degree of variability. Accurate and reliable NER can facilitate the\nextraction and analysis of important information. However, NER for other than\nEnglish is challenging due to limited data availability, as the high expertise,\ntime, and expenses are required to annotate its data. In this paper, by using\nthe limited data, we explore various factors including model structure, corpus\nannotation scheme and data augmentation techniques to improve the performance\nof a NER model for French. Our experiments demonstrate that these approaches\ncan significantly improve the model's F1 score from original CRF score of 62.41\nto 79.39. Our findings suggest that considering different extrinsic factors and\ncombining these techniques is a promising approach for improving NER\nperformance where the size of data is limited.\n","authors":["Grace Yang","Zhiyi Li","Yandong Liu","Jungyeul Park"],"pdf_url":"https://arxiv.org/pdf/2410.12750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14391v3","updated":"2024-10-16T17:01:16Z","published":"2023-11-24T10:15:34Z","title":"ÚFAL CorPipe at CRAC 2023: Larger Context Improves Multilingual\n  Coreference Resolution","summary":"  We present CorPipe, the winning entry to the CRAC 2023 Shared Task on\nMultilingual Coreference Resolution. Our system is an improved version of our\nearlier multilingual coreference pipeline, and it surpasses other participants\nby a large margin of 4.5 percent points. CorPipe first performs mention\ndetection, followed by coreference linking via an antecedent-maximization\napproach on the retrieved spans. Both tasks are trained jointly on all\navailable corpora using a shared pretrained language model. Our main\nimprovements comprise inputs larger than 512 subwords and changing the mention\ndecoding to support ensembling. The source code is available at\nhttps://github.com/ufal/crac2023-corpipe.\n","authors":["Milan Straka"],"pdf_url":"https://arxiv.org/pdf/2311.14391v3.pdf","comment":"Accepted to CRAC 2023 (the Sixth Workshop on Computational Models of\n  Reference, Anaphora and Coreference)"},{"id":"http://arxiv.org/abs/2209.07278v3","updated":"2024-10-16T16:56:17Z","published":"2022-09-15T13:11:39Z","title":"ÚFAL CorPipe at CRAC 2022: Effectivity of Multilingual Models for\n  Coreference Resolution","summary":"  We describe the winning submission to the CRAC 2022 Shared Task on\nMultilingual Coreference Resolution. Our system first solves mention detection\nand then coreference linking on the retrieved spans with an\nantecedent-maximization approach, and both tasks are fine-tuned jointly with\nshared Transformer weights. We report results of fine-tuning a wide range of\npretrained models. The center of this contribution are fine-tuned multilingual\nmodels. We found one large multilingual model with sufficiently large encoder\nto increase performance on all datasets across the board, with the benefit not\nlimited only to the underrepresented languages or groups of typologically\nrelative languages. The source code is available at\nhttps://github.com/ufal/crac2022-corpipe.\n","authors":["Milan Straka","Jana Straková"],"pdf_url":"https://arxiv.org/pdf/2209.07278v3.pdf","comment":"Accepted to CRAC 2022 (Fifth Workshop on Computational Models of\n  Reference, Anaphora and Coreference)"},{"id":"http://arxiv.org/abs/2410.12735v1","updated":"2024-10-16T16:51:01Z","published":"2024-10-16T16:51:01Z","title":"CREAM: Consistency Regularized Self-Rewarding Language Models","summary":"  Recent self-rewarding large language models (LLM) have successfully applied\nLLM-as-a-Judge to iteratively improve the alignment performance without the\nneed of human annotations for preference data. These methods commonly utilize\nthe same LLM to act as both the policy model (which generates responses) and\nthe reward model (which scores and ranks those responses). The ranked responses\nare then used as preference pairs to train the LLM via direct alignment\ntechnologies (e.g. DPO). However, it is noteworthy that throughout this\nprocess, there is no guarantee of accuracy in the rewarding and ranking, which\nis critical for ensuring accurate rewards and high-quality preference data.\nEmpirical results from relatively small LLMs (e.g., 7B parameters) also\nindicate that improvements from self-rewarding may diminish after several\niterations in certain situations, which we hypothesize is due to accumulated\nbias in the reward system. This bias can lead to unreliable preference data for\ntraining the LLM. To address this issue, we first formulate and analyze the\ngeneralized iterative preference fine-tuning framework for self-rewarding\nlanguage model. We then introduce the regularization to this generalized\nframework to mitigate the overconfident preference labeling in the\nself-rewarding process. Based on this theoretical insight, we propose a\nConsistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages\nthe rewarding consistency across different iterations to regularize the\nself-rewarding training, helping the model to learn from more reliable\npreference data. With this explicit regularization, our empirical results\ndemonstrate the superiority of CREAM in improving both reward consistency and\nalignment performance. The code is publicly available at\nhttps://github.com/Raibows/CREAM.\n","authors":["Zhaoyang Wang","Weilei He","Zhiyuan Liang","Xuchao Zhang","Chetan Bansal","Ying Wei","Weitong Zhang","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2410.12735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12722v1","updated":"2024-10-16T16:31:24Z","published":"2024-10-16T16:31:24Z","title":"WorldMedQA-V: a multilingual, multimodal medical examination dataset for\n  multimodal language models evaluation","summary":"  Multimodal/vision language models (VLMs) are increasingly being deployed in\nhealthcare settings worldwide, necessitating robust benchmarks to ensure their\nsafety, efficacy, and fairness. Multiple-choice question and answer (QA)\ndatasets derived from national medical examinations have long served as\nvaluable evaluation tools, but existing datasets are largely text-only and\navailable in a limited subset of languages and countries. To address these\nchallenges, we present WorldMedQA-V, an updated multilingual, multimodal\nbenchmarking dataset designed to evaluate VLMs in healthcare. WorldMedQA-V\nincludes 568 labeled multiple-choice QAs paired with 568 medical images from\nfour countries (Brazil, Israel, Japan, and Spain), covering original languages\nand validated English translations by native clinicians, respectively. Baseline\nperformance for common open- and closed-source models are provided in the local\nlanguage and English translations, and with and without images provided to the\nmodel. The WorldMedQA-V benchmark aims to better match AI systems to the\ndiverse healthcare environments in which they are deployed, fostering more\nequitable, effective, and representative applications.\n","authors":["João Matos","Shan Chen","Siena Placino","Yingya Li","Juan Carlos Climent Pardo","Daphna Idan","Takeshi Tohyama","David Restrepo","Luis F. Nakayama","Jose M. M. Pascual-Leone","Guergana Savova","Hugo Aerts","Leo A. Celi","A. Ian Wong","Danielle S. Bitterman","Jack Gallifant"],"pdf_url":"https://arxiv.org/pdf/2410.12722v1.pdf","comment":"submitted for review, total of 14 pages"},{"id":"http://arxiv.org/abs/2407.00463v5","updated":"2024-10-16T16:13:32Z","published":"2024-06-29T15:20:11Z","title":"Open-Source Conversational AI with SpeechBrain 1.0","summary":"  SpeechBrain is an open-source Conversational AI toolkit based on PyTorch,\nfocused particularly on speech processing tasks such as speech recognition,\nspeech enhancement, speaker recognition, text-to-speech, and much more. It\npromotes transparency and replicability by releasing both the pre-trained\nmodels and the complete \"recipes\" of code and algorithms required for training\nthem. This paper presents SpeechBrain 1.0, a significant milestone in the\nevolution of the toolkit, which now has over 200 recipes for speech, audio, and\nlanguage processing tasks, and more than 100 models available on Hugging Face.\nSpeechBrain 1.0 introduces new technologies to support diverse learning\nmodalities, Large Language Model (LLM) integration, and advanced decoding\nstrategies, along with novel models, tasks, and modalities. It also includes a\nnew benchmark repository, offering researchers a unified platform for\nevaluating models across diverse tasks.\n","authors":["Mirco Ravanelli","Titouan Parcollet","Adel Moumen","Sylvain de Langen","Cem Subakan","Peter Plantinga","Yingzhi Wang","Pooneh Mousavi","Luca Della Libera","Artem Ploujnikov","Francesco Paissan","Davide Borra","Salah Zaiem","Zeyu Zhao","Shucong Zhang","Georgios Karakasidis","Sung-Lin Yeh","Pierre Champion","Aku Rouhe","Rudolf Braun","Florian Mai","Juan Zuluaga-Gomez","Seyed Mahed Mousavi","Andreas Nautsch","Xuechen Liu","Sangeet Sagar","Jarod Duret","Salima Mdhaffar","Gaelle Laperriere","Mickael Rouvier","Renato De Mori","Yannick Esteve"],"pdf_url":"https://arxiv.org/pdf/2407.00463v5.pdf","comment":"Accepted to the Journal of Machine Learning research (JMLR), Machine\n  Learning Open Source Software"},{"id":"http://arxiv.org/abs/2410.12705v1","updated":"2024-10-16T16:11:49Z","published":"2024-10-16T16:11:49Z","title":"WorldCuisines: A Massive-Scale Benchmark for Multilingual and\n  Multicultural Visual Question Answering on Global Cuisines","summary":"  Vision Language Models (VLMs) often struggle with culture-specific knowledge,\nparticularly in languages other than English and in underrepresented cultural\ncontexts. To evaluate their understanding of such knowledge, we introduce\nWorldCuisines, a massive-scale benchmark for multilingual and multicultural,\nvisually grounded language understanding. This benchmark includes a visual\nquestion answering (VQA) dataset with text-image pairs across 30 languages and\ndialects, spanning 9 language families and featuring over 1 million data\npoints, making it the largest multicultural VQA benchmark to date. It includes\ntasks for identifying dish names and their origins. We provide evaluation\ndatasets in two sizes (12k and 60k instances) alongside a training dataset (1\nmillion instances). Our findings show that while VLMs perform better with\ncorrect location context, they struggle with adversarial contexts and\npredicting specific regional cuisines and languages. To support future\nresearch, we release a knowledge base with annotated food entries and images\nalong with the VQA data.\n","authors":["Genta Indra Winata","Frederikus Hudi","Patrick Amadeus Irawan","David Anugraha","Rifki Afina Putri","Yutong Wang","Adam Nohejl","Ubaidillah Ariq Prathama","Nedjma Ousidhoum","Afifa Amriani","Anar Rzayev","Anirban Das","Ashmari Pramodya","Aulia Adila","Bryan Wilie","Candy Olivia Mawalim","Ching Lam Cheng","Daud Abolade","Emmanuele Chersoni","Enrico Santus","Fariz Ikhwantri","Garry Kuwanto","Hanyang Zhao","Haryo Akbarianto Wibowo","Holy Lovenia","Jan Christian Blaise Cruz","Jan Wira Gotama Putra","Junho Myung","Lucky Susanto","Maria Angelica Riera Machin","Marina Zhukova","Michael Anugraha","Muhammad Farid Adilazuarda","Natasha Santosa","Peerat Limkonchotiwat","Raj Dabre","Rio Alexander Audino","Samuel Cahyawijaya","Shi-Xiong Zhang","Stephanie Yulia Salim","Yi Zhou","Yinxuan Gui","David Ifeoluwa Adelani","En-Shiun Annie Lee","Shogo Okada","Ayu Purwarianti","Alham Fikri Aji","Taro Watanabe","Derry Tanti Wijaya","Alice Oh","Chong-Wah Ngo"],"pdf_url":"https://arxiv.org/pdf/2410.12705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12704v1","updated":"2024-10-16T16:10:59Z","published":"2024-10-16T16:10:59Z","title":"Sarcasm Detection in a Less-Resourced Language","summary":"  The sarcasm detection task in natural language processing tries to classify\nwhether an utterance is sarcastic or not. It is related to sentiment analysis\nsince it often inverts surface sentiment. Because sarcastic sentences are\nhighly dependent on context, and they are often accompanied by various\nnon-verbal cues, the task is challenging. Most of related work focuses on\nhigh-resourced languages like English. To build a sarcasm detection dataset for\na less-resourced language, such as Slovenian, we leverage two modern\ntechniques: a machine translation specific medium-size transformer model, and a\nvery large generative language model. We explore the viability of translated\ndatasets and how the size of a pretrained transformer affects its ability to\ndetect sarcasm. We train ensembles of detection models and evaluate models'\nperformance. The results show that larger models generally outperform smaller\nones and that ensembling can slightly improve sarcasm detection performance.\nOur best ensemble approach achieves an $\\text{F}_1$-score of 0.765 which is\nclose to annotators' agreement in the source language.\n","authors":["Lazar Đoković","Marko Robnik-Šikonja"],"pdf_url":"https://arxiv.org/pdf/2410.12704v1.pdf","comment":"4 pages, published in the Slovenian Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2404.15219v2","updated":"2024-10-16T16:01:59Z","published":"2024-04-23T16:51:26Z","title":"Unsupervised End-to-End Task-Oriented Dialogue with LLMs: The Power of\n  the Noisy Channel","summary":"  Training task-oriented dialogue systems typically requires turn-level\nannotations for interacting with their APIs: e.g. a dialogue state and the\nsystem actions taken at each step. These annotations can be costly to produce,\nerror-prone, and require both domain and annotation expertise. With advances in\nLLMs, we hypothesize that unlabeled data and a schema definition are sufficient\nfor building a working task-oriented dialogue system, completely unsupervised.\nWe consider a novel unsupervised setting of only (1) a well-defined API schema\n(2) a set of unlabeled dialogues between a user and agent. We propose an\ninnovative approach using expectation-maximization (EM) that infers turn-level\nannotations as latent variables using a noisy channel model to build an\nend-to-end dialogue agent. Evaluating our approach on the MultiWOZ benchmark,\nour method more than doubles the dialogue success rate of a strong GPT-3.5\nbaseline.\n","authors":["Brendan King","Jeffrey Flanigan"],"pdf_url":"https://arxiv.org/pdf/2404.15219v2.pdf","comment":"To be presented at Empirical Methods in Natural Language Processing\n  (EMNLP 2024). 18 Pages, 8 Figures"},{"id":"http://arxiv.org/abs/2410.12694v1","updated":"2024-10-16T15:54:11Z","published":"2024-10-16T15:54:11Z","title":"VividMed: Vision Language Model with Versatile Visual Grounding for\n  Medicine","summary":"  Recent advancements in Vision Language Models (VLMs) have demonstrated\nremarkable promise in generating visually grounded responses. However, their\napplication in the medical domain is hindered by unique challenges. For\ninstance, most VLMs rely on a single method of visual grounding, whereas\ncomplex medical tasks demand more versatile approaches. Additionally, while\nmost VLMs process only 2D images, a large portion of medical images are 3D. The\nlack of medical data further compounds these obstacles. To address these\nchallenges, we present VividMed, a vision language model with versatile visual\ngrounding for medicine. Our model supports generating both semantic\nsegmentation masks and instance-level bounding boxes, and accommodates various\nimaging modalities, including both 2D and 3D data. We design a three-stage\ntraining procedure and an automatic data synthesis pipeline based on open\ndatasets and models. Besides visual grounding tasks, VividMed also excels in\nother common downstream tasks, including Visual Question Answering (VQA) and\nreport generation. Ablation studies empirically show that the integration of\nvisual grounding ability leads to improved performance on these tasks. Our code\nis publicly available at https://github.com/function2-llx/MMMM.\n","authors":["Lingxiao Luo","Bingda Tang","Xuanzhong Chen","Rong Han","Ting Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12691v1","updated":"2024-10-16T15:51:18Z","published":"2024-10-16T15:51:18Z","title":"Building Better: Avoiding Pitfalls in Developing Language Resources when\n  Data is Scarce","summary":"  Language is a symbolic capital that affects people's lives in many ways\n(Bourdieu, 1977, 1991). It is a powerful tool that accounts for identities,\ncultures, traditions, and societies in general. Hence, data in a given language\nshould be viewed as more than a collection of tokens. Good data collection and\nlabeling practices are key to building more human-centered and socially aware\ntechnologies. While there has been a rising interest in mid- to low-resource\nlanguages within the NLP community, work in this space has to overcome unique\nchallenges such as data scarcity and access to suitable annotators. In this\npaper, we collect feedback from those directly involved in and impacted by NLP\nartefacts for mid- to low-resource languages. We conduct a quantitative and\nqualitative analysis of the responses and highlight the main issues related to\n(1) data quality such as linguistic and cultural data suitability; and (2) the\nethics of common annotation practices such as the misuse of online community\nservices. Based on these findings, we make several recommendations for the\ncreation of high-quality language artefacts that reflect the cultural milieu of\nits speakers, while simultaneously respecting the dignity and labor of data\nworkers.\n","authors":["Nedjma Ousidhoum","Meriem Beloucif","Saif M. Mohammad"],"pdf_url":"https://arxiv.org/pdf/2410.12691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01474v2","updated":"2024-10-16T15:45:35Z","published":"2024-05-02T17:07:25Z","title":"Understanding Figurative Meaning through Explainable Visual Entailment","summary":"  Large Vision-Language Models (VLMs) have demonstrated strong capabilities in\ntasks requiring a fine-grained understanding of literal meaning in images and\ntext, such as visual question-answering or visual entailment. However, there\nhas been little exploration of these models' capabilities when presented with\nimages and captions containing figurative meaning, such as metaphors or humor.\nTo close this gap, we propose a new task framing the figurative meaning\nunderstanding problem as an explainable visual entailment task, where the model\nhas to predict whether the image (premise) entails a caption (hypothesis) and\njustify the predicted label with a textual explanation. The figurative\nphenomena can be present either in the image, the caption, or both. Utilizing a\nhuman-AI collaboration approach, we build the accompanying expert-verified\ndataset V-FLUTE, containing 6,027 {image, caption, label, explanation}\ninstances spanning five diverse figurative phenomena: metaphors, similes,\nidioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs\nstruggle to generalize from literal to figurative meaning, particularly when it\nis present in images. Further, we identify common types of errors in VLM\nreasoning via human evaluation.\n","authors":["Arkadiy Saakyan","Shreyas Kulkarni","Tuhin Chakrabarty","Smaranda Muresan"],"pdf_url":"https://arxiv.org/pdf/2405.01474v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11167v2","updated":"2024-10-16T15:40:51Z","published":"2024-02-17T02:25:57Z","title":"ToBlend: Token-Level Blending With an Ensemble of LLMs to Attack\n  AI-Generated Text Detection","summary":"  The robustness of AI-content detection models against sophisticated\nadversarial strategies, such as paraphrasing or word switching, is a rising\nconcern in natural language generation (NLG) applications. This study proposes\nToBlend, a novel token-level ensemble text generation method to challenge the\nrobustness of current AI-content detection approaches by utilizing multiple\nsets of candidate generative large language models (LLMs). By randomly sampling\ntoken(s) from candidate LLMs sets, we find ToBlend significantly drops the\nperformance of most mainstream AI-content detection methods. We evaluate the\ntext quality produced under different ToBlend settings based on annotations\nfrom experienced human experts. We proposed a fine-tuned Llama3.1 model to\ndistinguish the ToBlend generated text more accurately. Our findings underscore\nour proposed text generation approach's great potential in deceiving and\nimproving detection models. Our datasets, codes, and annotations are\nopen-sourced.\n","authors":["Fan Huang","Haewoon Kwak","Jisun An"],"pdf_url":"https://arxiv.org/pdf/2402.11167v2.pdf","comment":"Submitted to ARR Oct-2024 Cycle"},{"id":"http://arxiv.org/abs/2402.07204v4","updated":"2024-10-16T15:28:18Z","published":"2024-02-11T13:30:53Z","title":"ITINERA: Integrating Spatial Optimization with Large Language Models for\n  Open-domain Urban Itinerary Planning","summary":"  Citywalk, a recently popular form of urban travel, requires genuine\npersonalization and understanding of fine-grained requests compared to\ntraditional itinerary planning. In this paper, we introduce the novel task of\nOpen-domain Urban Itinerary Planning (OUIP), which generates personalized urban\nitineraries from user requests in natural language. We then present ITINERA, an\nOUIP system that integrates spatial optimization with large language models to\nprovide customized urban itineraries based on user needs. This involves\ndecomposing user requests, selecting candidate points of interest (POIs),\nordering the POIs based on cluster-aware spatial optimization, and generating\nthe itinerary. Experiments on real-world datasets and the performance of the\ndeployed system demonstrate our system's capacity to deliver personalized and\nspatially coherent itineraries compared to current solutions. Source codes of\nITINERA are available at https://github.com/YihongT/ITINERA.\n","authors":["Yihong Tang","Zhaokai Wang","Ao Qu","Yihao Yan","Zhaofeng Wu","Dingyi Zhuang","Jushi Kai","Kebing Hou","Xiaotong Guo","Jinhua Zhao","Zhan Zhao","Wei Ma"],"pdf_url":"https://arxiv.org/pdf/2402.07204v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12662v1","updated":"2024-10-16T15:20:08Z","published":"2024-10-16T15:20:08Z","title":"Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models","summary":"  Vision-language alignment in Large Vision-Language Models (LVLMs)\nsuccessfully enables LLMs to understand visual input. However, we find that\nexisting vision-language alignment methods fail to transfer the existing safety\nmechanism for text in LLMs to vision, which leads to vulnerabilities in toxic\nimage. To explore the cause of this problem, we give the insightful explanation\nof where and how the safety mechanism of LVLMs operates and conduct comparative\nanalysis between text and vision. We find that the hidden states at the\nspecific transformer layers play a crucial role in the successful activation of\nsafety mechanism, while the vision-language alignment at hidden states level in\ncurrent methods is insufficient. This results in a semantic shift for input\nimages compared to text in hidden states, therefore misleads the safety\nmechanism. To address this, we propose a novel Text-Guided vision-language\nAlignment method (TGA) for LVLMs. TGA retrieves the texts related to input\nvision and uses them to guide the projection of vision into the hidden states\nspace in LLMs. Experiments show that TGA not only successfully transfers the\nsafety mechanism for text in basic LLMs to vision in vision-language alignment\nfor LVLMs without any safety fine-tuning on the visual modality but also\nmaintains the general performance on various vision tasks (Safe and Good).\n","authors":["Shicheng Xu","Liang Pang","Yunchang Zhu","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.12662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12656v1","updated":"2024-10-16T15:17:20Z","published":"2024-10-16T15:17:20Z","title":"Evaluating Morphological Compositional Generalization in Large Language\n  Models","summary":"  Large language models (LLMs) have demonstrated significant progress in\nvarious natural language generation and understanding tasks. However, their\nlinguistic generalization capabilities remain questionable, raising doubts\nabout whether these models learn language similarly to humans. While humans\nexhibit compositional generalization and linguistic creativity in language use,\nthe extent to which LLMs replicate these abilities, particularly in morphology,\nis under-explored. In this work, we systematically investigate the\nmorphological generalization abilities of LLMs through the lens of\ncompositionality. We define morphemes as compositional primitives and design a\nnovel suite of generative and discriminative tasks to assess morphological\nproductivity and systematicity. Focusing on agglutinative languages such as\nTurkish and Finnish, we evaluate several state-of-the-art instruction-finetuned\nmultilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs\nstruggle with morphological compositional generalization particularly when\napplied to novel word roots, with performance declining sharply as\nmorphological complexity increases. While models can identify individual\nmorphological combinations better than chance, their performance lacks\nsystematicity, leading to significant accuracy gaps compared to humans.\n","authors":["Mete Ismayilzada","Defne Circi","Jonne Sälevä","Hale Sirin","Abdullatif Köksal","Bhuwan Dhingra","Antoine Bosselut","Lonneke van der Plas","Duygu Ataman"],"pdf_url":"https://arxiv.org/pdf/2410.12656v1.pdf","comment":"33 pages"},{"id":"http://arxiv.org/abs/2406.11785v2","updated":"2024-10-16T15:15:44Z","published":"2024-06-17T17:39:10Z","title":"CELL your Model: Contrastive Explanations for Large Language Models","summary":"  The advent of black-box deep neural network classification models has sparked\nthe need to explain their decisions. However, in the case of generative AI,\nsuch as large language models (LLMs), there is no class prediction to explain.\nRather, one can ask why an LLM output a particular response to a given prompt.\nIn this paper, we answer this question by proposing, to the best of our\nknowledge, the first contrastive explanation methods requiring simply\nblack-box/query access. Our explanations suggest that an LLM outputs a reply to\na given prompt because if the prompt was slightly modified, the LLM would have\ngiven a different response that is either less preferable or contradicts the\noriginal response. The key insight is that contrastive explanations simply\nrequire a scoring function that has meaning to the user and not necessarily a\nspecific real valued quantity (viz. class label). We offer two algorithms for\nfinding contrastive explanations: i) A myopic algorithm, which although\neffective in creating contrasts, requires many model calls and ii) A budgeted\nalgorithm, our main algorithmic contribution, which intelligently creates\ncontrasts adhering to a query budget, necessary for longer contexts. We show\nthe efficacy of these methods on diverse natural language tasks such as\nopen-text generation, automated red teaming, and explaining conversational\ndegradation.\n","authors":["Ronny Luss","Erik Miehling","Amit Dhurandhar"],"pdf_url":"https://arxiv.org/pdf/2406.11785v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20366v4","updated":"2024-10-16T15:09:14Z","published":"2024-09-30T15:04:17Z","title":"Disentangling Singlish Discourse Particles with Task-Driven\n  Representation","summary":"  Singlish, or formally Colloquial Singapore English, is an English-based\ncreole language originating from the SouthEast Asian country Singapore. The\nlanguage contains influences from Sinitic languages such as Chinese dialects,\nMalay, Tamil and so forth. A fundamental task to understanding Singlish is to\nfirst understand the pragmatic functions of its discourse particles, upon which\nSinglish relies heavily to convey meaning. This work offers a preliminary\neffort to disentangle the Singlish discourse particles (lah, meh and hor) with\ntask-driven representation learning. After disentanglement, we cluster these\ndiscourse particles to differentiate their pragmatic functions, and perform\nSinglish-to-English machine translation. Our work provides a computational\nmethod to understanding Singlish discourse particles, and opens avenues towards\na deeper comprehension of the language and its usage.\n","authors":["Linus Tze En Foo","Lynnette Hui Xian Ng"],"pdf_url":"https://arxiv.org/pdf/2409.20366v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13745v4","updated":"2024-10-16T15:07:41Z","published":"2024-08-25T07:10:36Z","title":"DOCE: Finding the Sweet Spot for Execution-Based Code Generation","summary":"  Recently, a diverse set of decoding and reranking procedures have been shown\neffective for LLM-based code generation. However, a comprehensive framework\nthat links and experimentally compares these methods is missing. We address\nthis by proposing Decoding Objectives for Code Execution, a comprehensive\nframework that includes candidate generation, $n$-best reranking, minimum Bayes\nrisk (MBR) decoding, and self-debugging as the core components. We then study\nthe contributions of these components through execution-based evaluation\nmetrics. Our findings highlight the importance of execution-based methods and\nthe difference gap between execution-based and execution-free methods.\nFurthermore, we assess the impact of filtering based on trial unit tests, a\nsimple and effective strategy that has been often overlooked in prior works. We\nalso propose self-debugging on multiple candidates, obtaining state-of-the-art\nperformance on reranking for code generation. We expect our framework to\nprovide a solid guideline for future research on code generation.\n","authors":["Haau-Sing Li","Patrick Fernandes","Iryna Gurevych","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2408.13745v4.pdf","comment":"10 pages (32 including appendix), 5 figures, 25 tables. Prompts are\n  provided in the GitHub repository to avoid potential text overlap with other\n  papers"},{"id":"http://arxiv.org/abs/2409.13832v3","updated":"2024-10-16T14:56:59Z","published":"2024-09-20T18:18:14Z","title":"GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music\n  Scores for All Singing Tasks","summary":"  The scarcity of high-quality and multi-task singing datasets significantly\nhinders the development of diverse controllable and personalized singing tasks,\nas existing singing datasets suffer from low quality, limited diversity of\nlanguages and singers, absence of multi-technique information and realistic\nmusic scores, and poor task suitability. To tackle these problems, we present\nGTSinger, a large global, multi-technique, free-to-use, high-quality singing\ncorpus with realistic music scores, designed for all singing tasks, along with\nits benchmarks. Particularly, (1) we collect 80.59 hours of high-quality\nsinging voices, forming the largest recorded singing dataset; (2) 20\nprofessional singers across nine widely spoken languages offer diverse timbres\nand styles; (3) we provide controlled comparison and phoneme-level annotations\nof six commonly used singing techniques, helping technique modeling and\ncontrol; (4) GTSinger offers realistic music scores, assisting real-world\nmusical composition; (5) singing voices are accompanied by manual\nphoneme-to-audio alignments, global style labels, and 16.16 hours of paired\nspeech for various singing tasks. Moreover, to facilitate the use of GTSinger,\nwe conduct four benchmark experiments: technique-controllable singing voice\nsynthesis, technique recognition, style transfer, and speech-to-singing\nconversion. The corpus and demos can be found at http://gtsinger.github.io. We\nprovide the dataset and the code for processing data and conducting benchmarks\nat https://huggingface.co/datasets/GTSinger/GTSinger and\nhttps://github.com/GTSinger/GTSinger.\n","authors":["Yu Zhang","Changhao Pan","Wenxiang Guo","Ruiqi Li","Zhiyuan Zhu","Jialei Wang","Wenhao Xu","Jingyu Lu","Zhiqing Hong","Chuxin Wang","LiChao Zhang","Jinzheng He","Ziyue Jiang","Yuxin Chen","Chen Yang","Jiecheng Zhou","Xinyu Cheng","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.13832v3.pdf","comment":"Accepted by NeurIPS 2024 (Spotlight)"},{"id":"http://arxiv.org/abs/2409.15360v3","updated":"2024-10-16T14:56:15Z","published":"2024-09-18T02:35:41Z","title":"Reward-Robust RLHF in LLMs","summary":"  As Large Language Models (LLMs) continue to progress toward more advanced\nforms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is\nincreasingly seen as a key pathway toward achieving Artificial General\nIntelligence (AGI). However, the reliance on reward-model-based (RM-based)\nalignment methods introduces significant challenges due to the inherent\ninstability and imperfections of Reward Models (RMs), which can lead to\ncritical issues such as reward hacking and misalignment with human intentions.\nIn this paper, we introduce a reward-robust RLHF framework aimed at addressing\nthese fundamental challenges, paving the way for more reliable and resilient\nlearning in LLMs. Our approach introduces a novel optimization objective that\ncarefully balances performance and robustness by incorporating Bayesian Reward\nModel Ensembles (BRME) to model the uncertainty set of reward functions. This\nallows the framework to integrate both nominal performance and minimum reward\nsignals, ensuring more stable learning even with imperfect RMs. Empirical\nresults demonstrate that our framework consistently outperforms baselines\nacross diverse benchmarks, showing improved accuracy and long-term stability.\nWe also provide a theoretical analysis, demonstrating that reward-robust RLHF\napproaches the stability of constant reward settings, which proves to be\nacceptable even in a stochastic-case analysis. Together, these contributions\nhighlight the framework potential to enhance both the performance and stability\nof LLM alignment.\n","authors":["Yuzi Yan","Xingzhou Lou","Jialian Li","Yiping Zhang","Jian Xie","Chao Yu","Yu Wang","Dong Yan","Yuan Shen"],"pdf_url":"https://arxiv.org/pdf/2409.15360v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13300v2","updated":"2024-10-16T14:52:16Z","published":"2024-07-18T09:05:49Z","title":"Robust ASR Error Correction with Conservative Data Filtering","summary":"  Error correction (EC) based on large language models is an emerging\ntechnology to enhance the performance of automatic speech recognition (ASR)\nsystems. Generally, training data for EC are collected by automatically pairing\na large set of ASR hypotheses (as sources) and their gold references (as\ntargets). However, the quality of such pairs is not guaranteed, and we observed\nvarious types of noise which can make the EC models brittle, e.g. inducing\novercorrection in out-of-domain (OOD) settings. In this work, we propose two\nfundamental criteria that EC training data should satisfy: namely, EC targets\nshould (1) improve linguistic acceptability over sources and (2) be inferable\nfrom the available context (e.g. source phonemes). Through these criteria, we\nidentify low-quality EC pairs and train the models not to make any correction\nin such cases, the process we refer to as conservative data filtering. In our\nexperiments, we focus on Japanese ASR using a strong Conformer-CTC as the\nbaseline and finetune Japanese LLMs for EC. Through our evaluation on a suite\nof 21 internal benchmarks, we demonstrate that our approach can significantly\nreduce overcorrection and improve both the accuracy and quality of ASR results\nin the challenging OOD settings.\n","authors":["Takuma Udagawa","Masayuki Suzuki","Masayasu Muraoka","Gakuto Kurata"],"pdf_url":"https://arxiv.org/pdf/2407.13300v2.pdf","comment":"Accepted to EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2406.09988v2","updated":"2024-10-16T14:48:38Z","published":"2024-06-14T12:52:42Z","title":"Details Make a Difference: Object State-Sensitive Neurorobotic Task\n  Planning","summary":"  The state of an object reflects its current status or condition and is\nimportant for a robot's task planning and manipulation. However, detecting an\nobject's state and generating a state-sensitive plan for robots is challenging.\nRecently, pre-trained Large Language Models (LLMs) and Vision-Language Models\n(VLMs) have shown impressive capabilities in generating plans. However, to the\nbest of our knowledge, there is hardly any investigation on whether LLMs or\nVLMs can also generate object state-sensitive plans. To study this, we\nintroduce an Object State-Sensitive Agent (OSSA), a task-planning agent\nempowered by pre-trained neural networks. We propose two methods for OSSA: (i)\na modular model consisting of a pre-trained vision processing module (dense\ncaptioning model, DCM) and a natural language processing model (LLM), and (ii)\na monolithic model consisting only of a VLM. To quantitatively evaluate the\nperformances of the two methods, we use tabletop scenarios where the task is to\nclear the table. We contribute a multimodal benchmark dataset that takes object\nstates into consideration. Our results show that both methods can be used for\nobject state-sensitive tasks, but the monolithic approach outperforms the\nmodular approach. The code for OSSA is available at\nhttps://github.com/Xiao-wen-Sun/OSSA\n","authors":["Xiaowen Sun","Xufeng Zhao","Jae Hee Lee","Wenhao Lu","Matthias Kerzel","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2406.09988v2.pdf","comment":"ICANN24, Switzerland"},{"id":"http://arxiv.org/abs/2410.12622v1","updated":"2024-10-16T14:42:23Z","published":"2024-10-16T14:42:23Z","title":"From Measurement Instruments to Training Data: Leveraging Theory-Driven\n  Synthetic Training Data for Measuring Social Constructs","summary":"  Computational text classification is a challenging task, especially for\nmulti-dimensional social constructs. Recently, there has been increasing\ndiscussion that synthetic training data could enhance classification by\noffering examples of how these constructs are represented in texts. In this\npaper, we systematically examine the potential of theory-driven synthetic\ntraining data for improving the measurement of social constructs. In\nparticular, we explore how researchers can transfer established knowledge from\nmeasurement instruments in the social sciences, such as survey scales or\nannotation codebooks, into theory-driven generation of synthetic data. Using\ntwo studies on measuring sexism and political topics, we assess the added value\nof synthetic training data for fine-tuning text classification models. Although\nthe results of the sexism study were less promising, our findings demonstrate\nthat synthetic data can be highly effective in reducing the need for labeled\ndata in political topic classification. With only a minimal drop in\nperformance, synthetic data allows for substituting large amounts of labeled\ndata. Furthermore, theory-driven synthetic data performed markedly better than\ndata generated without conceptual information in mind.\n","authors":["Lukas Birkenmaier","Matthias Roth","Indira Sen"],"pdf_url":"https://arxiv.org/pdf/2410.12622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12621v1","updated":"2024-10-16T14:40:32Z","published":"2024-10-16T14:40:32Z","title":"Weak-to-Strong Generalization beyond Accuracy: a Pilot Study in Safety,\n  Toxicity, and Legal Reasoning","summary":"  As large language models (LLMs) continue to advance, ensuring their alignment\nwith human values becomes increasingly critical. Traditional alignment methods\nheavily rely on human feedback to fine-tune models. With the emergence of\nsuperhuman models whose outputs may surpass human understanding, evaluating and\naligning these models using human judgments poses significant challenges. To\naddress the challenges, recent works use weak supervisors to elicit knowledge\nfrom much stronger models. However, there are important disanalogies between\nthe empirical setup in the existing works and the genuine goal of alignment. We\nremark that existing works investigate the phenomenon of weak-to-strong\ngeneration in analogous setup (i.e., binary classification), rather than\npractical alignment-relevant tasks (e.g., safety). In this paper, we bridge\nthis gap by extending weak-to-strong generation to the context of practical\nalignment. We empirically demonstrate the widespread phenomenon of\nweak-to-strong generation in three complicated alignment tasks: safety,\ntoxicity, and legal reasoning}. Furthermore, we explore efficient strategies\nfor improving alignment performance to enhance the quality of model outcomes.\nLastly, we summarize and analyze the challenges and potential solutions in\nregard to specific alignment tasks, which we hope to catalyze the research\nprogress on the topic of weak-to-strong generalization. Our code is released at\nhttps://github.com/yeruimeng/WTS.git.\n","authors":["Ruimeng Ye","Yang Xiao","Bo Hui"],"pdf_url":"https://arxiv.org/pdf/2410.12621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12617v1","updated":"2024-10-16T14:34:30Z","published":"2024-10-16T14:34:30Z","title":"Parsing Akkadian Verbs with Prolog","summary":"  This paper describes a parsing/generation system for finite verbal forms in\nAkkadian, with the possible addition of suffixes, implemented in Prolog. The\nwork described provides the framework and engine to interpret the D, N, and G\nstems along with accusative, dative and ventive endings.\n","authors":["Aaron Macks"],"pdf_url":"https://arxiv.org/pdf/2410.12617v1.pdf","comment":"6 pages, 9 figures, presented at ACL-02 the Association of\n  Computational Linguistics, 2002"},{"id":"http://arxiv.org/abs/2410.12613v1","updated":"2024-10-16T14:29:29Z","published":"2024-10-16T14:29:29Z","title":"Exploring Model Kinship for Merging Large Language Models","summary":"  Model merging has become one of the key technologies for enhancing the\ncapabilities and efficiency of Large Language Models (LLMs). However, our\nunderstanding of the expected performance gains and principles when merging any\ntwo models remains limited. In this work, we introduce model kinship, the\ndegree of similarity or relatedness between LLMs, analogous to biological\nevolution. With comprehensive empirical analysis, we find that there is a\ncertain relationship between model kinship and the performance gains after\nmodel merging, which can help guide our selection of candidate models. Inspired\nby this, we propose a new model merging strategy: Top-k Greedy Merging with\nModel Kinship, which can yield better performance on benchmark datasets.\nSpecifically, we discover that using model kinship as a criterion can assist us\nin continuously performing model merging, alleviating the degradation (local\noptima) in model evolution, whereas model kinship can serve as a guide to\nescape these traps. Code is available at\nhttps://github.com/zjunlp/ModelKinship.\n","authors":["Yedi Hu","Yunzhi Yao","Ningyu Zhang","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12613v1.pdf","comment":"Ongoing work"},{"id":"http://arxiv.org/abs/2305.14463v4","updated":"2024-10-16T14:27:49Z","published":"2023-05-23T18:37:30Z","title":"ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain\n  Readability Assessment","summary":"  We present a comprehensive evaluation of large language models for\nmultilingual readability assessment. Existing evaluation resources lack domain\nand language diversity, limiting the ability for cross-domain and cross-lingual\nanalyses. This paper introduces ReadMe++, a multilingual multi-domain dataset\nwith human annotations of 9757 sentences in Arabic, English, French, Hindi, and\nRussian, collected from 112 different data sources. This benchmark will\nencourage research on developing robust multilingual readability assessment\nmethods. Using ReadMe++, we benchmark multilingual and monolingual language\nmodels in the supervised, unsupervised, and few-shot prompting settings. The\ndomain and language diversity in ReadMe++ enable us to test more effective\nfew-shot prompting, and identify shortcomings in state-of-the-art unsupervised\nmethods. Our experiments also reveal exciting results of superior domain\ngeneralization and enhanced cross-lingual transfer capabilities by models\ntrained on ReadMe++. We will make our data publicly available and release a\npython package tool for multilingual sentence readability prediction using our\ntrained models at: https://github.com/tareknaous/readme\n","authors":["Tarek Naous","Michael J. Ryan","Anton Lavrouk","Mohit Chandra","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2305.14463v4.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.12608v1","updated":"2024-10-16T14:24:55Z","published":"2024-10-16T14:24:55Z","title":"Not All Votes Count! Programs as Verifiers Improve Self-Consistency of\n  Language Models for Math Reasoning","summary":"  Large language models (LLMs) have shown increasing proficiency in solving\nmathematical reasoning problems. However, many current open-source LLMs often\nstill make calculation and semantic understanding errors in their intermediate\nreasoning steps. In this work, we propose PROVE, a simple yet effective\nframework that uses program-based verification as a heuristic to filter out\npotentially incorrect reasoning paths before aggregating the final answers.\nInstead of relying on vanilla majority voting, our approach rejects solutions\nwhose corresponding program outputs are inconsistent with the generated\nsolution, aggregating only those validated by Python programs. We conducted\nextensive experiments on 13 open-source LLMs from various model families and\nsizes, ranging from 0.5B to 13B parameters, across seven math benchmarks. We\ndemonstrate that PROVE consistently outperforms vanilla majority voting as a\nheuristic for solving mathematical reasoning tasks across all datasets and\nmodel sizes. Notably, PROVE increases accuracy on the GSM8K benchmark from\n48.85% to 53.83% for Qwen2-0.5B-Instruct, from 65.66% to 73.01% for\nLlama-3.2-1B-Instruct, from 73.39% to 79.61% for Gemma-2-2b-it, and from 41.32%\nto 59.51% for Llama-2-7B-chat. Our codes are available at\nhttps://github.com/declare-lab/prove.\n","authors":["Vernon Y. H. Toh","Deepanway Ghosal","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2410.12608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12601v1","updated":"2024-10-16T14:21:52Z","published":"2024-10-16T14:21:52Z","title":"CCSBench: Evaluating Compositional Controllability in LLMs for\n  Scientific Document Summarization","summary":"  To broaden the dissemination of scientific knowledge to diverse audiences,\nscientific document summarization must simultaneously control multiple\nattributes such as length and empirical focus. However, existing research\ntypically focuses on controlling single attributes, leaving the compositional\ncontrol of multiple attributes underexplored. To address this gap, we introduce\nCCSBench, a benchmark for compositional controllable summarization in the\nscientific domain. Our benchmark enables fine-grained control over both\nexplicit attributes (e.g., length), which are objective and straightforward,\nand implicit attributes (e.g., empirical focus), which are more subjective and\nconceptual. We conduct extensive experiments on GPT-4, LLaMA2, and other\npopular LLMs under various settings. Our findings reveal significant\nlimitations in large language models' ability to balance trade-offs between\ncontrol attributes, especially implicit ones that require deeper understanding\nand abstract reasoning.\n","authors":["Yixi Ding","Jiaying Wu","Tongyao Zhu","Yanxia Qin","Qian Liu","Min-Yen Kan"],"pdf_url":"https://arxiv.org/pdf/2410.12601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12600v1","updated":"2024-10-16T14:17:53Z","published":"2024-10-16T14:17:53Z","title":"On the Risk of Evidence Pollution for Malicious Social Text Detection in\n  the Era of LLMs","summary":"  Evidence-enhanced detectors present remarkable abilities in identifying\nmalicious social text with related evidence. However, the rise of large\nlanguage models (LLMs) brings potential risks of evidence pollution to confuse\ndetectors. This paper explores how to manipulate evidence, simulating potential\nmisuse scenarios including basic pollution, and rephrasing or generating\nevidence by LLMs. To mitigate its negative impact, we propose three defense\nstrategies from both the data and model sides, including machine-generated text\ndetection, a mixture of experts, and parameter updating. Extensive experiments\non four malicious social text detection tasks with ten datasets present that\nevidence pollution, especially the generate strategy, significantly compromises\nexisting detectors. On the other hand, the defense strategies could mitigate\nevidence pollution, but they faced limitations for practical employment, such\nas the need for annotated data and huge inference costs. Further analysis\nillustrates that polluted evidence is of high quality, would compromise the\nmodel calibration, and could ensemble to amplify the negative impact.\n","authors":["Herun Wan","Minnan Luo","Zhixiong Su","Guang Dai","Xiang Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.12600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11894v4","updated":"2024-10-16T14:14:27Z","published":"2024-03-18T15:53:33Z","title":"From Explainable to Interpretable Deep Learning for Natural Language\n  Processing in Healthcare: How Far from Reality?","summary":"  Deep learning (DL) has substantially enhanced natural language processing\n(NLP) in healthcare research. However, the increasing complexity of DL-based\nNLP necessitates transparent model interpretability, or at least\nexplainability, for reliable decision-making. This work presents a thorough\nscoping review of explainable and interpretable DL in healthcare NLP. The term\n\"eXplainable and Interpretable Artificial Intelligence\" (XIAI) is introduced to\ndistinguish XAI from IAI. Different models are further categorized based on\ntheir functionality (model-, input-, output-based) and scope (local, global).\nOur analysis shows that attention mechanisms are the most prevalent emerging\nIAI technique. The use of IAI is growing, distinguishing it from XAI. The major\nchallenges identified are that most XIAI does not explore \"global\" modelling\nprocesses, the lack of best practices, and the lack of systematic evaluation\nand benchmarks. One important opportunity is to use attention mechanisms to\nenhance multi-modal XIAI for personalized medicine. Additionally, combining DL\nwith causal logic holds promise. Our discussion encourages the integration of\nXIAI in Large Language Models (LLMs) and domain-specific smaller models. In\nconclusion, XIAI adoption in healthcare requires dedicated in-house expertise.\nCollaboration with domain experts, end-users, and policymakers can lead to\nready-to-use XIAI methods across NLP and medical tasks. While challenges exist,\nXIAI techniques offer a valuable foundation for interpretable NLP algorithms in\nhealthcare.\n","authors":["Guangming Huang","Yingya Li","Shoaib Jameel","Yunfei Long","Giorgos Papanastasiou"],"pdf_url":"https://arxiv.org/pdf/2403.11894v4.pdf","comment":"This paper has been accepted by Computational and Structural\n  Biotechnology Journal"},{"id":"http://arxiv.org/abs/2402.19350v6","updated":"2024-10-16T14:12:47Z","published":"2024-02-29T16:56:36Z","title":"Prompting Explicit and Implicit Knowledge for Multi-hop Question\n  Answering Based on Human Reading Process","summary":"  Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to\nsimulate human reasoning and inference processes, achieving proficient\nperformance in multi-hop QA. However, a gap persists between PLMs' reasoning\nabilities and those of humans when tackling complex problems. Psychological\nstudies suggest a vital connection between explicit information in passages and\nhuman prior knowledge during reading. Nevertheless, current research has given\ninsufficient attention to linking input passages and PLMs' pre-training-based\nknowledge from the perspective of human cognition studies. In this study, we\nintroduce a Prompting Explicit and Implicit knowledge (PEI) framework, which\nuses prompts to connect explicit and implicit knowledge, aligning with human\nreading process for multi-hop QA. We consider the input passages as explicit\nknowledge, employing them to elicit implicit knowledge through unified prompt\nreasoning. Furthermore, our model incorporates type-specific reasoning via\nprompts, a form of implicit knowledge. Experimental results show that PEI\nperforms comparably to the state-of-the-art on HotpotQA. Ablation studies\nconfirm the efficacy of our model in bridging and integrating explicit and\nimplicit knowledge.\n","authors":["Guangming Huang","Yunfei Long","Cunjin Luo","Jiaxing Shen","Xia Sun"],"pdf_url":"https://arxiv.org/pdf/2402.19350v6.pdf","comment":"This paper has been accepted at COLING 2024"},{"id":"http://arxiv.org/abs/2410.03293v3","updated":"2024-10-16T14:11:21Z","published":"2024-10-04T10:06:55Z","title":"Five Years of COVID-19 Discourse on Instagram: A Labeled Instagram\n  Dataset of Over Half a Million Posts for Multilingual Sentiment Analysis","summary":"  The work presented in this paper makes three scientific contributions with a\nspecific focus on mining and analysis of COVID-19-related posts on Instagram.\nFirst, it presents a multilingual dataset of 500,153 Instagram posts about\nCOVID-19 published between January 2020 and September 2024. This dataset,\navailable at https://dx.doi.org/10.21227/d46p-v480, contains Instagram posts in\n161 different languages as well as 535,021 distinct hashtags. After the\ndevelopment of this dataset, multilingual sentiment analysis was performed,\nwhich involved classifying each post as positive, negative, or neutral. The\nresults of sentiment analysis are presented as a separate attribute in this\ndataset. Second, it presents the results of performing sentiment analysis per\nyear from 2020 to 2024. The findings revealed the trends in sentiment related\nto COVID-19 on Instagram since the beginning of the pandemic. For instance,\nbetween 2020 and 2024, the sentiment trends show a notable shift, with positive\nsentiment decreasing from 38.35% to 28.69%, while neutral sentiment rising from\n44.19% to 58.34%. Finally, the paper also presents findings of\nlanguage-specific sentiment analysis. This analysis highlighted similar and\ncontrasting trends of sentiment across posts published in different languages\non Instagram. For instance, out of all English posts, 49.68% were positive,\n14.84% were negative, and 35.48% were neutral. In contrast, among Hindi posts,\n4.40% were positive, 57.04% were negative, and 38.56% were neutral, reflecting\ndistinct differences in the sentiment distribution between these two languages.\n","authors":["Nirmalya Thakur"],"pdf_url":"https://arxiv.org/pdf/2410.03293v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08469v2","updated":"2024-10-16T14:09:14Z","published":"2024-10-11T02:42:13Z","title":"Semantic Token Reweighting for Interpretable and Controllable Text\n  Embeddings in CLIP","summary":"  A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial\nrole in translating textual input into an embedding space shared with images,\nthereby facilitating the interpretative analysis of vision tasks through\nnatural language. Despite the varying significance of different textual\nelements within a sentence depending on the context, efforts to account for\nvariation of importance in constructing text embeddings have been lacking. We\npropose a framework of Semantic Token Reweighting to build Interpretable text\nembeddings (SToRI), which incorporates controllability as well. SToRI refines\nthe text encoding process in CLIP by differentially weighting semantic elements\nbased on contextual importance, enabling finer control over emphasis responsive\nto data-driven insights and user preferences. The efficacy of SToRI is\ndemonstrated through comprehensive experiments on few-shot image classification\nand image retrieval tailored to user preferences.\n","authors":["Eunji Kim","Kyuhong Shim","Simyung Chang","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2410.08469v2.pdf","comment":"Accepted at EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.12586v1","updated":"2024-10-16T14:04:26Z","published":"2024-10-16T14:04:26Z","title":"Can We Reverse In-Context Knowledge Edits?","summary":"  In-context knowledge editing (IKE) enables efficient modification of large\nlanguage model (LLM) outputs without parameter changes and at zero-cost.\nHowever, it can be misused to manipulate responses opaquely, e.g., insert\nmisinformation or offensive content. Such malicious interventions could be\nincorporated into high-level wrapped APIs where the final input prompt is not\nshown to end-users. To address this issue, we investigate the detection and\nreversal of IKE-edits. First, we demonstrate that IKE-edits can be detected\nwith high accuracy (F1 > 80\\%) using only the top-10 output probabilities of\nthe next token, even in a black-box setting, e.g. proprietary LLMs with limited\noutput information. Further, we introduce the novel task of reversing IKE-edits\nusing specially tuned reversal tokens. We explore using both continuous and\ndiscrete reversal tokens, achieving over 80\\% accuracy in recovering original,\nunedited outputs across multiple LLMs. Our continuous reversal tokens prove\nparticularly effective, with minimal impact on unedited prompts. Through\nanalysis of output distributions, attention patterns, and token rankings, we\nprovide insights into IKE's effects on LLMs and how reversal tokens mitigate\nthem. This work represents a significant step towards enhancing LLM resilience\nagainst potential misuse of in-context editing, improving their transparency\nand trustworthiness.\n","authors":["Paul Youssef","Zhixue Zhao","Jörg Schlötterer","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2410.12586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12583v1","updated":"2024-10-16T14:01:22Z","published":"2024-10-16T14:01:22Z","title":"STRUX: An LLM for Decision-Making with Structured Explanations","summary":"  Countless decisions shape our daily lives, and it is paramount to understand\nthe how and why behind these choices. In this paper, we introduce a new LLM\ndecision-making framework called STRUX, which enhances LLM decision-making by\nproviding structured explanations. These include favorable and adverse facts\nrelated to the decision, along with their respective strengths. STRUX begins by\ndistilling lengthy information into a concise table of key facts. It then\nemploys a series of self-reflection steps to determine which of these facts are\npivotal, categorizing them as either favorable or adverse in relation to a\nspecific decision. Lastly, we fine-tune an LLM to identify and prioritize these\nkey facts to optimize decision-making. STRUX has been evaluated on the\nchallenging task of forecasting stock investment decisions based on earnings\ncall transcripts and demonstrated superior performance against strong\nbaselines. It enhances decision transparency by allowing users to understand\nthe impact of different factors, representing a meaningful step towards\npractical decision-making with LLMs.\n","authors":["Yiming Lu","Yebowen Hu","Hassan Foroosh","Wei Jin","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2410.12583v1.pdf","comment":"10 pages, 7 figures, submitted to NAACL 2025"},{"id":"http://arxiv.org/abs/2406.12593v2","updated":"2024-10-16T13:45:54Z","published":"2024-06-18T13:25:18Z","title":"PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval","summary":"  Differentiable Search Index (DSI) utilizes Pre-trained Language Models (PLMs)\nfor efficient document retrieval without relying on external indexes. However,\nDSI needs full re-training to handle updates in dynamic corpora, causing\nsignificant computational inefficiencies. We introduce PromptDSI, a\nprompt-based rehearsal-free approach for instance-wise incremental learning\ndocument retrieval. PromptDSI attaches prompts to the frozen PLM's encoder of\nDSI, leveraging its powerful representation to efficiently index new corpora\nwhile maintaining a balance between stability and plasticity. We eliminate the\ninitial forward pass of prompt-based continual learning methods that doubles\ntraining and inference time. Moreover, we propose a topic-aware prompt pool\nthat employs neural topic embeddings as fixed keys. This strategy ensures\ndiverse and effective prompt usage, addressing the challenge of parameter\nunderutilization caused by the collapse of the query-key matching mechanism.\nOur empirical evaluations demonstrate that BERT-based PromptDSI matches IncDSI\nin managing forgetting while improving new corpora performance by more than 4%\nHits@10 on NQ320k and upto 3% MRR@10 on MS MARCO 300k.\n","authors":["Tuan-Luc Huynh","Thuy-Trang Vu","Weiqing Wang","Yinwei Wei","Trung Le","Dragan Gasevic","Yuan-Fang Li","Thanh-Toan Do"],"pdf_url":"https://arxiv.org/pdf/2406.12593v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2406.12319v3","updated":"2024-10-16T13:39:49Z","published":"2024-06-18T06:43:04Z","title":"The Comparative Trap: Pairwise Comparisons Amplifies Biased Preferences\n  of LLM Evaluators","summary":"  As large language models (LLMs) are increasingly used as evaluators for\nnatural language generation tasks, ensuring unbiased assessments is essential.\nHowever, LLM evaluators often display biased preferences, such as favoring\nverbosity and authoritative tones. Our empirical analysis reveals that these\nbiases are exacerbated in pairwise evaluation, where LLMs directly compare two\noutputs and easily prioritize superficial attributes. In contrast, pointwise\nevaluation, which assesses outputs independently, is less susceptible to such\nbias because each output is judged in isolation. To address the limitations of\nthe pairwise evaluation, we introduce a novel evaluation method, PRePair, which\nintegrates pointwise reasoning within a pairwise framework. PRePair effectively\nalleviates biased preference, improving performance on the adversarial\nbenchmark (LLMBar) while outperforming pointwise evaluation on the standard\nbenchmark (MT-Bench).\n","authors":["Hawon Jeong","ChaeHun Park","Jimin Hong","Hojoon Lee","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2406.12319v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12558v1","updated":"2024-10-16T13:34:51Z","published":"2024-10-16T13:34:51Z","title":"A Claim Decomposition Benchmark for Long-form Answer Verification","summary":"  The advancement of LLMs has significantly boosted the performance of complex\nlong-form question answering tasks. However, one prominent issue of LLMs is the\ngenerated \"hallucination\" responses that are not factual. Consequently,\nattribution for each claim in responses becomes a common solution to improve\nthe factuality and verifiability. Existing researches mainly focus on how to\nprovide accurate citations for the response, which largely overlook the\nimportance of identifying the claims or statements for each response. To bridge\nthis gap, we introduce a new claim decomposition benchmark, which requires\nbuilding system that can identify atomic and checkworthy claims for LLM\nresponses. Specifically, we present the Chinese Atomic Claim Decomposition\nDataset (CACDD), which builds on the WebCPM dataset with additional expert\nannotations to ensure high data quality. The CACDD encompasses a collection of\n500 human-annotated question-answer pairs, including a total of 4956 atomic\nclaims. We further propose a new pipeline for human annotation and describe the\nchallenges of this task. In addition, we provide experiment results on\nzero-shot, few-shot and fine-tuned LLMs as baselines. The results show that the\nclaim decomposition is highly challenging and requires further explorations.\nAll code and data are publicly available at\n\\url{https://github.com/FBzzh/CACDD}.\n","authors":["Zhihao Zhang","Yixing Fan","Ruqing Zhang","Jiafeng Guo"],"pdf_url":"https://arxiv.org/pdf/2410.12558v1.pdf","comment":"Accepted by CCIR 2024"},{"id":"http://arxiv.org/abs/2406.18221v3","updated":"2024-10-16T13:31:05Z","published":"2024-06-26T10:08:47Z","title":"Enhancing Data Privacy in Large Language Models through Private\n  Association Editing","summary":"  Large language models (LLMs) require a significant redesign in solutions to\npreserve privacy in data-intensive applications due to their text-generation\ncapabilities. Indeed, LLMs tend to memorize and emit private information when\nmaliciously prompted. In this paper, we introduce Private Association Editing\n(PAE) as a novel defense approach for private data leakage. PAE is designed to\neffectively remove Personally Identifiable Information (PII) without retraining\nthe model. Experimental results demonstrate the effectiveness of PAE with\nrespect to alternative baseline methods. We believe PAE will serve as a\ncritical tool in the ongoing effort to protect data privacy in LLMs,\nencouraging the development of safer models for real-world applications.\n","authors":["Davide Venditti","Elena Sofia Ruzzetti","Giancarlo A. Xompero","Cristina Giannone","Andrea Favalli","Raniero Romagnoli","Fabio Massimo Zanzotto"],"pdf_url":"https://arxiv.org/pdf/2406.18221v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12543v1","updated":"2024-10-16T13:21:46Z","published":"2024-10-16T13:21:46Z","title":"LLM-based Translation Inference with Iterative Bilingual Understanding","summary":"  The remarkable understanding and generation capabilities of large language\nmodels (LLMs) have greatly improved translation performance. However, incorrect\nunderstanding of the sentence to be translated can degrade translation quality.\nTo address this issue, we proposed a novel Iterative Bilingual Understanding\nTranslation (IBUT) method based on the cross-lingual capabilities of LLMs and\nthe dual characteristics of translation tasks. The cross-lingual capability of\nLLMs enables the generation of contextual understanding for both the source and\ntarget languages separately. Furthermore, the dual characteristics allow IBUT\nto generate effective cross-lingual feedback, iteratively refining contextual\nunderstanding, thereby reducing errors and improving translation performance.\nExperimental results showed that the proposed IBUT outperforms several strong\ncomparison methods, especially being generalized to multiple domains (e.g.,\nnews, commonsense, and cultural translation benchmarks).\n","authors":["Andong Chen","Kehai Chen","Yang Xiang","Xuefeng Bai","Muyun Yang","Tiejun Zhao","Min zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12543v1.pdf","comment":"work in process"},{"id":"http://arxiv.org/abs/2406.14162v3","updated":"2024-10-16T13:16:25Z","published":"2024-06-20T10:04:09Z","title":"DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval\n  Augmented Generation","summary":"  Retrieval Augmented Generation (RAG) is widely employed to ground responses\nto queries on domain-specific documents. But do RAG implementations leave out\nimportant information when answering queries that need an integrated analysis\nof information (e.g., Tell me good news in the stock market today.)? To address\nthese concerns, RAG developers need to annotate information retrieval (IR) data\nfor their domain of interest, which is challenging because (1) domain-specific\nqueries usually need nuanced definitions of relevance beyond shallow semantic\nrelevance; and (2) human or GPT-4 annotation is costly and cannot cover all\n(query, document) pairs (i.e., annotation selection bias), thus harming the\neffectiveness in evaluating IR recall. To address these challenges, we propose\nDIRAS (Domain-specific Information Retrieval Annotation with Scalability), a\nmanual-annotation-free schema that fine-tunes open-sourced LLMs to consider\nnuanced relevance definition and annotate (partial) relevance labels with\ncalibrated relevance scores. Extensive evaluation shows that DIRAS enables\nsmaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking\nunseen (query, document) pairs, and is helpful for real-world RAG development.\nAll code, LLM generations, and human annotations can be found in\n\\url{https://github.com/EdisonNi-hku/DIRAS}.\n","authors":["Jingwei Ni","Tobias Schimanski","Meihong Lin","Mrinmaya Sachan","Elliott Ash","Markus Leippold"],"pdf_url":"https://arxiv.org/pdf/2406.14162v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12532v1","updated":"2024-10-16T13:10:27Z","published":"2024-10-16T13:10:27Z","title":"MedAide: Towards an Omni Medical Aide via Specialized LLM-based\n  Multi-Agent Collaboration","summary":"  Large Language Model (LLM)-driven interactive systems currently show\npotential promise in healthcare domains. Despite their remarkable capabilities,\nLLMs typically lack personalized recommendations and diagnosis analysis in\nsophisticated medical applications, causing hallucinations and performance\nbottlenecks. To address these challenges, this paper proposes MedAide, an\nLLM-based omni medical multi-agent collaboration framework for specialized\nhealthcare services. Specifically, MedAide first performs query rewriting\nthrough retrieval-augmented generation to accomplish accurate medical intent\nunderstanding. Immediately, we devise a contextual encoder to obtain intent\nprototype embeddings, which are used to recognize fine-grained intents by\nsimilarity matching. According to the intent relevance, the activated agents\ncollaborate effectively to provide integrated decision analysis. Extensive\nexperiments are conducted on four medical benchmarks with composite intents.\nExperimental results from automated metrics and expert doctor evaluations show\nthat MedAide outperforms current LLMs and improves their medical proficiency\nand strategic reasoning.\n","authors":["Jinjie Wei","Dingkang Yang","Yanshu Li","Qingyao Xu","Zhaoyu Chen","Mingcheng Li","Yue Jiang","Xiaolu Hou","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12532v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2407.09879v3","updated":"2024-10-16T12:57:56Z","published":"2024-07-13T13:03:45Z","title":"sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through\n  N-shot Guided Prompting","summary":"  Despite the remarkable success of LLMs in English, there is a significant gap\nin performance in non-English languages. In order to address this, we introduce\na novel recipe for creating a multilingual synthetic instruction tuning\ndataset, sPhinX, which is created by selectively translating instruction\nresponse pairs from English into 50 languages. We test the effectiveness of\nsPhinx by using it to fine-tune two state-of-the-art models, Mistral-7B and\nPhi-Small and then evaluating them across a comprehensive suite of multilingual\nbenchmarks that test reasoning, question answering, reading comprehension and\nmachine translation. Our results show that Mistral-7B and Phi-Small fine-tuned\nwith sPhinX perform better on an average by 5%pt for both the models when\ncompared to the base variants of these models. We also devise a strategy to\nincorporate N-shot examples in each fine-tuning sample which further boosts the\nperformance of these models by 9%pt and 4%pt respectively respectively compared\nto vanilla fine-tuning. To show efficacy of our data curation approach, we also\ndirectly translate our original dataset to the target languages, and observe an\nincrease of 7%pt and 4%pt on both the models respectively. sPhinX outperforms\nother multilingual instruction tuning datasets in both efficiency and\ndiversity, reducing dataset creation costs. It also maintains strong\nperformance on standard English LLM benchmarks, with minimal regression.\n","authors":["Sanchit Ahuja","Kumar Tanmay","Hardik Hansrajbhai Chauhan","Barun Patra","Kriti Aggarwal","Luciano Del Corro","Arindam Mitra","Tejas Indulal Dhamecha","Ahmed Awadallah","Monojit Choudhary","Vishrav Chaudhary","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2407.09879v3.pdf","comment":"20 pages, 12 tables, 5 figures"},{"id":"http://arxiv.org/abs/2409.19014v3","updated":"2024-10-16T12:55:55Z","published":"2024-09-24T01:40:50Z","title":"FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark","summary":"  Text-to-SQL systems have become crucial for translating natural language into\nSQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, the Execution Accuracy\n(EX), the most prevalent evaluation metric, still shows many false positives\nand negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our metric improves\nagreement with human experts (from 62 to 87.04 in Cohen's kappa) with\ncomprehensive context and sophisticated criteria. Our extensive experiments\nyield several key insights: (1) Models' performance increases by over 2.6\npoints on average, substantially affecting rankings on Spider and BIRD\nbenchmarks; (2) The underestimation of models in EX primarily stems from\nannotation quality issues; and (3) Model performance on particularly\nchallenging questions tends to be overestimated. This work contributes to a\nmore accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field.\n","authors":["Heegyu Kim","Taeyang Jeon","Seunghwan Choi","Seungtaek Choi","Hyunsouk Cho"],"pdf_url":"https://arxiv.org/pdf/2409.19014v3.pdf","comment":"preprint, under review"},{"id":"http://arxiv.org/abs/2410.12513v1","updated":"2024-10-16T12:45:35Z","published":"2024-10-16T12:45:35Z","title":"FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction","summary":"  Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments.\n","authors":["Akriti Jain","Saransh Sharma","Koyel Mukherjee","Soumyabrata Pal"],"pdf_url":"https://arxiv.org/pdf/2410.12513v1.pdf","comment":"17 pages, 6 figures, Submitted to ICLR 2025"},{"id":"http://arxiv.org/abs/2410.12511v1","updated":"2024-10-16T12:38:58Z","published":"2024-10-16T12:38:58Z","title":"Advancing Fairness in Natural Language Processing: From Traditional\n  Methods to Explainability","summary":"  The burgeoning field of Natural Language Processing (NLP) stands at a\ncritical juncture where the integration of fairness within its frameworks has\nbecome an imperative. This PhD thesis addresses the need for equity and\ntransparency in NLP systems, recognizing that fairness in NLP is not merely a\ntechnical challenge but a moral and ethical necessity, requiring a rigorous\nexamination of how these technologies interact with and impact diverse human\npopulations. Through this lens, this thesis undertakes a thorough investigation\ninto the development of equitable NLP methodologies and the evaluation of\nbiases that prevail in current systems.\n  First, it introduces an innovative algorithm to mitigate biases in\nmulti-class classifiers, tailored for high-risk NLP applications, surpassing\ntraditional methods in both bias mitigation and prediction accuracy. Then, an\nanalysis of the Bios dataset reveals the impact of dataset size on\ndiscriminatory biases and the limitations of standard fairness metrics. This\nawareness has led to explorations in the field of explainable AI, aiming for a\nmore complete understanding of biases where traditional metrics are limited.\nConsequently, the thesis presents COCKATIEL, a model-agnostic explainability\nmethod that identifies and ranks concepts in Transformer models, outperforming\nprevious approaches in sentiment analysis tasks. Finally, the thesis\ncontributes to bridging the gap between fairness and explainability by\nintroducing TaCo, a novel method to neutralize bias in Transformer model\nembeddings.\n  In conclusion, this thesis constitutes a significant interdisciplinary\nendeavor that intertwines explicability and fairness to challenge and reshape\ncurrent NLP paradigms. The methodologies and critiques presented contribute to\nthe ongoing discourse on fairness in machine learning, offering actionable\nsolutions for more equitable and responsible AI systems.\n","authors":["Fanny Jourdan"],"pdf_url":"https://arxiv.org/pdf/2410.12511v1.pdf","comment":"PhD Thesis, Toulouse University"},{"id":"http://arxiv.org/abs/2410.10114v2","updated":"2024-10-16T12:30:53Z","published":"2024-10-14T03:05:12Z","title":"Mixture of Experts Made Personalized: Federated Prompt Learning for\n  Vision-Language Models","summary":"  Prompt learning for pre-trained Vision-Language Models (VLMs) like CLIP has\ndemonstrated potent applicability across diverse downstream tasks. This\nlightweight approach has quickly gained traction from federated learning (FL)\nresearchers who seek to efficiently adapt VLMs to heterogeneous scenarios.\nHowever, current federated prompt learning methods are habitually restricted to\nthe traditional FL paradigm, where the participating clients are generally only\nallowed to download a single globally aggregated model from the server. While\njustifiable for training full-sized models under federated settings, in this\nwork, we argue that this paradigm is ill-suited for lightweight prompts. By\nfacilitating the clients to download multiple pre-aggregated prompts as fixed\nnon-local experts, we propose Personalized Federated Mixture of Adaptive\nPrompts (pFedMoAP), a novel FL framework that personalizes the prompt learning\nprocess through the lens of Mixture of Experts (MoE). pFedMoAP implements a\nlocal attention-based gating network that learns to generate enhanced text\nfeatures for better alignment with local image data on the client, benefiting\nfrom both local and downloaded non-local adaptive prompt experts. The non-local\nexperts are sparsely selected from a server-maintained pool, fostering\ncollaborative learning across clients. To evaluate the proposed algorithm, we\nconduct extensive experiments across 9 datasets under various heterogeneous\nfederated settings. The results show that pFedMoAP consistently outperforms the\nstate-of-the-art alternatives, underscoring its efficacy in personalizing\nprompt learning for CLIP within the federated learning paradigm.\n","authors":["Jun Luo","Chen Chen","Shandong Wu"],"pdf_url":"https://arxiv.org/pdf/2410.10114v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.12499v1","updated":"2024-10-16T12:22:47Z","published":"2024-10-16T12:22:47Z","title":"With a Grain of SALT: Are LLMs Fair Across Social Dimensions?","summary":"  This paper presents an analysis of biases in open-source Large Language\nModels (LLMs) across various genders, religions, and races. We introduce a\nmethodology for generating a bias detection dataset using seven bias triggers:\nGeneral Debate, Positioned Debate, Career Advice, Story Generation,\nProblem-Solving, Cover-Letter Writing, and CV Generation. We use GPT-4o to\ngenerate a diverse set of prompts for each trigger across various genders,\nreligious and racial groups. We evaluate models from Llama and Gemma family on\nthe generated dataset. We anonymise the LLM-generated text associated with each\ngroup using GPT-4o-mini and do a pairwise comparison using GPT-4o-as-a-Judge.\nTo quantify bias in the LLM-generated text we use the number of wins and losses\nin the pairwise comparison. Our analysis spans three languages, English,\nGerman, and Arabic to explore how language influences bias manifestation. Our\nfindings reveal that LLMs exhibit strong polarization toward certain groups\nacross each category, with a notable consistency observed across models.\nHowever, when switching languages, variations and anomalies emerge, often\nattributable to cultural cues and contextual differences.\n","authors":["Samee Arif","Zohaib Khan","Agha Ali Raza","Awais Athar"],"pdf_url":"https://arxiv.org/pdf/2410.12499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05770v2","updated":"2024-10-16T12:18:20Z","published":"2024-10-08T07:52:35Z","title":"Efficient Few-shot Learning for Multi-label Classification of Scientific\n  Documents with Many Classes","summary":"  Scientific document classification is a critical task and often involves many\nclasses. However, collecting human-labeled data for many classes is expensive\nand usually leads to label-scarce scenarios. Moreover, recent work has shown\nthat sentence embedding model fine-tuning for few-shot classification is\nefficient, robust, and effective. In this work, we propose FusionSent\n(Fusion-based Sentence Embedding Fine-tuning), an efficient and prompt-free\napproach for few-shot classification of scientific documents with many classes.\nFusionSent uses available training examples and their respective label texts to\ncontrastively fine-tune two different sentence embedding models. Afterward, the\nparameters of both fine-tuned models are fused to combine the complementary\nknowledge from the separate fine-tuning steps into a single model. Finally, the\nresulting sentence embedding model is frozen to embed the training instances,\nwhich are then used as input features to train a classification head. Our\nexperiments show that FusionSent significantly outperforms strong baselines by\nan average of $6.0$ $F_{1}$ points across multiple scientific document\nclassification datasets. In addition, we introduce a new dataset for\nmulti-label classification of scientific documents, which contains 203,961\nscientific articles and 130 classes from the arXiv category taxonomy. Code and\ndata are available at https://github.com/sebischair/FusionSent.\n","authors":["Tim Schopf","Alexander Blatzheim","Nektarios Machner","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2410.05770v2.pdf","comment":"Accepted to the 7th International Conference on Natural Language and\n  Speech Processing (ICNLSP 2024)"},{"id":"http://arxiv.org/abs/2408.09945v2","updated":"2024-10-16T12:15:39Z","published":"2024-08-19T12:34:31Z","title":"Benchmarking LLMs for Translating Classical Chinese Poetry:Evaluating\n  Adequacy, Fluency, and Elegance","summary":"  Large language models (LLMs) have shown remarkable performance in general\ntranslation tasks. However, the increasing demand for high-quality translations\nthat are not only adequate but also fluent and elegant. To assess the extent to\nwhich current LLMs can meet these demands, we introduce a suitable benchmark\nfor translating classical Chinese poetry into English. This task requires not\nonly adequacy in translating culturally and historically significant content\nbut also a strict adherence to linguistic fluency and poetic elegance. Our\nstudy reveals that existing LLMs fall short of this task. To address these\nissues, we propose RAT, a \\textbf{R}etrieval-\\textbf{A}ugmented machine\n\\textbf{T}ranslation method that enhances the translation process by\nincorporating knowledge related to classical poetry. Additionally, we propose\nan automatic evaluation metric based on GPT-4, which better assesses\ntranslation quality in terms of adequacy, fluency, and elegance, overcoming the\nlimitations of traditional metrics. Our dataset and code will be made\navailable.\n","authors":["Andong Chen","Lianzhang Lou","Kehai Chen","Xuefeng Bai","Yang Xiang","Muyun Yang","Tiejun Zhao","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.09945v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2408.08688v4","updated":"2024-10-16T12:15:19Z","published":"2024-08-16T12:01:55Z","title":"The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic\n  Preference Optimization Dataset Generation","summary":"  This paper presents a novel methodology for generating synthetic Preference\nOptimization (PO) datasets using multi-agent workflows. We evaluate the\neffectiveness and potential of these workflows in automating and enhancing the\ndataset generation process. PO dataset generation requires two modules: (1)\nresponse evaluation, and (2) response generation. In the response evaluation\nmodule, the responses from Large Language Models (LLMs) are evaluated and\nranked - a task typically carried out by human annotators that we automate\nusing LLMs. We assess the response evaluation module in a 2 step process. In\nstep 1, we assess LLMs as evaluators using three distinct prompting strategies.\nIn step 2, we apply the winning prompting strategy to compare the performance\nof LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. Our evaluation shows that\nGPT-4o-as-a-Judge is more consistent across all datasets. For the response\ngeneration module, we use the identified LLM evaluator configuration and\ncompare different configurations of the LLM Feedback Loop. We use the win rate\nto determine the best multi-agent configuration for generation. Experimenting\nwith various configurations, we find that the LLM Feedback Loop, with Llama as\nthe generator and Gemma as the reviewer, achieves a notable 71.8% and 73.8% win\nrate over single-agent Llama and Gemma, respectively. After identifying the\nbest configurations for both modules, we generate our PO datasets using the\nabove pipeline.\n","authors":["Samee Arif","Sualeha Farid","Abdul Hameed Azeemi","Awais Athar","Agha Ali Raza"],"pdf_url":"https://arxiv.org/pdf/2408.08688v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12492v1","updated":"2024-10-16T12:14:29Z","published":"2024-10-16T12:14:29Z","title":"End-to-end Planner Training for Language Modeling","summary":"  Through end-to-end training to predict the next token, LLMs have become\nvaluable tools for various tasks. Enhancing their core training in language\nmodeling can improve numerous downstream applications. A successful approach to\nenhance language modeling uses a separate planning module to predict abstract\nlabels of future sentences and conditions the LM on these predictions. However,\nthis method is non-differentiable, preventing joint end-to-end tuning of the\nplanner with the LM. We propose an effective method to improve this approach by\nenabling joint fine-tuning of the planner and the LM. We show that a naive way\nof approximating the gradient of selecting a label via the straight-through\nestimator is not effective. Instead, we propose to use the predicted label\nprobabilities as mixing weights to condition the LM on a weighted average of\nlabel embeddings in a differentiable manner. This not only enables joint\nfine-tuning of the planner and the LM, but also allows the LM to draw on the\nfull label distribution predicted by the planner, retaining more information.\nOur experimental results show consistent improvements in perplexity.\n","authors":["Nathan Cornille","Florian Mai","Jingyuan Sun","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2410.12492v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2410.12491v1","updated":"2024-10-16T12:14:25Z","published":"2024-10-16T12:14:25Z","title":"Insights from the Inverse: Reconstructing LLM Training Goals Through\n  Inverse RL","summary":"  Large language models (LLMs) trained with Reinforcement Learning from Human\nFeedback (RLHF) have demonstrated remarkable capabilities, but their underlying\nreward functions and decision-making processes remain opaque. This paper\nintroduces a novel approach to interpreting LLMs by applying inverse\nreinforcement learning (IRL) to recover their implicit reward functions. We\nconduct experiments on toxicity-aligned LLMs of varying sizes, extracting\nreward models that achieve up to 80.40% accuracy in predicting human\npreferences. Our analysis reveals key insights into the non-identifiability of\nreward functions, the relationship between model size and interpretability, and\npotential pitfalls in the RLHF process. We demonstrate that IRL-derived reward\nmodels can be used to fine-tune new LLMs, resulting in comparable or improved\nperformance on toxicity benchmarks. This work provides a new lens for\nunderstanding and improving LLM alignment, with implications for the\nresponsible development and deployment of these powerful systems.\n","authors":["Jared Joselowitz","Arjun Jagota","Satyapriya Krishna","Sonali Parbhoo"],"pdf_url":"https://arxiv.org/pdf/2410.12491v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.16535v2","updated":"2024-10-16T12:00:46Z","published":"2024-06-24T11:16:26Z","title":"Token-based Decision Criteria Are Suboptimal in In-context Learning","summary":"  In-Context Learning (ICL) typically utilizes classification criteria from\noutput probabilities of manually selected label tokens. However, we argue that\nsuch token-based classification criteria lead to suboptimal decision\nboundaries, despite delicate calibrations through translation and constrained\nrotation applied. To address this problem, we propose Hidden Calibration, which\nrenounces token probabilities and uses the nearest centroid classifier on the\nLM's last hidden states. In detail, we assign the label of the nearest centroid\npreviously estimated from a calibration set to the test sample as the predicted\nlabel. Our experiments on 6 models and 10 classification datasets indicate that\nHidden Calibration consistently outperforms current token-based baselines by\nabout 20%~50%, achieving a strong state-of-the-art in ICL. Our further analysis\ndemonstrates that Hidden Calibration finds better classification criteria with\nless inter-class overlap, and LMs provide linearly separable intra-class\nclusters with the help of demonstrations, which supports Hidden Calibration and\ngives new insights into the principle of ICL.\n","authors":["Hakaze Cho","Yoshihiro Sakai","Mariko Kato","Kenshiro Tanaka","Akira Ishii","Naoya Inoue"],"pdf_url":"https://arxiv.org/pdf/2406.16535v2.pdf","comment":"24 pages, 15 figures, 13 tables"},{"id":"http://arxiv.org/abs/2402.10052v2","updated":"2024-10-16T11:50:27Z","published":"2024-02-15T16:21:14Z","title":"UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in\n  Large Language Models","summary":"  Mitigating the retention of sensitive or private information in large\nlanguage models is essential for enhancing privacy and safety. Existing\nunlearning methods, like Gradient Ascent and Negative Preference Optimization,\ndirectly tune models to remove unwanted information. However, these methods\noften become unstable because they fine-tune by maximizing cross-entropy loss,\nwhich is the opposite of traditional loss minimization in learning. This\nreversal creates instability, especially on larger datasets, as the model\nstruggles to balance unlearning with maintaining language capacity, leading to\nover-unlearning. In this paper, we introduce UnDIAL (Unlearning via\nSelf-Distillation on Adjusted Logits), a novel and robust unlearning method.\nOur approach leverages self-distillation to adjust logits and selectively\nreduce the influence of targeted tokens. This technique ensures smooth\nconvergence and avoids catastrophic forgetting, even in challenging unlearning\ntasks with large datasets and sequential unlearning requests. Extensive\nexperiments show that UnDIAL can achieve both robustness in unlearning and\nscalability while maintaining stable training dynamics and resilience to\nhyperparameter tuning.\n","authors":["Yijiang River Dong","Hongzhou Lin","Mikhail Belkin","Ramon Huerta","Ivan Vulić"],"pdf_url":"https://arxiv.org/pdf/2402.10052v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12480v1","updated":"2024-10-16T11:50:02Z","published":"2024-10-16T11:50:02Z","title":"KcMF: A Knowledge-compliant Framework for Schema and Entity Matching\n  with Fine-tuning-free LLMs","summary":"  Schema and entity matching tasks are crucial for data integration and\nmanagement. While large language models (LLMs) have shown promising results in\nthese tasks, they suffer from hallucinations and confusion about task\ninstructions. In this paper, we present the Knowledge-Compliant Matching\nFramework (KcMF), an LLM-based approach that addresses these issues without the\nneed for domain-specific fine-tuning. KcMF employs a pseudo-code-based task\ndecomposition strategy to adopt task-specific natural language statements that\nguide LLM reasoning and reduce confusion. We also propose two mechanisms,\nDataset as Knowledge (DaK) and Example as Knowledge (EaK), to build domain\nknowledge sets when unstructured domain knowledge is lacking. Additionally, we\nintroduce a result-ensembling strategy to leverage multiple knowledge sources\nand suppress poorly formatted outputs. Comprehensive evaluations on schema and\nentity matching tasks demonstrate that KcMF outperforms previous non-LLM\nstate-of-the-art (SOTA) methods by an average F1 score of 22.9% and competes\neffectively with SOTA fine-tuned LLMs. Moreover, KcMF generalizes well across\ndifferent LLMs.\n","authors":["Yongqin Xu","Huan Li","Ke Chen","Lidan Shou"],"pdf_url":"https://arxiv.org/pdf/2410.12480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08440v3","updated":"2024-10-16T11:49:48Z","published":"2024-07-11T12:26:55Z","title":"Beyond Instruction Following: Evaluating Inferential Rule Following of\n  Large Language Models","summary":"  Although Large Language Models (LLMs) have demonstrated strong ability, they\nare further supposed to be controlled and guided by in real-world scenarios to\nbe safe, accurate, and intelligent. This demands the possession of capability\nof LLMs. However, no prior work has made a clear evaluation of the inferential\nrule-following capability of LLMs. Previous studies that try to evaluate the\ninferential rule-following capability of LLMs fail to distinguish the\ninferential rule-following scenarios from the instruction-following scenarios.\nTherefore, this paper first clarifies the concept of inferential rule-following\nand proposes a comprehensive benchmark, RuleBench, to evaluate a diversified\nrange of inferential rule-following abilities. Our experimental results on a\nvariety of LLMs show that they are still limited in following rules. Our\nanalysis based on the evaluation results provides insights into the\nimprovements for LLMs toward a better inferential rule-following intelligent\nagent. We further propose Inferential Rule-Following Tuning (IRFT). The\nexperimental results show that through IRFT, LLMs can learn abstract\nrule-following abilities from purely synthetic data and then generalize to\nRuleBench. The data and code can be found at:\nhttps://anonymous.4open.science/r/llm-rule-following-B3E3/\n","authors":["Wangtao Sun","Chenxiang Zhang","XueYou Zhang","Xuanqing Yu","Ziyang Huang","Pei Chen","Haotian Xu","Shizhu He","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.08440v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12478v1","updated":"2024-10-16T11:46:55Z","published":"2024-10-16T11:46:55Z","title":"MlingConf: A Comprehensive Study of Multilingual Confidence Estimation\n  on Large Language Models","summary":"  The tendency of Large Language Models (LLMs) to generate hallucinations\nraises concerns regarding their reliability. Therefore, confidence estimations\nindicating the extent of trustworthiness of the generations become essential.\nHowever, current LLM confidence estimations in languages other than English\nremain underexplored. This paper addresses this gap by introducing a\ncomprehensive investigation of Multilingual Confidence estimation (MlingConf)\non LLMs, focusing on both language-agnostic (LA) and language-specific (LS)\ntasks to explore the performance and language dominance effects of multilingual\nconfidence estimations on different tasks. The benchmark comprises four\nmeticulously checked and human-evaluate high-quality multilingual datasets for\nLA tasks and one for the LS task tailored to specific social, cultural, and\ngeographical contexts of a language. Our experiments reveal that on LA tasks\nEnglish exhibits notable linguistic dominance in confidence estimations than\nother languages, while on LS tasks, using question-related language to prompt\nLLMs demonstrates better linguistic dominance in multilingual confidence\nestimations. The phenomena inspire a simple yet effective native-tone prompting\nstrategy by employing language-specific prompts for LS tasks, effectively\nimproving LLMs' reliability and accuracy on LS tasks.\n","authors":["Boyang Xue","Hongru Wang","Rui Wang","Sheng Wang","Zezhong Wang","Yiming Du","Bin Liang","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2410.12478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12476v1","updated":"2024-10-16T11:46:32Z","published":"2024-10-16T11:46:32Z","title":"Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial\n  Generation","summary":"  Machine learning (ML) exhibits promise in the clinical domain. However, it is\nconstrained by data scarcity and ethical considerations, as the generation of\nclinical trials presents significant challenges due to stringent privacy\nregulations, high costs, and the extended duration required for conducting\nstudies with human participants. Despite the advancements of large language\nmodels (LLMs) in general generation tasks, their potential in facilitating the\ngeneration of synthetic clinical trials is under-explored. To address this gap,\nwe introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs\nto generate artificial yet realistic and diverse clinical trials with binary\nsuccess/failure labels. Experiments conducted on real clinical trials from the\n\\url{ClinicalTrials.gov} database demonstrate that our synthetic data can\neffectively augment real datasets. Furthermore, by fine-tuning a pre-trained\nmodel as a binary classifier on synthetic clinical trial datasets, we\ndemonstrate that this augmentation enhances model training for downstream tasks\nsuch as trial outcome prediction. Our findings suggest that LLMs for synthetic\nclinical trial generation hold promise for accelerating clinical research and\nupholding ethical standards for patient privacy. The code is publicly available\nat\nhttps://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.\n","authors":["Zerui Xu","Fang Wu","Tianfan Fu","Yue Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.12476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12470v1","updated":"2024-10-16T11:34:33Z","published":"2024-10-16T11:34:33Z","title":"Learning to Predict Usage Options of Product Reviews with LLM-Generated\n  Labels","summary":"  Annotating large datasets can be challenging. However, crowd-sourcing is\noften expensive and can lack quality, especially for non-trivial tasks. We\npropose a method of using LLMs as few-shot learners for annotating data in a\ncomplex natural language task where we learn a standalone model to predict\nusage options for products from customer reviews. We also propose a new\nevaluation metric for this scenario, HAMS4, that can be used to compare a set\nof strings with multiple reference sets. Learning a custom model offers\nindividual control over energy efficiency and privacy measures compared to\nusing the LLM directly for the sequence-to-sequence task. We compare this data\nannotation approach with other traditional methods and demonstrate how LLMs can\nenable considerable cost savings. We find that the quality of the resulting\ndata exceeds the level attained by third-party vendor services and that\nGPT-4-generated labels even reach the level of domain experts. We make the code\nand generated labels publicly available.\n","authors":["Leo Kohlenberg","Leonard Horns","Frederic Sadrieh","Nils Kiele","Matthis Clausen","Konstantin Ketterer","Avetis Navasardyan","Tamara Czinczoll","Gerard de Melo","Ralf Herbrich"],"pdf_url":"https://arxiv.org/pdf/2410.12470v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2410.12462v1","updated":"2024-10-16T11:23:03Z","published":"2024-10-16T11:23:03Z","title":"Bridging the Language Gaps in Large Language Models with Inference-Time\n  Cross-Lingual Intervention","summary":"  Large Language Models (LLMs) have shown remarkable capabilities in natural\nlanguage processing but exhibit significant performance gaps among different\nlanguages. Most existing approaches to address these disparities rely on\npretraining or fine-tuning, which are resource-intensive. To overcome these\nlimitations without incurring significant costs, we propose Inference-Time\nCross-Lingual Intervention (INCLINE), a novel framework that enhances LLM\nperformance on low-performing (source) languages by aligning their internal\nrepresentations with those of high-performing (target) languages during\ninference. INCLINE initially learns alignment matrices using parallel sentences\nfrom source and target languages through a Least-Squares optimization, and then\napplies these matrices during inference to transform the low-performing\nlanguage representations toward the high-performing language space. Extensive\nexperiments on nine benchmarks with five LLMs demonstrate that INCLINE\nsignificantly improves performance across diverse tasks and languages, compared\nto recent strong baselines. Our analysis demonstrates that INCLINE is highly\ncost-effective and applicable to a wide range of applications. In addition, we\nrelease the code to foster research along this line:\nhttps://github.com/weixuan-wang123/INCLINE.\n","authors":["Weixuan Wang","Minghao Wu","Barry Haddow","Alexandra Birch"],"pdf_url":"https://arxiv.org/pdf/2410.12462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12458v1","updated":"2024-10-16T11:16:34Z","published":"2024-10-16T11:16:34Z","title":"The Best of Both Worlds: Bridging Quality and Diversity in Data\n  Selection with Bipartite Graph","summary":"  The performance of large language models (LLMs) in natural language\nprocessing (NLP) tasks is significantly influenced by the quality and diversity\nof data used for supervised fine-tuning (SFT). Current data selection methods\noften focus solely on quality or diversity, leading to underperforming models\ndue to suboptimal training data. In this paper, we introduce GraphFilter, a\nnovel method that represents the dataset as a bipartite graph, linking\nsentences to their constituent n-grams. This representation effectively\ncaptures the relationships between sentences and linguistic patterns,\nfacilitating the selection of sentences that enhance n-gram diversity. To\nbalance quality and diversity during selection, we propose a priority function\nthat combines the quality metric with the diversity metric in a multiplicative\nmanner. GraphFilter iteratively selects high-priority sentences, updates the\nbipartite graph by removing covered n-grams, and re-calculates priorities to\nreflect the evolving data landscape. We conduct extensive experiments using\nthree model backbones across six widely used benchmarks. The results\ndemonstrate that GraphFilter outperforms all nine baseline approaches,\nachieving superior model performance and computational efficiency. Our analyses\nvalidate the effectiveness of our design choices, examine the subsets selected\nby GraphFilter and other methods, highlight the importance of instruction\ndiversity, and explore the role of quality and diversity in relation to subset\nsizes. GraphFilter establishes a new foundation for effective data selection\nstrategies, encouraging further research in data selection for LLMs.\n","authors":["Minghao Wu","Thuy-Trang Vu","Lizhen Qu","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2410.12458v1.pdf","comment":"19 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2406.07411v2","updated":"2024-10-16T10:56:24Z","published":"2024-06-11T16:15:06Z","title":"VersiCode: Towards Version-controllable Code Generation","summary":"  Large Language Models (LLMs) have made tremendous strides in code generation,\nbut existing research fails to account for the dynamic nature of software\ndevelopment, marked by frequent library updates. This gap significantly limits\nLLMs' deployment in realistic settings. In this paper, we propose two novel\ntasks aimed at bridging this gap: version-specific code completion (VSCC) and\nversion-aware code migration (VACM). In conjunction, we introduce VersiCode, a\ncomprehensive Python dataset specifically designed to evaluate LLMs on these\ntwo tasks, together with a novel evaluation metric, Critical Diff Check\n(CDC@1), which assesses code generation against evolving API requirements. We\nconduct an extensive evaluation on VersiCode, which reveals that\nversion-controllable code generation is indeed a significant challenge, even\nfor GPT-4o and other strong frontier models. We believe the novel tasks,\ndataset, and metric open up a new, important research direction that will\nfurther enhance LLMs' real-world applicability. The code and resources can be\nfound at https://github.com/wutong8023/VersiCode.\n","authors":["Tongtong Wu","Weigang Wu","Xingyu Wang","Kang Xu","Suyu Ma","Bo Jiang","Ping Yang","Zhenchang Xing","Yuan-Fang Li","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2406.07411v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12445v1","updated":"2024-10-16T10:49:22Z","published":"2024-10-16T10:49:22Z","title":"Open Ko-LLM Leaderboard2: Bridging Foundational and Practical Evaluation\n  for Korean LLMs","summary":"  The Open Ko-LLM Leaderboard has been instrumental in benchmarking Korean\nLarge Language Models (LLMs), yet it has certain limitations. Notably, the\ndisconnect between quantitative improvements on the overly academic leaderboard\nbenchmarks and the qualitative impact of the models should be addressed.\nFurthermore, the benchmark suite is largely composed of translated versions of\ntheir English counterparts, which may not fully capture the intricacies of the\nKorean language. To address these issues, we propose Open Ko-LLM Leaderboard2,\nan improved version of the earlier Open Ko-LLM Leaderboard. The original\nbenchmarks are entirely replaced with new tasks that are more closely aligned\nwith real-world capabilities. Additionally, four new native Korean benchmarks\nare introduced to better reflect the distinct characteristics of the Korean\nlanguage. Through these refinements, Open Ko-LLM Leaderboard2 seeks to provide\na more meaningful evaluation for advancing Korean LLMs.\n","authors":["Hyeonwoo Kim","Dahyun Kim","Jihoo Kim","Sukyung Lee","Yungi Kim","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2410.12445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12444v1","updated":"2024-10-16T10:48:14Z","published":"2024-10-16T10:48:14Z","title":"Expanding Chatbot Knowledge in Customer Service: Context-Aware Similar\n  Question Generation Using Large Language Models","summary":"  Reliable responses of service chatbots are often achieved by employing\nretrieval-based methods that restrict answers to a knowledge base comprising\npredefined question-answer pairs (QA pairs). To accommodate potential\nvariations in how a customer's query may be expressed, it emerges as the\nfavored solution to augment these QA pairs with similar questions that are\npossibly diverse while remaining semantic consistency. This augmentation task\nis known as Similar Question Generation (SQG). Traditional methods that heavily\nrely on human efforts or rule-based techniques suffer from limited diversity or\nsignificant semantic deviation from the source question, only capable of\nproducing a finite number of useful questions.\n  To address these limitations, we propose an SQG approach based on Large\nLanguage Models (LLMs), capable of producing a substantial number of diverse\nquestions while maintaining semantic consistency to the source QA pair. This is\nachieved by leveraging LLMs' natural language understanding capability through\nfine-tuning with specially designed prompts. The experiments conducted on a\nreal customer-service dataset demonstrate that our method surpasses baseline\nmethods by a significant margin in terms of semantic diversity. Human\nevaluation further confirms that integrating the answer that reflects the\ncustomer's intention is crucial for increasing the number of generated\nquestions that meet business requirements.\n","authors":["Mengze Hong","Yuanfeng Song","Di Jiang","Lu Wang","Zichang Guo","Chen Jason Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11507v2","updated":"2024-10-16T10:36:18Z","published":"2024-10-15T11:20:42Z","title":"Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic\n  Evaluation Framework for LLMs","summary":"  While various vertical domain large language models (LLMs) have been\ndeveloped, the challenge of automatically evaluating their performance across\ndifferent domains remains significant. Current benchmark-based evaluation\nmethods exhibit rigid, aimless interactions and rely on pre-collected static\ndatasets that are costly to build, inflexible across domains, and misaligned\nwith practical user needs. To address this issue, we revisit the evaluation\ncomponents and introduce two concepts: Benchmark+, which extends traditional\nquestion-answer benchmark into a more flexible \"strategy-criterion\" format; and\nAssessment+, which enhances the interaction process, enabling deeper\nexploration and supporting both quantitative metrics and qualitative insights.\nThese concepts capture the nuanced behaviors of LLMs through richer, multi-turn\ninteractions. We propose an agent-based evaluation framework called TestAgent,\nwhich implements these concepts through retrieval augmented generation and\nreinforcement learning. Experiments on tasks ranging from constructing vertical\ndomain evaluation to activating existing benchmarks demonstrate the\neffectiveness of TestAgent across various scenarios. We believe this work\noffers an interesting perspective on automatic evaluation for LLMs.\n","authors":["Wanying Wang","Zeyu Ma","Pengfei Liu","Mingang Chen"],"pdf_url":"https://arxiv.org/pdf/2410.11507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16264v2","updated":"2024-10-16T10:19:45Z","published":"2024-08-29T05:02:52Z","title":"LoraMap: Harnessing the Power of LoRA Connections","summary":"  Fact-checking techniques can mitigate hallucinations in Large Language Models\n(LLMs), a prominent issue in specialized domains. As parameter-efficient\ntechniques such as Low-Rank Adaptation (LoRA) can overcome substantial\ncomputational overhead, some studies have explored the integration of multiple\nLoRAs. While previous studies focus on parallel integration, this paper\ninvestigates methods to establish connections among multiple LoRAs. We create\nthree reasoning datasets tailored to fact-checking and fine-tune individual\nLoRAs, allowing them to view and reason from diverse perspectives. Then, we\nexplore strategies for allocating these reasoning LoRAs and introduce LoraMap,\nan approach to map connections between them. The results of the fact-checking\ntask demonstrate that the performance of LoraMap is superior to LoraHub, an\nexisting method for integrating LoRAs. LoraMap also outperforms with\nsignificantly fewer trainable parameters than LoraConcat, which concatenates\nLoRAs and further fine-tunes them.\n","authors":["Hyeryun Park","Jeongwon Kwak","Dongsuk Jang","Sumin Park","Jinwook Choi"],"pdf_url":"https://arxiv.org/pdf/2408.16264v2.pdf","comment":"17 pages, 12 figures, 7 tables"},{"id":"http://arxiv.org/abs/2410.12428v1","updated":"2024-10-16T10:16:34Z","published":"2024-10-16T10:16:34Z","title":"Conformity in Large Language Models","summary":"  The conformity effect describes the tendency of individuals to align their\nresponses with the majority. Studying this bias in large language models (LLMs)\nis crucial, as LLMs are increasingly used in various information-seeking and\ndecision-making tasks as conversation partners to improve productivity. Thus,\nconformity to incorrect responses can compromise their effectiveness. In this\npaper, we adapt psychological experiments to examine the extent of conformity\nin state-of-the-art LLMs. Our findings reveal that all models tested exhibit\nvarying levels of conformity toward the majority, regardless of their initial\nchoice or correctness, across different knowledge domains. Notably, we are the\nfirst to show that LLMs are more likely to conform when they are more uncertain\nin their own prediction. We further explore factors that influence conformity,\nsuch as training paradigms and input characteristics, finding that\ninstruction-tuned models are less susceptible to conformity, while increasing\nthe naturalness of majority tones amplifies conformity. Finally, we propose two\ninterventions--Devil's Advocate and Question Distillation--to mitigate\nconformity, providing insights into building more robust language models.\n","authors":["Xiaochen Zhu","Caiqi Zhang","Tom Stafford","Nigel Collier","Andreas Vlachos"],"pdf_url":"https://arxiv.org/pdf/2410.12428v1.pdf","comment":"16 pages (8 pages main body), 14 figures"},{"id":"http://arxiv.org/abs/2410.07129v2","updated":"2024-10-16T10:14:54Z","published":"2024-10-09T17:51:55Z","title":"Mental Disorders Detection in the Era of Large Language Models","summary":"  This paper compares the effectiveness of traditional machine learning\nmethods, encoder-based models, and large language models (LLMs) on the task of\ndetecting depression and anxiety. Five datasets were considered, each differing\nin format and the method used to define the target pathology class. We tested\nAutoML models based on linguistic features, several variations of encoder-based\nTransformers such as BERT, and state-of-the-art LLMs as pathology\nclassification models. The results demonstrated that LLMs outperform\ntraditional methods, particularly on noisy and small datasets where training\nexamples vary significantly in text length and genre. However, psycholinguistic\nfeatures and encoder-based models can achieve performance comparable to\nlanguage models when trained on texts from individuals with clinically\nconfirmed depression, highlighting their potential effectiveness in targeted\nclinical applications.\n","authors":["Gleb Kuzmin","Petr Strepetov","Maksim Stankevich","Artem Shelmanov","Ivan Smirnov"],"pdf_url":"https://arxiv.org/pdf/2410.07129v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11288v2","updated":"2024-10-16T09:59:17Z","published":"2024-06-17T07:51:44Z","title":"MFC-Bench: Benchmarking Multimodal Fact-Checking with Large\n  Vision-Language Models","summary":"  Large vision-language models (LVLMs) have significantly improved multimodal\nreasoning tasks, such as visual question answering and image captioning. These\nmodels embed multimodal facts within their parameters, rather than relying on\nexternal knowledge bases to store factual information explicitly. However, the\ncontent discerned by LVLMs may deviate from actual facts due to inherent bias\nor incorrect inference. To address this issue, we introduce MFC-Bench, a\nrigorous and comprehensive benchmark designed to evaluate the factual accuracy\nof LVLMs across three stages of verdict prediction for MFC: Manipulation,\nOut-of-Context, and Veracity Classification. Through our evaluation on\nMFC-Bench, we benchmarked a dozen diverse and representative LVLMs, uncovering\nthat current models still fall short in multimodal fact-checking and\ndemonstrate insensitivity to various forms of manipulated content. We hope that\nMFC-Bench could raise attention to the trustworthy AI potentially assisted by\nLVLMs in the future. The MFC-Bench and accompanying resources are publicly\naccessible at https://github.com/wskbest/MFC-Bench, contributing to ongoing\nresearch in the multimodal fact-checking field.\n","authors":["Shengkang Wang","Hongzhan Lin","Ziyang Luo","Zhen Ye","Guang Chen","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2406.11288v2.pdf","comment":"28 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.12413v1","updated":"2024-10-16T09:56:01Z","published":"2024-10-16T09:56:01Z","title":"Theoretical Analysis of Hierarchical Language Recognition and Generation\n  by Transformers without Positional Encoding","summary":"  In this study, we provide constructive proof that Transformers can recognize\nand generate hierarchical language efficiently with respect to model size, even\nwithout the need for a specific positional encoding. Specifically, we show that\ncausal masking and a starting token enable Transformers to compute positional\ninformation and depth within hierarchical structures. We demonstrate that\nTransformers without positional encoding can generate hierarchical languages.\nFurthermore, we suggest that explicit positional encoding might have a\ndetrimental effect on generalization with respect to sequence length.\n","authors":["Daichi Hayakawa","Issei Sato"],"pdf_url":"https://arxiv.org/pdf/2410.12413v1.pdf","comment":"55 pages, 11 figures"},{"id":"http://arxiv.org/abs/2409.07891v2","updated":"2024-10-16T09:53:40Z","published":"2024-09-12T09:51:56Z","title":"A corpus-based investigation of pitch contours of monosyllabic words in\n  conversational Taiwan Mandarin","summary":"  In Mandarin, the tonal contours of monosyllabic words produced in isolation\nor in careful speech are characterized by four lexical tones: a high-level tone\n(T1), a rising tone (T2), a dipping tone (T3) and a falling tone (T4). However,\nin spontaneous speech, the actual tonal realization of monosyllabic words can\ndeviate significantly from these canonical tones due to intra-syllabic\nco-articulation and inter-syllabic co-articulation with adjacent tones. In\naddition, Chuang et al. (2024) recently reported that the tonal contours of\ndisyllabic Mandarin words with T2-T4 tone pattern are co-determined by their\nmeanings. Following up on their research, we present a corpus-based\ninvestigation of how the pitch contours of monosyllabic words are realized in\nspontaneous conversational Mandarin, focusing on the effects of contextual\npredictors on the one hand, and the way in words' meanings co-determine pitch\ncontours on the other hand. We analyze the F0 contours of 3824 tokens of 63\ndifferent word types in a spontaneous Taiwan Mandarin corpus, using the\ngeneralized additive (mixed) model to decompose a given observed pitch contour\ninto a set of component pitch contours. We show that the tonal context\nsubstantially modify a word's canonical tone. Once the effect of tonal context\nis controlled for, T2 and T3 emerge as low flat tones, contrasting with T1 as a\nhigh tone, and with T4 as a high-to-mid falling tone. The neutral tone (T0),\nwhich in standard descriptions, is realized based on the preceding tone,\nemerges as a low tone in its own right, modified by the other predictors in the\nsame way as the standard tones T1, T2, T3, and T4. We also show that word, and\neven more so, word sense, co-determine words' F0 contours. Analyses of variable\nimportance using random forests further supported the substantial effect of\ntonal context and an effect of word sense.\n","authors":["Xiaoyun Jin","Mirjam Ernestus","R. Harald Baayen"],"pdf_url":"https://arxiv.org/pdf/2409.07891v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12409v1","updated":"2024-10-16T09:44:38Z","published":"2024-10-16T09:44:38Z","title":"Revealing the Barriers of Language Agents in Planning","summary":"  Autonomous planning has been an ongoing pursuit since the inception of\nartificial intelligence. Based on curated problem solvers, early planning\nagents could deliver precise solutions for specific tasks but lacked\ngeneralization. The emergence of large language models (LLMs) and their\npowerful reasoning capabilities has reignited interest in autonomous planning\nby automatically generating reasonable solutions for given tasks. However,\nprior research and our experiments show that current language agents still lack\nhuman-level planning abilities. Even the state-of-the-art reasoning model,\nOpenAI o1, achieves only 15.6% on one of the complex real-world planning\nbenchmarks. This highlights a critical question: What hinders language agents\nfrom achieving human-level planning? Although existing studies have highlighted\nweak performance in agent planning, the deeper underlying issues and the\nmechanisms and limitations of the strategies proposed to address them remain\ninsufficiently understood. In this work, we apply the feature attribution study\nand identify two key factors that hinder agent planning: the limited role of\nconstraints and the diminishing influence of questions. We also find that\nalthough current strategies help mitigate these challenges, they do not fully\nresolve them, indicating that agents still have a long way to go before\nreaching human-level intelligence.\n","authors":["Jian Xie","Kexun Zhang","Jiangjie Chen","Siyu Yuan","Kai Zhang","Yikai Zhang","Lei Li","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2410.12409v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2410.12407v1","updated":"2024-10-16T09:42:29Z","published":"2024-10-16T09:42:29Z","title":"Beyond Coarse-Grained Matching in Video-Text Retrieval","summary":"  Video-text retrieval has seen significant advancements, yet the ability of\nmodels to discern subtle differences in captions still requires verification.\nIn this paper, we introduce a new approach for fine-grained evaluation. Our\napproach can be applied to existing datasets by automatically generating hard\nnegative test captions with subtle single-word variations across nouns, verbs,\nadjectives, adverbs, and prepositions. We perform comprehensive experiments\nusing four state-of-the-art models across two standard benchmarks (MSR-VTT and\nVATEX) and two specially curated datasets enriched with detailed descriptions\n(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our\nanalyses show that the current evaluation benchmarks fall short in detecting a\nmodel's ability to perceive subtle single-word differences, 2) our fine-grained\nevaluation highlights the difficulty models face in distinguishing such subtle\nvariations. To enhance fine-grained understanding, we propose a new baseline\nthat can be easily combined with current methods. Experiments on our\nfine-grained evaluations demonstrate that this approach enhances a model's\nability to understand fine-grained differences.\n","authors":["Aozhu Chen","Hazel Doughty","Xirong Li","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12407v1.pdf","comment":"Accepted to ACCV 2024"},{"id":"http://arxiv.org/abs/2410.12406v1","updated":"2024-10-16T09:41:48Z","published":"2024-10-16T09:41:48Z","title":"Nominal Class Assignment in Swahili: A Computational Account","summary":"  We discuss the open question of the relation between semantics and nominal\nclass assignment in Swahili. We approach the problem from a computational\nperspective, aiming first to quantify the extent of this relation, and then to\nexplicate its nature, taking extra care to suppress morphosyntactic confounds.\nOur results are the first of their kind, providing a quantitative evaluation of\nthe semantic cohesion of each nominal class, as well as a nuanced taxonomic\ndescription of its semantic content.\n","authors":["Giada Palmieri","Konstantinos Kogkalidis"],"pdf_url":"https://arxiv.org/pdf/2410.12406v1.pdf","comment":"Tenth Italian Conference on Computational Linguistics (CliC-it-2024)"},{"id":"http://arxiv.org/abs/2410.12405v1","updated":"2024-10-16T09:38:13Z","published":"2024-10-16T09:38:13Z","title":"ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs","summary":"  Large language models (LLMs) have demonstrated impressive capabilities across\nvarious tasks, but their performance is highly sensitive to the prompts\nutilized. This variability poses challenges for accurate assessment and user\nsatisfaction. Current research frequently overlooks instance-level prompt\nvariations and their implications on subjective evaluations. To address these\nshortcomings, we introduce ProSA, a framework designed to evaluate and\ncomprehend prompt sensitivity in LLMs. ProSA incorporates a novel sensitivity\nmetric, PromptSensiScore, and leverages decoding confidence to elucidate\nunderlying mechanisms. Our extensive study, spanning multiple tasks, uncovers\nthat prompt sensitivity fluctuates across datasets and models, with larger\nmodels exhibiting enhanced robustness. We observe that few-shot examples can\nalleviate this sensitivity issue, and subjective evaluations are also\nsusceptible to prompt sensitivities, particularly in complex,\nreasoning-oriented tasks. Furthermore, our findings indicate that higher model\nconfidence correlates with increased prompt robustness. We believe this work\nwill serve as a helpful tool in studying prompt sensitivity of LLMs. The\nproject is released at: https://github.com/open-compass/ProSA .\n","authors":["Jingming Zhuo","Songyang Zhang","Xinyu Fang","Haodong Duan","Dahua Lin","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12405v1.pdf","comment":"EMNLP 2024, Findings"},{"id":"http://arxiv.org/abs/2308.01472v2","updated":"2024-10-16T09:37:11Z","published":"2023-08-02T23:39:29Z","title":"Reverse Stable Diffusion: What prompt was used to generate this image?","summary":"  Text-to-image diffusion models have recently attracted the interest of many\nresearchers, and inverting the diffusion process can play an important role in\nbetter understanding the generative process and how to engineer prompts in\norder to obtain the desired images. To this end, we study the task of\npredicting the prompt embedding given an image generated by a generative\ndiffusion model. We consider a series of white-box and black-box models (with\nand without access to the weights of the diffusion network) to deal with the\nproposed task. We propose a novel learning framework comprising a joint prompt\nregression and multi-label vocabulary classification objective that generates\nimproved prompts. To further improve our method, we employ a curriculum\nlearning procedure that promotes the learning of image-prompt pairs with lower\nlabeling noise (i.e. that are better aligned). We conduct experiments on the\nDiffusionDB data set, predicting text prompts from images generated by Stable\nDiffusion. In addition, we make an interesting discovery: training a diffusion\nmodel on the prompt generation task can make the model generate images that are\nmuch better aligned with the input prompts, when the model is directly reused\nfor text-to-image generation. Our code is publicly available for download at\nhttps://github.com/CroitoruAlin/Reverse-Stable-Diffusion.\n","authors":["Florinel-Alin Croitoru","Vlad Hondru","Radu Tudor Ionescu","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2308.01472v2.pdf","comment":"Accepted for publication in Computer Vision and Image Understanding"},{"id":"http://arxiv.org/abs/2405.00716v4","updated":"2024-10-16T09:18:58Z","published":"2024-04-25T15:51:06Z","title":"Large Language Models in the Clinic: A Comprehensive Benchmark","summary":"  The adoption of large language models (LLMs) to assist clinicians has\nattracted remarkable attention. Existing works mainly adopt the close-ended\nquestion-answering (QA) task with answer options for evaluation. However, many\nclinical decisions involve answering open-ended questions without pre-set\noptions. To better understand LLMs in the clinic, we construct a benchmark\nClinicBench. We first collect eleven existing datasets covering diverse\nclinical language generation, understanding, and reasoning tasks. Furthermore,\nwe construct six novel datasets and clinical tasks that are complex but common\nin real-world practice, e.g., open-ended decision-making, long document\nprocessing, and emerging drug analysis. We conduct an extensive evaluation of\ntwenty-two LLMs under both zero-shot and few-shot settings. Finally, we invite\nmedical experts to evaluate the clinical usefulness of LLMs. The benchmark data\nis available at https://github.com/AI-in-Health/ClinicBench.\n","authors":["Fenglin Liu","Zheng Li","Hongjian Zhou","Qingyu Yin","Jingfeng Yang","Xianfeng Tang","Chen Luo","Ming Zeng","Haoming Jiang","Yifan Gao","Priyanka Nigam","Sreyashi Nag","Bing Yin","Yining Hua","Xuan Zhou","Omid Rohanian","Anshul Thakur","Lei Clifton","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2405.00716v4.pdf","comment":"Accepted at EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.12391v1","updated":"2024-10-16T09:18:39Z","published":"2024-10-16T09:18:39Z","title":"Tracking Universal Features Through Fine-Tuning and Model Merging","summary":"  We study how features emerge, disappear, and persist across models fine-tuned\non different domains of text. More specifically, we start from a base one-layer\nTransformer language model that is trained on a combination of the BabyLM\ncorpus, and a collection of Python code from The Stack. This base model is\nadapted to two new domains of text: TinyStories, and the Lua programming\nlanguage, respectively; and then these two models are merged using these two\nmodels using spherical linear interpolation. Our exploration aims to provide\ndeeper insights into the stability and transformation of features across\ntypical transfer-learning scenarios using small-scale models and sparse\nauto-encoders.\n","authors":["Niels Horn","Desmond Elliott"],"pdf_url":"https://arxiv.org/pdf/2410.12391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12388v1","updated":"2024-10-16T09:13:23Z","published":"2024-10-16T09:13:23Z","title":"Prompt Compression for Large Language Models: A Survey","summary":"  Leveraging large language models (LLMs) for complex natural language tasks\ntypically requires long-form prompts to convey detailed requirements and\ninformation, which results in increased memory usage and inference costs. To\nmitigate these challenges, multiple efficient methods have been proposed, with\nprompt compression gaining significant research interest. This survey provides\nan overview of prompt compression techniques, categorized into hard prompt\nmethods and soft prompt methods. First, the technical approaches of these\nmethods are compared, followed by an exploration of various ways to understand\ntheir mechanisms, including the perspectives of attention optimization,\nParameter-Efficient Fine-Tuning (PEFT), modality fusion, and new synthetic\nlanguage. We also examine the downstream adaptations of various prompt\ncompression techniques. Finally, the limitations of current prompt compression\nmethods are analyzed, and several future directions are outlined, such as\noptimizing the compression encoder, combining hard and soft prompts methods,\nand leveraging insights from multimodality.\n","authors":["Zongqian Li","Yinhong Liu","Yixuan Su","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2410.12388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03514v2","updated":"2024-10-16T08:59:22Z","published":"2024-03-06T07:43:43Z","title":"CLongEval: A Chinese Benchmark for Evaluating Long-Context Large\n  Language Models","summary":"  Developing Large Language Models (LLMs) with robust long-context capabilities\nhas been the recent research focus, resulting in the emergence of long-context\nLLMs proficient in Chinese. However, the evaluation of these models remains\nunderdeveloped due to a lack of benchmarks. To address this gap, we present\nCLongEval, a comprehensive Chinese benchmark for evaluating long-context LLMs.\nCLongEval is characterized by three key features: (1) Sufficient data volume,\ncomprising 7 distinct tasks and 7,267 examples; (2) Broad applicability,\naccommodating to models with context windows size from 1K to 100K; (3) High\nquality, with over 2,000 manually annotated question-answer pairs in addition\nto the automatically constructed labels. With CLongEval, we undertake a\ncomprehensive assessment of 6 open-source long-context LLMs and 2 leading\ncommercial counterparts that feature both long-context abilities and\nproficiency in Chinese. We also provide in-depth analysis based on the\nempirical results, trying to shed light on the critical capabilities that\npresent challenges in long-context settings. The dataset, evaluation scripts,\nand model outputs are released.\n","authors":["Zexuan Qiu","Jingjing Li","Shijue Huang","Xiaoqi Jiao","Wanjun Zhong","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2403.03514v2.pdf","comment":"Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.12380v1","updated":"2024-10-16T08:55:49Z","published":"2024-10-16T08:55:49Z","title":"Evaluation of Attribution Bias in Retrieval-Augmented Large Language\n  Models","summary":"  Attributing answers to source documents is an approach used to enhance the\nverifiability of a model's output in retrieval augmented generation (RAG).\nPrior work has mainly focused on improving and evaluating the attribution\nquality of large language models (LLMs) in RAG, but this may come at the\nexpense of inducing biases in the attribution of answers. We define and examine\ntwo aspects in the evaluation of LLMs in RAG pipelines, namely attribution\nsensitivity and bias with respect to authorship information. We explicitly\ninform an LLM about the authors of source documents, instruct it to attribute\nits answers, and analyze (i) how sensitive the LLM's output is to the author of\nsource documents, and (ii) whether the LLM exhibits a bias towards\nhuman-written or AI-generated source documents. We design an experimental setup\nin which we use counterfactual evaluation to study three LLMs in terms of their\nattribution sensitivity and bias in RAG pipelines. Our results show that adding\nauthorship information to source documents can significantly change the\nattribution quality of LLMs by 3% to 18%. Moreover, we show that LLMs can have\nan attribution bias towards explicit human authorship, which can serve as a\ncompeting hypothesis for findings of prior work that shows that LLM-generated\ncontent may be preferred over human-written contents. Our findings indicate\nthat metadata of source documents can influence LLMs' trust, and how they\nattribute their answers. Furthermore, our research highlights attribution bias\nand sensitivity as a novel aspect of brittleness in LLMs.\n","authors":["Amin Abolghasemi","Leif Azzopardi","Seyyed Hadi Hashemi","Maarten de Rijke","Suzan Verberne"],"pdf_url":"https://arxiv.org/pdf/2410.12380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06499v4","updated":"2024-10-16T08:53:23Z","published":"2023-12-11T16:22:37Z","title":"TaCo: Targeted Concept Erasure Prevents Non-Linear Classifiers From\n  Detecting Protected Attributes","summary":"  Ensuring fairness in NLP models is crucial, as they often encode sensitive\nattributes like gender and ethnicity, leading to biased outcomes. Current\nconcept erasure methods attempt to mitigate this by modifying final latent\nrepresentations to remove sensitive information without retraining the entire\nmodel. However, these methods typically rely on linear classifiers, which leave\nmodels vulnerable to non-linear adversaries capable of recovering sensitive\ninformation.\n  We introduce Targeted Concept Erasure (TaCo), a novel approach that removes\nsensitive information from final latent representations, ensuring fairness even\nagainst non-linear classifiers. Our experiments show that TaCo outperforms\nstate-of-the-art methods, achieving greater reductions in the prediction\naccuracy of sensitive attributes by non-linear classifier while preserving\noverall task performance. Code is available on\nhttps://github.com/fanny-jourdan/TaCo.\n","authors":["Fanny Jourdan","Louis Béthune","Agustin Picard","Laurent Risser","Nicholas Asher"],"pdf_url":"https://arxiv.org/pdf/2312.06499v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12377v1","updated":"2024-10-16T08:49:17Z","published":"2024-10-16T08:49:17Z","title":"HerO at AVeriTeC: The Herd of Open Large Language Models for Verifying\n  Real-World Claims","summary":"  To tackle the AVeriTeC shared task hosted by the FEVER-24, we introduce a\nsystem that only employs publicly available large language models (LLMs) for\neach step of automated fact-checking, dubbed the Herd of Open LLMs for\nverifying real-world claims (HerO). HerO employs multiple LLMs for each step of\nautomated fact-checking. For evidence retrieval, a language model is used to\nenhance a query by generating hypothetical fact-checking documents. We prompt\npretrained and fine-tuned LLMs for question generation and veracity prediction\nby crafting prompts with retrieved in-context samples. HerO achieved 2nd place\non the leaderboard with the AVeriTeC score of 0.57, suggesting the potential of\nopen LLMs for verifying real-world claims. For future research, we make our\ncode publicly available at https://github.com/ssu-humane/HerO.\n","authors":["Yejun Yoon","Jaeyoon Jung","Seunghyun Yoon","Kunwoo Park"],"pdf_url":"https://arxiv.org/pdf/2410.12377v1.pdf","comment":"A system description paper for the AVeriTeC shared task, hosted by\n  the seventh FEVER workshop (co-located with EMNLP 2024)"},{"id":"http://arxiv.org/abs/2410.12375v1","updated":"2024-10-16T08:46:26Z","published":"2024-10-16T08:46:26Z","title":"PRefLexOR: Preference-based Recursive Language Modeling for Exploratory\n  Optimization of Reasoning and Agentic Thinking","summary":"  PRefLexOR (Preference-based Recursive Language Modeling for Exploratory\nOptimization of Reasoning) combines preference optimization with concepts from\nReinforcement Learning to enable models to self-teach through iterative\nreasoning improvements. We propose a recursive learning approach that engages\nthe model in multi-step reasoning, revisiting, and refining intermediate steps\nbefore producing a final output in training and inference phases. Through\nmultiple training stages, the model first learns to align its reasoning with\naccurate decision paths by optimizing the log odds between preferred and\nnon-preferred responses. During this process, PRefLexOR builds a dynamic\nknowledge graph by generating questions from random text chunks and\nretrieval-augmentation to contextualize relevant details from the entire\ntraining corpus. In the second stage, preference optimization enhances model\nperformance by using rejection sampling to fine-tune reasoning quality by\ncontinually producing in-situ training data while masking the reasoning steps.\nRecursive optimization within a thinking token framework introduces iterative\nfeedback loops, where the model refines reasoning, achieving deeper coherence,\nconsistency, and adaptability. Implemented in small language models with only 3\nbillion parameters, we should that even tiny models can iteratively teach\nthemselves to reason with greater depth and reflectivity. Our implementation is\nstraightforward and can be incorporated into any existing pretrained LLM. We\nfocus our examples on applications in biological materials science and\ndemonstrate the method in a variety of case studies that range from in-domain\nto cross-domain applications. Using reasoning strategies that include thinking\nand reflection modalities we build a multi-agent recursive self-improving\ninference approach to successively improve responses via repeated sampling in\ninference time.\n","authors":["Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2410.12375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12361v1","updated":"2024-10-16T08:24:09Z","published":"2024-10-16T08:24:09Z","title":"Proactive Agent: Shifting LLM Agents from Reactive Responses to Active\n  Assistance","summary":"  Agents powered by large language models have shown remarkable abilities in\nsolving complex tasks. However, most agent systems remain reactive, limiting\ntheir effectiveness in scenarios requiring foresight and autonomous\ndecision-making. In this paper, we tackle the challenge of developing proactive\nagents capable of anticipating and initiating tasks without explicit human\ninstructions. We propose a novel data-driven approach for this problem.\nFirstly, we collect real-world human activities to generate proactive task\npredictions. These predictions are then labeled by human annotators as either\naccepted or rejected. The labeled data is used to train a reward model that\nsimulates human judgment and serves as an automatic evaluator of the\nproactiveness of LLM agents. Building on this, we develop a comprehensive data\ngeneration pipeline to create a diverse dataset, ProactiveBench, containing\n6,790 events. Finally, we demonstrate that fine-tuning models with the proposed\nProactiveBench can significantly elicit the proactiveness of LLM agents.\nExperimental results show that our fine-tuned model achieves an F1-Score of\n66.47% in proactively offering assistance, outperforming all open-source and\nclose-source models. These results highlight the potential of our method in\ncreating more proactive and effective agent systems, paving the way for future\nadvancements in human-agent collaboration.\n","authors":["Yaxi Lu","Shenzhi Yang","Cheng Qian","Guirong Chen","Qinyu Luo","Yesai Wu","Huadong Wang","Xin Cong","Zhong Zhang","Yankai Lin","Weiwen Liu","Yasheng Wang","Zhiyuan Liu","Fangming Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.12361v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.09822v2","updated":"2024-10-16T08:20:43Z","published":"2024-09-15T18:43:11Z","title":"Causal Inference with Large Language Model: A Survey","summary":"  Causal inference has been a pivotal challenge across diverse domains such as\nmedicine and economics, demanding a complicated integration of human knowledge,\nmathematical reasoning, and data mining capabilities. Recent advancements in\nnatural language processing (NLP), particularly with the advent of large\nlanguage models (LLMs), have introduced promising opportunities for traditional\ncausal inference tasks. This paper reviews recent progress in applying LLMs to\ncausal inference, encompassing various tasks spanning different levels of\ncausation. We summarize the main causal problems and approaches, and present a\ncomparison of their evaluation results in different causal scenarios.\nFurthermore, we discuss key findings and outline directions for future\nresearch, underscoring the potential implications of integrating LLMs in\nadvancing causal inference methodologies.\n","authors":["Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2409.09822v2.pdf","comment":"12 pages, 2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2312.03003v3","updated":"2024-10-16T08:15:53Z","published":"2023-12-04T06:13:35Z","title":"Explore, Select, Derive, and Recall: Augmenting LLM with Human-like\n  Memory for Mobile Task Automation","summary":"  The advent of large language models (LLMs) has opened up new opportunities in\nthe field of mobile task automation. Their superior language understanding and\nreasoning capabilities allow users to automate complex and repetitive tasks.\nHowever, due to the inherent unreliability and high operational cost of LLMs,\ntheir practical applicability is quite limited. To address these issues, this\npaper introduces MobileGPT, an innovative LLM-based mobile task automator\nequipped with a human-like app memory. MobileGPT emulates the cognitive process\nof humans interacting with a mobile app -- explore, select, derive, and recall.\nThis approach allows for a more precise and efficient learning of a task's\nprocedure by breaking it down into smaller, modular sub-tasks that can be\nre-used, re-arranged, and adapted for various objectives. We implement\nMobileGPT using online LLMs services (GPT-3.5 and GPT-4) and evaluate its\nperformance on a dataset of 185 tasks across 18 mobile apps. The results\nindicate that MobileGPT can automate and learn new tasks with 82.7% accuracy,\nand is able to adapt them to different contexts with near perfect (98.75%)\naccuracy while reducing both latency and cost by 62.5% and 68.8%, respectively,\ncompared to the GPT-4 powered baseline.\n","authors":["Sunjae Lee","Junyoung Choi","Jungjae Lee","Munim Hasan Wasi","Hojun Choi","Steven Y. Ko","Sangeun Oh","Insik Shin"],"pdf_url":"https://arxiv.org/pdf/2312.03003v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12350v1","updated":"2024-10-16T08:13:54Z","published":"2024-10-16T08:13:54Z","title":"GECTurk WEB: An Explainable Online Platform for Turkish Grammatical\n  Error Detection and Correction","summary":"  Sophisticated grammatical error detection/correction tools are available for\na small set of languages such as English and Chinese. However, it is not\nstraightforward -- if not impossible -- to adapt them to morphologically rich\nlanguages with complex writing rules like Turkish which has more than 80\nmillion speakers. Even though several tools exist for Turkish, they primarily\nfocus on spelling errors rather than grammatical errors and lack features such\nas web interfaces, error explanations and feedback mechanisms. To fill this\ngap, we introduce GECTurk WEB, a light, open-source, and flexible web-based\nsystem that can detect and correct the most common forms of Turkish writing\nerrors, such as the misuse of diacritics, compound and foreign words, pronouns,\nlight verbs along with spelling mistakes. Our system provides native speakers\nand second language learners an easily accessible tool to detect/correct such\nmistakes and also to learn from their mistakes by showing the explanation for\nthe violated rule(s). The proposed system achieves 88,3 system usability score,\nand is shown to help learn/remember a grammatical rule (confirmed by 80% of the\nparticipants). The GECTurk WEB is available both as an offline tool at\nhttps://github.com/GGLAB-KU/gecturkweb or online at www.gecturk.net.\n","authors":["Ali Gebeşçe","Gözde Gül Şahin"],"pdf_url":"https://arxiv.org/pdf/2410.12350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01308v2","updated":"2024-10-16T08:08:51Z","published":"2024-08-02T15:00:05Z","title":"Reconsidering Degeneration of Token Embeddings with Definitions for\n  Encoder-based Pre-trained Language Models","summary":"  Learning token embeddings based on token co-occurrence statistics has proven\neffective for both pre-training and fine-tuning in natural language processing.\nHowever, recent studies have pointed out that the distribution of learned\nembeddings degenerates into anisotropy (i.e., non-uniform distribution), and\neven pre-trained language models (PLMs) suffer from a loss of semantics-related\ninformation in embeddings for low-frequency tokens. This study first analyzes\nthe fine-tuning dynamics of encoder-based PLMs and demonstrates their\nrobustness against degeneration. On the basis of this analysis, we propose\nDefinitionEMB, a method that utilizes definitions to re-construct isotropically\ndistributed and semantics-related token embeddings for encoder-based PLMs while\nmaintaining original robustness during fine-tuning. Our experiments demonstrate\nthe effectiveness of leveraging definitions from Wiktionary to re-construct\nsuch embeddings for two encoder-based PLMs: RoBERTa-base and BART-large.\nFurthermore, the re-constructed embeddings for low-frequency tokens improve the\nperformance of these models across various GLUE and four text summarization\ndatasets.\n","authors":["Ying Zhang","Dongyuan Li","Manabu Okumura"],"pdf_url":"https://arxiv.org/pdf/2408.01308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07109v2","updated":"2024-10-16T08:06:22Z","published":"2024-10-09T17:45:47Z","title":"I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in\n  Multi-Agent Settings with Social Hierarchy","summary":"  As Large Language Model (LLM)-based agents become increasingly autonomous and\nwill more freely interact with each other, studying interactions between them\nbecomes crucial to anticipate emergent phenomena and potential risks. Drawing\ninspiration from the widely popular Stanford Prison Experiment, we contribute\nto this line of research by studying interaction patterns of LLM agents in a\ncontext characterized by strict social hierarchy. We do so by specifically\nstudying two types of phenomena: persuasion and anti-social behavior in\nsimulated scenarios involving a guard and a prisoner agent who seeks to achieve\na specific goal (i.e., obtaining additional yard time or escape from prison).\nLeveraging 200 experimental scenarios for a total of 2,000 machine-machine\nconversations across five different popular LLMs, we provide a set of\nnoteworthy findings. We first document how some models consistently fail in\ncarrying out a conversation in our multi-agent setup where power dynamics are\nat play. Then, for the models that were able to engage in successful\ninteractions, we empirically show how the goal that an agent is set to achieve\nimpacts primarily its persuasiveness, while having a negligible effect with\nrespect to the agent's anti-social behavior. Third, we highlight how agents'\npersonas, and particularly the guard's personality, drive both the likelihood\nof successful persuasion from the prisoner and the emergence of anti-social\nbehaviors. Fourth, we show that even without explicitly prompting for specific\npersonalities, anti-social behavior emerges by simply assigning agents' roles.\nThese results bear implications for the development of interactive LLM agents\nas well as the debate on their societal impact.\n","authors":["Gian Maria Campedelli","Nicolò Penzo","Massimo Stefan","Roberto Dessì","Marco Guerini","Bruno Lepri","Jacopo Staiano"],"pdf_url":"https://arxiv.org/pdf/2410.07109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15729v3","updated":"2024-10-16T08:04:46Z","published":"2024-02-24T05:40:01Z","title":"How Do Humans Write Code? Large Models Do It the Same Way Too","summary":"  Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought\n(CoT) as the most popular method in Large Language Models (LLMs) mathematical\nreasoning tasks by utilizing external tool calls to circumvent computational\nerrors. However, our evaluation of the GPT-4 and Llama series reveals that\nusing PoT introduces more reasoning errors, such as incorrect formulas or\nflawed logic, compared to CoT. To address this issue, we propose Human-Think\nLanguage (HTL), which leverages a suite of strategies that help integrate PoT\nand CoT, encompassing: (1) a new generation paradigm that uses full CoT\nreasoning to control code generation. (2) Focus Attention, that directs model\nattention to the CoT reasoning during PoT to generate more logical code. (3)\nreinforcement learning that utilizes the accuracy of both CoT and PoT responses\nas rewards to prevent repetitive reasoning steps in LLMs when solving difficult\nmath problems. Our method achieves an average improvement of 6.5% on the\nLlama-Base model and 4.3% on the Mistral-Base model across 8 mathematical\ncalculation datasets. It also shows significant effectiveness on five\nout-of-domain datasets by controlling the model's information flow, exhibiting\nstrong transferability. Additionally, HTL shows the most significant\nimprovement in non-mathematical natural language inference task, contributing\nto a unified reasoning task framework\n","authors":["Long Li","Xuzheng He","Haozhe Wang","Linlin Wang","Liang He"],"pdf_url":"https://arxiv.org/pdf/2402.15729v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12341v1","updated":"2024-10-16T08:02:48Z","published":"2024-10-16T08:02:48Z","title":"A linguistic analysis of undesirable outcomes in the era of generative\n  AI","summary":"  Recent research has focused on the medium and long-term impacts of generative\nAI, posing scientific and societal challenges mainly due to the detection and\nreliability of machine-generated information, which is projected to form the\nmajor content on the Web soon. Prior studies show that LLMs exhibit a lower\nperformance in generation tasks (model collapse) as they undergo a fine-tuning\nprocess across multiple generations on their own generated content\n(self-consuming loop). In this paper, we present a comprehensive simulation\nframework built upon the chat version of LLama2, focusing particularly on the\nlinguistic aspects of the generated content, which has not been fully examined\nin existing studies. Our results show that the model produces less lexical rich\ncontent across generations, reducing diversity. The lexical richness has been\nmeasured using the linguistic measures of entropy and TTR as well as\ncalculating the POSTags frequency. The generated content has also been examined\nwith an $n$-gram analysis, which takes into account the word order, and\nsemantic networks, which consider the relation between different words. These\nfindings suggest that the model collapse occurs not only by decreasing the\ncontent diversity but also by distorting the underlying linguistic patterns of\nthe generated text, which both highlight the critical importance of carefully\nchoosing and curating the initial input text, which can alleviate the model\ncollapse problem. Furthermore, we conduct a qualitative analysis of the\nfine-tuned models of the pipeline to compare their performances on generic NLP\ntasks to the original model. We find that autophagy transforms the initial\nmodel into a more creative, doubtful and confused one, which might provide\ninaccurate answers and include conspiracy theories in the model responses,\nspreading false and biased information on the Web.\n","authors":["Daniele Gambetta","Gizem Gezici","Fosca Giannotti","Dino Pedreschi","Alistair Knott","Luca Pappalardo"],"pdf_url":"https://arxiv.org/pdf/2410.12341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17753v2","updated":"2024-10-16T07:59:39Z","published":"2024-06-25T17:40:47Z","title":"Measuring and Benchmarking Large Language Models' Capabilities to\n  Generate Persuasive Language","summary":"  We are exposed to much information trying to influence us, such as teaser\nmessages, debates, politically framed news, and propaganda - all of which use\npersuasive language. With the recent interest in Large Language Models (LLMs),\nwe study the ability of LLMs to produce persuasive text. As opposed to prior\nwork which focuses on particular domains or types of persuasion, we conduct a\ngeneral study across various domains to measure and benchmark to what degree\nLLMs produce persuasive language - both when explicitly instructed to rewrite\ntext to be more or less persuasive and when only instructed to paraphrase. We\nconstruct the new dataset Persuasive-Pairs of pairs of a short text and its\nrewrite by an LLM to amplify or diminish persuasive language. We multi-annotate\nthe pairs on a relative scale for persuasive language: a valuable resource in\nitself, and for training a regression model to score and benchmark persuasive\nlanguage, including for new LLMs across domains. In our analysis, we find that\ndifferent 'personas' in LLaMA3's system prompt change persuasive language\nsubstantially, even when only instructed to paraphrase.\n","authors":["Amalie Brogaard Pauli","Isabelle Augenstein","Ira Assent"],"pdf_url":"https://arxiv.org/pdf/2406.17753v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12329v1","updated":"2024-10-16T07:49:13Z","published":"2024-10-16T07:49:13Z","title":"Understanding the Role of LLMs in Multimodal Evaluation Benchmarks","summary":"  The rapid advancement of Multimodal Large Language Models (MLLMs) has been\naccompanied by the development of various benchmarks to evaluate their\ncapabilities. However, the true nature of these evaluations and the extent to\nwhich they assess multimodal reasoning versus merely leveraging the underlying\nLarge Language Model (LLM) backbone remain unclear. This paper presents a\ncomprehensive investigation into the role of LLM backbones in MLLM evaluation,\nfocusing on two critical aspects: the degree to which current benchmarks truly\nassess multimodal reasoning and the influence of LLM prior knowledge on\nperformance. Specifically, we introduce a modified evaluation protocol to\ndisentangle the contributions of the LLM backbone from multimodal integration,\nand an automatic knowledge identification technique for diagnosing whether LLMs\nequip the necessary knowledge for corresponding multimodal questions. Our study\nencompasses four diverse MLLM benchmarks and eight state-of-the-art MLLMs. Key\nfindings reveal that some benchmarks allow high performance even without visual\ninputs and up to 50\\% of error rates can be attributed to insufficient world\nknowledge in the LLM backbone, indicating a heavy reliance on language\ncapabilities. To address knowledge deficiencies, we propose a knowledge\naugmentation pipeline that achieves significant performance gains, with\nimprovements of up to 60\\% on certain datasets, resulting in a approximately 4x\nincrease in performance. Our work provides crucial insights into the role of\nthe LLM backbone in MLLMs, and highlights the need for more nuanced\nbenchmarking approaches.\n","authors":["Botian Jiang","Lei Li","Xiaonan Li","Zhaowei Li","Xiachong Feng","Lingpeng Kong","Qi Liu","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.12329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12327v1","updated":"2024-10-16T07:47:45Z","published":"2024-10-16T07:47:45Z","title":"Neuron-based Personality Trait Induction in Large Language Models","summary":"  Large language models (LLMs) have become increasingly proficient at\nsimulating various personality traits, an important capability for supporting\nrelated applications (e.g., role-playing). To further improve this capacity, in\nthis paper, we present a neuron-based approach for personality trait induction\nin LLMs, with three major technical contributions. First, we construct\nPersonalityBench, a large-scale dataset for identifying and evaluating\npersonality traits in LLMs. This dataset is grounded in the Big Five\npersonality traits from psychology and is designed to assess the generative\ncapabilities of LLMs towards specific personality traits. Second, by leveraging\nPersonalityBench, we propose an efficient method for identifying\npersonality-related neurons within LLMs by examining the opposite aspects of a\ngiven trait. Third, we develop a simple yet effective induction method that\nmanipulates the values of these identified personality-related neurons. This\nmethod enables fine-grained control over the traits exhibited by LLMs without\ntraining and modifying model parameters. Extensive experiments validate the\nefficacy of our neuron identification and trait induction methods. Notably, our\napproach achieves comparable performance as fine-tuned models, offering a more\nefficient and flexible solution for personality trait induction in LLMs. We\nprovide access to all the mentioned resources at\nhttps://github.com/RUCAIBox/NPTI.\n","authors":["Jia Deng","Tianyi Tang","Yanbin Yin","Wenhao Yang","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2410.12327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12325v1","updated":"2024-10-16T07:45:56Z","published":"2024-10-16T07:45:56Z","title":"Optimizing Low-Resource Language Model Training: Comprehensive Analysis\n  of Multi-Epoch, Multi-Lingual, and Two-Stage Approaches","summary":"  In this paper, we address the challenge of optimizing training setups for\nLarge Language Models (LLMs) of low-resource language with a limited amount of\ncorpus. Existing works adopt multi-epoch, multi-lingual, and two-stage training\nto utilize the limited target language corpus efficiently. However, there is\nstill a lack of understanding about the optimal hyperparameter setups for\ncombining these three approaches to train LLMs. We exhaustively explore\ntraining setups for low-resource language LLM, combining these three\napproaches, and found the following insights for efficiently reducing the cost\nof hyperparameter search: (1) As the amount of target language corpus\ndecreases, the optimal training approach shifts from monolingual single-stage\ntraining to multi-lingual two-stage training at a compute budget dependent\nthreshold. (2) The optimal model scale remains stable regardless of the amount\nof target language corpus, allowing the use of the compute-optimal scale of\nmonolingual training. (3) The optimal number of epochs can be extrapolated from\nsmaller-scale experiments to larger scale using our proposed model. Also, we\nprovide evidence that, in single-stage training, the target language validation\nloss follows a power law with respect to the target language ratio, with an\nexponent independent of the amount of data, model scale, and language pair.\n","authors":["Kosuke Akimoto","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2410.12325v1.pdf","comment":"16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.12323v1","updated":"2024-10-16T07:44:28Z","published":"2024-10-16T07:44:28Z","title":"Reversal of Thought: Enhancing Large Language Models with\n  Preference-Guided Reverse Reasoning Warm-up","summary":"  Large language models (LLMs) have shown remarkable performance in reasoning\ntasks but face limitations in mathematical and complex logical reasoning.\nExisting methods to improve LLMs' logical capabilities either involve traceable\nor verifiable logical sequences that generate more reliable responses by\nconstructing logical structures yet increase computational costs, or introduces\nrigid logic template rules, reducing flexibility. In this paper, we propose\nReversal of Thought (RoT), a novel framework aimed at enhancing the logical\nreasoning abilities of LLMs. RoT utilizes a Preference-Guided Reverse Reasoning\nwarm-up strategy, which integrates logical symbols for pseudocode planning\nthrough meta-cognitive mechanisms and pairwise preference self-evaluation to\ngenerate task-specific prompts solely through demonstrations, aligning with\nLLMs' cognitive preferences shaped by Reinforcement Learning with Human\nFeedback (RLHF). Through reverse reasoning, we ultilize a Cognitive Preference\nManager to assess knowledge boundaries and further expand LLMs' reasoning\ncapabilities by aggregating solution logic for known tasks and stylistic\ntemplates for unknown tasks. Experiments across various tasks demonstrate that\nRoT surpasses existing baselines in both reasoning accuracy and efficiency.\n","authors":["Jiahao Yuan","Dehui Du","Hao Zhang","Zixiang Di","Usman Naseem"],"pdf_url":"https://arxiv.org/pdf/2410.12323v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12311v1","updated":"2024-10-16T07:24:28Z","published":"2024-10-16T07:24:28Z","title":"Open Domain Question Answering with Conflicting Contexts","summary":"  Open domain question answering systems frequently rely on information\nretrieved from large collections of text (such as the Web) to answer questions.\nHowever, such collections of text often contain conflicting information, and\nindiscriminately depending on this information may result in untruthful and\ninaccurate answers. To understand the gravity of this problem, we collect a\nhuman-annotated dataset, Question Answering with Conflicting Contexts (QACC),\nand find that as much as 25% of unambiguous, open domain questions can lead to\nconflicting contexts when retrieved using Google Search. We evaluate and\nbenchmark three powerful Large Language Models (LLMs) with our dataset QACC and\ndemonstrate their limitations in effectively addressing questions with\nconflicting information. To explore how humans reason through conflicting\ncontexts, we request our annotators to provide explanations for their\nselections of correct answers. We demonstrate that by finetuning LLMs to\nexplain their answers, we can introduce richer information into their training\nthat guide them through the process of reasoning with conflicting contexts.\n","authors":["Siyi Liu","Qiang Ning","Kishaloy Halder","Wei Xiao","Zheng Qi","Phu Mon Htut","Yi Zhang","Neha Anna John","Bonan Min","Yassine Benajiba","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2410.12311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07167v2","updated":"2024-10-16T07:23:03Z","published":"2024-10-09T17:59:04Z","title":"Deciphering Cross-Modal Alignment in Large Vision-Language Models with\n  Modality Integration Rate","summary":"  We present the Modality Integration Rate (MIR), an effective, robust, and\ngeneralized metric to indicate the multi-modal pre-training quality of Large\nVision Language Models (LVLMs). Large-scale pre-training plays a critical role\nin building capable LVLMs, while evaluating its training quality without the\ncostly supervised fine-tuning stage is under-explored. Loss, perplexity, and\nin-context evaluation results are commonly used pre-training metrics for Large\nLanguage Models (LLMs), while we observed that these metrics are less\nindicative when aligning a well-trained LLM with a new modality. Due to the\nlack of proper metrics, the research of LVLMs in the critical pre-training\nstage is hindered greatly, including the training data choice, efficient module\ndesign, etc. In this paper, we propose evaluating the pre-training quality from\nthe inter-modal distribution distance perspective and present MIR, the Modality\nIntegration Rate, which is 1) \\textbf{Effective} to represent the pre-training\nquality and show a positive relation with the benchmark performance after\nsupervised fine-tuning. 2) \\textbf{Robust} toward different training/evaluation\ndata. 3) \\textbf{Generalize} across training configurations and architecture\nchoices. We conduct a series of pre-training experiments to explore the\neffectiveness of MIR and observe satisfactory results that MIR is indicative\nabout training data selection, training strategy schedule, and model\narchitecture design to get better pre-training results. We hope MIR could be a\nhelpful metric for building capable LVLMs and inspire the following research\nabout modality alignment in different areas. Our code is at:\nhttps://github.com/shikiw/Modality-Integration-Rate.\n","authors":["Qidong Huang","Xiaoyi Dong","Pan Zhang","Yuhang Zang","Yuhang Cao","Jiaqi Wang","Dahua Lin","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2410.07167v2.pdf","comment":"Project page: https://github.com/shikiw/Modality-Integration-Rate"},{"id":"http://arxiv.org/abs/2406.13993v2","updated":"2024-10-16T07:14:20Z","published":"2024-06-20T04:44:20Z","title":"Exploring Changes in Nation Perception with Nationality-Assigned\n  Personas in LLMs","summary":"  Persona assignment has become a common strategy for customizing LLM use to\nparticular tasks and contexts. In this study, we explore how evaluation of\ndifferent nations change when LLMs are assigned specific nationality personas.\nWe assign 193 different nationality personas (e.g., an American person) to four\nLLMs and examine how the LLM evaluations (or ''perceptions'')of countries\nchange. We find that all LLM-persona combinations tend to favor Western\nEuropean nations, though nation-personas push LLM behaviors to focus more on\nand treat the nation-persona's own region more favorably. Eastern European,\nLatin American, and African nations are treated more negatively by different\nnationality personas. We additionally find that evaluations by nation-persona\nLLMs of other nations correlate with human survey responses but fail to match\nthe values closely. Our study provides insight into how biases and stereotypes\nare realized within LLMs when adopting different national personas. In line\nwith the ''Blueprint for an AI Bill of Rights'', our findings underscore the\ncritical need for developing mechanisms to ensure that LLM outputs promote\nfairness and avoid over-generalization.\n","authors":["Mahammed Kamruzzaman","Gene Louis Kim"],"pdf_url":"https://arxiv.org/pdf/2406.13993v2.pdf","comment":"Pre-print, Under review"},{"id":"http://arxiv.org/abs/2410.05581v2","updated":"2024-10-16T07:07:20Z","published":"2024-10-08T00:37:16Z","title":"Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes\n  Fail to Improve?","summary":"  In the last decade, the generalization and adaptation abilities of deep\nlearning models were typically evaluated on fixed training and test\ndistributions. Contrary to traditional deep learning, large language models\n(LLMs) are (i) even more overparameterized, (ii) trained on unlabeled text\ncorpora curated from the Internet with minimal human intervention, and (iii)\ntrained in an online fashion. These stark contrasts prevent researchers from\ntransferring lessons learned on model generalization and adaptation in deep\nlearning contexts to LLMs. To this end, our short paper introduces empirical\nobservations that aim to shed light on further training of already pretrained\nlanguage models. Specifically, we demonstrate that training a model on a text\ndomain could degrade its perplexity on the test portion of the same domain. We\nobserve with our subsequent analysis that the performance degradation is\npositively correlated with the similarity between the additional and the\noriginal pretraining dataset of the LLM. Our further token-level perplexity\nobservations reveals that the perplexity degradation is due to a handful of\ntokens that are not informative about the domain. We hope these findings will\nguide us in determining when to adapt a model vs when to rely on its\nfoundational capabilities.\n","authors":["Fırat Öncel","Matthias Bethge","Beyza Ermis","Mirco Ravanelli","Cem Subakan","Çağatay Yıldız"],"pdf_url":"https://arxiv.org/pdf/2410.05581v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.12299v1","updated":"2024-10-16T06:58:49Z","published":"2024-10-16T06:58:49Z","title":"Semantics-Adaptive Activation Intervention for LLMs via Dynamic Steering\n  Vectors","summary":"  Large language models (LLMs) have achieved remarkable performance across many\ntasks, yet aligning them with desired behaviors remains challenging. Activation\nintervention has emerged as an effective and economical method to modify the\nbehavior of LLMs. Despite considerable interest in this area, current\nintervention methods exclusively employ a fixed steering vector to modify model\nactivations, lacking adaptability to diverse input semantics. To address this\nlimitation, we propose Semantics-Adaptive Dynamic Intervention (SADI), a novel\nmethod that constructs a dynamic steering vector to intervene model activations\nat inference time. More specifically, SADI utilizes activation differences in\ncontrastive pairs to precisely identify critical elements of an LLM (i.e.,\nattention heads, hidden states, and neurons) for targeted intervention. During\ninference, SADI dynamically steers model behavior by scaling element-wise\nactivations based on the directions of input semantics. Experimental results\nshow that SADI outperforms established baselines by substantial margins,\nimproving task performance without training. SADI's cost-effectiveness and\ngeneralizability across various LLM backbones and tasks highlight its potential\nas a versatile alignment technique. In addition, we release the code to foster\nresearch along this line:https://github.com/weixuan-wang123/SADI.\n","authors":["Weixuan Wang","Jingyuan Yang","Wei Peng"],"pdf_url":"https://arxiv.org/pdf/2410.12299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12298v1","updated":"2024-10-16T06:57:18Z","published":"2024-10-16T06:57:18Z","title":"Pyramid-Driven Alignment: Pyramid Principle Guided Integration of Large\n  Language Models and Knowledge Graphs","summary":"  Large Language Models (LLMs) possess impressive reasoning abilities but are\nprone to generating incorrect information, often referred to as hallucinations.\nWhile incorporating external Knowledge Graphs (KGs) can partially mitigate this\nissue, existing methods primarily treat KGs as static knowledge repositories,\noverlooking the critical disparity between KG and LLM knowledge, and failing to\nfully exploit the reasoning capabilities inherent in KGs. To address these\nlimitations, we propose Pyramid-Driven Alignment (PDA), a novel framework for\nseamlessly integrating LLMs with KGs. PDA utilizes Pyramid Principle analysis\nto construct a hierarchical pyramid structure. This structure is designed to\nreflect the input question and generate more validated deductive knowledge,\nthereby enhancing the alignment of LLMs and KGs and ensuring more cohesive\nintegration. Furthermore, PDA employs a recursive mechanism to harness the\nunderlying reasoning abilities of KGs, resulting in more accurate knowledge\nretrieval for question-answering tasks. Our experimental results reveal a\nsubstantial performance advantage of PDA over state-of-the-art baselines, with\nimprovements reaching 26.70% and 26.78%.\n","authors":["Lei Sun","Xinchen Wang","Youdi Li"],"pdf_url":"https://arxiv.org/pdf/2410.12298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12294v1","updated":"2024-10-16T06:51:09Z","published":"2024-10-16T06:51:09Z","title":"Towards LLM-based Cognitive Models of Students with Misconceptions","summary":"  Accurately modeling student cognition is crucial for developing effective\nAI-driven educational technologies. A key challenge is creating realistic\nstudent models that satisfy two essential properties: (1) accurately\nreplicating specific misconceptions, and (2) correctly solving problems where\nthese misconceptions are not applicable. This dual requirement reflects the\ncomplex nature of student understanding, where misconceptions coexist with\ncorrect knowledge. This paper investigates whether Large Language Models (LLMs)\ncan be instruction-tuned to meet this dual requirement and effectively simulate\nstudent thinking in algebra. We introduce MalAlgoPy, a novel Python library\nthat generates datasets reflecting authentic student solution patterns through\na graph-based representation of algebraic problem-solving. Utilizing MalAlgoPy,\nwe define and examine Cognitive Student Models (CSMs) - LLMs instruction tuned\nto faithfully emulate realistic student behavior. Our findings reveal that LLMs\ntrained on misconception examples can efficiently learn to replicate errors.\nHowever, the training diminishes the model's ability to solve problems\ncorrectly, particularly for problem types where the misconceptions are not\napplicable, thus failing to satisfy second property of CSMs. We demonstrate\nthat by carefully calibrating the ratio of correct to misconception examples in\nthe training data - sometimes as low as 0.25 - it is possible to develop CSMs\nthat satisfy both properties. Our insights enhance our understanding of\nAI-based student models and pave the way for effective adaptive learning\nsystems.\n","authors":["Shashank Sonkar","Xinghe Chen","Naiming Liu","Richard G. Baraniuk","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.12294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12292v1","updated":"2024-10-16T06:49:54Z","published":"2024-10-16T06:49:54Z","title":"How much do contextualized representations encode long-range context?","summary":"  We analyze contextual representations in neural autoregressive language\nmodels, emphasizing long-range contexts that span several thousand tokens. Our\nmethodology employs a perturbation setup and the metric\n\\emph{Anisotropy-Calibrated Cosine Similarity}, to capture the degree of\ncontextualization of long-range patterns from the perspective of representation\ngeometry. We begin the analysis with a case study on standard decoder-only\nTransformers, demonstrating that similar perplexity can exhibit markedly\ndifferent downstream task performance, which can be explained by the difference\nin contextualization of long-range content. Next, we extend the analysis to\nother models, covering recent novel architectural designs and various training\nconfigurations. The representation-level results illustrate a reduced capacity\nfor high-complexity (i.e., less compressible) sequences across architectures,\nand that fully recurrent models rely heavily on local context, whereas hybrid\nmodels more effectively encode the entire sequence structure. Finally,\npreliminary analysis of model size and training configurations on the encoding\nof long-range context suggest potential directions for improving existing\nlanguage models.\n","authors":["Simeng Sun","Cheng-Ping Hsieh"],"pdf_url":"https://arxiv.org/pdf/2410.12292v1.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.12288v1","updated":"2024-10-16T06:47:18Z","published":"2024-10-16T06:47:18Z","title":"A Prompt-Based Knowledge Graph Foundation Model for Universal In-Context\n  Reasoning","summary":"  Extensive knowledge graphs (KGs) have been constructed to facilitate\nknowledge-driven tasks across various scenarios. However, existing work usually\ndevelops separate reasoning models for different KGs, lacking the ability to\ngeneralize and transfer knowledge across diverse KGs and reasoning settings. In\nthis paper, we propose a prompt-based KG foundation model via in-context\nlearning, namely KG-ICL, to achieve a universal reasoning ability.\nSpecifically, we introduce a prompt graph centered with a query-related example\nfact as context to understand the query relation. To encode prompt graphs with\nthe generalization ability to unseen entities and relations in queries, we\nfirst propose a unified tokenizer that maps entities and relations in prompt\ngraphs to predefined tokens. Then, we propose two message passing neural\nnetworks to perform prompt encoding and KG reasoning, respectively. We conduct\nevaluation on 43 different KGs in both transductive and inductive settings.\nResults indicate that the proposed KG-ICL outperforms baselines on most\ndatasets, showcasing its outstanding generalization and universal reasoning\ncapabilities. The source code is accessible on GitHub:\nhttps://github.com/nju-websoft/KG-ICL.\n","authors":["Yuanning Cui","Zequn Sun","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2410.12288v1.pdf","comment":"Accepted in the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.12284v1","updated":"2024-10-16T06:43:02Z","published":"2024-10-16T06:43:02Z","title":"Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical\n  Decision-Support Setting","summary":"  The growing capabilities of AI models are leading to their wider use,\nincluding in safety-critical domains. Explainable AI (XAI) aims to make these\nmodels safer to use by making their inference process more transparent.\nHowever, current explainability methods are seldom evaluated in the way they\nare intended to be used: by real-world end users. To address this, we conducted\na large-scale user study with 85 healthcare practitioners in the context of\nhuman-AI collaborative chest X-ray analysis. We evaluated three types of\nexplanations: visual explanations (saliency maps), natural language\nexplanations, and a combination of both modalities. We specifically examined\nhow different explanation types influence users depending on whether the AI\nadvice and explanations are factually correct. We find that text-based\nexplanations lead to significant over-reliance, which is alleviated by\ncombining them with saliency maps. We also observe that the quality of\nexplanations, that is, how much factually correct information they entail, and\nhow much this aligns with AI correctness, significantly impacts the usefulness\nof the different explanation types.\n","authors":["Maxime Kayser","Bayar Menzat","Cornelius Emde","Bogdan Bercean","Alex Novak","Abdala Espinosa","Bartlomiej W. Papiez","Susanne Gaube","Thomas Lukasiewicz","Oana-Maria Camburu"],"pdf_url":"https://arxiv.org/pdf/2410.12284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12279v1","updated":"2024-10-16T06:35:56Z","published":"2024-10-16T06:35:56Z","title":"Beyond Oversmoothing: Evaluating DDPM and MSE for Scalable Speech\n  Synthesis in ASR","summary":"  Synthetically generated speech has rapidly approached human levels of\nnaturalness. However, the paradox remains that ASR systems, when trained on TTS\noutput that is judged as natural by humans, continue to perform badly on real\nspeech. In this work, we explore whether this phenomenon is due to the\noversmoothing behaviour of models commonly used in TTS, with a particular focus\non the behaviour of TTS-for-ASR as the amount of TTS training data is scaled\nup. We systematically compare Denoising Diffusion Probabilistic Models (DDPM)\nto Mean Squared Error (MSE) based models for TTS, when used for ASR model\ntraining. We test the scalability of the two approaches, varying both the\nnumber hours, and the number of different speakers. We find that for a given\nmodel size, DDPM can make better use of more data, and a more diverse set of\nspeakers, than MSE models. We achieve the best reported ratio between real and\nsynthetic speech WER to date (1.46), but also find that a large gap remains.\n","authors":["Christoph Minixhofer","Ondrej Klejch","Peter Bell"],"pdf_url":"https://arxiv.org/pdf/2410.12279v1.pdf","comment":"Under review at ICASSP 2025"},{"id":"http://arxiv.org/abs/2406.14036v2","updated":"2024-10-16T06:33:44Z","published":"2024-06-20T06:56:35Z","title":"Towards Infinite-Long Prefix in Transformer","summary":"  Prompting and context-based fine-tuning methods, which we call Prefix\nLearning, have been proposed to enhance the performance of language models on\nvarious downstream tasks. They are empirically efficient and effective,\nmatching the performance of full parameter fine-tuning, but the theoretical\nunderstandings are limited. In this paper, we aim to address this limitation by\nstudying their ability from the perspective of prefix length. In particular, we\nprovide a convergence guarantee for training an ultra-long prefix in a stylized\nsetting using the Neural Tangent Kernel (NTK) framework. Based on this strong\ntheoretical guarantee, we design and implement an algorithm that only needs to\nintroduce and fine-tune a few extra trainable parameters instead of an\ninfinite-long prefix in each layer of a transformer, and can approximate the\nprefix attention to a guaranteed polynomial-small error. Preliminary\nexperimental results on vision, natural language, and math data show that our\nmethod achieves superior or competitive performance compared to existing\nmethods like full parameters fine-tuning, P-Tuning V2, and LoRA. This\ndemonstrates our method is promising for parameter-efficient fine-tuning. Our\ncode can be found at\n\\url{https://github.com/ChristianYang37/chiwun/tree/main/src/NTK-Attention}.\n","authors":["Yingyu Liang","Zhenmei Shi","Zhao Song","Chiwun Yang"],"pdf_url":"https://arxiv.org/pdf/2406.14036v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11258v2","updated":"2024-10-16T06:32:50Z","published":"2024-06-17T06:48:31Z","title":"SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented\n  Generation","summary":"  Large Language Models (LLMs) have shown great potential in the biomedical\ndomain with the advancement of retrieval-augmented generation (RAG). However,\nexisting retrieval-augmented approaches face challenges in addressing diverse\nqueries and documents, particularly for medical knowledge queries, resulting in\nsub-optimal performance. To address these limitations, we propose a novel\nplug-and-play LLM-based retrieval method called Self-Rewarding Tree Search\n(SeRTS) based on Monte Carlo Tree Search (MCTS) and a self-rewarding paradigm.\nBy combining the reasoning capabilities of LLMs with the effectiveness of tree\nsearch, SeRTS boosts the zero-shot performance of retrieving high-quality and\ninformative results for RAG. We further enhance retrieval performance by\nfine-tuning LLMs with Proximal Policy Optimization (PPO) objectives using the\ntrajectories collected by SeRTS as feedback. Controlled experiments using the\nBioASQ-QA dataset with GPT-3.5-Turbo and LLama2-7b demonstrate that our method\nsignificantly improves the performance of the BM25 retriever and surpasses the\nstrong baseline of self-reflection in both efficiency and scalability.\nMoreover, SeRTS generates higher-quality feedback for PPO training than\nself-reflection. Our proposed method effectively adapts LLMs to document\nretrieval tasks, enhancing their ability to retrieve highly relevant documents\nfor RAG in the context of medical knowledge queries. This work presents a\nsignificant step forward in leveraging LLMs for accurate and comprehensive\nbiomedical question answering.\n","authors":["Minda Hu","Licheng Zong","Hongru Wang","Jingyan Zhou","Jingjing Li","Yichen Gao","Kam-Fai Wong","Yu Li","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2406.11258v2.pdf","comment":"This work has been accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.12278v1","updated":"2024-10-16T06:31:59Z","published":"2024-10-16T06:31:59Z","title":"Controlled Automatic Task-Specific Synthetic Data Generation for\n  Hallucination Detection","summary":"  We present a novel approach to automatically generate non-trivial\ntask-specific synthetic datasets for hallucination detection. Our approach\nfeatures a two-step generation-selection pipeline, using hallucination pattern\nguidance and a language style alignment during generation. Hallucination\npattern guidance leverages the most important task-specific hallucination\npatterns while language style alignment aligns the style of the synthetic\ndataset with benchmark text. To obtain robust supervised detectors from\nsynthetic datasets, we also adopt a data mixture strategy to improve\nperformance robustness and generalization. Our results on three datasets show\nthat our generated hallucination text is more closely aligned with\nnon-hallucinated text versus baselines, to train hallucination detectors with\nbetter generalization. Our hallucination detectors trained on synthetic\ndatasets outperform in-context-learning (ICL)-based detectors by a large margin\nof 32%. Our extensive experiments confirm the benefits of our approach with\ncross-task and cross-generator generalization. Our data-mixture-based training\nfurther improves the generalization and robustness of hallucination detection.\n","authors":["Yong Xie","Karan Aggarwal","Aitzaz Ahmad","Stephen Lau"],"pdf_url":"https://arxiv.org/pdf/2410.12278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11291v2","updated":"2024-10-16T06:25:57Z","published":"2024-10-15T05:26:57Z","title":"Enhancing Assamese NLP Capabilities: Introducing a Centralized Dataset\n  Repository","summary":"  This paper introduces a centralized, open-source dataset repository designed\nto advance NLP and NMT for Assamese, a low-resource language. The repository,\navailable at GitHub, supports various tasks like sentiment analysis, named\nentity recognition, and machine translation by providing both pre-training and\nfine-tuning corpora. We review existing datasets, highlighting the need for\nstandardized resources in Assamese NLP, and discuss potential applications in\nAI-driven research, such as LLMs, OCR, and chatbots. While promising,\nchallenges like data scarcity and linguistic diversity remain. The repository\naims to foster collaboration and innovation, promoting Assamese language\nresearch in the digital age.\n","authors":["S. Tamang","D. J. Bora"],"pdf_url":"https://arxiv.org/pdf/2410.11291v2.pdf","comment":"6 pages, 1 table, 1 figure"},{"id":"http://arxiv.org/abs/2407.12508v2","updated":"2024-10-16T06:25:50Z","published":"2024-07-17T11:45:02Z","title":"MERLIN: Multimodal Embedding Refinement via LLM-based Iterative\n  Navigation for Text-Video Retrieval-Rerank Pipeline","summary":"  The rapid expansion of multimedia content has made accurately retrieving\nrelevant videos from large collections increasingly challenging. Recent\nadvancements in text-video retrieval have focused on cross-modal interactions,\nlarge-scale foundation model training, and probabilistic modeling, yet often\nneglect the crucial user perspective, leading to discrepancies between user\nqueries and the content retrieved. To address this, we introduce MERLIN\n(Multimodal Embedding Refinement via LLM-based Iterative Navigation), a novel,\ntraining-free pipeline that leverages Large Language Models (LLMs) for\niterative feedback learning. MERLIN refines query embeddings from a user\nperspective, enhancing alignment between queries and video content through a\ndynamic question answering process. Experimental results on datasets like\nMSR-VTT, MSVD, and ActivityNet demonstrate that MERLIN substantially improves\nRecall@1, outperforming existing systems and confirming the benefits of\nintegrating LLMs into multimodal retrieval systems for more responsive and\ncontext-aware multimedia retrieval.\n","authors":["Donghoon Han","Eunhwan Park","Gisang Lee","Adam Lee","Nojun Kwak"],"pdf_url":"https://arxiv.org/pdf/2407.12508v2.pdf","comment":"EMNLP 2024 Industry Track Accepted (Camera-Ready Version)"},{"id":"http://arxiv.org/abs/2410.12271v1","updated":"2024-10-16T06:16:30Z","published":"2024-10-16T06:16:30Z","title":"Kallini et al. (2024) do not compare impossible languages with\n  constituency-based ones","summary":"  A central goal of linguistic theory is to find a precise characterization of\nthe notion \"possible human language\", in the form of a computational device\nthat is capable of describing all and only the languages that can be acquired\nby a typically developing human child. The success of recent large language\nmodels (LLMs) in NLP applications arguably raises the possibility that LLMs\nmight be computational devices that meet this goal. This would only be the case\nif, in addition to succeeding in learning human languages, LLMs struggle to\nlearn \"impossible\" human languages. Kallini et al. (2024; \"Mission: Impossible\nLanguage Models\", Proc. ACL) conducted experiments aiming to test this by\ntraining GPT-2 on a variety of synthetic languages, and found that it learns\nsome more successfully than others. They present these asymmetries as support\nfor the idea that LLMs' inductive biases align with what is regarded as\n\"possible\" for human languages, but the most significant comparison has a\nconfound that makes this conclusion unwarranted. In this paper I explain the\nconfound and suggest some ways forward towards constructing a comparison that\nappropriately tests the underlying issue.\n","authors":["Tim Hunter"],"pdf_url":"https://arxiv.org/pdf/2410.12271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04070v3","updated":"2024-10-16T06:15:35Z","published":"2024-10-05T08:00:55Z","title":"PAD: Personalized Alignment at Decoding-Time","summary":"  Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment.\n","authors":["Ruizhe Chen","Xiaotian Zhang","Meng Luo","Wenhao Chai","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2410.04070v3.pdf","comment":"This paper presents Personalized Alignment at Decoding-time (PAD), a\n  novel framework designed to align LLM outputs with diverse personalized\n  preferences during the inference phase"},{"id":"http://arxiv.org/abs/2410.12265v1","updated":"2024-10-16T06:06:06Z","published":"2024-10-16T06:06:06Z","title":"An Automatic and Cost-Efficient Peer-Review Framework for Language\n  Generation Evaluation","summary":"  With the rapid development of large language models (LLMs), how to\nefficiently evaluate them has become an important research question. Existing\nevaluation methods often suffer from high costs, limited test formats, the need\nof human references, and systematic evaluation biases. To address these\nlimitations, our study introduces the Auto-PRE, an automatic LLM evaluation\nframework based on peer review. In contrast to previous studies that rely on\nhuman annotations, Auto-PRE selects evaluator LLMs automatically based on their\ninherent traits including consistency, self-confidence, and pertinence. We\nconduct extensive experiments on three tasks: summary generation, non-factoid\nquestion-answering, and dialogue generation. Experimental results indicate our\nAuto-PRE achieves state-of-the-art performance at a lower cost. Moreover, our\nstudy highlights the impact of prompt strategies and evaluation formats on\nevaluation performance, offering guidance for method optimization in the\nfuture.\n","authors":["Junjie Chen","Weihang Su","Zhumin Chu","Haitao Li","Qinyao Ai","Yiqun Liu","Min Zhang","Shaoping Ma"],"pdf_url":"https://arxiv.org/pdf/2410.12265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10148v2","updated":"2024-10-16T05:59:33Z","published":"2024-10-14T04:29:57Z","title":"$α$-DPO: Adaptive Reward Margin is What Direct Preference\n  Optimization Needs","summary":"  Aligning large language models (LLMs) with human values and intentions is\ncrucial for their utility, honesty, and safety. Reinforcement learning from\nhuman feedback (RLHF) is a popular approach to achieve this alignment, but it\nfaces challenges in computational efficiency and training stability. Recent\nmethods like Direct Preference Optimization (DPO) and Simple Preference\nOptimization (SimPO) have proposed offline alternatives to RLHF, simplifying\nthe process by reparameterizing the reward function. However, DPO depends on a\npotentially suboptimal reference model, and SimPO's assumption of a fixed\ntarget reward margin may lead to suboptimal decisions in diverse data settings.\nIn this work, we propose $\\alpha$-DPO, an adaptive preference optimization\nalgorithm designed to address these limitations by introducing a dynamic reward\nmargin. Specifically, $\\alpha$-DPO employs an adaptive preference distribution,\nbalancing the policy model and the reference model to achieve personalized\nreward margins. We provide theoretical guarantees for $\\alpha$-DPO,\ndemonstrating its effectiveness as a surrogate optimization objective and its\nability to balance alignment and diversity through KL divergence control.\nEmpirical evaluations on AlpacaEval 2 and Arena-Hard show that $\\alpha$-DPO\nconsistently outperforms DPO and SimPO across various model settings,\nestablishing it as a robust approach for fine-tuning LLMs. Our method achieves\nsignificant improvements in win rates, highlighting its potential as a powerful\ntool for LLM alignment. The code is available at\nhttps://github.com/junkangwu/alpha-DPO\n","authors":["Junkang Wu","Xue Wang","Zhengyi Yang","Jiancan Wu","Jinyang Gao","Bolin Ding","Xiang Wang","Rong Jin","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2410.10148v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10792v6","updated":"2024-10-16T05:44:07Z","published":"2023-08-21T15:35:16Z","title":"Instruction Tuning for Large Language Models: A Survey","summary":"  This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), a crucial technique to enhance the capabilities and\ncontrollability of large language models (LLMs). Instruction tuning refers to\nthe process of further training LLMs on a dataset consisting of\n\\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the\ngap between the next-word prediction objective of LLMs and the users' objective\nof having LLMs adhere to human instructions. In this work, we make a systematic\nreview of the literature, including the general methodology of IT, the\nconstruction of IT datasets, the training of IT models, and applications to\ndifferent modalities, domains and applications, along with an analysis on\naspects that influence the outcome of IT (e.g., generation of instruction\noutputs, size of the instruction dataset, etc). We also review the potential\npitfalls of IT along with criticism against it, along with efforts pointing out\ncurrent deficiencies of existing strategies and suggest some avenues for\nfruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey\n","authors":["Shengyu Zhang","Linfeng Dong","Xiaoya Li","Sen Zhang","Xiaofei Sun","Shuhe Wang","Jiwei Li","Runyi Hu","Tianwei Zhang","Fei Wu","Guoyin Wang"],"pdf_url":"https://arxiv.org/pdf/2308.10792v6.pdf","comment":"V3; Last update: Oct 16, 2024"},{"id":"http://arxiv.org/abs/2406.11632v2","updated":"2024-10-16T05:22:53Z","published":"2024-06-17T15:13:52Z","title":"Unveiling the Power of Source: Source-based Minimum Bayes Risk Decoding\n  for Neural Machine Translation","summary":"  Maximum a posteriori decoding, a commonly used method for neural machine\ntranslation (NMT), aims to maximize the estimated posterior probability.\nHowever, high estimated probability does not always lead to high translation\nquality. Minimum Bayes Risk (MBR) decoding (\\citealp{kumar2004minimum}) offers\nan alternative by seeking hypotheses with the highest expected utility. In this\npaper, we show that Quality Estimation (QE) reranking\n(\\citealp{fernandes-etal-2022-quality}), which uses a QE model as a reranker,\ncan be viewed as a variant of MBR. Inspired by this, we propose source-based\nMBR (sMBR) decoding, a novel approach that utilizes synthetic sources\n(generated via back-translation or paraphrasing) as ``support hypotheses'' and\na reference-free quality estimation metric as the utility function, marking the\nfirst work to solely use sources in MBR decoding. Experiments show that sMBR\noutperforms QE reranking and the standard MBR decoding. Our findings suggest\nthat sMBR is a promising approach for NMT decoding.\n","authors":["Boxuan Lyu","Hidetaka Kamigaito","Kotaro Funakoshi","Manabu Okumura"],"pdf_url":"https://arxiv.org/pdf/2406.11632v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12248v1","updated":"2024-10-16T05:20:32Z","published":"2024-10-16T05:20:32Z","title":"CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for\n  Retrieval-Augmented Generation with Enhanced Data Diversity","summary":"  Retrieval-Augmented Generation (RAG) aims to enhance large language models\n(LLMs) to generate more accurate and reliable answers with the help of the\nretrieved context from external knowledge sources, thereby reducing the\nincidence of hallucinations. Despite the advancements, evaluating these systems\nremains a crucial research area due to the following issues: (1) Limited data\ndiversity: The insufficient diversity of knowledge sources and query types\nconstrains the applicability of RAG systems; (2) Obscure problems location:\nExisting evaluation methods have difficulty in locating the stage of the RAG\npipeline where problems occur; (3) Unstable retrieval evaluation: These methods\noften fail to effectively assess retrieval performance, particularly when the\nchunking strategy changes. To tackle these challenges, we propose a\nComprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough\nevaluation across the entire RAG pipeline, including chunking, retrieval,\nreranking, and generation. To effectively evaluate the first three phases, we\nintroduce multi-granularity keywords, including coarse-grained and fine-grained\nkeywords, to assess the retrieved context instead of relying on the annotation\nof golden chunks. Moreover, we release a holistic benchmark dataset tailored\nfor diverse data scenarios covering a wide range of document formats and query\ntypes. We demonstrate the utility of the CoFE-RAG framework by conducting\nexperiments to evaluate each stage of RAG systems. Our evaluation method\nprovides unique insights into the effectiveness of RAG systems in handling\ndiverse data scenarios, offering a more nuanced understanding of their\ncapabilities and limitations.\n","authors":["Jintao Liu","Ruixue Ding","Linhao Zhang","Pengjun Xie","Fie Huang"],"pdf_url":"https://arxiv.org/pdf/2410.12248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12247v1","updated":"2024-10-16T05:17:49Z","published":"2024-10-16T05:17:49Z","title":"EPS-MoE: Expert Pipeline Scheduler for Cost-Efficient MoE Inference","summary":"  Large Language Model (LLM) has revolutionized the field of artificial\nintelligence, with their capabilities expanding rapidly due to advances in deep\nlearning and increased computational resources. The mixture-of-experts (MoE)\nmodel has emerged as a prominent architecture in the field of LLM, better\nbalancing the model performance and computational efficiency. MoE architecture\nallows for effective scaling and efficient parallel processing, but the GEMM\n(General Matrix Multiply) of MoE and the large parameters introduce challenges\nin terms of computation efficiency and communication overhead, which becomes\nthe throughput bottleneck during inference. Applying a single parallelism\nstrategy like EP, DP, PP, etc. to MoE architecture usually achieves sub-optimal\ninference throughput, the straightforward combinations of existing different\nparallelisms on MoE can not obtain optimal inference throughput yet. This paper\nintroduces EPS-MoE, a novel expert pipeline scheduler for MoE that goes beyond\nthe existing inference parallelism schemes. Our approach focuses on optimizing\nthe computation of MoE FFN (FeedForward Network) modules by dynamically\nselecting the best kernel implementation of GroupGemm and DenseGemm for\ndifferent loads and adaptively overlapping these computations with\n\\textit{all2all} communication, leading to a substantial increase in\nthroughput. Our experimental results demonstrate an average 21% improvement in\nprefill throughput over existing parallel inference methods. Specifically, we\nvalidated our method on DeepSeekV2, a highly optimized model claimed to achieve\na prefill throughput of 100K tokens per second. By applying EPS-MoE, we further\naccelerated it to at least 120K tokens per second.\n","authors":["Yulei Qian","Fengcun Li","Xiangyang Ji","Xiaoyu Zhao","Jianchao Tan","Kefeng Zhang","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2410.12247v1.pdf","comment":"13 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.06949v2","updated":"2024-10-16T05:04:45Z","published":"2024-10-09T14:45:45Z","title":"Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent\n  Approach","summary":"  In real world software development, improper or missing exception handling\ncan severely impact the robustness and reliability of code. Exception handling\nmechanisms require developers to detect, capture, and manage exceptions\naccording to high standards, but many developers struggle with these tasks,\nleading to fragile code. This problem is particularly evident in open source\nprojects and impacts the overall quality of the software ecosystem. To address\nthis challenge, we explore the use of large language models (LLMs) to improve\nexception handling in code. Through extensive analysis, we identify three key\nissues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception\nTypes, and Distorted Handling Solutions. These problems are widespread across\nreal world repositories, suggesting that robust exception handling practices\nare often overlooked or mishandled. In response, we propose Seeker, a multi\nagent framework inspired by expert developer strategies for exception handling.\nSeeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist\nLLMs in detecting, capturing, and resolving exceptions more effectively. Our\nwork is the first systematic study on leveraging LLMs to enhance exception\nhandling practices, providing valuable insights for future improvements in code\nreliability.\n","authors":["Xuanming Zhang","Yuxuan Chen","Yuan Yuan","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2410.06949v2.pdf","comment":"26 pages, 7 figures. Submitted ICLR 2025"},{"id":"http://arxiv.org/abs/2406.14867v2","updated":"2024-10-16T05:03:04Z","published":"2024-06-21T05:05:39Z","title":"Investigating the Transferability of Code Repair for Low-Resource\n  Programming Languages","summary":"  Large language models (LLMs) have shown remarkable performance on code\ngeneration tasks. A recent use case is iterative code repair, where an LLM\nfixes an incorrect program by rationalizing about errors and generating new\ncode. Recent works augment the code repair process by integrating modern\ntechniques such as chain-of-thought reasoning or distillation, but only study\ntheir benefits on high-resource languages like Python, and ignore low-resource\nlanguages like Perl. To address this gap of knowledge, we investigate the\nbenefits of distilling code repair for both high and low resource languages to\ndetermine if the techniques that are effective in a high resource setting are\nalso applicable in a low resource setting. Our evaluation shows that distilling\nthe ability to repair code has language dependent benefits. To explain this\nbehavior, we perform a further analysis and find that contrary to preexisting\nbeliefs, the correlation between reasoning ability and code correction ability\nis weak. We hypothesize this weak correlation is magnified in low-resource\nsettings where base models lack deep knowledge of a programming language,\nleading to wavering benefits of code repair.\n","authors":["Kyle Wong","Alfonso Amayuelas","Liangming Pan","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2406.14867v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06554v2","updated":"2024-10-16T04:48:08Z","published":"2024-10-09T05:17:08Z","title":"The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield\n  Better Language Models","summary":"  Reinforcement Learning from Human Feedback significantly enhances Natural\nLanguage Processing by aligning language models with human expectations. A\ncritical factor in this alignment is the strength of reward models used during\ntraining. This study explores whether stronger reward models invariably lead to\nbetter language models. In this paper, through experiments on relevance,\nfactuality, and completeness tasks using the QA-FEEDBACK dataset and reward\nmodels based on Longformer, we uncover a surprising paradox: language models\ntrained with moderately accurate reward models outperform those guided by\nhighly accurate ones. This challenges the widely held belief that stronger\nreward models always lead to better language models, and opens up new avenues\nfor future research into the key factors driving model performance and how to\nchoose the most suitable reward models. Code and additional details are\navailable at https://github.com/EIT-NLP/AccuracyParadox-RLHF.\n","authors":["Yanjun Chen","Dawei Zhu","Yirong Sun","Xinghao Chen","Wei Zhang","Xiaoyu Shen"],"pdf_url":"https://arxiv.org/pdf/2410.06554v2.pdf","comment":"10 pages, 27 figures (including 18 in the appendix), submitted to\n  EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.12228v1","updated":"2024-10-16T04:44:15Z","published":"2024-10-16T04:44:15Z","title":"Triple Modality Fusion: Aligning Visual, Textual, and Graph Data with\n  Large Language Models for Multi-Behavior Recommendations","summary":"  Integrating diverse data modalities is crucial for enhancing the performance\nof personalized recommendation systems. Traditional models, which often rely on\nsingular data sources, lack the depth needed to accurately capture the\nmultifaceted nature of item features and user behaviors. This paper introduces\na novel framework for multi-behavior recommendations, leveraging the fusion of\ntriple-modality, which is visual, textual, and graph data through alignment\nwith large language models (LLMs). By incorporating visual information, we\ncapture contextual and aesthetic item characteristics; textual data provides\ninsights into user interests and item features in detail; and graph data\nelucidates relationships within the item-behavior heterogeneous graphs. Our\nproposed model called Triple Modality Fusion (TMF) utilizes the power of LLMs\nto align and integrate these three modalities, achieving a comprehensive\nrepresentation of user behaviors. The LLM models the user's interactions\nincluding behaviors and item features in natural languages. Initially, the LLM\nis warmed up using only natural language-based prompts. We then devise the\nmodality fusion module based on cross-attention and self-attention mechanisms\nto integrate different modalities from other models into the same embedding\nspace and incorporate them into an LLM. Extensive experiments demonstrate the\neffectiveness of our approach in improving recommendation accuracy. Further\nablation studies validate the effectiveness of our model design and benefits of\nthe TMF.\n","authors":["Luyi Ma","Xiaohan Li","Zezhong Fan","Jianpeng Xu","Jason Cho","Praveen Kanumala","Kaushiki Nag","Sushant Kumar","Kannan Achan"],"pdf_url":"https://arxiv.org/pdf/2410.12228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12222v1","updated":"2024-10-16T04:36:17Z","published":"2024-10-16T04:36:17Z","title":"On A Scale From 1 to 5: Quantifying Hallucination in Faithfulness\n  Evaluation","summary":"  Hallucination has been a popular topic in natural language generation (NLG).\nIn real-world applications, unfaithful content can result in bad data quality\nor loss of trust from end users. Thus, it is crucial to fact-check before\nadopting NLG for production usage, which can be expensive if done manually. In\nthis paper, we investigate automated faithfulness evaluation in guided NLG. We\ndeveloped a rubrics template and use large language models (LLMs) to score the\ngeneration into quantifiable scales. We compared popular LLMs as well as the\nwidely adopted natural language inference (NLI) models in scoring quality and\nsensitivity. In addition, we developed methods to generation synthetic\nunfaithful data, as well as a heuristics to quantify the percentage of\nhallucination. Our results on 4 travel-domain industry dataset show that GPT-4\ncan provide accurate judgement and explanation on whether a source and a\ngeneration are factually consistent. Furthermore, we found that tuning NLI\nmodels on synthetic data can improve performance. Lastly, we present insights\non latency and cost for deploying such system.\n","authors":["Xiaonan Jing","Srinivas Billa","Danny Godbout"],"pdf_url":"https://arxiv.org/pdf/2410.12222v1.pdf","comment":"14 pages, 13 figures"},{"id":"http://arxiv.org/abs/2406.17232v2","updated":"2024-10-16T04:36:09Z","published":"2024-06-25T02:37:29Z","title":"Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human\n  Belief Networks","summary":"  Creating human-like large language model (LLM) agents is crucial for faithful\nsocial simulation. Having LLMs role-play based on demographic information\nsometimes improves human likeness but often does not. This study assessed\nwhether LLM alignment with human behavior can be improved by integrating\ninformation from empirically-derived human belief networks. Using data from a\nhuman survey, we estimated a belief network encompassing 64 topics loading on\nnine non-overlapping latent factors. We then seeded LLM-based agents with an\nopinion on one topic, and assessed the alignment of its expressed opinions on\nremaining test topics with corresponding human data. Role-playing based on\ndemographic information alone did not align LLM and human opinions, but seeding\nthe agent with a single belief greatly improved alignment for topics related in\nthe belief network, and not for topics outside the network. These results\nsuggest a novel path for human-LLM belief alignment in work seeking to simulate\nand understand patterns of belief distributions in society.\n","authors":["Yun-Shiuan Chuang","Krirk Nirunwiroj","Zach Studdiford","Agam Goyal","Vincent V. Frigo","Sijia Yang","Dhavan Shah","Junjie Hu","Timothy T. Rogers"],"pdf_url":"https://arxiv.org/pdf/2406.17232v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12219v1","updated":"2024-10-16T04:29:46Z","published":"2024-10-16T04:29:46Z","title":"OmnixR: Evaluating Omni-modality Language Models on Reasoning across\n  Modalities","summary":"  We introduce OmnixR, an evaluation suite designed to benchmark SoTA\nOmni-modality Language Models, such as GPT-4o and Gemini. Evaluating OLMs,\nwhich integrate multiple modalities such as text, vision, and audio, presents\nunique challenges. Particularly, the user message might often consist of\nmultiple modalities, such that OLMs have to establish holistic understanding\nand reasoning across modalities to accomplish the task. Existing benchmarks are\nlimited to single modality or dual-modality tasks, overlooking comprehensive\nmulti-modal assessments of model reasoning. To address this, OmnixR offers two\nevaluation variants: (1)synthetic subset: a synthetic dataset generated\nautomatically by translating text into multiple modalities--audio, images,\nvideo, and hybrids (Omnify). (2)realistic subset: a real-world dataset,\nmanually curated and annotated by experts, for evaluating cross-modal reasoning\nin natural settings. OmnixR presents a unique evaluation towards assessing OLMs\nover a diverse mix of modalities, such as a question that involves video,\naudio, and text, providing a rigorous cross-modal reasoning testbed unlike any\nexisting benchmarks. Our experiments find that all state-of-the-art OLMs\nstruggle with OmnixR questions that require integrating information from\nmultiple modalities to answer. Further analysis highlights differences in\nreasoning behavior, underscoring the challenges of omni-modal AI alignment.\n","authors":["Lichang Chen","Hexiang Hu","Mingda Zhang","Yiwen Chen","Zifeng Wang","Yandong Li","Pranav Shyam","Tianyi Zhou","Heng Huang","Ming-Hsuan Yang","Boqing Gong"],"pdf_url":"https://arxiv.org/pdf/2410.12219v1.pdf","comment":"19 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2410.12217v1","updated":"2024-10-16T04:26:40Z","published":"2024-10-16T04:26:40Z","title":"Accurate and Data-Efficient Toxicity Prediction when Annotators Disagree","summary":"  When annotators disagree, predicting the labels given by individual\nannotators can capture nuances overlooked by traditional label aggregation. We\nintroduce three approaches to predicting individual annotator ratings on the\ntoxicity of text by incorporating individual annotator-specific information: a\nneural collaborative filtering (NCF) approach, an in-context learning (ICL)\napproach, and an intermediate embedding-based architecture. We also study the\nutility of demographic information for rating prediction. NCF showed limited\nutility; however, integrating annotator history, demographics, and survey\ninformation permits both the embedding-based architecture and ICL to\nsubstantially improve prediction accuracy, with the embedding-based\narchitecture outperforming the other methods. We also find that, if\ndemographics are predicted from survey information, using these imputed\ndemographics as features performs comparably to using true demographic data.\nThis suggests that demographics may not provide substantial information for\nmodeling ratings beyond what is captured in survey responses. Our findings\nraise considerations about the relative utility of different types of annotator\ninformation and provide new approaches for modeling annotators in subjective\nNLP tasks.\n","authors":["Harbani Jaggi","Kashyap Murali","Eve Fleisig","Erdem Bıyık"],"pdf_url":"https://arxiv.org/pdf/2410.12217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.20246v5","updated":"2024-10-16T04:26:04Z","published":"2023-10-31T08:09:20Z","title":"Breaking Language Barriers in Multilingual Mathematical Reasoning:\n  Insights and Observations","summary":"  Existing research predominantly focuses on developing powerful language\nlearning models (LLMs) for mathematical reasoning within monolingual languages,\nwith few explorations in preserving efficacy in a multilingual context. To\nbridge this gap, this paper pioneers exploring and training powerful\nMultilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we\nconstruct the first multilingual math reasoning instruction dataset,\nMGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue\nof training data scarcity in xMR tasks. Based on the collected dataset, we\npropose different training strategies to build powerful xMR LLMs, named\nMathOctopus, notably outperform conventional open-source LLMs and exhibit\nsuperiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B\nreaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond\nremarkable results, we unearth several pivotal observations and insights from\nextensive experiments: (1) When extending the rejection sampling strategy to\nthe multilingual context, it proves effective for model performances, albeit\nlimited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT)\nacross multiple languages not only significantly enhances model performance\nmultilingually but also elevates their monolingual performance. This indicates\nthat crafting multilingual corpora can be regarded as a vital strategy for\nenhancing model performance in a specific language, especially in mathematical\nreasoning tasks. For instance, MathOctopus-7B improves its counterparts that\ntrained on English from 42.2% to 50.8% on GSM8K testset. Codes are available at\nhttps://github.com/microsoft/MathOctopus.\n","authors":["Nuo Chen","Zinan Zheng","Ning Wu","Ming Gong","Dongmei Zhang","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2310.20246v5.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2407.17487v3","updated":"2024-10-16T04:24:59Z","published":"2024-07-03T08:27:51Z","title":"Explainable Natural Language Processing for Corporate Sustainability\n  Analysis","summary":"  Sustainability commonly refers to entities, such as individuals, companies,\nand institutions, having a non-detrimental (or even positive) impact on the\nenvironment, society, and the economy. With sustainability becoming a synonym\nof acceptable and legitimate behaviour, it is being increasingly demanded and\nregulated. Several frameworks and standards have been proposed to measure the\nsustainability impact of corporations, including United Nations' sustainable\ndevelopment goals and the recently introduced global sustainability reporting\nframework, amongst others. However, the concept of corporate sustainability is\ncomplex due to the diverse and intricate nature of firm operations (i.e.\ngeography, size, business activities, interlinks with other stakeholders). As a\nresult, corporate sustainability assessments are plagued by subjectivity both\nwithin data that reflect corporate sustainability efforts (i.e. corporate\nsustainability disclosures) and the analysts evaluating them. This subjectivity\ncan be distilled into distinct challenges, such as incompleteness, ambiguity,\nunreliability and sophistication on the data dimension, as well as limited\nresources and potential bias on the analyst dimension. Put together,\nsubjectivity hinders effective cost attribution to entities non-compliant with\nprevailing sustainability expectations, potentially rendering sustainability\nefforts and its associated regulations futile. To this end, we argue that\nExplainable Natural Language Processing (XNLP) can significantly enhance\ncorporate sustainability analysis. Specifically, linguistic understanding\nalgorithms (lexical, semantic, syntactic), integrated with XAI capabilities\n(interpretability, explainability, faithfulness), can bridge gaps in analyst\nresources and mitigate subjectivity problems within data.\n","authors":["Keane Ong","Rui Mao","Ranjan Satapathy","Ricardo Shirota Filho","Erik Cambria","Johan Sulaeman","Gianmarco Mengaldo"],"pdf_url":"https://arxiv.org/pdf/2407.17487v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11086v2","updated":"2024-10-16T04:23:12Z","published":"2024-10-14T20:59:59Z","title":"JOOCI: a Framework for Learning Comprehensive Speech Representations","summary":"  Information in speech can be divided into two categories: what is being said\n(content) and how it is expressed (other). Current state-of-the-art (SOTA)\ntechniques model speech at fixed segments, usually 10-25 ms, using a single\nembedding. Given the orthogonal nature of other and content information,\nattempting to optimize both within a single embedding results in suboptimal\nsolutions. This approach divides the models capacity, limiting its ability to\nbuild complex hierarchical features effectively. In this work, we present an\nend-to-end speech representation learning framework designed to jointly\noptimize the other and content information (JOOCI) in speech. By using separate\nlearnable parameters, JOOCI addresses this optimization challenge by modeling\nother and content information independently. Our results show that JOOCI\nconsistently outperforms other SOTA models of similar size (100 million\nparameters) and pre-training data used (960 hours) by a significant margin when\nevaluated on a range of speech downstream tasks in the SUPERB benchmark, as\nshown in Table 1.\n","authors":["Hemant Yadav","Rajiv Ratn Shah","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2410.11086v2.pdf","comment":"Submitted to ICLR 2025"},{"id":"http://arxiv.org/abs/2410.10861v2","updated":"2024-10-16T03:53:40Z","published":"2024-10-07T16:54:18Z","title":"Translation Canvas: An Explainable Interface to Pinpoint and Analyze\n  Translation Systems","summary":"  With the rapid advancement of machine translation research, evaluation\ntoolkits have become essential for benchmarking system progress. Tools like\nCOMET and SacreBLEU offer single quality score assessments that are effective\nfor pairwise system comparisons. However, these tools provide limited insights\nfor fine-grained system-level comparisons and the analysis of instance-level\ndefects. To address these limitations, we introduce Translation Canvas, an\nexplainable interface designed to pinpoint and analyze translation systems'\nperformance: 1) Translation Canvas assists machine translation researchers in\ncomprehending system-level model performance by identifying common errors\n(their frequency and severity) and analyzing relationships between different\nsystems based on various evaluation metrics. 2) It supports fine-grained\nanalysis by highlighting error spans with explanations and selectively\ndisplaying systems' predictions. According to human evaluation, Translation\nCanvas demonstrates superior performance over COMET and SacreBLEU packages\nunder enjoyability and understandability criteria.\n","authors":["Chinmay Dandekar","Wenda Xu","Xi Xu","Siqi Ouyang","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2410.10861v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.12997v2","updated":"2024-10-16T03:52:13Z","published":"2024-06-18T18:37:24Z","title":"Discovering Elementary Discourse Units in Textual Data Using Canonical\n  Correlation Analysis","summary":"  Canonical Correlation Analysis (CCA) has been exploited immensely for\nlearning latent representations in various fields. This study takes a step\nfurther by demonstrating the potential of CCA in identifying Elementary\nDiscourse Units(EDUs) that captures the latent information within the textual\ndata. The probabilistic interpretation of CCA discussed in this study utilizes\nthe two-view nature of textual data, i.e. the consecutive sentences in a\ndocument or turns in a dyadic conversation, and has a strong theoretical\nfoundation. Furthermore, this study proposes a model for Elementary Discourse\nUnit(EDU) segmentation that discovers EDUs in textual data without any\nsupervision. To validate the model, the EDUs are utilized as textual unit for\ncontent selection in textual similarity task. Empirical results on Semantic\nTextual Similarity(STSB) and Mohler datasets confirm that, despite represented\nas a unigram, the EDUs deliver competitive results and can even beat various\nsophisticated supervised techniques. The model is simple, linear, adaptable and\nlanguage independent making it an ideal baseline particularly when labeled\ntraining data is scarce or nonexistent.\n","authors":["Akanksha Mehndiratta","Krishna Asawa"],"pdf_url":"https://arxiv.org/pdf/2406.12997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17969v2","updated":"2024-10-16T03:35:22Z","published":"2024-05-28T08:56:33Z","title":"Knowledge Circuits in Pretrained Transformers","summary":"  The remarkable capabilities of modern large language models are rooted in\ntheir vast repositories of knowledge encoded within their parameters, enabling\nthem to perceive the world and engage in reasoning. The inner workings of how\nthese models store knowledge have long been a subject of intense interest and\ninvestigation among researchers. To date, most studies have concentrated on\nisolated components within these models, such as the Multilayer Perceptrons and\nattention head. In this paper, we delve into the computation graph of the\nlanguage model to uncover the knowledge circuits that are instrumental in\narticulating specific knowledge. The experiments, conducted with GPT2 and\nTinyLLAMA, have allowed us to observe how certain information heads, relation\nheads, and Multilayer Perceptrons collaboratively encode knowledge within the\nmodel. Moreover, we evaluate the impact of current knowledge editing techniques\non these knowledge circuits, providing deeper insights into the functioning and\nconstraints of these editing methodologies. Finally, we utilize knowledge\ncircuits to analyze and interpret language model behaviors such as\nhallucinations and in-context learning. We believe the knowledge circuits hold\npotential for advancing our understanding of Transformers and guiding the\nimproved design of knowledge editing. Code and data are available in\nhttps://github.com/zjunlp/KnowledgeCircuits.\n","authors":["Yunzhi Yao","Ningyu Zhang","Zekun Xi","Mengru Wang","Ziwen Xu","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.17969v2.pdf","comment":"NeurIPS 2024, 32 pages"},{"id":"http://arxiv.org/abs/2407.07321v2","updated":"2024-10-16T03:33:58Z","published":"2024-07-10T02:33:09Z","title":"Examining Long-Context Large Language Models for Environmental Review\n  Document Comprehension","summary":"  As LLMs become increasingly ubiquitous, researchers have tried various\ntechniques to augment the knowledge provided to these models. Long context and\nretrieval-augmented generation (RAG) are two such methods that have recently\ngained popularity. In this work, we examine the benefits of both of these\ntechniques by utilizing question answering (QA) task in a niche domain. While\nthe effectiveness of LLM-based QA systems has already been established at an\nacceptable level in popular domains such as trivia and literature, it has not\noften been established in niche domains that traditionally require specialized\nexpertise. We construct the NEPAQuAD1.0 benchmark to evaluate the performance\nof five long-context LLMs -- Claude Sonnet, Gemini, GPT-4, Llama 3.1, and\nMistral -- when answering questions originating from Environmental Impact\nStatements prepared by U.S. federal government agencies in accordance with the\nNational Environmental Environmental Act (NEPA). We specifically measure the\nability of LLMs to understand the nuances of legal, technical, and\ncompliance-related information present in NEPA documents in different\ncontextual scenarios. We test the LLMs' internal prior NEPA knowledge by\nproviding questions without any context, as well as assess how LLMs synthesize\nthe contextual information present in long NEPA documents to facilitate the\nquestion/answering task. We compare the performance of the models in handling\ndifferent types of questions (e.g., problem-solving, divergent, etc.). Our\nresults suggest that RAG powered models significantly outperform those provided\nwith only the PDF context in terms of answer accuracy, regardless of the choice\nof the LLM. Our further analysis reveals that many models perform better\nanswering closed type questions (Yes/No) than divergent and problem-solving\nquestions.\n","authors":["Hung Phan","Anurag Acharya","Rounak Meyur","Sarthak Chaturvedi","Shivam Sharma","Mike Parker","Dan Nally","Ali Jannesari","Karl Pazdernik","Mahantesh Halappanavar","Sai Munikoti","Sameera Horawalavithana"],"pdf_url":"https://arxiv.org/pdf/2407.07321v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2410.12194v1","updated":"2024-10-16T03:30:09Z","published":"2024-10-16T03:30:09Z","title":"Negative-Prompt-driven Alignment for Generative Language Model","summary":"  Large language models have achieved remarkable capabilities, but aligning\ntheir outputs with human values and preferences remains a significant\nchallenge. Existing alignment methods primarily focus on positive examples\nwhile overlooking the importance of negative responses in guiding models away\nfrom undesirable behaviors. For instance, the widely-used alignment datasets\nreveals a scarcity of explicit negative examples that contradict human values,\nhindering its ability to discourage harmful or biased outputs during training.\nTo address this limitation, we propose NEAT, i.e., NEgative-prompt-driven\nAlignmenT, to introduce negative prompts to generate undesirable responses\nalongside positive examples during the optimization process. NEAT explicitly\npenalizes the model for producing harmful outputs, guiding it not only toward\ndesirable behaviors but also steering it away from generating undesirable,\nbiased responses. This dual feedback mechanism enables better alignment with\nhuman preferences, crucial in contexts where avoiding harm is paramount.\nStarting from a pre-trained language model, NEAT performs online alignment by\nincorporating a ranking loss derived from an expanded preference dataset\ncontaining both positive and negative examples. Extensive experiments validate\nNEAT's effectiveness in significantly enhancing language models' alignment with\nhuman values and preferences.\n","authors":["Shiqi Qiao","Ning Xv","Biao Liu","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2410.12194v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2410.12790v1","updated":"2024-10-16T17:59:49Z","published":"2024-10-16T17:59:49Z","title":"Dual Prototype Evolving for Test-Time Generalization of Vision-Language\n  Models","summary":"  Test-time adaptation, which enables models to generalize to diverse data with\nunlabeled test samples, holds significant value in real-world scenarios.\nRecently, researchers have applied this setting to advanced pre-trained\nvision-language models (VLMs), developing approaches such as test-time prompt\ntuning to further extend their practical applicability. However, these methods\ntypically focus solely on adapting VLMs from a single modality and fail to\naccumulate task-specific knowledge as more samples are processed. To address\nthis, we introduce Dual Prototype Evolving (DPE), a novel test-time adaptation\napproach for VLMs that effectively accumulates task-specific knowledge from\nmulti-modalities. Specifically, we create and evolve two sets of\nprototypes--textual and visual--to progressively capture more accurate\nmulti-modal representations for target classes during test time. Moreover, to\npromote consistent multi-modal representations, we introduce and optimize\nlearnable residuals for each test sample to align the prototypes from both\nmodalities. Extensive experimental results on 15 benchmark datasets demonstrate\nthat our proposed DPE consistently outperforms previous state-of-the-art\nmethods while also exhibiting competitive computational efficiency. Code is\navailable at https://github.com/zhangce01/DPE-CLIP.\n","authors":["Ce Zhang","Simon Stepputtis","Katia Sycara","Yaqi Xie"],"pdf_url":"https://arxiv.org/pdf/2410.12790v1.pdf","comment":"Accepted by NeurIPS 2024. Project page:\n  https://zhangce01.github.io/DPE-CLIP"},{"id":"http://arxiv.org/abs/2410.12787v1","updated":"2024-10-16T17:59:02Z","published":"2024-10-16T17:59:02Z","title":"The Curse of Multi-Modalities: Evaluating Hallucinations of Large\n  Multimodal Models across Language, Visual, and Audio","summary":"  Recent advancements in large multimodal models (LMMs) have significantly\nenhanced performance across diverse tasks, with ongoing efforts to further\nintegrate additional modalities such as video and audio. However, most existing\nLMMs remain vulnerable to hallucinations, the discrepancy between the factual\nmultimodal input and the generated textual output, which has limited their\napplicability in various real-world scenarios. This paper presents the first\nsystematic investigation of hallucinations in LMMs involving the three most\ncommon modalities: language, visual, and audio. Our study reveals two key\ncontributors to hallucinations: overreliance on unimodal priors and spurious\ninter-modality correlations. To address these challenges, we introduce the\nbenchmark The Curse of Multi-Modalities (CMM), which comprehensively evaluates\nhallucinations in LMMs, providing a detailed analysis of their underlying\nissues. Our findings highlight key vulnerabilities, including imbalances in\nmodality integration and biases from training data, underscoring the need for\nbalanced cross-modal learning and enhanced hallucination mitigation strategies.\nBased on our observations and findings, we suggest potential research\ndirections that could enhance the reliability of LMMs.\n","authors":["Sicong Leng","Yun Xing","Zesen Cheng","Yang Zhou","Hang Zhang","Xin Li","Deli Zhao","Shijian Lu","Chunyan Miao","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2410.12787v1.pdf","comment":"Project Page: cmm-damovl.site"},{"id":"http://arxiv.org/abs/2410.12781v1","updated":"2024-10-16T17:54:06Z","published":"2024-10-16T17:54:06Z","title":"Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage\n  Gaussian Splats","summary":"  We propose Long-LRM, a generalizable 3D Gaussian reconstruction model that is\ncapable of reconstructing a large scene from a long sequence of input images.\nSpecifically, our model can process 32 source images at 960x540 resolution\nwithin only 1.3 seconds on a single A100 80G GPU. Our architecture features a\nmixture of the recent Mamba2 blocks and the classical transformer blocks which\nallowed many more tokens to be processed than prior work, enhanced by efficient\ntoken merging and Gaussian pruning steps that balance between quality and\nefficiency. Unlike previous feed-forward models that are limited to processing\n1~4 input images and can only reconstruct a small portion of a large scene,\nLong-LRM reconstructs the entire scene in a single feed-forward step. On\nlarge-scale scene datasets such as DL3DV-140 and Tanks and Temples, our method\nachieves performance comparable to optimization-based approaches while being\ntwo orders of magnitude more efficient. Project page:\nhttps://arthurhero.github.io/projects/llrm\n","authors":["Chen Ziwen","Hao Tan","Kai Zhang","Sai Bi","Fujun Luan","Yicong Hong","Li Fuxin","Zexiang Xu"],"pdf_url":"https://arxiv.org/pdf/2410.12781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12777v1","updated":"2024-10-16T17:51:25Z","published":"2024-10-16T17:51:25Z","title":"Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned\n  Concepts","summary":"  With the rapid progress of diffusion-based content generation, significant\nefforts are being made to unlearn harmful or copyrighted concepts from\npretrained diffusion models (DMs) to prevent potential model misuse. However,\nit is observed that even when DMs are properly unlearned before release,\nmalicious finetuning can compromise this process, causing DMs to relearn the\nunlearned concepts. This occurs partly because certain benign concepts (e.g.,\n\"skin\") retained in DMs are related to the unlearned ones (e.g., \"nudity\"),\nfacilitating their relearning via finetuning. To address this, we propose\nmeta-unlearning on DMs. Intuitively, a meta-unlearned DM should behave like an\nunlearned DM when used as is; moreover, if the meta-unlearned DM undergoes\nmalicious finetuning on unlearned concepts, the related benign concepts\nretained within it will be triggered to self-destruct, hindering the relearning\nof unlearned concepts. Our meta-unlearning framework is compatible with most\nexisting unlearning methods, requiring only the addition of an\neasy-to-implement meta objective. We validate our approach through empirical\nexperiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4\nand SDXL), supported by extensive ablation studies. Our code is available at\nhttps://github.com/sail-sg/Meta-Unlearning.\n","authors":["Hongcheng Gao","Tianyu Pang","Chao Du","Taihang Hu","Zhijie Deng","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.12777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12769v1","updated":"2024-10-16T17:44:58Z","published":"2024-10-16T17:44:58Z","title":"Towards Zero-Shot Camera Trap Image Categorization","summary":"  This paper describes the search for an alternative approach to the automatic\ncategorization of camera trap images. First, we benchmark state-of-the-art\nclassifiers using a single model for all images. Next, we evaluate methods\ncombining MegaDetector with one or more classifiers and Segment Anything to\nassess their impact on reducing location-specific overfitting. Last, we propose\nand test two approaches using large language and foundational models, such as\nDINOv2, BioCLIP, BLIP, and ChatGPT, in a zero-shot scenario. Evaluation carried\nout on two publicly available datasets (WCT from New Zealand, CCT20 from the\nSouthwestern US) and a private dataset (CEF from Central Europe) revealed that\ncombining MegaDetector with two separate classifiers achieves the highest\naccuracy. This approach reduced the relative error of a single BEiTV2\nclassifier by approximately 42\\% on CCT20, 48\\% on CEF, and 75\\% on WCT.\nBesides, as the background is removed, the error in terms of accuracy in new\nlocations is reduced to half. The proposed zero-shot pipeline based on DINOv2\nand FAISS achieved competitive results (1.0\\% and 4.7\\% smaller on CCT20, and\nCEF, respectively), which highlights the potential of zero-shot approaches for\ncamera trap image categorization.\n","authors":["Jiří Vyskočil","Lukas Picek"],"pdf_url":"https://arxiv.org/pdf/2410.12769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12763v1","updated":"2024-10-16T17:37:43Z","published":"2024-10-16T17:37:43Z","title":"Gravity-aligned Rotation Averaging with Circular Regression","summary":"  Reconstructing a 3D scene from unordered images is pivotal in computer vision\nand robotics, with applications spanning crowd-sourced mapping and beyond.\nWhile global Structure-from-Motion (SfM) techniques are scalable and fast, they\noften compromise on accuracy. To address this, we introduce a principled\napproach that integrates gravity direction into the rotation averaging phase of\nglobal pipelines, enhancing camera orientation accuracy and reducing the\ndegrees of freedom. This additional information is commonly available in recent\nconsumer devices, such as smartphones, mixed-reality devices and drones, making\nthe proposed method readily accessible. Rooted in circular regression, our\nalgorithm has similar convergence guarantees as linear regression. It also\nsupports scenarios where only a subset of cameras have known gravity.\nAdditionally, we propose a mechanism to refine error-prone gravity. We achieve\nstate-of-the-art accuracy on four large-scale datasets. Particularly, the\nproposed method improves upon the SfM baseline by 13 AUC@$1^\\circ$ points, on\naverage, while running eight times faster. It also outperforms the standard\nplanar pose graph optimization technique by 23 AUC@$1^\\circ$ points. The code\nis at https://github.com/colmap/glomap.\n","authors":["Linfei Pan","Marc Pollefeys","Dániel Baráth"],"pdf_url":"https://arxiv.org/pdf/2410.12763v1.pdf","comment":"accepted at ECCV2024"},{"id":"http://arxiv.org/abs/2410.12761v1","updated":"2024-10-16T17:32:23Z","published":"2024-10-16T17:32:23Z","title":"SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And\n  Video Generation","summary":"  Recent advances in diffusion models have significantly enhanced their ability\nto generate high-quality images and videos, but they have also increased the\nrisk of producing unsafe content. Existing unlearning/editing-based methods for\nsafe generation remove harmful concepts from models but face several\nchallenges: (1) They cannot instantly remove harmful concepts without training.\n(2) Their safe generation capabilities depend on collected training data. (3)\nThey alter model weights, risking degradation in quality for content unrelated\nto toxic concepts. To address these, we propose SAFREE, a novel, training-free\napproach for safe T2I and T2V, that does not alter the model's weights.\nSpecifically, we detect a subspace corresponding to a set of toxic concepts in\nthe text embedding space and steer prompt embeddings away from this subspace,\nthereby filtering out harmful content while preserving intended semantics. To\nbalance the trade-off between filtering toxicity and preserving safe concepts,\nSAFREE incorporates a novel self-validating filtering mechanism that\ndynamically adjusts the denoising steps when applying the filtered embeddings.\nAdditionally, we incorporate adaptive re-attention mechanisms within the\ndiffusion latent space to selectively diminish the influence of features\nrelated to toxic concepts at the pixel level. In the end, SAFREE ensures\ncoherent safety checking, preserving the fidelity, quality, and safety of the\noutput. SAFREE achieves SOTA performance in suppressing unsafe content in T2I\ngeneration compared to training-free baselines and effectively filters targeted\nconcepts while maintaining high-quality images. It also shows competitive\nresults against training-based methods. We extend SAFREE to various T2I\nbackbones and T2V tasks, showcasing its flexibility and generalization. SAFREE\nprovides a robust and adaptable safeguard for ensuring safe visual generation.\n","authors":["Jaehong Yoon","Shoubin Yu","Vaidehi Patil","Huaxiu Yao","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2410.12761v1.pdf","comment":"The first two authors contributed equally; Project page:\n  https://safree-safe-t2i-t2v.github.io/"},{"id":"http://arxiv.org/abs/2410.10551v2","updated":"2024-10-16T17:04:50Z","published":"2024-10-14T14:32:05Z","title":"Preserving Cardiac Integrity: A Topology-Infused Approach to Whole Heart\n  Segmentation","summary":"  Whole heart segmentation (WHS) supports cardiovascular disease (CVD)\ndiagnosis, disease monitoring, treatment planning, and prognosis. Deep learning\nhas become the most widely used method for WHS applications in recent years.\nHowever, segmentation of whole-heart structures faces numerous challenges\nincluding heart shape variability during the cardiac cycle, clinical artifacts\nlike motion and poor contrast-to-noise ratio, domain shifts in multi-center\ndata, and the distinct modalities of CT and MRI. To address these limitations\nand improve segmentation quality, this paper introduces a new\ntopology-preserving module that is integrated into deep neural networks. The\nimplementation achieves anatomically plausible segmentation by using learned\ntopology-preserving fields, which are based entirely on 3D convolution and are\ntherefore very effective for 3D voxel data. We incorporate natural constraints\nbetween structures into the end-to-end training and enrich the feature\nrepresentation of the neural network. The effectiveness of the proposed method\nis validated on an open-source medical heart dataset, specifically using the\nWHS++ data. The results demonstrate that the architecture performs\nexceptionally well, achieving a Dice coefficient of 0.939 during testing. This\nindicates full topology preservation for individual structures and\nsignificantly outperforms other baselines in preserving the overall scene\ntopology.\n","authors":["Chenyu Zhang","Wenxue Guan","Xiaodan Xing","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2410.10551v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12742v1","updated":"2024-10-16T17:01:28Z","published":"2024-10-16T17:01:28Z","title":"PND-Net: Plant Nutrition Deficiency and Disease Classification using\n  Graph Convolutional Network","summary":"  Crop yield production could be enhanced for agricultural growth if various\nplant nutrition deficiencies, and diseases are identified and detected at early\nstages. The deep learning methods have proven its superior performances in the\nautomated detection of plant diseases and nutrition deficiencies from visual\nsymptoms in leaves. This article proposes a new deep learning method for plant\nnutrition deficiencies and disease classification using a graph convolutional\nnetwork (GNN), added upon a base convolutional neural network (CNN). Sometimes,\na global feature descriptor might fail to capture the vital region of a\ndiseased leaf, which causes inaccurate classification of disease. To address\nthis issue, regional feature learning is crucial for a holistic feature\naggregation. In this work, region-based feature summarization at multi-scales\nis explored using spatial pyramidal pooling for discriminative feature\nrepresentation. A GCN is developed to capacitate learning of finer details for\nclassifying plant diseases and insufficiency of nutrients. The proposed method,\ncalled Plant Nutrition Deficiency and Disease Network (PND-Net), is evaluated\non two public datasets for nutrition deficiency, and two for disease\nclassification using four CNNs. The best classification performances are: (a)\n90.00% Banana and 90.54% Coffee nutrition deficiency; and (b) 96.18% Potato\ndiseases and 84.30% on PlantDoc datasets using Xception backbone. Furthermore,\nadditional experiments have been carried out for generalization, and the\nproposed method has achieved state-of-the-art performances on two public\ndatasets, namely the Breast Cancer Histopathology Image Classification\n(BreakHis 40X: 95.50%, and BreakHis 100X: 96.79% accuracy) and Single cells in\nPap smear images for cervical cancer classification (SIPaKMeD: 99.18%\naccuracy). Also, PND-Net achieves improved performances using five-fold cross\nvalidation.\n","authors":["Asish Bera","Debotosh Bhattacharjee","Ondrej Krejcar"],"pdf_url":"https://arxiv.org/pdf/2410.12742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12725v1","updated":"2024-10-16T16:36:23Z","published":"2024-10-16T16:36:23Z","title":"Optimizing 3D Geometry Reconstruction from Implicit Neural\n  Representations","summary":"  Implicit neural representations have emerged as a powerful tool in learning\n3D geometry, offering unparalleled advantages over conventional representations\nlike mesh-based methods. A common type of INR implicitly encodes a shape's\nboundary as the zero-level set of the learned continuous function and learns a\nmapping from a low-dimensional latent space to the space of all possible shapes\nrepresented by its signed distance function. However, most INRs struggle to\nretain high-frequency details, which are crucial for accurate geometric\ndepiction, and they are computationally expensive. To address these\nlimitations, we present a novel approach that both reduces computational\nexpenses and enhances the capture of fine details. Our method integrates\nperiodic activation functions, positional encodings, and normals into the\nneural network architecture. This integration significantly enhances the\nmodel's ability to learn the entire space of 3D shapes while preserving\nintricate details and sharp features, areas where conventional representations\noften fall short.\n","authors":["Shen Fan","Przemyslaw Musialski"],"pdf_url":"https://arxiv.org/pdf/2410.12725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12718v1","updated":"2024-10-16T16:28:08Z","published":"2024-10-16T16:28:08Z","title":"RAFA-Net: Region Attention Network For Food Items And Agricultural\n  Stress Recognition","summary":"  Deep Convolutional Neural Networks (CNNs) have facilitated remarkable success\nin recognizing various food items and agricultural stress. A decent performance\nboost has been witnessed in solving the agro-food challenges by mining and\nanalyzing of region-based partial feature descriptors. Also, computationally\nexpensive ensemble learning schemes using multiple CNNs have been studied in\nearlier works. This work proposes a region attention scheme for modelling\nlong-range dependencies by building a correlation among different regions\nwithin an input image. The attention method enhances feature representation by\nlearning the usefulness of context information from complementary regions.\nSpatial pyramidal pooling and average pooling pair aggregate partial\ndescriptors into a holistic representation. Both pooling methods establish\nspatial and channel-wise relationships without incurring extra parameters. A\ncontext gating scheme is applied to refine the descriptiveness of weighted\nattentional features, which is relevant for classification. The proposed Region\nAttention network for Food items and Agricultural stress recognition method,\ndubbed RAFA-Net, has been experimented on three public food datasets, and has\nachieved state-of-the-art performances with distinct margins. The highest top-1\naccuracies of RAFA-Net are 91.69%, 91.56%, and 96.97% on the UECFood-100,\nUECFood-256, and MAFood-121 datasets, respectively. In addition, better\naccuracies have been achieved on two benchmark agricultural stress datasets.\nThe best top-1 accuracies on the Insect Pest (IP-102) and PlantDoc-27 plant\ndisease datasets are 92.36%, and 85.54%, respectively; implying RAFA-Net's\ngeneralization capability.\n","authors":["Asish Bera","Ondrej Krejcar","Debotosh Bhattacharjee"],"pdf_url":"https://arxiv.org/pdf/2410.12718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12705v1","updated":"2024-10-16T16:11:49Z","published":"2024-10-16T16:11:49Z","title":"WorldCuisines: A Massive-Scale Benchmark for Multilingual and\n  Multicultural Visual Question Answering on Global Cuisines","summary":"  Vision Language Models (VLMs) often struggle with culture-specific knowledge,\nparticularly in languages other than English and in underrepresented cultural\ncontexts. To evaluate their understanding of such knowledge, we introduce\nWorldCuisines, a massive-scale benchmark for multilingual and multicultural,\nvisually grounded language understanding. This benchmark includes a visual\nquestion answering (VQA) dataset with text-image pairs across 30 languages and\ndialects, spanning 9 language families and featuring over 1 million data\npoints, making it the largest multicultural VQA benchmark to date. It includes\ntasks for identifying dish names and their origins. We provide evaluation\ndatasets in two sizes (12k and 60k instances) alongside a training dataset (1\nmillion instances). Our findings show that while VLMs perform better with\ncorrect location context, they struggle with adversarial contexts and\npredicting specific regional cuisines and languages. To support future\nresearch, we release a knowledge base with annotated food entries and images\nalong with the VQA data.\n","authors":["Genta Indra Winata","Frederikus Hudi","Patrick Amadeus Irawan","David Anugraha","Rifki Afina Putri","Yutong Wang","Adam Nohejl","Ubaidillah Ariq Prathama","Nedjma Ousidhoum","Afifa Amriani","Anar Rzayev","Anirban Das","Ashmari Pramodya","Aulia Adila","Bryan Wilie","Candy Olivia Mawalim","Ching Lam Cheng","Daud Abolade","Emmanuele Chersoni","Enrico Santus","Fariz Ikhwantri","Garry Kuwanto","Hanyang Zhao","Haryo Akbarianto Wibowo","Holy Lovenia","Jan Christian Blaise Cruz","Jan Wira Gotama Putra","Junho Myung","Lucky Susanto","Maria Angelica Riera Machin","Marina Zhukova","Michael Anugraha","Muhammad Farid Adilazuarda","Natasha Santosa","Peerat Limkonchotiwat","Raj Dabre","Rio Alexander Audino","Samuel Cahyawijaya","Shi-Xiong Zhang","Stephanie Yulia Salim","Yi Zhou","Yinxuan Gui","David Ifeoluwa Adelani","En-Shiun Annie Lee","Shogo Okada","Ayu Purwarianti","Alham Fikri Aji","Taro Watanabe","Derry Tanti Wijaya","Alice Oh","Chong-Wah Ngo"],"pdf_url":"https://arxiv.org/pdf/2410.12705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12700v1","updated":"2024-10-16T16:03:42Z","published":"2024-10-16T16:03:42Z","title":"Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via\n  Lightweight Value Optimization","summary":"  Recent advancements in diffusion models trained on large-scale data have\nenabled the generation of indistinguishable human-level images, yet they often\nproduce harmful content misaligned with human values, e.g., social bias, and\noffensive content. Despite extensive research on Large Language Models (LLMs),\nthe challenge of Text-to-Image (T2I) model alignment remains largely\nunexplored. Addressing this problem, we propose LiVO (Lightweight Value\nOptimization), a novel lightweight method for aligning T2I models with human\nvalues. LiVO only optimizes a plug-and-play value encoder to integrate a\nspecified value principle with the input prompt, allowing the control of\ngenerated images over both semantics and values. Specifically, we design a\ndiffusion model-tailored preference optimization loss, which theoretically\napproximates the Bradley-Terry model used in LLM alignment but provides a more\nflexible trade-off between image quality and value conformity. To optimize the\nvalue encoder, we also develop a framework to automatically construct a\ntext-image preference dataset of 86k (prompt, aligned image, violating image,\nvalue principle) samples. Without updating most model parameters and through\nadaptive value selection from the input prompt, LiVO significantly reduces\nharmful outputs and achieves faster convergence, surpassing several strong\nbaselines and taking an initial step towards ethically aligned T2I models.\n","authors":["Xingqi Wang","Xiaoyuan Yi","Xing Xie","Jia Jia"],"pdf_url":"https://arxiv.org/pdf/2410.12700v1.pdf","comment":"Accepted by ACM Multimedia 2024. The dataset and code can be found at\n  https://github.com/achernarwang/LiVO"},{"id":"http://arxiv.org/abs/2410.12696v1","updated":"2024-10-16T15:59:02Z","published":"2024-10-16T15:59:02Z","title":"AdaptiveDrag: Semantic-Driven Dragging on Diffusion-Based Image Editing","summary":"  Recently, several point-based image editing methods (e.g., DragDiffusion,\nFreeDrag, DragNoise) have emerged, yielding precise and high-quality results\nbased on user instructions. However, these methods often make insufficient use\nof semantic information, leading to less desirable results. In this paper, we\nproposed a novel mask-free point-based image editing method, AdaptiveDrag,\nwhich provides a more flexible editing approach and generates images that\nbetter align with user intent. Specifically, we design an auto mask generation\nmodule using super-pixel division for user-friendliness. Next, we leverage a\npre-trained diffusion model to optimize the latent, enabling the dragging of\nfeatures from handle points to target points. To ensure a comprehensive\nconnection between the input image and the drag process, we have developed a\nsemantic-driven optimization. We design adaptive steps that are supervised by\nthe positions of the points and the semantic regions derived from super-pixel\nsegmentation. This refined optimization process also leads to more realistic\nand accurate drag results. Furthermore, to address the limitations in the\ngenerative consistency of the diffusion model, we introduce an innovative\ncorresponding loss during the sampling process. Building on these effective\ndesigns, our method delivers superior generation results using only the single\ninput image and the handle-target point pairs. Extensive experiments have been\nconducted and demonstrate that the proposed method outperforms others in\nhandling various drag instructions (e.g., resize, movement, extension) across\ndifferent domains (e.g., animals, human face, land space, clothing).\n","authors":["DuoSheng Chen","Binghui Chen","Yifeng Geng","Liefeng Bo"],"pdf_url":"https://arxiv.org/pdf/2410.12696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12695v1","updated":"2024-10-16T15:58:47Z","published":"2024-10-16T15:58:47Z","title":"MultiCamCows2024 -- A Multi-view Image Dataset for AI-driven\n  Holstein-Friesian Cattle Re-Identification on a Working Farm","summary":"  We present MultiCamCows2024, a farm-scale image dataset filmed across\nmultiple cameras for the biometric identification of individual\nHolstein-Friesian cattle exploiting their unique black and white coat-patterns.\nCaptured by three ceiling-mounted visual sensors covering adjacent barn areas\nover seven days on a working dairy farm, the dataset comprises 101, 329 images\nof 90 cows, plus the underlying original CCTV footage. The dataset is provided\nalongside full computer vision recognition baselines, that is both a supervised\nand self-supervised learning framework for individual cow identification\ntrained on cattle tracklets. We report a performance above 96% single image\nidentification accuracy from the dataset and demonstrate that combining data\nfrom multiple cameras during learning enhances self-supervised identification.\nWe show that our framework enables fully automatic cattle identification,\nbarring only the simple human verification of tracklet integrity during data\ncollection. Crucially, our study highlights that multi-camera, supervised and\nself-supervised components in tandem not only deliver highly accurate\nindividual cow identification but also achieve this efficiently with no\nlabelling of cattle identities by humans at all. We argue that this improvement\nin efficacy has practical implications for livestock management, behaviour\nanalysis, and agricultural monitoring. For full reproducibility and practical\nease of use, we publish all key software and code including re-identification\ncomponents and the species detector with this paper.\n","authors":["Phoenix Yu","Tilo Burghardt","Andrew W Dowsey","Neill W Campbell"],"pdf_url":"https://arxiv.org/pdf/2410.12695v1.pdf","comment":"26 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.12694v1","updated":"2024-10-16T15:54:11Z","published":"2024-10-16T15:54:11Z","title":"VividMed: Vision Language Model with Versatile Visual Grounding for\n  Medicine","summary":"  Recent advancements in Vision Language Models (VLMs) have demonstrated\nremarkable promise in generating visually grounded responses. However, their\napplication in the medical domain is hindered by unique challenges. For\ninstance, most VLMs rely on a single method of visual grounding, whereas\ncomplex medical tasks demand more versatile approaches. Additionally, while\nmost VLMs process only 2D images, a large portion of medical images are 3D. The\nlack of medical data further compounds these obstacles. To address these\nchallenges, we present VividMed, a vision language model with versatile visual\ngrounding for medicine. Our model supports generating both semantic\nsegmentation masks and instance-level bounding boxes, and accommodates various\nimaging modalities, including both 2D and 3D data. We design a three-stage\ntraining procedure and an automatic data synthesis pipeline based on open\ndatasets and models. Besides visual grounding tasks, VividMed also excels in\nother common downstream tasks, including Visual Question Answering (VQA) and\nreport generation. Ablation studies empirically show that the integration of\nvisual grounding ability leads to improved performance on these tasks. Our code\nis publicly available at https://github.com/function2-llx/MMMM.\n","authors":["Lingxiao Luo","Bingda Tang","Xuanzhong Chen","Rong Han","Ting Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02181v2","updated":"2024-10-16T15:53:15Z","published":"2024-08-05T01:50:09Z","title":"AssemAI: Interpretable Image-Based Anomaly Detection for Manufacturing\n  Pipelines","summary":"  Anomaly detection in manufacturing pipelines remains a critical challenge,\nintensified by the complexity and variability of industrial environments. This\npaper introduces AssemAI, an interpretable image-based anomaly detection system\ntailored for smart manufacturing pipelines. Utilizing a curated image dataset\nfrom an industry-focused rocket assembly pipeline, we address the challenge of\nimbalanced image data and demonstrate the importance of image-based methods in\nanomaly detection. Our primary contributions include deriving an image dataset,\nfine-tuning an object detection model YOLO-FF, and implementing a custom\nanomaly detection model for assembly pipelines. The proposed approach leverages\ndomain knowledge in data preparation, model development and reasoning. We\nimplement several anomaly detection models on the derived image dataset,\nincluding a Convolutional Neural Network, Vision Transformer (ViT), and\npre-trained versions of these models. Additionally, we incorporate\nexplainability techniques at both user and model levels, utilizing ontology for\nuser-level explanations and SCORE-CAM for in-depth feature and model analysis.\nFinally, the best-performing anomaly detection model and YOLO-FF are deployed\nin a real-time setting. Our results include ablation studies on the baselines\nand a comprehensive evaluation of the proposed system. This work highlights the\nbroader impact of advanced image-based anomaly detection in enhancing the\nreliability and efficiency of smart manufacturing processes. The image dataset,\ncodes to reproduce the results and additional experiments are available at\nhttps://github.com/renjithk4/AssemAI.\n","authors":["Renjith Prasad","Chathurangi Shyalika","Ramtin Zand","Fadi El Kalach","Revathy Venkataramanan","Ramy Harik","Amit Sheth"],"pdf_url":"https://arxiv.org/pdf/2408.02181v2.pdf","comment":"8 Pages, 6 Figures, 4 Tables, Predictive Models in Engineering\n  Applications special session (MLPMEA )at International Conference on Machine\n  Learning and Applications (ICMLA) 2024"},{"id":"http://arxiv.org/abs/2410.12692v1","updated":"2024-10-16T15:52:32Z","published":"2024-10-16T15:52:32Z","title":"Machine Learning Approach to Brain Tumor Detection and Classification","summary":"  Brain tumor detection and classification are critical tasks in medical image\nanalysis, particularly in early-stage diagnosis, where accurate and timely\ndetection can significantly improve treatment outcomes. In this study, we apply\nvarious statistical and machine learning models to detect and classify brain\ntumors using brain MRI images. We explore a variety of statistical models\nincluding linear, logistic, and Bayesian regressions, and the machine learning\nmodels including decision tree, random forest, single-layer perceptron,\nmulti-layer perceptron, convolutional neural network (CNN), recurrent neural\nnetwork, and long short-term memory. Our findings show that CNN outperforms\nother models, achieving the best performance. Additionally, we confirm that the\nCNN model can also work for multi-class classification, distinguishing between\nfour categories of brain MRI images such as normal, glioma, meningioma, and\npituitary tumor images. This study demonstrates that machine learning\napproaches are suitable for brain tumor detection and classification,\nfacilitating real-world medical applications in assisting radiologists with\nearly and accurate diagnosis.\n","authors":["Alice Oh","Inyoung Noh","Jian Choo","Jihoo Lee","Justin Park","Kate Hwang","Sanghyeon Kim","Soo Min Oh"],"pdf_url":"https://arxiv.org/pdf/2410.12692v1.pdf","comment":"7 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.12686v1","updated":"2024-10-16T15:48:28Z","published":"2024-10-16T15:48:28Z","title":"Automatic Mapping of Anatomical Landmarks from Free-Text Using Large\n  Language Models: Insights from Llama-2","summary":"  Anatomical landmarks are vital in medical imaging for navigation and anomaly\ndetection. Modern large language models (LLMs), like Llama-2, offer promise for\nautomating the mapping of these landmarks in free-text radiology reports to\ncorresponding positions in image data. Recent studies propose LLMs may develop\ncoherent representations of generative processes. Motivated by these insights,\nwe investigated whether LLMs accurately represent the spatial positions of\nanatomical landmarks. Through experiments with Llama-2 models, we found that\nthey can linearly represent anatomical landmarks in space with considerable\nrobustness to different prompts. These results underscore the potential of LLMs\nto enhance the efficiency and accuracy of medical imaging workflows.\n","authors":["Mohamad Abdi","Gerardo Hemosillo Valadez","Halid Ziya Yerebakan"],"pdf_url":"https://arxiv.org/pdf/2410.12686v1.pdf","comment":"6 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2405.01474v2","updated":"2024-10-16T15:45:35Z","published":"2024-05-02T17:07:25Z","title":"Understanding Figurative Meaning through Explainable Visual Entailment","summary":"  Large Vision-Language Models (VLMs) have demonstrated strong capabilities in\ntasks requiring a fine-grained understanding of literal meaning in images and\ntext, such as visual question-answering or visual entailment. However, there\nhas been little exploration of these models' capabilities when presented with\nimages and captions containing figurative meaning, such as metaphors or humor.\nTo close this gap, we propose a new task framing the figurative meaning\nunderstanding problem as an explainable visual entailment task, where the model\nhas to predict whether the image (premise) entails a caption (hypothesis) and\njustify the predicted label with a textual explanation. The figurative\nphenomena can be present either in the image, the caption, or both. Utilizing a\nhuman-AI collaboration approach, we build the accompanying expert-verified\ndataset V-FLUTE, containing 6,027 {image, caption, label, explanation}\ninstances spanning five diverse figurative phenomena: metaphors, similes,\nidioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs\nstruggle to generalize from literal to figurative meaning, particularly when it\nis present in images. Further, we identify common types of errors in VLM\nreasoning via human evaluation.\n","authors":["Arkadiy Saakyan","Shreyas Kulkarni","Tuhin Chakrabarty","Smaranda Muresan"],"pdf_url":"https://arxiv.org/pdf/2405.01474v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12673v1","updated":"2024-10-16T15:37:29Z","published":"2024-10-16T15:37:29Z","title":"MambaBEV: An efficient 3D detection model with Mamba2","summary":"  A stable 3D object detection model based on BEV paradigm with temporal\ninformation is very important for autonomous driving systems. However, current\ntemporal fusion model use convolutional layer or deformable self-attention is\nnot conducive to the exchange of global information of BEV space and has more\ncomputational cost. Recently, a newly proposed based model specialized in\nprocessing sequence called mamba has shown great potential in multiple\ndownstream task. In this work, we proposed a mamba2-based BEV 3D object\ndetection model named MambaBEV. We also adapt an end to end self driving\nparadigm to test the performance of the model. Our work performs pretty good\nresults on nucences datasets:Our base version achieves 51.7% NDS. Our code will\nbe available soon.\n","authors":["Zihan You","Hao Wang","Qichao Zhao","Jinxiang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.12673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12669v1","updated":"2024-10-16T15:34:13Z","published":"2024-10-16T15:34:13Z","title":"3DIS: Depth-Driven Decoupled Instance Synthesis for Text-to-Image\n  Generation","summary":"  The increasing demand for controllable outputs in text-to-image generation\nhas spurred advancements in multi-instance generation (MIG), allowing users to\ndefine both instance layouts and attributes. However, unlike image-conditional\ngeneration methods such as ControlNet, MIG techniques have not been widely\nadopted in state-of-the-art models like SD2 and SDXL, primarily due to the\nchallenge of building robust renderers that simultaneously handle instance\npositioning and attribute rendering. In this paper, we introduce Depth-Driven\nDecoupled Instance Synthesis (3DIS), a novel framework that decouples the MIG\nprocess into two stages: (i) generating a coarse scene depth map for accurate\ninstance positioning and scene composition, and (ii) rendering fine-grained\nattributes using pre-trained ControlNet on any foundational model, without\nadditional training. Our 3DIS framework integrates a custom adapter into LDM3D\nfor precise depth-based layouts and employs a finetuning-free method for\nenhanced instance-level attribute rendering. Extensive experiments on\nCOCO-Position and COCO-MIG benchmarks demonstrate that 3DIS significantly\noutperforms existing methods in both layout precision and attribute rendering.\nNotably, 3DIS offers seamless compatibility with diverse foundational models,\nproviding a robust, adaptable solution for advanced multi-instance generation.\nThe code is available at: https://github.com/limuloo/3DIS.\n","authors":["Dewei Zhou","Ji Xie","Zongxin Yang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2410.12669v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.12662v1","updated":"2024-10-16T15:20:08Z","published":"2024-10-16T15:20:08Z","title":"Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models","summary":"  Vision-language alignment in Large Vision-Language Models (LVLMs)\nsuccessfully enables LLMs to understand visual input. However, we find that\nexisting vision-language alignment methods fail to transfer the existing safety\nmechanism for text in LLMs to vision, which leads to vulnerabilities in toxic\nimage. To explore the cause of this problem, we give the insightful explanation\nof where and how the safety mechanism of LVLMs operates and conduct comparative\nanalysis between text and vision. We find that the hidden states at the\nspecific transformer layers play a crucial role in the successful activation of\nsafety mechanism, while the vision-language alignment at hidden states level in\ncurrent methods is insufficient. This results in a semantic shift for input\nimages compared to text in hidden states, therefore misleads the safety\nmechanism. To address this, we propose a novel Text-Guided vision-language\nAlignment method (TGA) for LVLMs. TGA retrieves the texts related to input\nvision and uses them to guide the projection of vision into the hidden states\nspace in LLMs. Experiments show that TGA not only successfully transfers the\nsafety mechanism for text in basic LLMs to vision in vision-language alignment\nfor LVLMs without any safety fine-tuning on the visual modality but also\nmaintains the general performance on various vision tasks (Safe and Good).\n","authors":["Shicheng Xu","Liang Pang","Yunchang Zhu","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.12662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08601v3","updated":"2024-10-16T15:16:31Z","published":"2024-02-13T17:08:35Z","title":"Latent Inversion with Timestep-aware Sampling for Training-free\n  Non-rigid Editing","summary":"  Text-guided non-rigid editing involves complex edits for input images, such\nas changing motion or compositions within their surroundings. Since it requires\nmanipulating the input structure, existing methods often struggle with\npreserving object identity and background, particularly when combined with\nStable Diffusion. In this work, we propose a training-free approach for\nnon-rigid editing with Stable Diffusion, aimed at improving the identity\npreservation quality without compromising editability. Our approach comprises\nthree stages: text optimization, latent inversion, and timestep-aware text\ninjection sampling. Inspired by the success of Imagic, we employ their text\noptimization for smooth editing. Then, we introduce latent inversion to\npreserve the input image's identity without additional model fine-tuning. To\nfully utilize the input reconstruction ability of latent inversion, we suggest\ntimestep-aware text injection sampling. This effectively retains the structure\nof the input image by injecting the source text prompt in early sampling steps\nand then transitioning to the target prompt in subsequent sampling steps. This\nstrategic approach seamlessly harmonizes with text optimization, facilitating\ncomplex non-rigid edits to the input without losing the original identity. We\ndemonstrate the effectiveness of our method in terms of identity preservation,\neditability, and aesthetic quality through extensive experiments.\n","authors":["Yunji Jung","Seokju Lee","Tair Djanibekov","Hyunjung Shim","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2402.08601v3.pdf","comment":"This manuscript has been submitted to Pattern Recognition Letters"},{"id":"http://arxiv.org/abs/2410.12641v1","updated":"2024-10-16T15:00:31Z","published":"2024-10-16T15:00:31Z","title":"Cascade learning in multi-task encoder-decoder networks for concurrent\n  bone segmentation and glenohumeral joint assessment in shoulder CT scans","summary":"  Osteoarthritis is a degenerative condition affecting bones and cartilage,\noften leading to osteophyte formation, bone density loss, and joint space\nnarrowing. Treatment options to restore normal joint function vary depending on\nthe severity of the condition. This work introduces an innovative deep-learning\nframework processing shoulder CT scans. It features the semantic segmentation\nof the proximal humerus and scapula, the 3D reconstruction of bone surfaces,\nthe identification of the glenohumeral (GH) joint region, and the staging of\nthree common osteoarthritic-related pathologies: osteophyte formation (OS), GH\nspace reduction (JS), and humeroscapular alignment (HSA). The pipeline\ncomprises two cascaded CNN architectures: 3D CEL-UNet for segmentation and 3D\nArthro-Net for threefold classification. A retrospective dataset of 571 CT\nscans featuring patients with various degrees of GH osteoarthritic-related\npathologies was used to train, validate, and test the pipeline. Root mean\nsquared error and Hausdorff distance median values for 3D reconstruction were\n0.22mm and 1.48mm for the humerus and 0.24mm and 1.48mm for the scapula,\noutperforming state-of-the-art architectures and making it potentially suitable\nfor a PSI-based shoulder arthroplasty preoperative plan context. The\nclassification accuracy for OS, JS, and HSA consistently reached around 90%\nacross all three categories. The computational time for the inference pipeline\nwas less than 15s, showcasing the framework's efficiency and compatibility with\northopedic radiology practice. The outcomes represent a promising advancement\ntoward the medical translation of artificial intelligence tools. This progress\naims to streamline the preoperative planning pipeline delivering high-quality\nbone surfaces and supporting surgeons in selecting the most suitable surgical\napproach according to the unique patient joint conditions.\n","authors":["Luca Marsilio","Davide Marzorati","Matteo Rossi","Andrea Moglia","Luca Mainardi","Alfonso Manzotti","Pietro Cerveri"],"pdf_url":"https://arxiv.org/pdf/2410.12641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12628v1","updated":"2024-10-16T14:50:47Z","published":"2024-10-16T14:50:47Z","title":"DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse\n  Synthetic Data and Global-to-Local Adaptive Perception","summary":"  Document Layout Analysis is crucial for real-world document understanding\nsystems, but it encounters a challenging trade-off between speed and accuracy:\nmultimodal methods leveraging both text and visual features achieve higher\naccuracy but suffer from significant latency, whereas unimodal methods relying\nsolely on visual features offer faster processing speeds at the expense of\naccuracy. To address this dilemma, we introduce DocLayout-YOLO, a novel\napproach that enhances accuracy while maintaining speed advantages through\ndocument-specific optimizations in both pre-training and model design. For\nrobust document pre-training, we introduce the Mesh-candidate BestFit\nalgorithm, which frames document synthesis as a two-dimensional bin packing\nproblem, generating the large-scale, diverse DocSynth-300K dataset.\nPre-training on the resulting DocSynth-300K dataset significantly improves\nfine-tuning performance across various document types. In terms of model\noptimization, we propose a Global-to-Local Controllable Receptive Module that\nis capable of better handling multi-scale variations of document elements.\nFurthermore, to validate performance across different document types, we\nintroduce a complex and challenging benchmark named DocStructBench. Extensive\nexperiments on downstream datasets demonstrate that DocLayout-YOLO excels in\nboth speed and accuracy. Code, data, and models are available at\nhttps://github.com/opendatalab/DocLayout-YOLO.\n","authors":["Zhiyuan Zhao","Hengrui Kang","Bin Wang","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2410.12628v1.pdf","comment":"Github Repo: https://github.com/opendatalab/DocLayout-YOLO"},{"id":"http://arxiv.org/abs/2410.12613v1","updated":"2024-10-16T14:29:29Z","published":"2024-10-16T14:29:29Z","title":"Exploring Model Kinship for Merging Large Language Models","summary":"  Model merging has become one of the key technologies for enhancing the\ncapabilities and efficiency of Large Language Models (LLMs). However, our\nunderstanding of the expected performance gains and principles when merging any\ntwo models remains limited. In this work, we introduce model kinship, the\ndegree of similarity or relatedness between LLMs, analogous to biological\nevolution. With comprehensive empirical analysis, we find that there is a\ncertain relationship between model kinship and the performance gains after\nmodel merging, which can help guide our selection of candidate models. Inspired\nby this, we propose a new model merging strategy: Top-k Greedy Merging with\nModel Kinship, which can yield better performance on benchmark datasets.\nSpecifically, we discover that using model kinship as a criterion can assist us\nin continuously performing model merging, alleviating the degradation (local\noptima) in model evolution, whereas model kinship can serve as a guide to\nescape these traps. Code is available at\nhttps://github.com/zjunlp/ModelKinship.\n","authors":["Yedi Hu","Yunzhi Yao","Ningyu Zhang","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12613v1.pdf","comment":"Ongoing work"},{"id":"http://arxiv.org/abs/2401.12452v3","updated":"2024-10-16T14:19:28Z","published":"2024-01-23T02:41:06Z","title":"Self-supervised Learning of LiDAR 3D Point Clouds via 2D-3D Neural\n  Calibration","summary":"  This paper introduces a novel self-supervised learning framework for\nenhancing 3D perception in autonomous driving scenes. Specifically, our\napproach, namely NCLR, focuses on 2D-3D neural calibration, a novel pretext\ntask that estimates the rigid pose aligning camera and LiDAR coordinate\nsystems. First, we propose the learnable transformation alignment to bridge the\ndomain gap between image and point cloud data, converting features into a\nunified representation space for effective comparison and matching. Second, we\nidentify the overlapping area between the image and point cloud with the fused\nfeatures. Third, we establish dense 2D-3D correspondences to estimate the rigid\npose. The framework not only learns fine-grained matching from points to pixels\nbut also achieves alignment of the image and point cloud at a holistic level,\nunderstanding their relative pose. We demonstrate the efficacy of NCLR by\napplying the pre-trained backbone to downstream tasks, such as LiDAR-based 3D\nsemantic segmentation, object detection, and panoptic segmentation.\nComprehensive experiments on various datasets illustrate the superiority of\nNCLR over existing self-supervised methods. The results confirm that joint\nlearning from different modalities significantly enhances the network's\nunderstanding abilities and effectiveness of learned representation. The code\nis publicly available at https://github.com/Eaphan/NCLR.\n","authors":["Yifan Zhang","Siyu Ren","Junhui Hou","Jinjian Wu","Yixuan Yuan","Guangming Shi"],"pdf_url":"https://arxiv.org/pdf/2401.12452v3.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.11808v2","updated":"2024-10-16T14:18:53Z","published":"2024-03-18T14:05:52Z","title":"Dynamic Tuning Towards Parameter and Inference Efficiency for ViT\n  Adaptation","summary":"  Existing parameter-efficient fine-tuning (PEFT) methods have achieved\nsignificant success on vision transformers (ViTs) adaptation by improving\nparameter efficiency. However, the exploration of enhancing inference\nefficiency during adaptation remains underexplored. This limits the broader\napplication of pre-trained ViT models, especially when the model is\ncomputationally extensive. In this paper, we propose Dynamic Tuning (DyT), a\nnovel approach to improve both parameter and inference efficiency for ViT\nadaptation. Specifically, besides using the lightweight adapter modules, we\npropose a token dispatcher to distinguish informative tokens from less\nimportant ones, allowing the latter to dynamically skip the original block,\nthereby reducing the redundant computation during inference. Additionally, we\nexplore multiple design variants to find the best practice of DyT. Finally,\ninspired by the mixture-of-experts (MoE) mechanism, we introduce an enhanced\nadapter to further boost the adaptation performance. We validate DyT across\nvarious tasks, including image/video recognition and semantic segmentation. For\ninstance, DyT achieves superior performance compared to existing PEFT methods\nwhile evoking only 71% of their FLOPs on the VTAB-1K benchmark.\n","authors":["Wangbo Zhao","Jiasheng Tang","Yizeng Han","Yibing Song","Kai Wang","Gao Huang","Fan Wang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2403.11808v2.pdf","comment":"Accepted to NeurIPS2024"},{"id":"http://arxiv.org/abs/2407.09797v2","updated":"2024-10-16T14:16:21Z","published":"2024-07-13T07:58:48Z","title":"ScaleFlow++: Robust and Accurate Estimation of 3D Motion from Video","summary":"  Perceiving and understanding 3D motion is a core technology in fields such as\nautonomous driving, robots, and motion prediction. This paper proposes a 3D\nmotion perception method called ScaleFlow++ that is easy to generalize. With\njust a pair of RGB images, ScaleFlow++ can robustly estimate optical flow and\nmotion-in-depth (MID). Most existing methods directly regress MID from two RGB\nframes or optical flow, resulting in inaccurate and unstable results. Our key\ninsight is cross-scale matching, which extracts deep motion clues by matching\nobjects in pairs of images at different scales. Unlike previous methods,\nScaleFlow++ integrates optical flow and MID estimation into a unified\narchitecture, estimating optical flow and MID end-to-end based on feature\nmatching. Moreover, we also proposed modules such as global initialization\nnetwork, global iterative optimizer, and hybrid training pipeline to integrate\nglobal motion information, reduce the number of iterations, and prevent\noverfitting during training. On KITTI, ScaleFlow++ achieved the best monocular\nscene flow estimation performance, reducing SF-all from 6.21 to 5.79. The\nevaluation of MID even surpasses RGBD-based methods. In addition, ScaleFlow++\nhas achieved stunning zero-shot generalization performance in both rigid and\nnonrigid scenes. Code is available at\n\\url{https://github.com/HanLingsgjk/CSCV}.\n","authors":["Han Ling","Quansen Sun"],"pdf_url":"https://arxiv.org/pdf/2407.09797v2.pdf","comment":"14 pages; Previously this version appeared as arXiv:2409.12202 which\n  was submitted as a new work by accident"},{"id":"http://arxiv.org/abs/2410.12595v1","updated":"2024-10-16T14:12:26Z","published":"2024-10-16T14:12:26Z","title":"CMAL: A Novel Cross-Modal Associative Learning Framework for\n  Vision-Language Pre-Training","summary":"  With the flourishing of social media platforms, vision-language pre-training\n(VLP) recently has received great attention and many remarkable progresses have\nbeen achieved. The success of VLP largely benefits from the information\ncomplementation and enhancement between different modalities. However, most of\nrecent studies focus on cross-modal contrastive learning (CMCL) to promote\nimage-text alignment by pulling embeddings of positive sample pairs together\nwhile pushing those of negative pairs apart, which ignores the natural\nasymmetry property between different modalities and requires large-scale\nimage-text corpus to achieve arduous progress. To mitigate this predicament, we\npropose CMAL, a Cross-Modal Associative Learning framework with anchor points\ndetection and cross-modal associative learning for VLP. Specifically, we first\nrespectively embed visual objects and textual tokens into separate hypersphere\nspaces to learn intra-modal hidden features, and then design a cross-modal\nassociative prompt layer to perform anchor point masking and swap feature\nfilling for constructing a hybrid cross-modal associative prompt. Afterwards,\nwe exploit a unified semantic encoder to learn their cross-modal interactive\nfeatures for context adaptation. Finally, we design an associative mapping\nclassification layer to learn potential associative mappings between modalities\nat anchor points, within which we develop a fresh self-supervised associative\nmapping classification task to boost CMAL's performance. Experimental results\nverify the effectiveness of CMAL, showing that it achieves competitive\nperformance against previous CMCL-based methods on four common downstream\nvision-and-language tasks, with significantly fewer corpus. Especially, CMAL\nobtains new state-of-the-art results on SNLI-VE and REC (testA).\n","authors":["Zhiyuan Ma","Jianjun Li","Guohui Li","Kaiyan Huang"],"pdf_url":"https://arxiv.org/pdf/2410.12595v1.pdf","comment":"vision-language pre-training, contrastive learning, cross-modal,\n  associative learning, associative mapping classification"},{"id":"http://arxiv.org/abs/2410.12592v1","updated":"2024-10-16T14:10:53Z","published":"2024-10-16T14:10:53Z","title":"Cocoon: Robust Multi-Modal Perception with Uncertainty-Aware Sensor\n  Fusion","summary":"  An important paradigm in 3D object detection is the use of multiple\nmodalities to enhance accuracy in both normal and challenging conditions,\nparticularly for long-tail scenarios. To address this, recent studies have\nexplored two directions of adaptive approaches: MoE-based adaptive fusion,\nwhich struggles with uncertainties arising from distinct object configurations,\nand late fusion for output-level adaptive fusion, which relies on separate\ndetection pipelines and limits comprehensive understanding. In this work, we\nintroduce Cocoon, an object- and feature-level uncertainty-aware fusion\nframework. The key innovation lies in uncertainty quantification for\nheterogeneous representations, enabling fair comparison across modalities\nthrough the introduction of a feature aligner and a learnable surrogate ground\ntruth, termed feature impression. We also define a training objective to ensure\nthat their relationship provides a valid metric for uncertainty quantification.\nCocoon consistently outperforms existing static and adaptive methods in both\nnormal and challenging conditions, including those with natural and artificial\ncorruptions. Furthermore, we show the validity and efficacy of our uncertainty\nmetric across diverse datasets.\n","authors":["Minkyoung Cho","Yulong Cao","Jiachen Sun","Qingzhao Zhang","Marco Pavone","Jeong Joon Park","Heng Yang","Z. Morley Mao"],"pdf_url":"https://arxiv.org/pdf/2410.12592v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2410.12591v1","updated":"2024-10-16T14:10:48Z","published":"2024-10-16T14:10:48Z","title":"Rethinking Visual Counterfactual Explanations Through Region Constraint","summary":"  Visual counterfactual explanations (VCEs) have recently gained immense\npopularity as a tool for clarifying the decision-making process of image\nclassifiers. This trend is largely motivated by what these explanations promise\nto deliver -- indicate semantically meaningful factors that change the\nclassifier's decision. However, we argue that current state-of-the-art\napproaches lack a crucial component -- the region constraint -- whose absence\nprevents from drawing explicit conclusions, and may even lead to faulty\nreasoning due to phenomenons like confirmation bias. To address the issue of\nprevious methods, which modify images in a very entangled and widely dispersed\nmanner, we propose region-constrained VCEs (RVCEs), which assume that only a\npredefined image region can be modified to influence the model's prediction. To\neffectively sample from this subclass of VCEs, we propose Region-Constrained\nCounterfactual Schr\\\"odinger Bridges (RCSB), an adaptation of a tractable\nsubclass of Schr\\\"odinger Bridges to the problem of conditional inpainting,\nwhere the conditioning signal originates from the classifier of interest. In\naddition to setting a new state-of-the-art by a large margin, we extend RCSB to\nallow for exact counterfactual reasoning, where the predefined region contains\nonly the factor of interest, and incorporating the user to actively interact\nwith the RVCE by predefining the regions manually.\n","authors":["Bartlomiej Sobieski","Jakub Grzywaczewski","Bartlomiej Sadlej","Matthew Tivnan","Przemyslaw Biecek"],"pdf_url":"https://arxiv.org/pdf/2410.12591v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.12589v1","updated":"2024-10-16T14:10:15Z","published":"2024-10-16T14:10:15Z","title":"From Lab to Pocket: A Novel Continual Learning-based Mobile Application\n  for Screening COVID-19","summary":"  Artificial intelligence (AI) has emerged as a promising tool for predicting\nCOVID-19 from medical images. In this paper, we propose a novel continual\nlearning-based approach and present the design and implementation of a mobile\napplication for screening COVID-19. Our approach demonstrates the ability to\nadapt to evolving datasets, including data collected from different locations\nor hospitals, varying virus strains, and diverse clinical presentations,\nwithout retraining from scratch. We have evaluated state-of-the-art continual\nlearning methods for detecting COVID-19 from chest X-rays and selected the\nbest-performing model for our mobile app. We evaluated various deep learning\narchitectures to select the best-performing one as a foundation model for\ncontinual learning. Both regularization and memory-based methods for continual\nlearning were tested, using different memory sizes to develop the optimal\ncontinual learning model for our app. DenseNet161 emerged as the best\nfoundation model with 96.87\\% accuracy, and Learning without Forgetting (LwF)\nwas the top continual learning method with an overall performance of 71.99\\%.\nThe mobile app design considers both patient and doctor perspectives. It\nincorporates the continual learning DenseNet161 LwF model on a cloud server,\nenabling the model to learn from new instances of chest X-rays and their\nclassifications as they are submitted. The app is designed, implemented, and\nevaluated to ensure it provides an efficient tool for COVID-19 screening. The\napp is available to download from\nhttps://github.com/DannyFGitHub/COVID-19PneumoCheckApp.\n","authors":["Danny Falero","Muhammad Ashad Kabir","Nusrat Homaira"],"pdf_url":"https://arxiv.org/pdf/2410.12589v1.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2410.08469v2","updated":"2024-10-16T14:09:14Z","published":"2024-10-11T02:42:13Z","title":"Semantic Token Reweighting for Interpretable and Controllable Text\n  Embeddings in CLIP","summary":"  A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial\nrole in translating textual input into an embedding space shared with images,\nthereby facilitating the interpretative analysis of vision tasks through\nnatural language. Despite the varying significance of different textual\nelements within a sentence depending on the context, efforts to account for\nvariation of importance in constructing text embeddings have been lacking. We\npropose a framework of Semantic Token Reweighting to build Interpretable text\nembeddings (SToRI), which incorporates controllability as well. SToRI refines\nthe text encoding process in CLIP by differentially weighting semantic elements\nbased on contextual importance, enabling finer control over emphasis responsive\nto data-driven insights and user preferences. The efficacy of SToRI is\ndemonstrated through comprehensive experiments on few-shot image classification\nand image retrieval tailored to user preferences.\n","authors":["Eunji Kim","Kyuhong Shim","Simyung Chang","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2410.08469v2.pdf","comment":"Accepted at EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.12584v1","updated":"2024-10-16T14:04:06Z","published":"2024-10-16T14:04:06Z","title":"Self-DenseMobileNet: A Robust Framework for Lung Nodule Classification\n  using Self-ONN and Stacking-based Meta-Classifier","summary":"  In this study, we propose a novel and robust framework, Self-DenseMobileNet,\ndesigned to enhance the classification of nodules and non-nodules in chest\nradiographs (CXRs). Our approach integrates advanced image standardization and\nenhancement techniques to optimize the input quality, thereby improving\nclassification accuracy. To enhance predictive accuracy and leverage the\nstrengths of multiple models, the prediction probabilities from\nSelf-DenseMobileNet were transformed into tabular data and used to train eight\nclassical machine learning (ML) models; the top three performers were then\ncombined via a stacking algorithm, creating a robust meta-classifier that\nintegrates their collective insights for superior classification performance.\nTo enhance the interpretability of our results, we employed class activation\nmapping (CAM) to visualize the decision-making process of the best-performing\nmodel. Our proposed framework demonstrated remarkable performance on internal\nvalidation data, achieving an accuracy of 99.28\\% using a Meta-Random Forest\nClassifier. When tested on an external dataset, the framework maintained strong\ngeneralizability with an accuracy of 89.40\\%. These results highlight a\nsignificant improvement in the classification of CXRs with lung nodules.\n","authors":["Md. Sohanur Rahman","Muhammad E. H. Chowdhury","Hasib Ryan Rahman","Mosabber Uddin Ahmed","Muhammad Ashad Kabir","Sanjiban Sekhar Roy","Rusab Sarmun"],"pdf_url":"https://arxiv.org/pdf/2410.12584v1.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2410.11639v2","updated":"2024-10-16T13:48:37Z","published":"2024-10-15T14:29:47Z","title":"Efficient and Effective Universal Adversarial Attack against\n  Vision-Language Pre-training Models","summary":"  Vision-language pre-training (VLP) models, trained on large-scale image-text\npairs, have become widely used across a variety of downstream\nvision-and-language (V+L) tasks. This widespread adoption raises concerns about\ntheir vulnerability to adversarial attacks. Non-universal adversarial attacks,\nwhile effective, are often impractical for real-time online applications due to\ntheir high computational demands per data instance. Recently, universal\nadversarial perturbations (UAPs) have been introduced as a solution, but\nexisting generator-based UAP methods are significantly time-consuming. To\novercome the limitation, we propose a direct optimization-based UAP approach,\ntermed DO-UAP, which significantly reduces resource consumption while\nmaintaining high attack performance. Specifically, we explore the necessity of\nmultimodal loss design and introduce a useful data augmentation strategy.\nExtensive experiments conducted on three benchmark VLP datasets, six popular\nVLP models, and three classical downstream tasks demonstrate the efficiency and\neffectiveness of DO-UAP. Specifically, our approach drastically decreases the\ntime consumption by 23-fold while achieving a better attack performance.\n","authors":["Fan Yang","Yihao Huang","Kailong Wang","Ling Shi","Geguang Pu","Yang Liu","Haoyu Wang"],"pdf_url":"https://arxiv.org/pdf/2410.11639v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2402.15704v4","updated":"2024-10-16T13:41:53Z","published":"2024-02-24T03:44:06Z","title":"Adaptive Convolutional Neural Network for Image Super-resolution","summary":"  Convolutional neural networks can automatically learn features via deep\nnetwork architectures and given input samples. However, the robustness of\nobtained models may face challenges in varying scenes. Bigger differences in\nnetwork architecture are beneficial to extract more diversified structural\ninformation to strengthen the robustness of an obtained super-resolution model.\nIn this paper, we proposed a adaptive convolutional neural network for image\nsuper-resolution (ADSRNet). To capture more information, ADSRNet is implemented\nby a heterogeneous parallel network. The upper network can enhance relation of\ncontext information, salient information relation of a kernel mapping and\nrelations of shallow and deep layers to improve performance of image\nsuper-resolution. That can strengthen adaptability of an obtained\nsuper-resolution model for different scenes. The lower network utilizes a\nsymmetric architecture to enhance relations of different layers to mine more\nstructural information, which is complementary with a upper network for image\nsuper-resolution. The relevant experimental results show that the proposed\nADSRNet is effective to deal with image resolving. Codes are obtained at\nhttps://github.com/hellloxiaotian/ADSRNet.\n","authors":["Chunwei Tian","Xuanyu Zhang","Tao Wang","Yongjun Zhang","Qi Zhu","Chia-Wen Lin"],"pdf_url":"https://arxiv.org/pdf/2402.15704v4.pdf","comment":"11pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.12564v1","updated":"2024-10-16T13:38:31Z","published":"2024-10-16T13:38:31Z","title":"FTII-Bench: A Comprehensive Multimodal Benchmark for Flow Text with\n  Image Insertion","summary":"  Benefiting from the revolutionary advances in large language models (LLMs)\nand foundational vision models, large vision-language models (LVLMs) have also\nmade significant progress. However, current benchmarks focus on tasks that\nevaluating only a single aspect of LVLM capabilities (e.g., recognition,\ndetection, understanding). These tasks fail to fully demonstrate LVLMs'\npotential in complex application scenarios. To comprehensively assess the\nperformance of existing LVLMs, we propose a more challenging task called the\nFlow Text with Image Insertion task (FTII). This task requires LVLMs to\nsimultaneously possess outstanding abilities in image comprehension,\ninstruction understanding, and long-text interpretation. Specifically, given\nseveral text paragraphs and a set of candidate images, as the text paragraphs\naccumulate, the LVLMs are required to select the most suitable image from the\ncandidates to insert after the corresponding paragraph. Constructing a\nbenchmark for such a task is highly challenging, particularly in determining\nthe sequence of flowing text and images. To address this challenge, we turn to\nprofessional news reports, which naturally contain a gold standard for\nimage-text sequences. Based on this, we introduce the Flow Text with Image\nInsertion Benchmark (FTII-Bench), which includes 318 high-quality Chinese\nimage-text news articles and 307 high-quality English image-text news articles,\ncovering 10 different news domains. Using these 625 high-quality articles, we\nconstruct problems of two different types with multiple levels of difficulty.\nFurthermore, we establish two different evaluation pipelines based on the CLIP\nmodel and existing LVLMs. We evaluate 9 open-source and 2 closed-source LVLMs\nas well as 2 CLIP-based models. Results indicate that even the most advanced\nmodels (e.g., GPT-4o) face significant challenges when tackling the FTII task.\n","authors":["Jiacheng Ruan","Yebin Yang","Zehao Lin","Feiyu Xiong","Zeyun Tang","Zhiyu Li"],"pdf_url":"https://arxiv.org/pdf/2410.12564v1.pdf","comment":"Work in progress. 9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.12562v1","updated":"2024-10-16T13:38:01Z","published":"2024-10-16T13:38:01Z","title":"Adaptive Prompt Learning with SAM for Few-shot Scanning Probe Microscope\n  Image Segmentation","summary":"  The Segment Anything Model (SAM) has demonstrated strong performance in image\nsegmentation of natural scene images. However, its effectiveness diminishes\nmarkedly when applied to specific scientific domains, such as Scanning Probe\nMicroscope (SPM) images. This decline in accuracy can be attributed to the\ndistinct data distribution and limited availability of the data inherent in the\nscientific images. On the other hand, the acquisition of adequate SPM datasets\nis both time-intensive and laborious as well as skill-dependent. To address\nthese challenges, we propose an Adaptive Prompt Learning with SAM (APL-SAM)\nframework tailored for few-shot SPM image segmentation. Our approach\nincorporates two key innovations to enhance SAM: 1) An Adaptive Prompt Learning\nmodule leverages few-shot embeddings derived from limited support set to learn\nadaptively central representatives, serving as visual prompts. This innovation\neliminates the need for time-consuming online user interactions for providing\nprompts, such as exhaustively marking points and bounding boxes slice by slice;\n2) A multi-source, multi-level mask decoder specifically designed for few-shot\nSPM image segmentation is introduced, which can effectively capture the\ncorrespondence between the support and query images. To facilitate\ncomprehensive training and evaluation, we introduce a new dataset, SPM-Seg,\ncurated for SPM image segmentation. Extensive experiments on this dataset\nreveal that the proposed APL-SAM framework significantly outperforms the\noriginal SAM, achieving over a 30% improvement in terms of Dice Similarity\nCoefficient with only one-shot guidance. Moreover, APL-SAM surpasses\nstate-of-the-art few-shot segmentation methods and even fully supervised\napproaches in performance. Code and dataset used in this study will be made\navailable upon acceptance.\n","authors":["Yao Shen","Ziwei Wei","Chunmeng Liu","Shuming Wei","Qi Zhao","Kaiyang Zeng","Guangyao Li"],"pdf_url":"https://arxiv.org/pdf/2410.12562v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.12561v1","updated":"2024-10-16T13:36:47Z","published":"2024-10-16T13:36:47Z","title":"Development of Image Collection Method Using YOLO and Siamese Network","summary":"  As we enter the era of big data, collecting high-quality data is very\nimportant. However, collecting data by humans is not only very time-consuming\nbut also expensive. Therefore, many scientists have devised various methods to\ncollect data using computers. Among them, there is a method called web\ncrawling, but the authors found that the crawling method has a problem in that\nunintended data is collected along with the user. The authors found that this\ncan be filtered using the object recognition model YOLOv10. However, there are\ncases where data that is not properly filtered remains. Here, image\nreclassification was performed by additionally utilizing the distance output\nfrom the Siamese network, and higher performance was recorded than other\nclassification models. (average \\_f1 score YOLO+MobileNet\n0.678->YOLO+SiameseNet 0.772)) The user can specify a distance threshold to\nadjust the balance between data deficiency and noise-robustness. The authors\nalso found that the Siamese network can achieve higher performance with fewer\nresources because the cropped images are used for object recognition when\nprocessing images in the Siamese network. (Class 20 mean-based f1 score,\nnon-crop+Siamese(MobileNetV3-Small) 80.94 -> crop\npreprocessing+Siamese(MobileNetV3-Small) 82.31) In this way, the image\nretrieval system that utilizes two consecutive models to reduce errors can save\nusers' time and effort, and build better quality data faster and with fewer\nresources than before.\n","authors":["Chan Young Shin","Ah Hyun Lee","Jun Young Lee","Ji Min Lee","Soo Jin Park"],"pdf_url":"https://arxiv.org/pdf/2410.12561v1.pdf","comment":"15 pages, 13 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.12557v1","updated":"2024-10-16T13:34:40Z","published":"2024-10-16T13:34:40Z","title":"One Step Diffusion via Shortcut Models","summary":"  Diffusion models and flow-matching models have enabled generating diverse and\nrealistic images by learning to transfer noise to data. However, sampling from\nthese models involves iterative denoising over many neural network passes,\nmaking generation slow and expensive. Previous approaches for speeding up\nsampling require complex training regimes, such as multiple training phases,\nmultiple networks, or fragile scheduling. We introduce shortcut models, a\nfamily of generative models that use a single network and training phase to\nproduce high-quality samples in a single or multiple sampling steps. Shortcut\nmodels condition the network not only on the current noise level but also on\nthe desired step size, allowing the model to skip ahead in the generation\nprocess. Across a wide range of sampling step budgets, shortcut models\nconsistently produce higher quality samples than previous approaches, such as\nconsistency models and reflow. Compared to distillation, shortcut models reduce\ncomplexity to a single network and training phase and additionally allow\nvarying step budgets at inference time.\n","authors":["Kevin Frans","Danijar Hafner","Sergey Levine","Pieter Abbeel"],"pdf_url":"https://arxiv.org/pdf/2410.12557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12542v1","updated":"2024-10-16T13:20:57Z","published":"2024-10-16T13:20:57Z","title":"Evaluating Utility of Memory Efficient Medical Image Generation: A Study\n  on Lung Nodule Segmentation","summary":"  The scarcity of publicly available medical imaging data limits the\ndevelopment of effective AI models. This work proposes a memory-efficient\npatch-wise denoising diffusion probabilistic model (DDPM) for generating\nsynthetic medical images, focusing on CT scans with lung nodules. Our approach\ngenerates high-utility synthetic images with nodule segmentation while\nefficiently managing memory constraints, enabling the creation of training\ndatasets. We evaluate the method in two scenarios: training a segmentation\nmodel exclusively on synthetic data, and augmenting real-world training data\nwith synthetic images. In the first case, models trained solely on synthetic\ndata achieve Dice scores comparable to those trained on real-world data\nbenchmarks. In the second case, augmenting real-world data with synthetic\nimages significantly improves segmentation performance. The generated images\ndemonstrate their potential to enhance medical image datasets in scenarios with\nlimited real-world data.\n","authors":["Kathrin Khadra","Utku Türkbey"],"pdf_url":"https://arxiv.org/pdf/2410.12542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11795v2","updated":"2024-10-16T13:10:32Z","published":"2024-10-15T17:19:46Z","title":"Efficient Diffusion Models: A Comprehensive Survey from Principles to\n  Practices","summary":"  As one of the most popular and sought-after generative models in the recent\nyears, diffusion models have sparked the interests of many researchers and\nsteadily shown excellent advantage in various generative tasks such as image\nsynthesis, video generation, molecule design, 3D scene rendering and multimodal\ngeneration, relying on their dense theoretical principles and reliable\napplication practices. The remarkable success of these recent efforts on\ndiffusion models comes largely from progressive design principles and efficient\narchitecture, training, inference, and deployment methodologies. However, there\nhas not been a comprehensive and in-depth review to summarize these principles\nand practices to help the rapid understanding and application of diffusion\nmodels. In this survey, we provide a new efficiency-oriented perspective on\nthese existing efforts, which mainly focuses on the profound principles and\nefficient practices in architecture designs, model training, fast inference and\nreliable deployment, to guide further theoretical research, algorithm migration\nand model application for new scenarios in a reader-friendly way.\n\\url{https://github.com/ponyzym/Efficient-DMs-Survey}\n","authors":["Zhiyuan Ma","Yuzhu Zhang","Guoli Jia","Liangliang Zhao","Yichao Ma","Mingjie Ma","Gaofeng Liu","Kaiyan Zhang","Jianjun Li","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.11795v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12526v1","updated":"2024-10-16T13:03:15Z","published":"2024-10-16T13:03:15Z","title":"Shaping a Stabilized Video by Mitigating Unintended Changes for\n  Concept-Augmented Video Editing","summary":"  Text-driven video editing utilizing generative diffusion models has garnered\nsignificant attention due to their potential applications. However, existing\napproaches are constrained by the limited word embeddings provided in\npre-training, which hinders nuanced editing targeting open concepts with\nspecific attributes. Directly altering the keywords in target prompts often\nresults in unintended disruptions to the attention mechanisms. To achieve more\nflexible editing easily, this work proposes an improved concept-augmented video\nediting approach that generates diverse and stable target videos flexibly by\ndevising abstract conceptual pairs. Specifically, the framework involves\nconcept-augmented textual inversion and a dual prior supervision mechanism. The\nformer enables plug-and-play guidance of stable diffusion for video editing,\neffectively capturing target attributes for more stylized results. The dual\nprior supervision mechanism significantly enhances video stability and\nfidelity. Comprehensive evaluations demonstrate that our approach generates\nmore stable and lifelike videos, outperforming state-of-the-art methods.\n","authors":["Mingce Guo","Jingxuan He","Shengeng Tang","Zhangye Wang","Lechao Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.12526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12524v1","updated":"2024-10-16T13:02:45Z","published":"2024-10-16T13:02:45Z","title":"MambaPainter: Neural Stroke-Based Rendering in a Single Step","summary":"  Stroke-based rendering aims to reconstruct an input image into an oil\npainting style by predicting brush stroke sequences. Conventional methods\nperform this prediction stroke-by-stroke or require multiple inference steps\ndue to the limitations of a predictable number of strokes. This procedure leads\nto inefficient translation speed, limiting their practicality. In this study,\nwe propose MambaPainter, capable of predicting a sequence of over 100 brush\nstrokes in a single inference step, resulting in rapid translation. We achieve\nthis sequence prediction by incorporating the selective state-space model.\nAdditionally, we introduce a simple extension to patch-based rendering, which\nwe use to translate high-resolution images, improving the visual quality with a\nminimal increase in computational cost. Experimental results demonstrate that\nMambaPainter can efficiently translate inputs to oil painting-style images\ncompared to state-of-the-art methods. The codes are available at\nhttps://github.com/STomoya/MambaPainter.\n","authors":["Tomoya Sawada","Marie Katsurai"],"pdf_url":"https://arxiv.org/pdf/2410.12524v1.pdf","comment":"Accepted to SIGGRAPH Asia 2024 posters"},{"id":"http://arxiv.org/abs/2410.12520v1","updated":"2024-10-16T12:58:08Z","published":"2024-10-16T12:58:08Z","title":"QueensCAMP: an RGB-D dataset for robust Visual SLAM","summary":"  Visual Simultaneous Localization and Mapping (VSLAM) is a fundamental\ntechnology for robotics applications. While VSLAM research has achieved\nsignificant advancements, its robustness under challenging situations, such as\npoor lighting, dynamic environments, motion blur, and sensor failures, remains\na challenging issue. To address these challenges, we introduce a novel RGB-D\ndataset designed for evaluating the robustness of VSLAM systems. The dataset\ncomprises real-world indoor scenes with dynamic objects, motion blur, and\nvarying illumination, as well as emulated camera failures, including lens dirt,\ncondensation, underexposure, and overexposure. Additionally, we offer\nopen-source scripts for injecting camera failures into any images, enabling\nfurther customization by the research community. Our experiments demonstrate\nthat ORB-SLAM2, a traditional VSLAM algorithm, and TartanVO, a Deep\nLearning-based VO algorithm, can experience performance degradation under these\nchallenging conditions. Therefore, this dataset and the camera failure\nopen-source tools provide a valuable resource for developing more robust VSLAM\nsystems capable of handling real-world challenges.\n","authors":["Hudson M. S. Bruno","Esther L. Colombini","Sidney N. Givigi Jr"],"pdf_url":"https://arxiv.org/pdf/2410.12520v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2403.17834v2","updated":"2024-10-16T12:49:19Z","published":"2024-03-26T16:19:56Z","title":"Developing Generalist Foundation Models from a Multimodal Dataset for 3D\n  Computed Tomography","summary":"  While computer vision has achieved tremendous success with multimodal\nencoding and direct textual interaction with images via chat-based large\nlanguage models, similar advancements in medical imaging AI, particularly in 3D\nimaging, have been limited due to the scarcity of comprehensive datasets. To\naddress this critical gap, we introduce CT-RATE, the first dataset that pairs\n3D medical images with corresponding textual reports. CT-RATE comprises 25,692\nnon-contrast 3D chest CT scans from 21,304 unique patients. Through various\nreconstructions, these scans are expanded to 50,188 volumes, totaling over 14.3\nmillion 2D slices. Each scan is accompanied by its corresponding radiology\nreport. Leveraging CT-RATE, we develop CT-CLIP, a CT-focused contrastive\nlanguage-image pretraining framework designed for broad applications without\nthe need for task-specific training. We demonstrate how CT-CLIP can be used in\ntwo tasks: multi-abnormality detection and case retrieval. Remarkably, in\nmulti-abnormality detection, CT-CLIP outperforms state-of-the-art fully\nsupervised models across all key metrics, effectively eliminating the need for\nmanual annotation. In case retrieval, it efficiently retrieves relevant cases\nusing either image or textual queries, thereby enhancing knowledge\ndissemination. By combining CT-CLIP's vision encoder with a pretrained large\nlanguage model, we create CT-CHAT, a vision-language foundational chat model\nfor 3D chest CT volumes. Finetuned on over 2.7 million question-answer pairs\nderived from the CT-RATE dataset, CT-CHAT surpasses other multimodal AI\nassistants, underscoring the necessity for specialized methods in 3D medical\nimaging. Collectively, the open-source release of CT-RATE, CT-CLIP, and CT-CHAT\nnot only addresses critical challenges in 3D medical imaging but also lays the\ngroundwork for future innovations in medical AI and improved patient care.\n","authors":["Ibrahim Ethem Hamamci","Sezgin Er","Furkan Almas","Ayse Gulnihan Simsek","Sevval Nil Esirgun","Irem Dogan","Muhammed Furkan Dasdelen","Omer Faruk Durugol","Bastian Wittmann","Tamaz Amiranashvili","Enis Simsar","Mehmet Simsar","Emine Bensu Erdemir","Abdullah Alanbay","Anjany Sekuboyina","Berkan Lafci","Christian Bluethgen","Mehmet Kemal Ozdemir","Bjoern Menze"],"pdf_url":"https://arxiv.org/pdf/2403.17834v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10114v2","updated":"2024-10-16T12:30:53Z","published":"2024-10-14T03:05:12Z","title":"Mixture of Experts Made Personalized: Federated Prompt Learning for\n  Vision-Language Models","summary":"  Prompt learning for pre-trained Vision-Language Models (VLMs) like CLIP has\ndemonstrated potent applicability across diverse downstream tasks. This\nlightweight approach has quickly gained traction from federated learning (FL)\nresearchers who seek to efficiently adapt VLMs to heterogeneous scenarios.\nHowever, current federated prompt learning methods are habitually restricted to\nthe traditional FL paradigm, where the participating clients are generally only\nallowed to download a single globally aggregated model from the server. While\njustifiable for training full-sized models under federated settings, in this\nwork, we argue that this paradigm is ill-suited for lightweight prompts. By\nfacilitating the clients to download multiple pre-aggregated prompts as fixed\nnon-local experts, we propose Personalized Federated Mixture of Adaptive\nPrompts (pFedMoAP), a novel FL framework that personalizes the prompt learning\nprocess through the lens of Mixture of Experts (MoE). pFedMoAP implements a\nlocal attention-based gating network that learns to generate enhanced text\nfeatures for better alignment with local image data on the client, benefiting\nfrom both local and downloaded non-local adaptive prompt experts. The non-local\nexperts are sparsely selected from a server-maintained pool, fostering\ncollaborative learning across clients. To evaluate the proposed algorithm, we\nconduct extensive experiments across 9 datasets under various heterogeneous\nfederated settings. The results show that pFedMoAP consistently outperforms the\nstate-of-the-art alternatives, underscoring its efficacy in personalizing\nprompt learning for CLIP within the federated learning paradigm.\n","authors":["Jun Luo","Chen Chen","Shandong Wu"],"pdf_url":"https://arxiv.org/pdf/2410.10114v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.12501v1","updated":"2024-10-16T12:27:10Z","published":"2024-10-16T12:27:10Z","title":"DH-VTON: Deep Text-Driven Virtual Try-On via Hybrid Attention Learning","summary":"  Virtual Try-ON (VTON) aims to synthesis specific person images dressed in\ngiven garments, which recently receives numerous attention in online shopping\nscenarios. Currently, the core challenges of the VTON task mainly lie in the\nfine-grained semantic extraction (i.e.,deep semantics) of the given reference\ngarments during depth estimation and effective texture preservation when the\ngarments are synthesized and warped onto human body. To cope with these issues,\nwe propose DH-VTON, a deep text-driven virtual try-on model featuring a special\nhybrid attention learning strategy and deep garment semantic preservation\nmodule. By standing on the shoulder of a well-built pre-trained\npaint-by-example (abbr. PBE) approach, we present our DH-VTON pipeline in this\nwork. Specifically, to extract the deep semantics of the garments, we first\nintroduce InternViT-6B as fine-grained feature learner, which can be trained to\nalign with the large-scale intrinsic knowledge with deep text semantics\n(e.g.,\"neckline\" or \"girdle\") to make up for the deficiency of the commonly\nadopted CLIP encoder. Based on this, to enhance the customized dressing\nabilities, we further introduce Garment-Feature ControlNet Plus (abbr. GFC+)\nmodule and propose to leverage a fresh hybrid attention strategy for training,\nwhich can adaptively integrate fine-grained characteristics of the garments\ninto the different layers of the VTON model, so as to achieve multi-scale\nfeatures preservation effects. Extensive experiments on several representative\ndatasets demonstrate that our method outperforms previous diffusion-based and\nGAN-based approaches, showing competitive performance in preserving garment\ndetails and generating authentic human images.\n","authors":["Jiabao Wei","Zhiyuan Ma"],"pdf_url":"https://arxiv.org/pdf/2410.12501v1.pdf","comment":"5 pages, 6 figures, ICASSP2025"},{"id":"http://arxiv.org/abs/2410.12490v1","updated":"2024-10-16T12:13:17Z","published":"2024-10-16T12:13:17Z","title":"Stabilize the Latent Space for Image Autoregressive Modeling: A Unified\n  Perspective","summary":"  Latent-based image generative models, such as Latent Diffusion Models (LDMs)\nand Mask Image Models (MIMs), have achieved notable success in image generation\ntasks. These models typically leverage reconstructive autoencoders like VQGAN\nor VAE to encode pixels into a more compact latent space and learn the data\ndistribution in the latent space instead of directly from pixels. However, this\npractice raises a pertinent question: Is it truly the optimal choice? In\nresponse, we begin with an intriguing observation: despite sharing the same\nlatent space, autoregressive models significantly lag behind LDMs and MIMs in\nimage generation. This finding contrasts sharply with the field of NLP, where\nthe autoregressive model GPT has established a commanding presence. To address\nthis discrepancy, we introduce a unified perspective on the relationship\nbetween latent space and generative models, emphasizing the stability of latent\nspace in image generative modeling. Furthermore, we propose a simple but\neffective discrete image tokenizer to stabilize the latent space for image\ngenerative modeling. Experimental results show that image autoregressive\nmodeling with our tokenizer (DiGIT) benefits both image understanding and image\ngeneration with the next token prediction principle, which is inherently\nstraightforward for GPT models but challenging for other generative models.\nRemarkably, for the first time, a GPT-style autoregressive model for images\noutperforms LDMs, which also exhibits substantial improvement akin to GPT when\nscaling up model size. Our findings underscore the potential of an optimized\nlatent space and the integration of discrete tokenization in advancing the\ncapabilities of image generative models. The code is available at\n\\url{https://github.com/DAMO-NLP-SG/DiGIT}.\n","authors":["Yongxin Zhu","Bocheng Li","Hang Zhang","Xin Li","Linli Xu","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2410.12490v1.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.12489v1","updated":"2024-10-16T12:09:38Z","published":"2024-10-16T12:09:38Z","title":"Synthetic Augmentation for Anatomical Landmark Localization using DDPMs","summary":"  Deep learning techniques for anatomical landmark localization (ALL) have\nshown great success, but their reliance on large annotated datasets remains a\nproblem due to the tedious and costly nature of medical data acquisition and\nannotation. While traditional data augmentation, variational autoencoders\n(VAEs), and generative adversarial networks (GANs) have already been used to\nsynthetically expand medical datasets, diffusion-based generative models have\nrecently started to gain attention for their ability to generate high-quality\nsynthetic images. In this study, we explore the use of denoising diffusion\nprobabilistic models (DDPMs) for generating medical images and their\ncorresponding heatmaps of landmarks to enhance the training of a supervised\ndeep learning model for ALL. Our novel approach involves a DDPM with a\n2-channel input, incorporating both the original medical image and its heatmap\nof annotated landmarks. We also propose a novel way to assess the quality of\nthe generated images using a Markov Random Field (MRF) model for landmark\nmatching and a Statistical Shape Model (SSM) to check landmark plausibility,\nbefore we evaluate the DDPM-augmented dataset in the context of an ALL task\ninvolving hand X-Rays.\n","authors":["Arnela Hadzic","Lea Bogensperger","Simon Johannes Joham","Martin Urschler"],"pdf_url":"https://arxiv.org/pdf/2410.12489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04492v3","updated":"2024-10-16T12:07:27Z","published":"2024-10-06T14:11:39Z","title":"Interpret Your Decision: Logical Reasoning Regularization for\n  Generalization in Visual Classification","summary":"  Vision models excel in image classification but struggle to generalize to\nunseen data, such as classifying images from unseen domains or discovering\nnovel categories. In this paper, we explore the relationship between logical\nreasoning and deep learning generalization in visual classification. A logical\nregularization termed L-Reg is derived which bridges a logical analysis\nframework to image classification. Our work reveals that L-Reg reduces the\ncomplexity of the model in terms of the feature distribution and classifier\nweights. Specifically, we unveil the interpretability brought by L-Reg, as it\nenables the model to extract the salient features, such as faces to persons,\nfor classification. Theoretical analysis and experiments demonstrate that L-Reg\nenhances generalization across various scenarios, including multi-domain\ngeneralization and generalized category discovery. In complex real-world\nscenarios where images span unknown classes and unseen domains, L-Reg\nconsistently improves generalization, highlighting its practical efficacy.\n","authors":["Zhaorui Tan","Xi Yang","Qiufeng Wang","Anh Nguyen","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2410.04492v3.pdf","comment":"Accepted by NeurIPS2024 as Spotlight"},{"id":"http://arxiv.org/abs/2406.15020v3","updated":"2024-10-16T11:58:16Z","published":"2024-06-21T09:49:34Z","title":"A3D: Does Diffusion Dream about 3D Alignment?","summary":"  We tackle the problem of text-driven 3D generation from a geometry alignment\nperspective. Given a set of text prompts, we aim to generate a collection of\nobjects with semantically corresponding parts aligned across them. Recent\nmethods based on Score Distillation have succeeded in distilling the knowledge\nfrom 2D diffusion models to high-quality representations of the 3D objects.\nThese methods handle multiple text queries separately, and therefore the\nresulting objects have a high variability in object pose and structure.\nHowever, in some applications, such as 3D asset design, it may be desirable to\nobtain a set of objects aligned with each other. In order to achieve the\nalignment of the corresponding parts of the generated objects, we propose to\nembed these objects into a common latent space and optimize the continuous\ntransitions between these objects. We enforce two kinds of properties of these\ntransitions: smoothness of the transition and plausibility of the intermediate\nobjects along the transition. We demonstrate that both of these properties are\nessential for good alignment. We provide several practical scenarios that\nbenefit from alignment between the objects, including 3D editing and object\nhybridization, and experimentally demonstrate the effectiveness of our method.\nhttps://voyleg.github.io/a3d/\n","authors":["Savva Ignatyev","Nina Konovalova","Daniil Selikhanovych","Oleg Voynov","Nikolay Patakin","Ilya Olkov","Dmitry Senushkin","Alexey Artemov","Anton Konushin","Alexander Filippov","Peter Wonka","Evgeny Burnaev"],"pdf_url":"https://arxiv.org/pdf/2406.15020v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03394v2","updated":"2024-10-16T11:48:21Z","published":"2024-06-05T15:44:54Z","title":"Gaussian Primitives for Deformable Image Registration","summary":"  Deformable Image Registration (DIR) is essential for aligning medical images\nthat exhibit anatomical variations, facilitating applications such as disease\ntracking and radiotherapy planning. While classical iterative methods and deep\nlearning approaches have achieved success in DIR, they are often hindered by\ncomputational inefficiency or poor generalization. In this paper, we introduce\nGaussianDIR, a novel, case-specific optimization DIR method inspired by 3D\nGaussian splatting. In general, GaussianDIR represents image deformations using\na sparse set of mobile and flexible Gaussian primitives, each defined by a\ncenter position, covariance, and local rigid transformation. This compact and\nexplicit representation reduces noise and computational overhead while\nimproving interpretability. Furthermore, the movement of individual voxel is\nderived via blending the local rigid transformation of the neighboring Gaussian\nprimitives. By this, GaussianDIR captures both global smoothness and local\nrigidity as well as reduces the computational burden. To address varying levels\nof deformation complexity, GaussianDIR also integrates an adaptive density\ncontrol mechanism that dynamically adjusts the density of Gaussian primitives.\nAdditionally, we employ multi-scale Gaussian primitives to capture both coarse\nand fine deformations, reducing optimization to local minima. Experimental\nresults on brain MRI, lung CT, and cardiac MRI datasets demonstrate that\nGaussianDIR outperforms existing DIR methods in both accuracy and efficiency,\nhighlighting its potential for clinical applications. Finally, as a\ntraining-free approach, it challenges the stereotype that iterative methods are\ninherently slow and transcend the limitations of poor generalization.\n","authors":["Jihe Li","Xiang Liu","Fabian Zhang","Xia Li","Xixin Cao","Ye Zhang","Joachim Buhmann"],"pdf_url":"https://arxiv.org/pdf/2406.03394v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12474v1","updated":"2024-10-16T11:42:11Z","published":"2024-10-16T11:42:11Z","title":"Mind the Gap Between Prototypes and Images in Cross-domain Finetuning","summary":"  In cross-domain few-shot classification (CFC), recent works mainly focus on\nadapting a simple transformation head on top of a frozen pre-trained backbone\nwith few labeled data to project embeddings into a task-specific metric space\nwhere classification can be performed by measuring similarities between image\ninstance and prototype representations. Technically, an assumption implicitly\nadopted in such a framework is that the prototype and image instance embeddings\nshare the same representation transformation. However, in this paper, we find\nthat there naturally exists a gap, which resembles the modality gap, between\nthe prototype and image instance embeddings extracted from the frozen\npre-trained backbone, and simply applying the same transformation during the\nadaptation phase constrains exploring the optimal representations and shrinks\nthe gap between prototype and image representations. To solve this problem, we\npropose a simple yet effective method, contrastive prototype-image adaptation\n(CoPA), to adapt different transformations respectively for prototypes and\nimages similarly to CLIP by treating prototypes as text prompts. Extensive\nexperiments on Meta-Dataset demonstrate that CoPA achieves the state-of-the-art\nperformance more efficiently. Meanwhile, further analyses also indicate that\nCoPA can learn better representation clusters, enlarge the gap, and achieve\nminimal validation loss at the enlarged gap.\n","authors":["Hongduan Tian","Feng Liu","Zhanke Zhou","Tongliang Liu","Chengqi Zhang","Bo Han"],"pdf_url":"https://arxiv.org/pdf/2410.12474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06241v2","updated":"2024-10-16T11:39:54Z","published":"2024-10-08T17:56:33Z","title":"BroadWay: Boost Your Text-to-Video Generation Model in a Training-free\n  Way","summary":"  The text-to-video (T2V) generation models, offering convenient visual\ncreation, have recently garnered increasing attention. Despite their\nsubstantial potential, the generated videos may present artifacts, including\nstructural implausibility, temporal inconsistency, and a lack of motion, often\nresulting in near-static video. In this work, we have identified a correlation\nbetween the disparity of temporal attention maps across different blocks and\nthe occurrence of temporal inconsistencies. Additionally, we have observed that\nthe energy contained within the temporal attention maps is directly related to\nthe magnitude of motion amplitude in the generated videos. Based on these\nobservations, we present BroadWay, a training-free method to improve the\nquality of text-to-video generation without introducing additional parameters,\naugmenting memory or sampling time. Specifically, BroadWay is composed of two\nprincipal components: 1) Temporal Self-Guidance improves the structural\nplausibility and temporal consistency of generated videos by reducing the\ndisparity between the temporal attention maps across various decoder blocks. 2)\nFourier-based Motion Enhancement enhances the magnitude and richness of motion\nby amplifying the energy of the map. Extensive experiments demonstrate that\nBroadWay significantly improves the quality of text-to-video generation with\nnegligible additional cost.\n","authors":["Jiazi Bu","Pengyang Ling","Pan Zhang","Tong Wu","Xiaoyi Dong","Yuhang Zang","Yuhang Cao","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06241v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09408v2","updated":"2024-10-16T11:28:19Z","published":"2024-08-18T08:38:20Z","title":"VrdONE: One-stage Video Visual Relation Detection","summary":"  Video Visual Relation Detection (VidVRD) focuses on understanding how\nentities interact over time and space in videos, a key step for gaining deeper\ninsights into video scenes beyond basic visual tasks. Traditional methods for\nVidVRD, challenged by its complexity, typically split the task into two parts:\none for identifying what relation categories are present and another for\ndetermining their temporal boundaries. This split overlooks the inherent\nconnection between these elements. Addressing the need to recognize entity\npairs' spatiotemporal interactions across a range of durations, we propose\nVrdONE, a streamlined yet efficacious one-stage model. VrdONE combines the\nfeatures of subjects and objects, turning predicate detection into 1D instance\nsegmentation on their combined representations. This setup allows for both\nrelation category identification and binary mask generation in one go,\neliminating the need for extra steps like proposal generation or\npost-processing. VrdONE facilitates the interaction of features across various\nframes, adeptly capturing both short-lived and enduring relations.\nAdditionally, we introduce the Subject-Object Synergy (SOS) module, enhancing\nhow subjects and objects perceive each other before combining. VrdONE achieves\nstate-of-the-art performances on the VidOR benchmark and ImageNet-VidVRD,\nshowcasing its superior capability in discerning relations across different\ntemporal scales. The code is available at https://github.com/lucaspk512/vrdone.\n","authors":["Xinjie Jiang","Chenxi Zheng","Xuemiao Xu","Bangzhen Liu","Weiying Zheng","Huaidong Zhang","Shengfeng He"],"pdf_url":"https://arxiv.org/pdf/2408.09408v2.pdf","comment":"12 pages, 8 figures, accepted by ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2410.11190v2","updated":"2024-10-16T11:19:56Z","published":"2024-10-15T02:10:45Z","title":"Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex\n  Capabilities","summary":"  GPT-4o, an all-encompassing model, represents a milestone in the development\nof large multi-modal language models. It can understand visual, auditory, and\ntextual modalities, directly output audio, and support flexible duplex\ninteraction. Models from the open-source community often achieve some\nfunctionalities of GPT-4o, such as visual understanding and voice chat.\nNevertheless, training a unified model that incorporates all modalities is\nchallenging due to the complexities of multi-modal data, intricate model\narchitectures, and training processes. In this paper, we introduce Mini-Omni2,\na visual-audio assistant capable of providing real-time, end-to-end voice\nresponses to visoin and audio queries. By integrating pretrained visual and\nauditory encoders, Mini-Omni2 maintains performance in individual modalities.\nWe propose a three-stage training process to align modalities, allowing the\nlanguage model to handle multi-modal inputs and outputs after training on a\nlimited dataset. For interaction, we introduce a command-based interruption\nmechanism, enabling more flexible interaction with users. To the best of our\nknowledge, Mini-Omni2 is one of the closest reproductions of GPT-4o, which have\nsimilar form of functionality, and we hope it can offer valuable insights for\nsubsequent research.\n","authors":["Zhifei Xie","Changqiao Wu"],"pdf_url":"https://arxiv.org/pdf/2410.11190v2.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.00355v2","updated":"2024-10-16T10:45:59Z","published":"2024-08-01T07:52:07Z","title":"DNTextSpotter: Arbitrary-Shaped Scene Text Spotting via Improved\n  Denoising Training","summary":"  More and more end-to-end text spotting methods based on Transformer\narchitecture have demonstrated superior performance. These methods utilize a\nbipartite graph matching algorithm to perform one-to-one optimal matching\nbetween predicted objects and actual objects. However, the instability of\nbipartite graph matching can lead to inconsistent optimization targets, thereby\naffecting the training performance of the model. Existing literature applies\ndenoising training to solve the problem of bipartite graph matching instability\nin object detection tasks. Unfortunately, this denoising training method cannot\nbe directly applied to text spotting tasks, as these tasks need to perform\nirregular shape detection tasks and more complex text recognition tasks than\nclassification. To address this issue, we propose a novel denoising training\nmethod (DNTextSpotter) for arbitrary-shaped text spotting. Specifically, we\ndecompose the queries of the denoising part into noised positional queries and\nnoised content queries. We use the four Bezier control points of the Bezier\ncenter curve to generate the noised positional queries. For the noised content\nqueries, considering that the output of the text in a fixed positional order is\nnot conducive to aligning position with content, we employ a masked character\nsliding method to initialize noised content queries, thereby assisting in the\nalignment of text content and position. To improve the model's perception of\nthe background, we further utilize an additional loss function for background\ncharacters classification in the denoising training part.Although DNTextSpotter\nis conceptually simple, it outperforms the state-of-the-art methods on four\nbenchmarks (Total-Text, SCUT-CTW1500, ICDAR15, and Inverse-Text), especially\nyielding an improvement of 11.3% against the best approach in Inverse-Text\ndataset.\n","authors":["Yu Xie","Qian Qiao","Jun Gao","Tianxiang Wu","Jiaqing Fan","Yue Zhang","Jielei Zhang","Huyang Sun"],"pdf_url":"https://arxiv.org/pdf/2408.00355v2.pdf","comment":"Accepted by ACM'MM2024"},{"id":"http://arxiv.org/abs/2410.12441v1","updated":"2024-10-16T10:36:29Z","published":"2024-10-16T10:36:29Z","title":"A Primal-dual algorithm for image reconstruction with ICNNs","summary":"  We address the optimization problem in a data-driven variational\nreconstruction framework, where the regularizer is parameterized by an\ninput-convex neural network (ICNN). While gradient-based methods are commonly\nused to solve such problems, they struggle to effectively handle non-smoothness\nwhich often leads to slow convergence. Moreover, the nested structure of the\nneural network complicates the application of standard non-smooth optimization\ntechniques, such as proximal algorithms. To overcome these challenges, we\nreformulate the problem and eliminate the network's nested structure. By\nrelating this reformulation to epigraphical projections of the activation\nfunctions, we transform the problem into a convex optimization problem that can\nbe efficiently solved using a primal-dual algorithm. We also prove that this\nreformulation is equivalent to the original variational problem. Through\nexperiments on several imaging tasks, we demonstrate that the proposed approach\noutperforms subgradient methods in terms of both speed and stability.\n","authors":["Hok Shing Wong","Matthias J. Ehrhardt","Subhadip Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2410.12441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00275v2","updated":"2024-10-16T10:27:14Z","published":"2024-09-30T23:04:55Z","title":"On Large Uni- and Multi-modal Models for Unsupervised Classification of\n  Social Media Images: Nature's Contribution to People as a case study","summary":"  Social media images have proven to be a valuable source of information for\nunderstanding human interactions with important subjects such as cultural\nheritage, biodiversity, and nature, among others. The task of grouping such\nimages into a number of semantically meaningful clusters without labels is\nchallenging due to the high diversity and complex nature of the visual content\nin addition to their large volume. On the other hand, recent advances in Large\nVisual Models (LVMs), Large Language Models (LLMs), and Large Visual Language\nModels (LVLMs) provide an important opportunity to explore new productive and\nscalable solutions. This work proposes, analyzes, and compares various\napproaches based on one or more state-of-the-art LVM, LLM, and LVLM, for\nmapping social media images into a number of predefined classes. As a case\nstudy, we consider the problem of understanding the interactions between humans\nand nature, also known as Nature's Contribution to People or Cultural Ecosystem\nServices (CES). Our experiments show that the highest-performing approaches,\nwith accuracy above 95%, still require the creation of a small labeled dataset.\nThese include the fine-tuned LVM DINOv2 and the LVLM LLaVA-1.5 combined with a\nfine-tuned LLM. The top fully unsupervised approaches, achieving accuracy above\n84%, are the LVLMs, specifically the proprietary GPT-4 model and the public\nLLaVA-1.5 model. Additionally, the LVM DINOv2, when applied in a 10-shot\nlearning setup, delivered competitive results with an accuracy of 83.99%,\nclosely matching the performance of the LVLM LLaVA-1.5.\n","authors":["Rohaifa Khaldi","Domingo Alcaraz-Segura","Ignacio Sánchez-Herrera","Javier Martinez-Lopez","Carlos Javier Navarro","Siham Tabik"],"pdf_url":"https://arxiv.org/pdf/2410.00275v2.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2405.17257v2","updated":"2024-10-16T10:08:50Z","published":"2024-05-27T15:14:47Z","title":"Topological reconstruction of sampled surfaces via Morse theory","summary":"  In this work, we study the perception problem for sampled surfaces (possibly\nwith boundary) using tools from computational topology, specifically, how to\nidentify their underlying topology starting from point-cloud samples in space,\nsuch as those obtained with 3D scanners. We present a reconstruction algorithm\nbased on a careful topological study of the point sample that allows us to\nobtain a cellular decomposition of it using a Morse function. No triangulation\nor local implicit equations are used as intermediate steps, avoiding in this\nway reconstruction-induced artifices. The algorithm can be run without any\nprior knowledge of the surface topology, density or regularity of the\npoint-sample. The results consist of a piece-wise decomposition of the given\nsurface as a union of Morse cells (i.e. topological disks), suitable for tasks\nsuch as mesh-independent reparametrization or noise-filtering, and a small-rank\ncellular complex determining the topology of the surface. The algorithm, which\nwe test with several real and synthetic surfaces, can be applied to smooth\nsurfaces with or without boundary, embedded in an ambient space of any\ndimension.\n","authors":["Franco Coltraro","Jaume Amorós","Maria Alberich-Carramiñana","Carme Torras"],"pdf_url":"https://arxiv.org/pdf/2405.17257v2.pdf","comment":"39 pages, 17 figures, 1 table, 1 algorithm, 1 appendix"},{"id":"http://arxiv.org/abs/2410.12419v1","updated":"2024-10-16T10:04:22Z","published":"2024-10-16T10:04:22Z","title":"Attention-Guided Perturbation for Consistency Regularization in\n  Semi-Supervised Medical Image Segmentation","summary":"  Medical image segmentation is a pivotal step in diagnostic and therapeutic\nprocesses. However, the acquisition of high-quality annotated data is often\nconstrained by scarcity and cost. Semi-supervised learning offers a promising\napproach to enhance model performance by using unlabeled data. While\nconsistency regularization is a prevalent method in semi-supervised image\nsegmentation, there is a dearth of research on perturbation strategies tailored\nfor semi-supervised medical image segmentation tasks. This paper introduces an\nattention-guided perturbation strategy for semi-supervised consistency\nregularization in the context of medical image segmentation. We add the\nperturbation based on the attention from the model in the image and feature\nlevel to achieve consistency regularization. The method is adept at\naccommodating the intricate structures and high-dimensional semantics inherent\nin medical images, thereby enhancing the performance of semi-supervised\nsegmentation tasks. Our method achieved state-of-the-art results on benchmark\ndatasets, including a 90.4\\% Dice score on the ACDC dataset in the 7-case\nscenario.\n","authors":["Yuxuan Cheng","Chenxi Shao","Jie Ma","Guoliang Li"],"pdf_url":"https://arxiv.org/pdf/2410.12419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11288v2","updated":"2024-10-16T09:59:17Z","published":"2024-06-17T07:51:44Z","title":"MFC-Bench: Benchmarking Multimodal Fact-Checking with Large\n  Vision-Language Models","summary":"  Large vision-language models (LVLMs) have significantly improved multimodal\nreasoning tasks, such as visual question answering and image captioning. These\nmodels embed multimodal facts within their parameters, rather than relying on\nexternal knowledge bases to store factual information explicitly. However, the\ncontent discerned by LVLMs may deviate from actual facts due to inherent bias\nor incorrect inference. To address this issue, we introduce MFC-Bench, a\nrigorous and comprehensive benchmark designed to evaluate the factual accuracy\nof LVLMs across three stages of verdict prediction for MFC: Manipulation,\nOut-of-Context, and Veracity Classification. Through our evaluation on\nMFC-Bench, we benchmarked a dozen diverse and representative LVLMs, uncovering\nthat current models still fall short in multimodal fact-checking and\ndemonstrate insensitivity to various forms of manipulated content. We hope that\nMFC-Bench could raise attention to the trustworthy AI potentially assisted by\nLVLMs in the future. The MFC-Bench and accompanying resources are publicly\naccessible at https://github.com/wskbest/MFC-Bench, contributing to ongoing\nresearch in the multimodal fact-checking field.\n","authors":["Shengkang Wang","Hongzhan Lin","Ziyang Luo","Zhen Ye","Guang Chen","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2406.11288v2.pdf","comment":"28 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.12414v1","updated":"2024-10-16T09:59:11Z","published":"2024-10-16T09:59:11Z","title":"Triplet: Triangle Patchlet for Mesh-Based Inverse Rendering and Scene\n  Parameters Approximation","summary":"  Recent advancements in Radiance Fields have significantly improved novel-view\nsynthesis. However, in many real-world applications, the more advanced\nchallenge lies in inverse rendering, which seeks to derive the physical\nproperties of a scene, including light, geometry, textures, and materials.\nMeshes, as a traditional representation adopted by many simulation pipeline,\nhowever, still show limited influence in radiance field for inverse rendering.\nThis paper introduces a novel framework called Triangle Patchlet (abbr.\nTriplet), a mesh-based representation, to comprehensively approximate these\nscene parameters. We begin by assembling Triplets with either randomly\ngenerated points or sparse points obtained from camera calibration where all\nfaces are treated as an independent element. Next, we simulate the physical\ninteraction of light and optimize the scene parameters using traditional\ngraphics rendering techniques like rasterization and ray tracing, accompanying\nwith density control and propagation. An iterative mesh extracting process is\nalso suggested, where we continue to optimize on geometry and materials with\ngraph-based operation. We also introduce several regulation terms to enable\nbetter generalization of materials property. Our framework could precisely\nestimate the light, materials and geometry with mesh without prior of light,\nmaterials and geometry in a unified framework. Experiments demonstrate that our\napproach can achieve state-of-the-art visual quality while reconstructing\nhigh-quality geometry and accurate material properties.\n","authors":["Jiajie Yang"],"pdf_url":"https://arxiv.org/pdf/2410.12414v1.pdf","comment":"https://github.com/RANDO11199/Triplet"},{"id":"http://arxiv.org/abs/2410.12411v1","updated":"2024-10-16T09:52:38Z","published":"2024-10-16T09:52:38Z","title":"AdaCropFollow: Self-Supervised Online Adaptation for Visual Under-Canopy\n  Navigation","summary":"  Under-canopy agricultural robots can enable various applications like precise\nmonitoring, spraying, weeding, and plant manipulation tasks throughout the\ngrowing season. Autonomous navigation under the canopy is challenging due to\nthe degradation in accuracy of RTK-GPS and the large variability in the visual\nappearance of the scene over time. In prior work, we developed a supervised\nlearning-based perception system with semantic keypoint representation and\ndeployed this in various field conditions. A large number of failures of this\nsystem can be attributed to the inability of the perception model to adapt to\nthe domain shift encountered during deployment. In this paper, we propose a\nself-supervised online adaptation method for adapting the semantic keypoint\nrepresentation using a visual foundational model, geometric prior, and pseudo\nlabeling. Our preliminary experiments show that with minimal data and\nfine-tuning of parameters, the keypoint prediction model trained with labels on\nthe source domain can be adapted in a self-supervised manner to various\nchallenging target domains onboard the robot computer using our method. This\ncan enable fully autonomous row-following capability in under-canopy robots\nacross fields and crops without requiring human intervention.\n","authors":["Arun N. Sivakumar","Federico Magistri","Mateus V. Gasparino","Jens Behley","Cyrill Stachniss","Girish Chowdhary"],"pdf_url":"https://arxiv.org/pdf/2410.12411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10441v2","updated":"2024-10-16T09:45:06Z","published":"2024-10-14T12:35:12Z","title":"Free Video-LLM: Prompt-guided Visual Perception for Efficient\n  Training-free Video LLMs","summary":"  Vision-language large models have achieved remarkable success in various\nmulti-modal tasks, yet applying them to video understanding remains challenging\ndue to the inherent complexity and computational demands of video data. While\ntraining-based video-LLMs deliver high performance, they often require\nsubstantial resources for training and inference. Conversely, training-free\napproaches offer a more efficient alternative by adapting pre-trained\nimage-LLMs models for video tasks without additional training, but they face\ninference efficiency bottlenecks due to the large number of visual tokens\ngenerated from video frames. In this work, we present a novel prompt-guided\nvisual perception framework (abbreviated as Free Video-LLM) for efficient\ninference of training-free video LLMs. The proposed framework decouples\nspatial-temporal dimension and performs temporal frame sampling and spatial RoI\ncropping respectively based on task-specific prompts. Our method effectively\nreduces the number of visual tokens while maintaining high performance across\nmultiple video question-answering benchmarks. Extensive experiments demonstrate\nthat our approach achieves competitive results with significantly fewer tokens,\noffering an optimal trade-off between accuracy and computational efficiency\ncompared to state-of-the-art video LLMs. The code will be available at\nhttps://github.com/contrastive/FreeVideoLLM.\n","authors":["Kai Han","Jianyuan Guo","Yehui Tang","Wei He","Enhua Wu","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2410.10441v2.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2410.12407v1","updated":"2024-10-16T09:42:29Z","published":"2024-10-16T09:42:29Z","title":"Beyond Coarse-Grained Matching in Video-Text Retrieval","summary":"  Video-text retrieval has seen significant advancements, yet the ability of\nmodels to discern subtle differences in captions still requires verification.\nIn this paper, we introduce a new approach for fine-grained evaluation. Our\napproach can be applied to existing datasets by automatically generating hard\nnegative test captions with subtle single-word variations across nouns, verbs,\nadjectives, adverbs, and prepositions. We perform comprehensive experiments\nusing four state-of-the-art models across two standard benchmarks (MSR-VTT and\nVATEX) and two specially curated datasets enriched with detailed descriptions\n(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our\nanalyses show that the current evaluation benchmarks fall short in detecting a\nmodel's ability to perceive subtle single-word differences, 2) our fine-grained\nevaluation highlights the difficulty models face in distinguishing such subtle\nvariations. To enhance fine-grained understanding, we propose a new baseline\nthat can be easily combined with current methods. Experiments on our\nfine-grained evaluations demonstrate that this approach enhances a model's\nability to understand fine-grained differences.\n","authors":["Aozhu Chen","Hazel Doughty","Xirong Li","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12407v1.pdf","comment":"Accepted to ACCV 2024"},{"id":"http://arxiv.org/abs/2407.08374v3","updated":"2024-10-16T09:37:14Z","published":"2024-07-11T10:35:53Z","title":"Enhancing Robustness of Vision-Language Models through Orthogonality\n  Learning and Self-Regularization","summary":"  Efficient fine-tuning of vision-language models (VLMs) like CLIP for specific\ndownstream tasks is gaining significant attention. Previous works primarily\nfocus on prompt learning to adapt the CLIP into a variety of downstream tasks,\nhowever, suffering from task overfitting when fine-tuned on a small data set.\nIn this paper, we introduce an orthogonal fine-tuning method for efficiently\nfine-tuning pretrained weights and enabling enhanced robustness and\ngeneralization, while a self-regularization strategy is further exploited to\nmaintain the stability in terms of zero-shot generalization of VLMs, dubbed\nOrthSR. Specifically, trainable orthogonal matrices are injected seamlessly\ninto the transformer architecture and enforced with orthogonality constraint\nduring the training, benefiting from the norm-preserving property and thus\nleading to stable and faster convergence, while keeping the pre-trained weights\nfrozen. To alleviate deviation from fine-tuning, a self-regularization strategy\nis further employed to retain the generalization of the model during the\ntraining within a bypass manner. In addition, to enrich the sample diversity\nfor downstream tasks under the small dataset scenario, we first explore\nattentive CutOut data augmentation to boost the efficient fine-tuning, leading\nto better model fitting capacity for specific downstream task. Then we support\nthe theoretical analysis on how our approach improves the specific downstream\nperformance and maintains the generalizability. For the first time, we revisit\nthe CLIP and CoOp with our method to effectively improve the model on few-shot\nimage classficiation scenario on par with the elaborated prompt learning\nmethods.\n","authors":["Jinlong Li","Dong Zhao","Zequn Jie","Elisa Ricci","Lin Ma","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2407.08374v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.01472v2","updated":"2024-10-16T09:37:11Z","published":"2023-08-02T23:39:29Z","title":"Reverse Stable Diffusion: What prompt was used to generate this image?","summary":"  Text-to-image diffusion models have recently attracted the interest of many\nresearchers, and inverting the diffusion process can play an important role in\nbetter understanding the generative process and how to engineer prompts in\norder to obtain the desired images. To this end, we study the task of\npredicting the prompt embedding given an image generated by a generative\ndiffusion model. We consider a series of white-box and black-box models (with\nand without access to the weights of the diffusion network) to deal with the\nproposed task. We propose a novel learning framework comprising a joint prompt\nregression and multi-label vocabulary classification objective that generates\nimproved prompts. To further improve our method, we employ a curriculum\nlearning procedure that promotes the learning of image-prompt pairs with lower\nlabeling noise (i.e. that are better aligned). We conduct experiments on the\nDiffusionDB data set, predicting text prompts from images generated by Stable\nDiffusion. In addition, we make an interesting discovery: training a diffusion\nmodel on the prompt generation task can make the model generate images that are\nmuch better aligned with the input prompts, when the model is directly reused\nfor text-to-image generation. Our code is publicly available for download at\nhttps://github.com/CroitoruAlin/Reverse-Stable-Diffusion.\n","authors":["Florinel-Alin Croitoru","Vlad Hondru","Radu Tudor Ionescu","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2308.01472v2.pdf","comment":"Accepted for publication in Computer Vision and Image Understanding"},{"id":"http://arxiv.org/abs/2410.12402v1","updated":"2024-10-16T09:31:24Z","published":"2024-10-16T09:31:24Z","title":"De-Identification of Medical Imaging Data: A Comprehensive Tool for\n  Ensuring Patient Privacy","summary":"  Medical data employed in research frequently comprises sensitive patient\nhealth information (PHI), which is subject to rigorous legal frameworks such as\nthe General Data Protection Regulation (GDPR) or the Health Insurance\nPortability and Accountability Act (HIPAA). Consequently, these types of data\nmust be pseudonymized prior to utilisation, which presents a significant\nchallenge for many researchers. Given the vast array of medical data, it is\nnecessary to employ a variety of de-identification techniques. To facilitate\nthe anonymization process for medical imaging data, we have developed an\nopen-source tool that can be used to de-identify DICOM magnetic resonance\nimages, computer tomography images, whole slide images and magnetic resonance\ntwix raw data. Furthermore, the implementation of a neural network enables the\nremoval of text within the images. The proposed tool automates an elaborate\nanonymization pipeline for multiple types of inputs, reducing the need for\nadditional tools used for de-identification of imaging data. We make our code\npublicly available at\nhttps://github.com/code-lukas/medical_image_deidentification.\n","authors":["Moritz Rempe","Lukas Heine","Constantin Seibold","Fabian Hörst","Jens Kleesiek"],"pdf_url":"https://arxiv.org/pdf/2410.12402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19783v2","updated":"2024-10-16T09:28:22Z","published":"2024-05-30T07:48:32Z","title":"Instruction-Guided Visual Masking","summary":"  Instruction following is crucial in contemporary LLM. However, when extended\nto multimodal setting, it often suffers from misalignment between specific\ntextual instruction and targeted local region of an image. To achieve more\naccurate and nuanced multimodal instruction following, we introduce\nInstruction-guided Visual Masking (IVM), a new versatile visual grounding model\nthat is compatible with diverse multimodal models, such as LMM and robot model.\nBy constructing visual masks for instruction-irrelevant regions, IVM-enhanced\nmultimodal models can effectively focus on task-relevant image regions to\nbetter align with complex instructions. Specifically, we design a visual\nmasking data generation pipeline and create an IVM-Mix-1M dataset with 1\nmillion image-instruction pairs. We further introduce a new learning technique,\nDiscriminator Weighted Supervised Learning (DWSL) for preferential IVM training\nthat prioritizes high-quality data samples. Experimental results on generic\nmultimodal tasks such as VQA and embodied robotic control demonstrate the\nversatility of IVM, which as a plug-and-play tool, significantly boosts the\nperformance of diverse multimodal models, yielding new state-of-the-art results\nacross challenging multimodal benchmarks. Code, model and data are available at\nhttps://github.com/2toinf/IVM.\n","authors":["Jinliang Zheng","Jianxiong Li","Sijie Cheng","Yinan Zheng","Jiaming Li","Jihao Liu","Yu Liu","Jingjing Liu","Xianyuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2405.19783v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.12396v1","updated":"2024-10-16T09:25:11Z","published":"2024-10-16T09:25:11Z","title":"Feature Augmentation for Self-supervised Contrastive Learning: A Closer\n  Look","summary":"  Self-supervised contrastive learning heavily relies on the view variance\nbrought by data augmentation, so that it can learn a view-invariant pre-trained\nrepresentation. Beyond increasing the view variance for contrast, this work\nfocuses on improving the diversity of training data, to improve the\ngeneralization and robustness of the pre-trained models. To this end, we\npropose a unified framework to conduct data augmentation in the feature space,\nknown as feature augmentation. This strategy is domain-agnostic, which augments\nsimilar features to the original ones and thus improves the data diversity. We\nperform a systematic investigation of various feature augmentation\narchitectures, the gradient-flow skill, and the relationship between feature\naugmentation and traditional data augmentation. Our study reveals some\npractical principles for feature augmentation in self-contrastive learning. By\nintegrating feature augmentation on the instance discrimination or the instance\nsimilarity paradigm, we consistently improve the performance of pre-trained\nfeature learning and gain better generalization over the downstream image\nclassification and object detection task.\n","authors":["Yong Zhang","Rui Zhu","Shifeng Zhang","Xu Zhou","Shifeng Chen","Xiaofan Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12396v1.pdf","comment":"IJCNN 2024"},{"id":"http://arxiv.org/abs/2410.12394v1","updated":"2024-10-16T09:23:02Z","published":"2024-10-16T09:23:02Z","title":"Real-time Stereo-based 3D Object Detection for Streaming Perception","summary":"  The ability to promptly respond to environmental changes is crucial for the\nperception system of autonomous driving. Recently, a new task called streaming\nperception was proposed. It jointly evaluate the latency and accuracy into a\nsingle metric for video online perception. In this work, we introduce\nStreamDSGN, the first real-time stereo-based 3D object detection framework\ndesigned for streaming perception. StreamDSGN is an end-to-end framework that\ndirectly predicts the 3D properties of objects in the next moment by leveraging\nhistorical information, thereby alleviating the accuracy degradation of\nstreaming perception. Further, StreamDSGN applies three strategies to enhance\nthe perception accuracy: (1) A feature-flow-based fusion method, which\ngenerates a pseudo-next feature at the current moment to address the\nmisalignment issue between feature and ground truth. (2) An extra regression\nloss for explicit supervision of object motion consistency in consecutive\nframes. (3) A large kernel backbone with a large receptive field for\neffectively capturing long-range spatial contextual features caused by changes\nin object positions. Experiments on the KITTI Tracking dataset show that,\ncompared with the strong baseline, StreamDSGN significantly improves the\nstreaming average precision by up to 4.33%. Our code is available at\nhttps://github.com/weiyangdaren/streamDSGN-pytorch.\n","authors":["Changcai Li","Zonghua Gu","Gang Chen","Libo Huang","Wei Zhang","Huihui Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.12394v1.pdf","comment":"Streaming Perception, 3D Object Detection, NeurIPS2024 poster"},{"id":"http://arxiv.org/abs/2410.12381v1","updated":"2024-10-16T09:04:57Z","published":"2024-10-16T09:04:57Z","title":"HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of\n  Large Multimodal Models Through Coding Tasks","summary":"  Coding tasks have been valuable for evaluating Large Language Models (LLMs),\nas they demand the comprehension of high-level instructions, complex reasoning,\nand the implementation of functional programs -- core capabilities for\nadvancing Artificial General Intelligence. Despite the progress in Large\nMultimodal Models (LMMs), which extend LLMs with visual perception and\nunderstanding capabilities, there remains a notable lack of coding benchmarks\nthat rigorously assess these models, particularly in tasks that emphasize\nvisual reasoning. To address this gap, we introduce HumanEval-V, a novel and\nlightweight benchmark specifically designed to evaluate LMMs' visual\nunderstanding and reasoning capabilities through code generation. HumanEval-V\nincludes 108 carefully crafted, entry-level Python coding tasks derived from\nplatforms like CodeForces and Stack Overflow. Each task is adapted by modifying\nthe context and algorithmic patterns of the original problems, with visual\nelements redrawn to ensure distinction from the source, preventing potential\ndata leakage. LMMs are required to complete the code solution based on the\nprovided visual context and a predefined Python function signature outlining\nthe task requirements. Every task is equipped with meticulously handcrafted\ntest cases to ensure a thorough and reliable evaluation of model-generated\nsolutions. We evaluate 19 state-of-the-art LMMs using HumanEval-V, uncovering\nsignificant challenges. Proprietary models like GPT-4o achieve only 13% pass@1\nand 36.4% pass@10, while open-weight models with 70B parameters score below 4%\npass@1. Ablation studies further reveal the limitations of current LMMs in\nvision reasoning and coding capabilities. These results underscore key areas\nfor future research to enhance LMMs' capabilities. We have open-sourced our\ncode and benchmark at https://github.com/HumanEval-V/HumanEval-V-Benchmark.\n","authors":["Fengji Zhang","Linquan Wu","Huiyu Bai","Guancheng Lin","Xiao Li","Xiao Yu","Yue Wang","Bei Chen","Jacky Keung"],"pdf_url":"https://arxiv.org/pdf/2410.12381v1.pdf","comment":"homepage https://humaneval-v.github.io/"},{"id":"http://arxiv.org/abs/2410.12379v1","updated":"2024-10-16T08:55:09Z","published":"2024-10-16T08:55:09Z","title":"Stylistic Multi-Task Analysis of Ukiyo-e Woodblock Prints","summary":"  In this work we present a large-scale dataset of \\textit{Ukiyo-e} woodblock\nprints. Unlike previous works and datasets in the artistic domain that\nprimarily focus on western art, this paper explores this pre-modern Japanese\nart form with the aim of broadening the scope for stylistic analysis and to\nprovide a benchmark to evaluate a variety of art focused Computer Vision\napproaches. Our dataset consists of over $175.000$ prints with corresponding\nmetadata (\\eg artist, era, and creation date) from the 17th century to present\nday. By approaching stylistic analysis as a Multi-Task problem we aim to more\nefficiently utilize the available metadata, and learn more general\nrepresentations of style. We show results for well-known baselines and\nstate-of-the-art multi-task learning frameworks to enable future comparison,\nand to encourage stylistic analysis on this artistic domain.\n","authors":["Selina Khan","Nanne van Noord"],"pdf_url":"https://arxiv.org/pdf/2410.12379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07914v3","updated":"2024-10-16T08:52:42Z","published":"2024-09-12T10:30:44Z","title":"InterACT: Inter-dependency Aware Action Chunking with Hierarchical\n  Attention Transformers for Bimanual Manipulation","summary":"  Bimanual manipulation presents unique challenges compared to unimanual tasks\ndue to the complexity of coordinating two robotic arms. In this paper, we\nintroduce InterACT: Inter-dependency aware Action Chunking with Hierarchical\nAttention Transformers, a novel imitation learning framework designed\nspecifically for bimanual manipulation. InterACT leverages hierarchical\nattention mechanisms to effectively capture inter-dependencies between dual-arm\njoint states and visual inputs. The framework comprises a Hierarchical\nAttention Encoder, which processes multi-modal inputs through segment-wise and\ncross-segment attention mechanisms, and a Multi-arm Decoder that generates each\narm's action predictions in parallel, while sharing information between the\narms through synchronization blocks by providing the other arm's intermediate\noutput as context. Our experiments, conducted on various simulated and\nreal-world bimanual manipulation tasks, demonstrate that InterACT outperforms\nexisting methods. Detailed ablation studies further validate the significance\nof key components, including the impact of CLS tokens, cross-segment encoders,\nand synchronization blocks on task performance. We provide supplementary\nmaterials and videos on our project page.\n","authors":["Andrew Lee","Ian Chuang","Ling-Yuan Chen","Iman Soltani"],"pdf_url":"https://arxiv.org/pdf/2409.07914v3.pdf","comment":"Accepted at Conference on Robot Learning (CoRL) 2024"},{"id":"http://arxiv.org/abs/2410.12372v1","updated":"2024-10-16T08:44:23Z","published":"2024-10-16T08:44:23Z","title":"GAN Based Top-Down View Synthesis in Reinforcement Learning Environments","summary":"  Human actions are based on the mental perception of the environment. Even\nwhen all the aspects of an environment are not visible, humans have an internal\nmental model that can generalize the partially visible scenes to fully\nconstructed and connected views. This internal mental model uses learned\nabstract representations of spatial and temporal aspects of the environments\nencountered in the past.\n  Artificial agents in reinforcement learning environments also benefit by\nlearning a representation of the environment from experience. It provides the\nagent with viewpoints that are not directly visible to it, helping it make\nbetter policy decisions. It can also be used to predict the future state of the\nenvironment.\n  This project explores learning the top-down view of an RL environment based\non the artificial agent's first-person view observations with a generative\nadversarial network(GAN). The top-down view is useful as it provides a complete\noverview of the environment by building a map of the entire environment. It\nprovides information about the objects' dimensions and shapes along with their\nrelative positions with one another. Initially, when only a partial observation\nof the environment is visible to the agent, only a partial top-down view is\ngenerated. As the agent explores the environment through a set of actions, the\ngenerated top-down view becomes complete. This generated top-down view can\nassist the agent in deducing better policy decisions. The focus of the project\nis to learn the top-down view of an RL environment. It doesn't deal with any\nReinforcement Learning task.\n","authors":["Usama Younus","Vinoj Jayasundara","Shivam Mishra","Suleyman Aslan"],"pdf_url":"https://arxiv.org/pdf/2410.12372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12369v1","updated":"2024-10-16T08:41:19Z","published":"2024-10-16T08:41:19Z","title":"Context-Infused Visual Grounding for Art","summary":"  Many artwork collections contain textual attributes that provide rich and\ncontextualised descriptions of artworks. Visual grounding offers the potential\nfor localising subjects within these descriptions on images, however, existing\napproaches are trained on natural images and generalise poorly to art. In this\npaper, we present CIGAr (Context-Infused GroundingDINO for Art), a visual\ngrounding approach which utilises the artwork descriptions during training as\ncontext, thereby enabling visual grounding on art. In addition, we present a\nnew dataset, Ukiyo-eVG, with manually annotated phrase-grounding annotations,\nand we set a new state-of-the-art for object detection on two artwork datasets.\n","authors":["Selina Khan","Nanne van Noord"],"pdf_url":"https://arxiv.org/pdf/2410.12369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08401v2","updated":"2024-10-16T08:38:06Z","published":"2024-04-12T11:15:15Z","title":"No Bells, Just Whistles: Sports Field Registration by Leveraging\n  Geometric Properties","summary":"  Broadcast sports field registration is traditionally addressed as a\nhomography estimation task, mapping the visible image area to a planar field\nmodel, predominantly focusing on the main camera shot. Addressing the\nshortcomings of previous approaches, we propose a novel calibration pipeline\nenabling camera calibration using a 3D soccer field model and extending the\nprocess to assess the multiple-view nature of broadcast videos. Our approach\nbegins with a keypoint generation pipeline derived from SoccerNet dataset\nannotations, leveraging the geometric properties of the court. Subsequently, we\nexecute classical camera calibration through DLT algorithm in a minimalist\nfashion, without further refinement. Through extensive experimentation on\nreal-world soccer broadcast datasets such as SoccerNet-Calibration, WorldCup\n2014 and TS- WorldCup, our method demonstrates superior performance in both\nmultiple- and single-view 3D camera calibration while maintaining competitive\nresults in homography estimation compared to state-of-the-art techniques.\n","authors":["Marc Gutiérrez-Pérez","Antonio Agudo"],"pdf_url":"https://arxiv.org/pdf/2404.08401v2.pdf","comment":"Accepted in CVPRW 2024"},{"id":"http://arxiv.org/abs/2409.03200v2","updated":"2024-10-16T08:36:17Z","published":"2024-09-05T02:46:36Z","title":"Active Fake: DeepFake Camouflage","summary":"  DeepFake technology has gained significant attention due to its ability to\nmanipulate facial attributes with high realism, raising serious societal\nconcerns. Face-Swap DeepFake is the most harmful among these techniques, which\nfabricates behaviors by swapping original faces with synthesized ones. Existing\nforensic methods, primarily based on Deep Neural Networks (DNNs), effectively\nexpose these manipulations and have become important authenticity indicators.\nHowever, these methods mainly concentrate on capturing the blending\ninconsistency in DeepFake faces, raising a new security issue, termed Active\nFake, emerges when individuals intentionally create blending inconsistency in\ntheir authentic videos to evade responsibility. This tactic is called DeepFake\nCamouflage. To achieve this, we introduce a new framework for creating DeepFake\ncamouflage that generates blending inconsistencies while ensuring\nimperceptibility, effectiveness, and transferability. This framework, optimized\nvia an adversarial learning strategy, crafts imperceptible yet effective\ninconsistencies to mislead forensic detectors. Extensive experiments\ndemonstrate the effectiveness and robustness of our method, highlighting the\nneed for further research in active fake detection.\n","authors":["Pu Sun","Honggang Qi","Yuezun Li"],"pdf_url":"https://arxiv.org/pdf/2409.03200v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10394v2","updated":"2024-10-16T08:20:44Z","published":"2024-10-14T11:30:18Z","title":"PIVOT-R: Primitive-Driven Waypoint-Aware World Model for Robotic\n  Manipulation","summary":"  Language-guided robotic manipulation is a challenging task that requires an\nembodied agent to follow abstract user instructions to accomplish various\ncomplex manipulation tasks. Previous work trivially fitting the data without\nrevealing the relation between instruction and low-level executable actions,\nthese models are prone to memorizing the surficial pattern of the data instead\nof acquiring the transferable knowledge, and thus are fragile to dynamic\nenvironment changes. To address this issue, we propose a PrIrmitive-driVen\nwaypOinT-aware world model for Robotic manipulation (PIVOT-R) that focuses\nsolely on the prediction of task-relevant waypoints. Specifically, PIVOT-R\nconsists of a Waypoint-aware World Model (WAWM) and a lightweight action\nprediction module. The former performs primitive action parsing and\nprimitive-driven waypoint prediction, while the latter focuses on decoding\nlow-level actions. Additionally, we also design an asynchronous hierarchical\nexecutor (AHE), which can use different execution frequencies for different\nmodules of the model, thereby helping the model reduce computational redundancy\nand improve model execution efficiency. Our PIVOT-R outperforms\nstate-of-the-art (SoTA) open-source models on the SeaWave benchmark, achieving\nan average relative improvement of 19.45% across four levels of instruction\ntasks. Moreover, compared to the synchronously executed PIVOT-R, the execution\nefficiency of PIVOT-R with AHE is increased by 28-fold, with only a 2.9% drop\nin performance. These results provide compelling evidence that our PIVOT-R can\nsignificantly improve both the performance and efficiency of robotic\nmanipulation.\n","authors":["Kaidong Zhang","Pengzhen Ren","Bingqian Lin","Junfan Lin","Shikui Ma","Hang Xu","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2410.10394v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.12346v1","updated":"2024-10-16T08:07:18Z","published":"2024-10-16T08:07:18Z","title":"Towards Flexible and Efficient Diffusion Low Light Enhancer","summary":"  Diffusion-based Low-Light Image Enhancement (LLIE) has demonstrated\nsignificant success in improving the visibility of low-light images. However,\nthe substantial computational burden introduced by the iterative sampling\nprocess remains a major concern. Current acceleration methods, whether\ntraining-based or training-free, often lead to significant performance\ndegradation. As a result, to achieve an efficient student model with\nperformance comparable to that of existing multi-step teacher model, it is\nusually necessary to retrain a more capable teacher model. This approach\nintroduces inflexibility, as it requires additional training to enhance the\nteacher's performance. To address these challenges, we propose\n\\textbf{Re}flectance-aware \\textbf{D}iffusion with \\textbf{Di}stilled\n\\textbf{T}rajectory (\\textbf{ReDDiT}), a step distillation framework\nspecifically designed for LLIE. ReDDiT trains a student model to replicate the\nteacher's trajectory in fewer steps while also possessing the ability to\nsurpass the teacher's performance. Specifically, we first introduce a\ntrajectory decoder from the teacher model to provide guidance. Subsequently, a\nreflectance-aware trajectory refinement module is incorporated into the\ndistillation process to enable more deterministic guidance from the teacher\nmodel. Our framework achieves comparable performance to previous\ndiffusion-based methods with redundant steps in just 2 steps while establishing\nnew state-of-the-art (SOTA) results with 8 or 4 steps. Comprehensive\nexperimental evaluations on 10 benchmark datasets validate the effectiveness of\nour method, consistently outperforming existing SOTA methods.\n","authors":["Guanzhou Lan","Qianli Ma","Yuqi Yang","Zhigang Wang","Dong Wang","Yuan Yuan","Bin Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.12346v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2410.12342v1","updated":"2024-10-16T08:02:49Z","published":"2024-10-16T08:02:49Z","title":"TAS: Distilling Arbitrary Teacher and Student via a Hybrid Assistant","summary":"  Most knowledge distillation (KD) methodologies predominantly focus on\nteacher-student pairs with similar architectures, such as both being\nconvolutional neural networks (CNNs). However, the potential and flexibility of\nKD can be greatly improved by expanding it to novel Cross-Architecture KD\n(CAKD), where the knowledge of homogeneous and heterogeneous teachers can be\ntransferred flexibly to a given student. The primary challenge in CAKD lies in\nthe substantial feature gaps between heterogeneous models, originating from the\ndistinction of their inherent inductive biases and module functions. To this\nend, we introduce an assistant model as a bridge to facilitate smooth feature\nknowledge transfer between heterogeneous teachers and students. More\nimportantly, within our proposed design principle, the assistant model combines\nthe advantages of cross-architecture inductive biases and module functions by\nmerging convolution and attention modules derived from both student and teacher\nmodule functions. Furthermore, we observe that heterogeneous features exhibit\ndiverse spatial distributions in CAKD, hindering the effectiveness of\nconventional pixel-wise mean squared error (MSE) loss. Therefore, we leverage a\nspatial-agnostic InfoNCE loss to align features after spatial smoothing,\nthereby improving the feature alignments in CAKD. Our proposed method is\nevaluated across some homogeneous model pairs and arbitrary heterogeneous\ncombinations of CNNs, ViTs, and MLPs, achieving state-of-the-art performance\nfor distilled models with a maximum gain of 11.47% on CIFAR-100 and 3.67% on\nImageNet-1K. Our code and models will be released.\n","authors":["Guopeng Li","Qiang Wang","Ke Yan","Shouhong Ding","Yuan Gao","Gui-Song Xia"],"pdf_url":"https://arxiv.org/pdf/2410.12342v1.pdf","comment":"18 pages, 6 figures, and 12 tables"},{"id":"http://arxiv.org/abs/2410.12337v1","updated":"2024-10-16T07:59:07Z","published":"2024-10-16T07:59:07Z","title":"ARIC: An Activity Recognition Dataset in Classroom Surveillance Images","summary":"  The application of activity recognition in the ``AI + Education\" field is\ngaining increasing attention. However, current work mainly focuses on the\nrecognition of activities in manually captured videos and a limited number of\nactivity types, with little attention given to recognizing activities in\nsurveillance images from real classrooms. Activity recognition in classroom\nsurveillance images faces multiple challenges, such as class imbalance and high\nactivity similarity. To address this gap, we constructed a novel multimodal\ndataset focused on classroom surveillance image activity recognition called\nARIC (Activity Recognition In Classroom). The ARIC dataset has advantages of\nmultiple perspectives, 32 activity categories, three modalities, and real-world\nclassroom scenarios. In addition to the general activity recognition tasks, we\nalso provide settings for continual learning and few-shot continual learning.\nWe hope that the ARIC dataset can act as a facilitator for future analysis and\nresearch for open teaching scenarios. You can download preliminary data from\nhttps://ivipclab.github.io/publication_ARIC/ARIC.\n","authors":["Linfeng Xu","Fanman Meng","Qingbo Wu","Lili Pan","Heqian Qiu","Lanxiao Wang","Kailong Chen","Kanglei Geng","Yilei Qian","Haojie Wang","Shuchang Zhou","Shimou Ling","Zejia Liu","Nanlin Chen","Yingjie Xu","Shaoxu Cheng","Bowen Tan","Ziyong Xu","Hongliang Li"],"pdf_url":"https://arxiv.org/pdf/2410.12337v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2409.03354"},{"id":"http://arxiv.org/abs/2410.12332v1","updated":"2024-10-16T07:52:57Z","published":"2024-10-16T07:52:57Z","title":"MC-Bench: A Benchmark for Multi-Context Visual Grounding in the Era of\n  MLLMs","summary":"  While multimodal large language models (MLLMs) have demonstrated\nextraordinary vision-language understanding capabilities and shown potential to\nserve as general-purpose assistants, their abilities to solve instance-level\nvisual-language problems beyond a single image warrant further exploration. In\norder to assess these unproven abilities of MLLMs, this paper proposes a new\nvisual grounding task called multi-context visual grounding, which aims to\nlocalize instances of interest across multiple images based on open-ended text\nprompts. To facilitate this research, we meticulously construct a new dataset\nMC-Bench for benchmarking the visual grounding capabilities of MLLMs. MC-Bench\nfeatures 2K high-quality and manually annotated samples, consisting of\ninstance-level labeled image pairs and corresponding text prompts that indicate\nthe target instances in the images. In total, there are three distinct styles\nof text prompts, covering 20 practical skills. We benchmark over 20\nstate-of-the-art MLLMs and foundation models with potential multi-context\nvisual grounding capabilities. Our evaluation reveals a non-trivial performance\ngap between existing MLLMs and humans across all metrics. We also observe that\nexisting MLLMs typically outperform foundation models without LLMs only on\nimage-level metrics, and the specialist MLLMs trained on single images often\nstruggle to generalize to multi-image scenarios. Moreover, a simple stepwise\nbaseline integrating advanced MLLM and a detector can significantly surpass\nprior end-to-end MLLMs. We hope our MC-Bench and empirical findings can\nencourage the research community to further explore and enhance the untapped\npotentials of MLLMs in instance-level tasks, particularly in multi-image\ncontexts. Project page: https://xuyunqiu.github.io/MC-Bench/.\n","authors":["Yunqiu Xu","Linchao Zhu","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2410.12332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01068v2","updated":"2024-10-16T07:49:27Z","published":"2024-09-02T08:43:50Z","title":"Progressive Retinal Image Registration via Global and Local Deformable\n  Transformations","summary":"  Retinal image registration plays an important role in the ophthalmological\ndiagnosis process. Since there exist variances in viewing angles and anatomical\nstructures across different retinal images, keypoint-based approaches become\nthe mainstream methods for retinal image registration thanks to their\nrobustness and low latency. These methods typically assume the retinal surfaces\nare planar, and adopt feature matching to obtain the homography matrix that\nrepresents the global transformation between images. Yet, such a planar\nhypothesis inevitably introduces registration errors since retinal surface is\napproximately curved. This limitation is more prominent when registering image\npairs with significant differences in viewing angles. To address this problem,\nwe propose a hybrid registration framework called HybridRetina, which\nprogressively registers retinal images with global and local deformable\ntransformations. For that, we use a keypoint detector and a deformation network\ncalled GAMorph to estimate the global transformation and local deformable\ntransformation, respectively. Specifically, we integrate multi-level pixel\nrelation knowledge to guide the training of GAMorph. Additionally, we utilize\nan edge attention module that includes the geometric priors of the images,\nensuring the deformation field focuses more on the vascular regions of clinical\ninterest. Experiments on two widely-used datasets, FIRE and FLoRI21, show that\nour proposed HybridRetina significantly outperforms some state-of-the-art\nmethods. The code is available at\nhttps://github.com/lyp-deeplearning/awesome-retinal-registration.\n","authors":["Yepeng Liu","Baosheng Yu","Tian Chen","Yuliang Gu","Bo Du","Yongchao Xu","Jun Cheng"],"pdf_url":"https://arxiv.org/pdf/2409.01068v2.pdf","comment":"Accepted at BIBM 2024"},{"id":"http://arxiv.org/abs/2410.12328v1","updated":"2024-10-16T07:48:53Z","published":"2024-10-16T07:48:53Z","title":"Improved Anomaly Detection through Conditional Latent Space VAE\n  Ensembles","summary":"  We propose a novel Conditional Latent space Variational Autoencoder (CL-VAE)\nto perform improved pre-processing for anomaly detection on data with known\ninlier classes and unknown outlier classes. This proposed variational\nautoencoder (VAE) improves latent space separation by conditioning on\ninformation within the data. The method fits a unique prior distribution to\neach class in the dataset, effectively expanding the classic prior distribution\nfor VAEs to include a Gaussian mixture model. An ensemble of these VAEs are\nmerged in the latent spaces to form a group consensus that greatly improves the\naccuracy of anomaly detection across data sets. Our approach is compared\nagainst the capabilities of a typical VAE, a CNN, and a PCA, with regards AUC\nfor anomaly detection. The proposed model shows increased accuracy in anomaly\ndetection, achieving an AUC of 97.4% on the MNIST dataset compared to 95.7% for\nthe second best model. In addition, the CL-VAE shows increased benefits from\nensembling, a more interpretable latent space, and an increased ability to\nlearn patterns in complex data with limited model sizes.\n","authors":["Oskar Åström","Alexandros Sopasakis"],"pdf_url":"https://arxiv.org/pdf/2410.12328v1.pdf","comment":"13 pages of main article, 19 pages including references and appendix,\n  4 figures"},{"id":"http://arxiv.org/abs/2410.12324v1","updated":"2024-10-16T07:44:56Z","published":"2024-10-16T07:44:56Z","title":"PAPL-SLAM: Principal Axis-Anchored Monocular Point-Line SLAM","summary":"  In point-line SLAM systems, the utilization of line structural information\nand the optimization of lines are two significant problems. The former is\nusually addressed through structural regularities, while the latter typically\ninvolves using minimal parameter representations of lines in optimization.\nHowever, separating these two steps leads to the loss of constraint information\nto each other. We anchor lines with similar directions to a principal axis and\noptimize them with $n+2$ parameters for $n$ lines, solving both problems\ntogether. Our method considers scene structural information, which can be\neasily extended to different world hypotheses while significantly reducing the\nnumber of line parameters to be optimized, enabling rapid and accurate mapping\nand tracking. To further enhance the system's robustness and avoid mismatch, we\nhave modeled the line-axis probabilistic data association and provided the\nalgorithm for axis creation, updating, and optimization. Additionally,\nconsidering that most real-world scenes conform to the Atlanta World\nhypothesis, we provide a structural line detection strategy based on vertical\npriors and vanishing points. Experimental results and ablation studies on\nvarious indoor and outdoor datasets demonstrate the effectiveness of our\nsystem.\n","authors":["Guanghao Li","Yu Cao","Qi Chen","Yifan Yang","Jian Pu"],"pdf_url":"https://arxiv.org/pdf/2410.12324v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.12312v1","updated":"2024-10-16T07:25:24Z","published":"2024-10-16T07:25:24Z","title":"FaceChain-FACT: Face Adapter with Decoupled Training for\n  Identity-preserved Personalization","summary":"  In the field of human-centric personalized image generation, the\nadapter-based method obtains the ability to customize and generate portraits by\ntext-to-image training on facial data. This allows for identity-preserved\npersonalization without additional fine-tuning in inference. Although there are\nimprovements in efficiency and fidelity, there is often a significant\nperformance decrease in test following ability, controllability, and diversity\nof generated faces compared to the base model. In this paper, we analyze that\nthe performance degradation is attributed to the failure to decouple identity\nfeatures from other attributes during extraction, as well as the failure to\ndecouple the portrait generation training from the overall generation task. To\naddress these issues, we propose the Face Adapter with deCoupled Training\n(FACT) framework, focusing on both model architecture and training strategy. To\ndecouple identity features from others, we leverage a transformer-based\nface-export encoder and harness fine-grained identity features. To decouple the\nportrait generation training, we propose Face Adapting Increment\nRegularization~(FAIR), which effectively constrains the effect of face adapters\non the facial region, preserving the generative ability of the base model.\nAdditionally, we incorporate a face condition drop and shuffle mechanism,\ncombined with curriculum learning, to enhance facial controllability and\ndiversity. As a result, FACT solely learns identity preservation from training\ndata, thereby minimizing the impact on the original text-to-image capabilities\nof the base model. Extensive experiments show that FACT has both\ncontrollability and fidelity in both text-to-image generation and inpainting\nsolutions for portrait generation.\n","authors":["Cheng Yu","Haoyu Xie","Lei Shang","Yang Liu","Jun Dan","Baigui Sun","Liefeng Bo"],"pdf_url":"https://arxiv.org/pdf/2410.12312v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.07167v2","updated":"2024-10-16T07:23:03Z","published":"2024-10-09T17:59:04Z","title":"Deciphering Cross-Modal Alignment in Large Vision-Language Models with\n  Modality Integration Rate","summary":"  We present the Modality Integration Rate (MIR), an effective, robust, and\ngeneralized metric to indicate the multi-modal pre-training quality of Large\nVision Language Models (LVLMs). Large-scale pre-training plays a critical role\nin building capable LVLMs, while evaluating its training quality without the\ncostly supervised fine-tuning stage is under-explored. Loss, perplexity, and\nin-context evaluation results are commonly used pre-training metrics for Large\nLanguage Models (LLMs), while we observed that these metrics are less\nindicative when aligning a well-trained LLM with a new modality. Due to the\nlack of proper metrics, the research of LVLMs in the critical pre-training\nstage is hindered greatly, including the training data choice, efficient module\ndesign, etc. In this paper, we propose evaluating the pre-training quality from\nthe inter-modal distribution distance perspective and present MIR, the Modality\nIntegration Rate, which is 1) \\textbf{Effective} to represent the pre-training\nquality and show a positive relation with the benchmark performance after\nsupervised fine-tuning. 2) \\textbf{Robust} toward different training/evaluation\ndata. 3) \\textbf{Generalize} across training configurations and architecture\nchoices. We conduct a series of pre-training experiments to explore the\neffectiveness of MIR and observe satisfactory results that MIR is indicative\nabout training data selection, training strategy schedule, and model\narchitecture design to get better pre-training results. We hope MIR could be a\nhelpful metric for building capable LVLMs and inspire the following research\nabout modality alignment in different areas. Our code is at:\nhttps://github.com/shikiw/Modality-Integration-Rate.\n","authors":["Qidong Huang","Xiaoyi Dong","Pan Zhang","Yuhang Zang","Yuhang Cao","Jiaqi Wang","Dahua Lin","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2410.07167v2.pdf","comment":"Project page: https://github.com/shikiw/Modality-Integration-Rate"},{"id":"http://arxiv.org/abs/2408.04804v2","updated":"2024-10-16T07:20:58Z","published":"2024-08-09T01:21:15Z","title":"Hyper-YOLO: When Visual Object Detection Meets Hypergraph Computation","summary":"  We introduce Hyper-YOLO, a new object detection method that integrates\nhypergraph computations to capture the complex high-order correlations among\nvisual features. Traditional YOLO models, while powerful, have limitations in\ntheir neck designs that restrict the integration of cross-level features and\nthe exploitation of high-order feature interrelationships. To address these\nchallenges, we propose the Hypergraph Computation Empowered Semantic Collecting\nand Scattering (HGC-SCS) framework, which transposes visual feature maps into a\nsemantic space and constructs a hypergraph for high-order message propagation.\nThis enables the model to acquire both semantic and structural information,\nadvancing beyond conventional feature-focused learning. Hyper-YOLO incorporates\nthe proposed Mixed Aggregation Network (MANet) in its backbone for enhanced\nfeature extraction and introduces the Hypergraph-Based Cross-Level and\nCross-Position Representation Network (HyperC2Net) in its neck. HyperC2Net\noperates across five scales and breaks free from traditional grid structures,\nallowing for sophisticated high-order interactions across levels and positions.\nThis synergy of components positions Hyper-YOLO as a state-of-the-art\narchitecture in various scale models, as evidenced by its superior performance\non the COCO dataset. Specifically, Hyper-YOLO-N significantly outperforms the\nadvanced YOLOv8-N and YOLOv9-T with 12\\% $\\text{AP}^{val}$ and 9\\%\n$\\text{AP}^{val}$ improvements. The source codes are at\nttps://github.com/iMoonLab/Hyper-YOLO.\n","authors":["Yifan Feng","Jiangang Huang","Shaoyi Du","Shihui Ying","Jun-Hai Yong","Yipeng Li","Guiguang Ding","Rongrong Ji","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2408.04804v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12307v1","updated":"2024-10-16T07:18:36Z","published":"2024-10-16T07:18:36Z","title":"DAT: Improving Adversarial Robustness via Generative Amplitude Mix-up in\n  Frequency Domain","summary":"  To protect deep neural networks (DNNs) from adversarial attacks, adversarial\ntraining (AT) is developed by incorporating adversarial examples (AEs) into\nmodel training. Recent studies show that adversarial attacks disproportionately\nimpact the patterns within the phase of the sample's frequency spectrum --\ntypically containing crucial semantic information -- more than those in the\namplitude, resulting in the model's erroneous categorization of AEs. We find\nthat, by mixing the amplitude of training samples' frequency spectrum with\nthose of distractor images for AT, the model can be guided to focus on phase\npatterns unaffected by adversarial perturbations. As a result, the model's\nrobustness can be improved. Unfortunately, it is still challenging to select\nappropriate distractor images, which should mix the amplitude without affecting\nthe phase patterns. To this end, in this paper, we propose an optimized\nAdversarial Amplitude Generator (AAG) to achieve a better tradeoff between\nimproving the model's robustness and retaining phase patterns. Based on this\ngenerator, together with an efficient AE production procedure, we design a new\nDual Adversarial Training (DAT) strategy. Experiments on various datasets show\nthat our proposed DAT leads to significantly improved robustness against\ndiverse adversarial attacks.\n","authors":["Fengpeng Li","Kemou Li","Haiwei Wu","Jinyu Tian","Jiantao Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.12307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11610v2","updated":"2024-10-16T07:09:12Z","published":"2024-10-15T13:46:19Z","title":"Depth Estimation From Monocular Images With Enhanced Encoder-Decoder\n  Architecture","summary":"  Estimating depth from a single 2D image is a challenging task because of the\nneed for stereo or multi-view data, which normally provides depth information.\nThis paper deals with this challenge by introducing a novel deep learning-based\napproach using an encoder-decoder architecture, where the Inception-ResNet-v2\nmodel is utilized as the encoder. According to the available literature, this\nis the first instance of using Inception-ResNet-v2 as an encoder for monocular\ndepth estimation, illustrating better performance than previous models. The use\nof Inception-ResNet-v2 enables our model to capture complex objects and\nfine-grained details effectively that are generally difficult to predict.\nBesides, our model incorporates multi-scale feature extraction to enhance depth\nprediction accuracy across different kinds of object sizes and distances. We\npropose a composite loss function consisting of depth loss, gradient edge loss,\nand SSIM loss, where the weights are fine-tuned to optimize the weighted sum,\nensuring better balance across different aspects of depth estimation.\nExperimental results on the NYU Depth V2 dataset show that our model achieves\nstate-of-the-art performance, with an ARE of 0.064, RMSE of 0.228, and accuracy\n($\\delta$ $<1.25$) of 89.3%. These metrics demonstrate that our model\neffectively predicts depth, even in challenging circumstances, providing a\nscalable solution for real-world applications in robotics, 3D reconstruction,\nand augmented reality.\n","authors":["Dabbrata Das","Argho Deb Das","Farhan Sadaf"],"pdf_url":"https://arxiv.org/pdf/2410.11610v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17137v3","updated":"2024-10-16T07:08:57Z","published":"2023-11-28T18:59:02Z","title":"Generative Models: What Do They Know? Do They Know Things? Let's Find\n  Out!","summary":"  Generative models excel at mimicking real scenes, suggesting they might\ninherently encode important intrinsic scene properties. In this paper, we aim\nto explore the following key questions: (1) What intrinsic knowledge do\ngenerative models like GANs, Autoregressive models, and Diffusion models\nencode? (2) Can we establish a general framework to recover intrinsic\nrepresentations from these models, regardless of their architecture or model\ntype? (3) How minimal can the required learnable parameters and labeled data be\nto successfully recover this knowledge? (4) Is there a direct link between the\nquality of a generative model and the accuracy of the recovered scene\nintrinsics?\n  Our findings indicate that a small Low-Rank Adaptators (LoRA) can recover\nintrinsic images-depth, normals, albedo and shading-across different generators\n(Autoregressive, GANs and Diffusion) while using the same decoder head that\ngenerates the image. As LoRA is lightweight, we introduce very few learnable\nparameters (as few as 0.04% of Stable Diffusion model weights for a rank of 2),\nand we find that as few as 250 labeled images are enough to generate intrinsic\nimages with these LoRA modules. Finally, we also show a positive correlation\nbetween the generative model's quality and the accuracy of the recovered\nintrinsics through control experiments.\n","authors":["Xiaodan Du","Nicholas Kolkin","Greg Shakhnarovich","Anand Bhattad"],"pdf_url":"https://arxiv.org/pdf/2311.17137v3.pdf","comment":"https://intrinsic-lora.github.io/"},{"id":"http://arxiv.org/abs/2403.14166v3","updated":"2024-10-16T07:07:21Z","published":"2024-03-21T06:34:46Z","title":"Mini-Splatting: Representing Scenes with a Constrained Number of\n  Gaussians","summary":"  In this study, we explore the challenge of efficiently representing scenes\nwith a constrained number of Gaussians. Our analysis shifts from traditional\ngraphics and 2D computer vision to the perspective of point clouds,\nhighlighting the inefficient spatial distribution of Gaussian representation as\na key limitation in model performance. To address this, we introduce strategies\nfor densification including blur split and depth reinitialization, and\nsimplification through intersection preserving and sampling. These techniques\nreorganize the spatial positions of the Gaussians, resulting in significant\nimprovements across various datasets and benchmarks in terms of rendering\nquality, resource consumption, and storage compression. Our Mini-Splatting\nintegrates seamlessly with the original rasterization pipeline, providing a\nstrong baseline for future research in Gaussian-Splatting-based works.\n\\href{https://github.com/fatPeter/mini-splatting}{Code is available}.\n","authors":["Guangchi Fang","Bing Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14166v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12295v1","updated":"2024-10-16T06:55:02Z","published":"2024-10-16T06:55:02Z","title":"Consistency Calibration: Improving Uncertainty Calibration via\n  Consistency among Perturbed Neighbors","summary":"  Calibration is crucial in deep learning applications, especially in fields\nlike healthcare and autonomous driving, where accurate confidence estimates are\nvital for decision-making. However, deep neural networks often suffer from\nmiscalibration, with reliability diagrams and Expected Calibration Error (ECE)\nbeing the only standard perspective for evaluating calibration performance. In\nthis paper, we introduce the concept of consistency as an alternative\nperspective on model calibration, inspired by uncertainty estimation literature\nin large language models (LLMs). We highlight its advantages over the\ntraditional reliability-based view. Building on this concept, we propose a\npost-hoc calibration method called Consistency Calibration (CC), which adjusts\nconfidence based on the model's consistency across perturbed inputs. CC is\nparticularly effective in locally uncertainty estimation, as it requires no\nadditional data samples or label information, instead generating input\nperturbations directly from the source data. Moreover, we show that performing\nperturbations at the logit level significantly improves computational\nefficiency. We validate the effectiveness of CC through extensive comparisons\nwith various post-hoc and training-time calibration methods, demonstrating\nstate-of-the-art performance on standard datasets such as CIFAR-10, CIFAR-100,\nand ImageNet, as well as on long-tailed datasets like ImageNet-LT.\n","authors":["Linwei Tao","Haolan Guo","Minjing Dong","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2410.12295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12284v1","updated":"2024-10-16T06:43:02Z","published":"2024-10-16T06:43:02Z","title":"Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical\n  Decision-Support Setting","summary":"  The growing capabilities of AI models are leading to their wider use,\nincluding in safety-critical domains. Explainable AI (XAI) aims to make these\nmodels safer to use by making their inference process more transparent.\nHowever, current explainability methods are seldom evaluated in the way they\nare intended to be used: by real-world end users. To address this, we conducted\na large-scale user study with 85 healthcare practitioners in the context of\nhuman-AI collaborative chest X-ray analysis. We evaluated three types of\nexplanations: visual explanations (saliency maps), natural language\nexplanations, and a combination of both modalities. We specifically examined\nhow different explanation types influence users depending on whether the AI\nadvice and explanations are factually correct. We find that text-based\nexplanations lead to significant over-reliance, which is alleviated by\ncombining them with saliency maps. We also observe that the quality of\nexplanations, that is, how much factually correct information they entail, and\nhow much this aligns with AI correctness, significantly impacts the usefulness\nof the different explanation types.\n","authors":["Maxime Kayser","Bayar Menzat","Cornelius Emde","Bogdan Bercean","Alex Novak","Abdala Espinosa","Bartlomiej W. Papiez","Susanne Gaube","Thomas Lukasiewicz","Oana-Maria Camburu"],"pdf_url":"https://arxiv.org/pdf/2410.12284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12278v1","updated":"2024-10-16T06:31:59Z","published":"2024-10-16T06:31:59Z","title":"Controlled Automatic Task-Specific Synthetic Data Generation for\n  Hallucination Detection","summary":"  We present a novel approach to automatically generate non-trivial\ntask-specific synthetic datasets for hallucination detection. Our approach\nfeatures a two-step generation-selection pipeline, using hallucination pattern\nguidance and a language style alignment during generation. Hallucination\npattern guidance leverages the most important task-specific hallucination\npatterns while language style alignment aligns the style of the synthetic\ndataset with benchmark text. To obtain robust supervised detectors from\nsynthetic datasets, we also adopt a data mixture strategy to improve\nperformance robustness and generalization. Our results on three datasets show\nthat our generated hallucination text is more closely aligned with\nnon-hallucinated text versus baselines, to train hallucination detectors with\nbetter generalization. Our hallucination detectors trained on synthetic\ndatasets outperform in-context-learning (ICL)-based detectors by a large margin\nof 32%. Our extensive experiments confirm the benefits of our approach with\ncross-task and cross-generator generalization. Our data-mixture-based training\nfurther improves the generalization and robustness of hallucination detection.\n","authors":["Yong Xie","Karan Aggarwal","Aitzaz Ahmad","Stephen Lau"],"pdf_url":"https://arxiv.org/pdf/2410.12278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19454v2","updated":"2024-10-16T06:31:28Z","published":"2024-09-28T20:40:18Z","title":"See Where You Read with Eye Gaze Tracking and Large Language Model","summary":"  Losing track of reading progress during line switching can be frustrating.\nEye gaze tracking technology offers a potential solution by highlighting read\nparagraphs, aiding users in avoiding wrong line switches. However, the gap\nbetween gaze tracking accuracy (2-3 cm) and text line spacing (3-5 mm) makes\ndirect application impractical. Existing methods leverage the linear reading\npattern but fail during jump reading. This paper presents a reading tracking\nand highlighting system that supports both linear and jump reading. Based on\nexperimental insights from the gaze nature study of 16 users, two gaze error\nmodels are designed to enable both jump reading detection and relocation. The\nsystem further leverages the large language model's contextual perception\ncapability in aiding reading tracking. A reading tracking domain-specific\nline-gaze alignment opportunity is also exploited to enable dynamic and\nfrequent calibration of the gaze results. Controlled experiments demonstrate\nreliable linear reading tracking, as well as 84% accuracy in tracking jump\nreading. Furthermore, real field tests with 18 volunteers demonstrated the\nsystem's effectiveness in tracking and highlighting read paragraphs, improving\nreading efficiency, and enhancing user experience.\n","authors":["Sikai Yang","Gang Yan"],"pdf_url":"https://arxiv.org/pdf/2409.19454v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2406.11548v5","updated":"2024-10-16T06:29:55Z","published":"2024-06-17T13:44:53Z","title":"AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic\n  Manipulation","summary":"  The ability to reflect on and correct failures is crucial for robotic systems\nto interact stably with real-life objects. Observing the generalization and\nreasoning capabilities of Multimodal Large Language Models (MLLMs), previous\napproaches have aimed to utilize these models to enhance robotic systems\naccordingly. However, these methods typically focus on high-level planning\ncorrections using an additional MLLM, with limited utilization of failed\nsamples to correct low-level contact poses which is particularly prone to occur\nduring articulated object manipulation. To address this gap, we propose an\nAutonomous Interactive Correction (AIC) MLLM, which makes use of previous\nlow-level interaction experiences to correct SE(3) pose predictions for\narticulated object. Specifically, AIC MLLM is initially fine-tuned to acquire\nboth pose prediction and feedback prompt comprehension abilities. We design two\ntypes of prompt instructions for interactions with objects: 1) visual masks to\nhighlight unmovable parts for position correction, and 2) textual descriptions\nto indicate potential directions for rotation correction. During inference, a\nFeedback Information Extraction module is introduced to recognize the failure\ncause, allowing AIC MLLM to adaptively correct the pose prediction using the\ncorresponding prompts. To further enhance manipulation stability, we devise a\nTest Time Adaptation strategy that enables AIC MLLM to better adapt to the\ncurrent scene configuration. Finally, extensive experiments are conducted in\nboth simulated and real-world environments to evaluate the proposed method. The\nresults demonstrate that our AIC MLLM can efficiently correct failure samples\nby leveraging interaction experience prompts. Our project website is\nhttps://sites.google.com/view/aic-mllm.\n","authors":["Chuyan Xiong","Chengyu Shen","Xiaoqi Li","Kaichen Zhou","Jiaming Liu","Ruiping Wang","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2406.11548v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02237v2","updated":"2024-10-16T06:28:58Z","published":"2024-10-03T06:16:50Z","title":"Key-Grid: Unsupervised 3D Keypoints Detection using Grid Heatmap\n  Features","summary":"  Detecting 3D keypoints with semantic consistency is widely used in many\nscenarios such as pose estimation, shape registration and robotics. Currently,\nmost unsupervised 3D keypoint detection methods focus on the rigid-body\nobjects. However, when faced with deformable objects, the keypoints they\nidentify do not preserve semantic consistency well. In this paper, we introduce\nan innovative unsupervised keypoint detector Key-Grid for both the rigid-body\nand deformable objects, which is an autoencoder framework. The encoder predicts\nkeypoints and the decoder utilizes the generated keypoints to reconstruct the\nobjects. Unlike previous work, we leverage the identified keypoint in formation\nto form a 3D grid feature heatmap called grid heatmap, which is used in the\ndecoder section. Grid heatmap is a novel concept that represents the latent\nvariables for grid points sampled uniformly in the 3D cubic space, where these\nvariables are the shortest distance between the grid points and the skeleton\nconnected by keypoint pairs. Meanwhile, we incorporate the information from\neach layer of the encoder into the decoder section. We conduct an extensive\nevaluation of Key-Grid on a list of benchmark datasets. Key-Grid achieves the\nstate-of-the-art performance on the semantic consistency and position accuracy\nof keypoints. Moreover, we demonstrate the robustness of Key-Grid to noise and\ndownsampling. In addition, we achieve SE-(3) invariance of keypoints though\ngeneralizing Key-Grid to a SE(3)-invariant backbone.\n","authors":["Chengkai Hou","Zhengrong Xue","Bingyang Zhou","Jinghan Ke","Lin Shao","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2410.02237v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12274v1","updated":"2024-10-16T06:28:49Z","published":"2024-10-16T06:28:49Z","title":"Fusion from Decomposition: A Self-Supervised Approach for Image Fusion\n  and Beyond","summary":"  Image fusion is famous as an alternative solution to generate one\nhigh-quality image from multiple images in addition to image restoration from a\nsingle degraded image. The essence of image fusion is to integrate\ncomplementary information from source images. Existing fusion methods struggle\nwith generalization across various tasks and often require labor-intensive\ndesigns, in which it is difficult to identify and extract useful information\nfrom source images due to the diverse requirements of each fusion task.\nAdditionally, these methods develop highly specialized features for different\ndownstream applications, hindering the adaptation to new and diverse downstream\ntasks. To address these limitations, we introduce DeFusion++, a novel framework\nthat leverages self-supervised learning (SSL) to enhance the versatility of\nfeature representation for different image fusion tasks. DeFusion++ captures\nthe image fusion task-friendly representations from large-scale data in a\nself-supervised way, overcoming the constraints of limited fusion datasets.\nSpecifically, we introduce two innovative pretext tasks: common and unique\ndecomposition (CUD) and masked feature modeling (MFM). CUD decomposes source\nimages into abstract common and unique components, while MFM refines these\ncomponents into robust fused features. Jointly training of these tasks enables\nDeFusion++ to produce adaptable representations that can effectively extract\nuseful information from various source images, regardless of the fusion task.\nThe resulting fused representations are also highly adaptable for a wide range\nof downstream tasks, including image segmentation and object detection.\nDeFusion++ stands out by producing versatile fused representations that can\nenhance both the quality of image fusion and the effectiveness of downstream\nhigh-level vision tasks, simplifying the process with the elegant fusion\nframework.\n","authors":["Pengwei Liang","Junjun Jiang","Qing Ma","Xianming Liu","Jiayi Ma"],"pdf_url":"https://arxiv.org/pdf/2410.12274v1.pdf","comment":"18page"},{"id":"http://arxiv.org/abs/2407.12508v2","updated":"2024-10-16T06:25:50Z","published":"2024-07-17T11:45:02Z","title":"MERLIN: Multimodal Embedding Refinement via LLM-based Iterative\n  Navigation for Text-Video Retrieval-Rerank Pipeline","summary":"  The rapid expansion of multimedia content has made accurately retrieving\nrelevant videos from large collections increasingly challenging. Recent\nadvancements in text-video retrieval have focused on cross-modal interactions,\nlarge-scale foundation model training, and probabilistic modeling, yet often\nneglect the crucial user perspective, leading to discrepancies between user\nqueries and the content retrieved. To address this, we introduce MERLIN\n(Multimodal Embedding Refinement via LLM-based Iterative Navigation), a novel,\ntraining-free pipeline that leverages Large Language Models (LLMs) for\niterative feedback learning. MERLIN refines query embeddings from a user\nperspective, enhancing alignment between queries and video content through a\ndynamic question answering process. Experimental results on datasets like\nMSR-VTT, MSVD, and ActivityNet demonstrate that MERLIN substantially improves\nRecall@1, outperforming existing systems and confirming the benefits of\nintegrating LLMs into multimodal retrieval systems for more responsive and\ncontext-aware multimedia retrieval.\n","authors":["Donghoon Han","Eunhwan Park","Gisang Lee","Adam Lee","Nojun Kwak"],"pdf_url":"https://arxiv.org/pdf/2407.12508v2.pdf","comment":"EMNLP 2024 Industry Track Accepted (Camera-Ready Version)"},{"id":"http://arxiv.org/abs/2410.12270v1","updated":"2024-10-16T06:14:15Z","published":"2024-10-16T06:14:15Z","title":"DaDiff: Domain-aware Diffusion Model for Nighttime UAV Tracking","summary":"  Domain adaptation is an inspiring solution to the misalignment issue of\nday/night image features for nighttime UAV tracking. However, the one-step\nadaptation paradigm is inadequate in addressing the prevalent difficulties\nposed by low-resolution (LR) objects when viewed from the UAVs at night, owing\nto the blurry edge contour and limited detail information. Moreover, these\napproaches struggle to perceive LR objects disturbed by nighttime noise. To\naddress these challenges, this work proposes a novel progressive alignment\nparadigm, named domain-aware diffusion model (DaDiff), aligning nighttime LR\nobject features to the daytime by virtue of progressive and stable generations.\nThe proposed DaDiff includes an alignment encoder to enhance the detail\ninformation of nighttime LR objects, a tracking-oriented layer designed to\nachieve close collaboration with tracking tasks, and a successive distribution\ndiscriminator presented to distinguish different feature distributions at each\ndiffusion timestep successively. Furthermore, an elaborate nighttime UAV\ntracking benchmark is constructed for LR objects, namely NUT-LR, consisting of\n100 annotated sequences. Exhaustive experiments have demonstrated the\nrobustness and feature alignment ability of the proposed DaDiff. The source\ncode and video demo are available at https://github.com/vision4robotics/DaDiff.\n","authors":["Haobo Zuo","Changhong Fu","Guangze Zheng","Liangliang Yao","Kunhan Lu","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2410.12270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19554v2","updated":"2024-10-16T06:12:53Z","published":"2024-09-29T04:43:10Z","title":"Tri-Cam: Practical Eye Gaze Tracking via Camera Network","summary":"  As human eyes serve as conduits of rich information, unveiling emotions,\nintentions, and even aspects of an individual's health and overall well-being,\ngaze tracking also enables various human-computer interaction applications, as\nwell as insights in psychological and medical research. However, existing gaze\ntracking solutions fall short at handling free user movement, and also require\nlaborious user effort in system calibration. We introduce Tri-Cam, a practical\ndeep learning-based gaze tracking system using three affordable RGB webcams. It\nfeatures a split network structure for efficient training, as well as\ndesignated network designs to handle the separated gaze tracking tasks. Tri-Cam\nis also equipped with an implicit calibration module, which makes use of mouse\nclick opportunities to reduce calibration overhead on the user's end. We\nevaluate Tri-Cam against Tobii, the state-of-the-art commercial eye tracker,\nachieving comparable accuracy, while supporting a wider free movement area. In\nconclusion, Tri-Cam provides a user-friendly, affordable, and robust gaze\ntracking solution that could practically enable various applications.\n","authors":["Sikai Yang"],"pdf_url":"https://arxiv.org/pdf/2409.19554v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2410.12269v1","updated":"2024-10-16T06:09:27Z","published":"2024-10-16T06:09:27Z","title":"LoD-Loc: Aerial Visual Localization using LoD 3D Map with Neural\n  Wireframe Alignment","summary":"  We propose a new method named LoD-Loc for visual localization in the air.\nUnlike existing localization algorithms, LoD-Loc does not rely on complex 3D\nrepresentations and can estimate the pose of an Unmanned Aerial Vehicle (UAV)\nusing a Level-of-Detail (LoD) 3D map. LoD-Loc mainly achieves this goal by\naligning the wireframe derived from the LoD projected model with that predicted\nby the neural network. Specifically, given a coarse pose provided by the UAV\nsensor, LoD-Loc hierarchically builds a cost volume for uniformly sampled pose\nhypotheses to describe pose probability distribution and select a pose with\nmaximum probability. Each cost within this volume measures the degree of line\nalignment between projected and predicted wireframes. LoD-Loc also devises a\n6-DoF pose optimization algorithm to refine the previous result with a\ndifferentiable Gaussian-Newton method. As no public dataset exists for the\nstudied problem, we collect two datasets with map levels of LoD3.0 and LoD2.0,\nalong with real RGB queries and ground-truth pose annotations. We benchmark our\nmethod and demonstrate that LoD-Loc achieves excellent performance, even\nsurpassing current state-of-the-art methods that use textured 3D models for\nlocalization. The code and dataset are available at\nhttps://victorzoo.github.io/LoD-Loc.github.io/.\n","authors":["Juelin Zhu","Shen Yan","Long Wang","Shengyue Zhang","Yu Liu","Maojun Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12269v1.pdf","comment":"Accepted by NeurIPS 2024; for Project page, see\n  https://victorzoo.github.io/LoD-Loc.github.io/"},{"id":"http://arxiv.org/abs/2408.10894v3","updated":"2024-10-16T06:00:24Z","published":"2024-08-20T14:27:03Z","title":"ViLReF: An Expert Knowledge Enabled Vision-Language Retinal Foundation\n  Model","summary":"  Subtle semantic differences in retinal image and text data present great\nchallenges for pre-training visual-language models. Moreover, false negative\nsamples, i.e., image-text pairs having the same semantics but incorrectly\nregarded as negatives, disrupt the visual-language pre-training process and\naffect the model's learning ability. This work aims to develop a retinal\nfoundation model, called ViLReF, by pre-training on a paired dataset comprising\n451,956 retinal images and corresponding diagnostic text reports. In our\nvision-language pre-training strategy, we leverage expert knowledge to\nfacilitate the extraction of labels and propose a novel constraint, the\nWeighted Similarity Coupling Loss, to adjust the speed of pushing sample pairs\nfurther apart dynamically within the feature space. Furthermore, we employ a\nbatch expansion module with dynamic memory queues, maintained by momentum\nencoders, to supply extra samples and compensate for the vacancies caused by\neliminating false negatives. Extensive experiments are conducted on multiple\ndatasets for downstream classification and segmentation tasks. The experimental\nresults demonstrate the powerful zero-shot and transfer learning capabilities\nof ViLReF, verifying the effectiveness of our pre-training strategy. Our ViLReF\nmodel is available at: https://github.com/T6Yang/ViLReF.\n","authors":["Shengzhu Yang","Jiawei Du","Jia Guo","Weihang Zhang","Hanruo Liu","Huiqi Li","Ningli Wang"],"pdf_url":"https://arxiv.org/pdf/2408.10894v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12259v1","updated":"2024-10-16T05:58:08Z","published":"2024-10-16T05:58:08Z","title":"Optimizing YOLOv5s Object Detection through Knowledge Distillation\n  algorithm","summary":"  This paper explores the application of knowledge distillation technology in\ntarget detection tasks, especially the impact of different distillation\ntemperatures on the performance of student models. By using YOLOv5l as the\nteacher network and a smaller YOLOv5s as the student network, we found that\nwith the increase of distillation temperature, the student's detection accuracy\ngradually improved, and finally achieved mAP50 and mAP50-95 indicators that\nwere better than the original YOLOv5s model at a specific temperature.\nExperimental results show that appropriate knowledge distillation strategies\ncan not only improve the accuracy of the model but also help improve the\nreliability and stability of the model in practical applications. This paper\nalso records in detail the accuracy curve and loss function descent curve\nduring the model training process and shows that the model converges to a\nstable state after 150 training cycles. These findings provide a theoretical\nbasis and technical reference for further optimizing target detection\nalgorithms.\n","authors":["Guanming Huang","Aoran Shen","Yuxiang Hu","Junliang Du","Jiacheng Hu","Yingbin Liang"],"pdf_url":"https://arxiv.org/pdf/2410.12259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12245v1","updated":"2024-10-16T05:16:14Z","published":"2024-10-16T05:16:14Z","title":"Advancing Healthcare: Innovative ML Approaches for Improved Medical\n  Imaging in Data-Constrained Environments","summary":"  Healthcare industries face challenges when experiencing rare diseases due to\nlimited samples. Artificial Intelligence (AI) communities overcome this\nsituation to create synthetic data which is an ethical and privacy issue in the\nmedical domain. This research introduces the CAT-U-Net framework as a new\napproach to overcome these limitations, which enhances feature extraction from\nmedical images without the need for large datasets. The proposed framework adds\nan extra concatenation layer with downsampling parts, thereby improving its\nability to learn from limited data while maintaining patient privacy. To\nvalidate, the proposed framework's robustness, different medical conditioning\ndatasets were utilized including COVID-19, brain tumors, and wrist fractures.\nThe framework achieved nearly 98% reconstruction accuracy, with a Dice\ncoefficient close to 0.946. The proposed CAT-U-Net has the potential to make a\nbig difference in medical image diagnostics in settings with limited data.\n","authors":["Al Amin","Kamrul Hasan","Saleh Zein-Sabatto","Liang Hong","Sachin Shetty","Imtiaz Ahmed","Tariqul Islam"],"pdf_url":"https://arxiv.org/pdf/2410.12245v1.pdf","comment":"7 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.07176v2","updated":"2024-10-16T05:11:30Z","published":"2024-07-09T18:42:41Z","title":"Scaling Up Personalized Image Aesthetic Assessment via Task Vector\n  Customization","summary":"  The task of personalized image aesthetic assessment seeks to tailor aesthetic\nscore prediction models to match individual preferences with just a few\nuser-provided inputs. However, the scalability and generalization capabilities\nof current approaches are considerably restricted by their reliance on an\nexpensive curated database. To overcome this long-standing scalability\nchallenge, we present a unique approach that leverages readily available\ndatabases for general image aesthetic assessment and image quality assessment.\nSpecifically, we view each database as a distinct image score regression task\nthat exhibits varying degrees of personalization potential. By determining\noptimal combinations of task vectors, known to represent specific traits of\neach database, we successfully create personalized models for individuals. This\napproach of integrating multiple models allows us to harness a substantial\namount of data. Our extensive experiments demonstrate the effectiveness of our\napproach in generalizing to previously unseen domains-a challenge previous\napproaches have struggled to achieve-making it highly applicable to real-world\nscenarios. Our novel approach significantly advances the field by offering\nscalable solutions for personalized aesthetic assessment and establishing high\nstandards for future research.\nhttps://yeolj00.github.io/personal-projects/personalized-aesthetics/\n","authors":["Jooyeol Yun","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2407.07176v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2410.12242v1","updated":"2024-10-16T05:08:00Z","published":"2024-10-16T05:08:00Z","title":"EG-HumanNeRF: Efficient Generalizable Human NeRF Utilizing Human Prior\n  for Sparse View","summary":"  Generalizable neural radiance field (NeRF) enables neural-based digital human\nrendering without per-scene retraining. When combined with human prior\nknowledge, high-quality human rendering can be achieved even with sparse input\nviews. However, the inference of these methods is still slow, as a large number\nof neural network queries on each ray are required to ensure the rendering\nquality. Moreover, occluded regions often suffer from artifacts, especially\nwhen the input views are sparse. To address these issues, we propose a\ngeneralizable human NeRF framework that achieves high-quality and real-time\nrendering with sparse input views by extensively leveraging human prior\nknowledge. We accelerate the rendering with a two-stage sampling reduction\nstrategy: first constructing boundary meshes around the human geometry to\nreduce the number of ray samples for sampling guidance regression, and then\nvolume rendering using fewer guided samples. To improve rendering quality,\nespecially in occluded regions, we propose an occlusion-aware attention\nmechanism to extract occlusion information from the human priors, followed by\nan image space refinement network to improve rendering quality. Furthermore,\nfor volume rendering, we adopt a signed ray distance function (SRDF)\nformulation, which allows us to propose an SRDF loss at every sample position\nto improve the rendering quality further. Our experiments demonstrate that our\nmethod outperforms the state-of-the-art methods in rendering quality and has a\ncompetitive rendering speed compared with speed-prioritized novel view\nsynthesis methods.\n","authors":["Zhaorong Wang","Yoshihiro Kanamori","Yuki Endo"],"pdf_url":"https://arxiv.org/pdf/2410.12242v1.pdf","comment":"project page: https://github.com/LarsPh/EG-HumanNeRF"},{"id":"http://arxiv.org/abs/2410.12240v1","updated":"2024-10-16T05:00:51Z","published":"2024-10-16T05:00:51Z","title":"Leveraging Spatial Attention and Edge Context for Optimized Feature\n  Selection in Visual Localization","summary":"  Visual localization determines an agent's precise position and orientation\nwithin an environment using visual data. It has become a critical task in the\nfield of robotics, particularly in applications such as autonomous navigation.\nThis is due to the ability to determine an agent's pose using cost-effective\nsensors such as RGB cameras. Recent methods in visual localization employ scene\ncoordinate regression to determine the agent's pose. However, these methods\nface challenges as they attempt to regress 2D-3D correspondences across the\nentire image region, despite not all regions providing useful information. To\naddress this issue, we introduce an attention network that selectively targets\ninformative regions of the image. Using this network, we identify the\nhighest-scoring features to improve the feature selection process and combine\nthe result with edge detection. This integration ensures that the features\nchosen for the training buffer are located within robust regions, thereby\nimproving 2D-3D correspondence and overall localization performance. Our\napproach was tested on the outdoor benchmark dataset, demonstrating superior\nresults compared to previous methods.\n","authors":["Nanda Febri Istighfarin","HyungGi Jo"],"pdf_url":"https://arxiv.org/pdf/2410.12240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00769v3","updated":"2024-10-16T04:53:55Z","published":"2024-02-01T16:58:11Z","title":"AnimateLCM: Computation-Efficient Personalized Style Video Generation\n  without Personalized Video Data","summary":"  This paper introduces an effective method for computation-efficient\npersonalized style video generation without requiring access to any\npersonalized video data. It reduces the necessary generation time of similarly\nsized video diffusion models from 25 seconds to around 1 second while\nmaintaining the same level of performance. The method's effectiveness lies in\nits dual-level decoupling learning approach: 1) separating the learning of\nvideo style from video generation acceleration, which allows for personalized\nstyle video generation without any personalized style video data, and 2)\nseparating the acceleration of image generation from the acceleration of video\nmotion generation, enhancing training efficiency and mitigating the negative\neffects of low-quality video data.\n","authors":["Fu-Yun Wang","Zhaoyang Huang","Weikang Bian","Xiaoyu Shi","Keqiang Sun","Guanglu Song","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2402.00769v3.pdf","comment":"Accepted as a Short Paper by SIGGRAPH ASIA 2024 Technical\n  Communications. This is a short version of the original work. Project Page:\n  https://animatelcm.github.io/"},{"id":"http://arxiv.org/abs/2410.12225v1","updated":"2024-10-16T04:42:10Z","published":"2024-10-16T04:42:10Z","title":"Evaluating Cascaded Methods of Vision-Language Models for Zero-Shot\n  Detection and Association of Hardhats for Increased Construction Safety","summary":"  This paper evaluates the use of vision-language models (VLMs) for zero-shot\ndetection and association of hardhats to enhance construction safety. Given the\nsignificant risk of head injuries in construction, proper enforcement of\nhardhat use is critical. We investigate the applicability of foundation models,\nspecifically OWLv2, for detecting hardhats in real-world construction site\nimages. Our contributions include the creation of a new benchmark dataset,\nHardhat Safety Detection Dataset, by filtering and combining existing datasets\nand the development of a cascaded detection approach. Experimental results on\n5,210 images demonstrate that the OWLv2 model achieves an average precision of\n0.6493 for hardhat detection. We further analyze the limitations and potential\nimprovements for real-world applications, highlighting the strengths and\nweaknesses of current foundation models in safety perception domains.\n","authors":["Lucas Choi","Ross Greer"],"pdf_url":"https://arxiv.org/pdf/2410.12225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12214v1","updated":"2024-10-16T04:19:28Z","published":"2024-10-16T04:19:28Z","title":"Order-Aware Interactive Segmentation","summary":"  Interactive segmentation aims to accurately segment target objects with\nminimal user interactions. However, current methods often fail to accurately\nseparate target objects from the background, due to a limited understanding of\norder, the relative depth between objects in a scene. To address this issue, we\npropose OIS: order-aware interactive segmentation, where we explicitly encode\nthe relative depth between objects into order maps. We introduce a novel\norder-aware attention, where the order maps seamlessly guide the user\ninteractions (in the form of clicks) to attend to the image features. We\nfurther present an object-aware attention module to incorporate a strong\nobject-level understanding to better differentiate objects with similar order.\nOur approach allows both dense and sparse integration of user clicks, enhancing\nboth accuracy and efficiency as compared to prior works. Experimental results\ndemonstrate that OIS achieves state-of-the-art performance, improving mIoU\nafter one click by 7.61 on the HQSeg44K dataset and 1.32 on the DAVIS dataset\nas compared to the previous state-of-the-art SegNext, while also doubling\ninference speed compared to current leading methods. The project page is\nhttps://ukaukaaaa.github.io/projects/OIS/index.html\n","authors":["Bin Wang","Anwesa Choudhuri","Meng Zheng","Zhongpai Gao","Benjamin Planche","Andong Deng","Qin Liu","Terrence Chen","Ulas Bagci","Ziyan Wu"],"pdf_url":"https://arxiv.org/pdf/2410.12214v1.pdf","comment":"Interactive demo can be found in project page:\n  https://ukaukaaaa.github.io/projects/OIS/index.html"},{"id":"http://arxiv.org/abs/2407.16874v2","updated":"2024-10-16T04:18:45Z","published":"2024-07-23T22:46:21Z","title":"Vision-Based Adaptive Robotics for Autonomous Surface Crack Repair","summary":"  Surface cracks in infrastructure can lead to significant deterioration and\ncostly maintenance if not efficiently repaired. Manual repair methods are\nlabor-intensive, time-consuming, and imprecise and thus difficult to scale to\nlarge areas. While advancements in robotic perception and manipulation have\nprogressed autonomous crack repair, existing methods still face three key\nchallenges: accurate localization of cracks within the robot's coordinate\nframe, (ii) adaptability to varying crack depths and widths, and (iii)\nvalidation of the repair process under realistic conditions. This paper\npresents an adaptive, autonomous system for surface crack detection and repair\nusing robotics with advanced sensing technologies to enhance precision and\nsafety for humans. The system uses an RGB-D camera for crack detection, a laser\nscanner for precise measurement, and an extruder and pump for material\ndeposition. To address one of the key challenges, the laser scanner is used to\nenhance the crack coordinates for accurate localization. Furthermore, our\napproach demonstrates that an adaptive crack-filling method is more efficient\nand effective than a fixed-speed approach, with experimental results confirming\nboth precision and consistency. In addition, to ensure real-world applicability\nand testing repeatability, we introduce a novel validation procedure using\n3D-printed crack specimens that accurately simulate real-world conditions. This\nresearch contributes to the evolving field of human-robot interaction in\nconstruction by demonstrating how adaptive robotic systems can reduce the need\nfor manual labor, improve safety, and enhance the efficiency of maintenance\noperations, ultimately paving the way for more sophisticated and integrated\nconstruction robotics.\n","authors":["Joshua Genova","Eric Cabrera","Vedhus Hoskere"],"pdf_url":"https://arxiv.org/pdf/2407.16874v2.pdf","comment":"22 pages, 14 figures, submitted to Advanced Engineering Informatics"},{"id":"http://arxiv.org/abs/2409.02529v3","updated":"2024-10-16T04:08:57Z","published":"2024-09-04T08:42:42Z","title":"Sample what you cant compress","summary":"  For learned image representations, basic autoencoders often produce blurry\nresults. Reconstruction quality can be improved by incorporating additional\npenalties such as adversarial (GAN) and perceptual losses. Arguably, these\napproaches lack a principled interpretation. Concurrently, in generative\nsettings diffusion has demonstrated a remarkable ability to create crisp, high\nquality results and has solid theoretical underpinnings (from variational\ninference to direct study as the Fisher Divergence). Our work combines\nautoencoder representation learning with diffusion and is, to our knowledge,\nthe first to demonstrate the efficacy of jointly learning a continuous encoder\nand decoder under a diffusion-based loss. We demonstrate that this approach\nyields better reconstruction quality as compared to GAN-based autoencoders\nwhile being easier to tune. We also show that the resulting representation is\neasier to model with a latent diffusion model as compared to the representation\nobtained from a state-of-the-art GAN-based loss. Since our decoder is\nstochastic, it can generate details not encoded in the otherwise deterministic\nlatent representation; we therefore name our approach \"Sample what you can't\ncompress\", or SWYCC for short.\n","authors":["Vighnesh Birodkar","Gabriel Barcik","James Lyon","Sergey Ioffe","David Minnen","Joshua V. Dillon"],"pdf_url":"https://arxiv.org/pdf/2409.02529v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10122v2","updated":"2024-10-16T04:04:01Z","published":"2024-10-14T03:22:26Z","title":"MuseTalk: Real-Time High Quality Lip Synchronization with Latent Space\n  Inpainting","summary":"  Achieving high-resolution, identity consistency, and accurate lip-speech\nsynchronization in face visual dubbing presents significant challenges,\nparticularly for real-time applications like live video streaming. We propose\nMuseTalk, which generates lip-sync targets in a latent space encoded by a\nVariational Autoencoder, enabling high-fidelity talking face video generation\nwith efficient inference. Specifically, we project the occluded lower half of\nthe face image and itself as an reference into a low-dimensional latent space\nand use a multi-scale U-Net to fuse audio and visual features at various\nlevels. We further propose a novel sampling strategy during training, which\nselects reference images with head poses closely matching the target, allowing\nthe model to focus on precise lip movement by filtering out redundant\ninformation. Additionally, we analyze the mechanism of lip-sync loss and reveal\nits relationship with input information volume. Extensive experiments show that\nMuseTalk consistently outperforms recent state-of-the-art methods in visual\nfidelity and achieves comparable lip-sync accuracy. As MuseTalk supports the\nonline generation of face at 256x256 at more than 30 FPS with negligible\nstarting latency, it paves the way for real-time applications.\n","authors":["Yue Zhang","Minhao Liu","Zhaokang Chen","Bin Wu","Yubin Zeng","Chao Zhan","Yingjie He","Junxin Huang","Wenjiang Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.10122v2.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.07669v2","updated":"2024-10-16T03:45:50Z","published":"2024-10-10T07:29:24Z","title":"Delta-ICM: Entropy Modeling with Delta Function for Learned Image\n  Compression","summary":"  Image Coding for Machines (ICM) is becoming more important as research in\ncomputer vision progresses. ICM is a vital research field that pursues the use\nof images for image recognition models, facilitating efficient image\ntransmission and storage. The demand for recognition models is growing rapidly\namong the general public, and their performance continues to improve. To meet\nthese needs, exchanging image data between consumer devices and cloud AI using\nICM technology could be one possible solution. In ICM, various image\ncompression methods have adopted Learned Image Compression (LIC). LIC includes\nan entropy model for estimating the bitrate of latent features, and the design\nof this model significantly affects its performance. Typically, LIC methods\nassume that the distribution of latent features follows a normal distribution.\nThis assumption is effective for compressing images intended for human vision.\nHowever, employing an entropy model based on normal distribution is inefficient\nin ICM due to the limitation of image parts that require precise decoding. To\naddress this, we propose Delta-ICM, which uses a probability distribution based\non a delta function. Assuming the delta distribution as a distribution of\nlatent features reduces the entropy of image portions unnecessary for machines.\nWe compress the remaining portions using an entropy model based on normal\ndistribution, similar to existing methods. Delta-ICM selects between the\nentropy model based on the delta distribution and the one based on the normal\ndistribution for each latent feature. Our method outperforms existing ICM\nmethods in image compression performance aimed at machines.\n","authors":["Takahiro Shindo","Taiju Watanabe","Yui Tatsumi","Hiroshi Watanabe"],"pdf_url":"https://arxiv.org/pdf/2410.07669v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07464v2","updated":"2024-10-16T03:44:41Z","published":"2024-07-10T08:40:39Z","title":"Video-to-Audio Generation with Hidden Alignment","summary":"  Generating semantically and temporally aligned audio content in accordance\nwith video input has become a focal point for researchers, particularly\nfollowing the remarkable breakthrough in text-to-video generation. In this\nwork, we aim to offer insights into the video-to-audio generation paradigm,\nfocusing on three crucial aspects: vision encoders, auxiliary embeddings, and\ndata augmentation techniques. Beginning with a foundational model built on a\nsimple yet surprisingly effective intuition, we explore various vision encoders\nand auxiliary embeddings through ablation studies. Employing a comprehensive\nevaluation pipeline that emphasizes generation quality and video-audio\nsynchronization alignment, we demonstrate that our model exhibits\nstate-of-the-art video-to-audio generation capabilities. Furthermore, we\nprovide critical insights into the impact of different data augmentation\nmethods on enhancing the generation framework's overall capacity. We showcase\npossibilities to advance the challenge of generating synchronized audio from\nsemantic and temporal perspectives. We hope these insights will serve as a\nstepping stone toward developing more realistic and accurate audio-visual\ngeneration models.\n","authors":["Manjie Xu","Chenxing Li","Xinyi Tu","Yong Ren","Rilin Chen","Yu Gu","Wei Liang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2407.07464v2.pdf","comment":"https://sites.google.com/view/vta-ldm"},{"id":"http://arxiv.org/abs/2405.17969v2","updated":"2024-10-16T03:35:22Z","published":"2024-05-28T08:56:33Z","title":"Knowledge Circuits in Pretrained Transformers","summary":"  The remarkable capabilities of modern large language models are rooted in\ntheir vast repositories of knowledge encoded within their parameters, enabling\nthem to perceive the world and engage in reasoning. The inner workings of how\nthese models store knowledge have long been a subject of intense interest and\ninvestigation among researchers. To date, most studies have concentrated on\nisolated components within these models, such as the Multilayer Perceptrons and\nattention head. In this paper, we delve into the computation graph of the\nlanguage model to uncover the knowledge circuits that are instrumental in\narticulating specific knowledge. The experiments, conducted with GPT2 and\nTinyLLAMA, have allowed us to observe how certain information heads, relation\nheads, and Multilayer Perceptrons collaboratively encode knowledge within the\nmodel. Moreover, we evaluate the impact of current knowledge editing techniques\non these knowledge circuits, providing deeper insights into the functioning and\nconstraints of these editing methodologies. Finally, we utilize knowledge\ncircuits to analyze and interpret language model behaviors such as\nhallucinations and in-context learning. We believe the knowledge circuits hold\npotential for advancing our understanding of Transformers and guiding the\nimproved design of knowledge editing. Code and data are available in\nhttps://github.com/zjunlp/KnowledgeCircuits.\n","authors":["Yunzhi Yao","Ningyu Zhang","Zekun Xi","Mengru Wang","Ziwen Xu","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.17969v2.pdf","comment":"NeurIPS 2024, 32 pages"},{"id":"http://arxiv.org/abs/2410.12195v1","updated":"2024-10-16T03:33:40Z","published":"2024-10-16T03:33:40Z","title":"Sparse Prototype Network for Explainable Pedestrian Behavior Prediction","summary":"  Predicting pedestrian behavior is challenging yet crucial for applications\nsuch as autonomous driving and smart city. Recent deep learning models have\nachieved remarkable performance in making accurate predictions, but they fail\nto provide explanations of their inner workings. One reason for this problem is\nthe multi-modal inputs. To bridge this gap, we present Sparse Prototype Network\n(SPN), an explainable method designed to simultaneously predict a pedestrian's\nfuture action, trajectory, and pose. SPN leverages an intermediate prototype\nbottleneck layer to provide sample-based explanations for its predictions. The\nprototypes are modality-independent, meaning that they can correspond to any\nmodality from the input. Therefore, SPN can extend to arbitrary combinations of\nmodalities. Regularized by mono-semanticity and clustering constraints, the\nprototypes learn consistent and human-understandable features and achieve\nstate-of-the-art performance on action, trajectory and pose prediction on TITAN\nand PIE. Finally, we propose a metric named Top-K Mono-semanticity Scale to\nquantitatively evaluate the explainability. Qualitative results show the\npositive correlation between sparsity and explainability. Code available at\nhttps://github.com/Equinoxxxxx/SPN.\n","authors":["Yan Feng","Alexander Carballo","Kazuya Takeda"],"pdf_url":"https://arxiv.org/pdf/2410.12195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12191v1","updated":"2024-10-16T03:25:16Z","published":"2024-10-16T03:25:16Z","title":"Test-time adaptation for image compression with distribution\n  regularization","summary":"  Current test- or compression-time adaptation image compression (TTA-IC)\napproaches, which leverage both latent and decoder refinements as a two-step\nadaptation scheme, have potentially enhanced the rate-distortion (R-D)\nperformance of learned image compression models on cross-domain compression\ntasks, \\textit{e.g.,} from natural to screen content images. However, compared\nwith the emergence of various decoder refinement variants, the latent\nrefinement, as an inseparable ingredient, is barely tailored to cross-domain\nscenarios. To this end, we aim to develop an advanced latent refinement method\nby extending the effective hybrid latent refinement (HLR) method, which is\ndesigned for \\textit{in-domain} inference improvement but shows noticeable\ndegradation of the rate cost in \\textit{cross-domain} tasks. Specifically, we\nfirst provide theoretical analyses, in a cue of marginalization approximation\nfrom in- to cross-domain scenarios, to uncover that the vanilla HLR suffers\nfrom an underlying mismatch between refined Gaussian conditional and hyperprior\ndistributions, leading to deteriorated joint probability approximation of\nmarginal distribution with increased rate consumption. To remedy this issue, we\nintroduce a simple Bayesian approximation-endowed \\textit{distribution\nregularization} to encourage learning a better joint probability approximation\nin a plug-and-play manner. Extensive experiments on six in- and cross-domain\ndatasets demonstrate that our proposed method not only improves the R-D\nperformance compared with other latent refinement counterparts, but also can be\nflexibly integrated into existing TTA-IC methods with incremental benefits.\n","authors":["Kecheng Chen","Pingping Zhang","Tiexin Qin","Shiqi Wang","Hong Yan","Haoliang Li"],"pdf_url":"https://arxiv.org/pdf/2410.12191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11211v2","updated":"2024-10-16T03:03:35Z","published":"2024-10-15T02:55:07Z","title":"CVCP-Fusion: On Implicit Depth Estimation for 3D Bounding Box Prediction","summary":"  Combining LiDAR and Camera-view data has become a common approach for 3D\nObject Detection. However, previous approaches combine the two input streams at\na point-level, throwing away semantic information derived from camera features.\nIn this paper we propose Cross-View Center Point-Fusion, a state-of-the-art\nmodel to perform 3D object detection by combining camera and LiDAR-derived\nfeatures in the BEV space to preserve semantic density from the camera stream\nwhile incorporating spacial data from the LiDAR stream. Our architecture\nutilizes aspects from previously established algorithms, Cross-View\nTransformers and CenterPoint, and runs their backbones in parallel, allowing\nefficient computation for real-time processing and application. In this paper\nwe find that while an implicitly calculated depth-estimate may be sufficiently\naccurate in a 2D map-view representation, explicitly calculated geometric and\nspacial information is needed for precise bounding box prediction in the 3D\nworld-view space.\n","authors":["Pranav Gupta","Rishabh Rengarajan","Viren Bankapur","Vedansh Mannem","Lakshit Ahuja","Surya Vijay","Kevin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.11211v2.pdf","comment":"7 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:2205.02833 by other authors"},{"id":"http://arxiv.org/abs/2410.12183v1","updated":"2024-10-16T03:01:44Z","published":"2024-10-16T03:01:44Z","title":"TransAgent: Transfer Vision-Language Foundation Models with\n  Heterogeneous Agent Collaboration","summary":"  Vision-language foundation models (such as CLIP) have recently shown their\npower in transfer learning, owing to large-scale image-text pre-training.\nHowever, target domain data in the downstream tasks can be highly different\nfrom the pre-training phase, which makes it hard for such a single model to\ngeneralize well. Alternatively, there exists a wide range of expert models that\ncontain diversified vision and/or language knowledge pre-trained on different\nmodalities, tasks, networks, and datasets. Unfortunately, these models are\n\"isolated agents\" with heterogeneous structures, and how to integrate their\nknowledge for generalizing CLIP-like models has not been fully explored. To\nbridge this gap, we propose a general and concise TransAgent framework, which\ntransports the knowledge of the isolated agents in a unified manner, and\neffectively guides CLIP to generalize with multi-source knowledge distillation.\nWith such a distinct framework, we flexibly collaborate with 11 heterogeneous\nagents to empower vision-language foundation models, without further cost in\nthe inference phase. Finally, our TransAgent achieves state-of-the-art\nperformance on 11 visual recognition datasets. Under the same low-shot setting,\nit outperforms the popular CoOp with around 10% on average, and 20% on EuroSAT\nwhich contains large domain shifts.\n","authors":["Yiwei Guo","Shaobin Zhuang","Kunchang Li","Yu Qiao","Yali Wang"],"pdf_url":"https://arxiv.org/pdf/2410.12183v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.10471v2","updated":"2024-10-16T02:34:40Z","published":"2024-10-14T13:06:04Z","title":"ReLayout: Towards Real-World Document Understanding via Layout-enhanced\n  Pre-training","summary":"  Recent approaches for visually-rich document understanding (VrDU) uses\nmanually annotated semantic groups, where a semantic group encompasses all\nsemantically relevant but not obviously grouped words. As OCR tools are unable\nto automatically identify such grouping, we argue that current VrDU approaches\nare unrealistic. We thus introduce a new variant of the VrDU task, real-world\nvisually-rich document understanding (ReVrDU), that does not allow for using\nmanually annotated semantic groups. We also propose a new method, ReLayout,\ncompliant with the ReVrDU scenario, which learns to capture semantic grouping\nthrough arranging words and bringing the representations of words that belong\nto the potential same semantic group closer together. Our experimental results\ndemonstrate the performance of existing methods is deteriorated with the ReVrDU\ntask, while ReLayout shows superiour performance.\n","authors":["Zhouqiang Jiang","Bowen Wang","Junhao Chen","Yuta Nakashima"],"pdf_url":"https://arxiv.org/pdf/2410.10471v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12165v1","updated":"2024-10-16T02:06:27Z","published":"2024-10-16T02:06:27Z","title":"Dual-Model Distillation for Efficient Action Classification with Hybrid\n  Edge-Cloud Solution","summary":"  As Artificial Intelligence models, such as Large Video-Language models\n(VLMs), grow in size, their deployment in real-world applications becomes\nincreasingly challenging due to hardware limitations and computational costs.\nTo address this, we design a hybrid edge-cloud solution that leverages the\nefficiency of smaller models for local processing while deferring to larger,\nmore accurate cloud-based models when necessary. Specifically, we propose a\nnovel unsupervised data generation method, Dual-Model Distillation (DMD), to\ntrain a lightweight switcher model that can predict when the edge model's\noutput is uncertain and selectively offload inference to the large model in the\ncloud. Experimental results on the action classification task show that our\nframework not only requires less computational overhead, but also improves\naccuracy compared to using a large model alone. Our framework provides a\nscalable and adaptable solution for action classification in\nresource-constrained environments, with potential applications beyond\nhealthcare. Noteworthy, while DMD-generated data is used for optimizing\nperformance and resource usage in our pipeline, we expect the concept of DMD to\nfurther support future research on knowledge alignment across multiple models.\n","authors":["Timothy Wei","Hsien Xin Peng","Elaine Xu","Bryan Zhao","Lei Ding","Diji Yang"],"pdf_url":"https://arxiv.org/pdf/2410.12165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12158v1","updated":"2024-10-16T01:38:59Z","published":"2024-10-16T01:38:59Z","title":"SAM-Guided Masked Token Prediction for 3D Scene Understanding","summary":"  Foundation models have significantly enhanced 2D task performance, and recent\nworks like Bridge3D have successfully applied these models to improve 3D scene\nunderstanding through knowledge distillation, marking considerable\nadvancements. Nonetheless, challenges such as the misalignment between 2D and\n3D representations and the persistent long-tail distribution in 3D datasets\nstill restrict the effectiveness of knowledge distillation from 2D to 3D using\nfoundation models. To tackle these issues, we introduce a novel SAM-guided\ntokenization method that seamlessly aligns 3D transformer structures with\nregion-level knowledge distillation, replacing the traditional KNN-based\ntokenization techniques. Additionally, we implement a group-balanced\nre-weighting strategy to effectively address the long-tail problem in knowledge\ndistillation. Furthermore, inspired by the recent success of masked feature\nprediction, our framework incorporates a two-stage masked token prediction\nprocess in which the student model predicts both the global embeddings and the\ntoken-wise local embeddings derived from the teacher models trained in the\nfirst stage. Our methodology has been validated across multiple datasets,\nincluding SUN RGB-D, ScanNet, and S3DIS, for tasks like 3D object detection and\nsemantic segmentation. The results demonstrate significant improvements over\ncurrent State-of-the-art self-supervised methods, establishing new benchmarks\nin this field.\n","authors":["Zhimin Chen","Liang Yang","Yingwei Li","Longlong Jing","Bing Li"],"pdf_url":"https://arxiv.org/pdf/2410.12158v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.13607v4","updated":"2024-10-16T01:11:04Z","published":"2024-06-19T14:58:49Z","title":"Ultra-High-Definition Image Restoration: New Benchmarks and A Dual\n  Interaction Prior-Driven Solution","summary":"  Ultra-High-Definition (UHD) image restoration has acquired remarkable\nattention due to its practical demand. In this paper, we construct UHD snow and\nrain benchmarks, named UHD-Snow and UHD-Rain, to remedy the deficiency in this\nfield. The UHD-Snow/UHD-Rain is established by simulating the physics process\nof rain/snow into consideration and each benchmark contains 3200 degraded/clear\nimage pairs of 4K resolution. Furthermore, we propose an effective UHD image\nrestoration solution by considering gradient and normal priors in model design\nthanks to these priors' spatial and detail contributions. Specifically, our\nmethod contains two branches: (a) feature fusion and reconstruction branch in\nhigh-resolution space and (b) prior feature interaction branch in\nlow-resolution space. The former learns high-resolution features and fuses\nprior-guided low-resolution features to reconstruct clear images, while the\nlatter utilizes normal and gradient priors to mine useful spatial features and\ndetail features to guide high-resolution recovery better. To better utilize\nthese priors, we introduce single prior feature interaction and dual prior\nfeature interaction, where the former respectively fuses normal and gradient\npriors with high-resolution features to enhance prior ones, while the latter\ncalculates the similarity between enhanced prior ones and further exploits dual\nguided filtering to boost the feature interaction of dual priors. We conduct\nexperiments on both new and existing public datasets and demonstrate the\nstate-of-the-art performance of our method on UHD image low-light enhancement,\ndehazing, deblurring, desonwing, and deraining. The source codes and benchmarks\nare available at \\url{https://github.com/wlydlut/UHDDIP}.\n","authors":["Liyan Wang","Cong Wang","Jinshan Pan","Xiaofeng Liu","Weixiang Zhou","Xiaoran Sun","Wei Wang","Zhixun Su"],"pdf_url":"https://arxiv.org/pdf/2406.13607v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.09946v3","updated":"2024-10-16T01:10:44Z","published":"2023-05-17T04:56:11Z","title":"AdaMSS: Adaptive Multi-Modality Segmentation-to-Survival Learning for\n  Survival Outcome Prediction from PET/CT Images","summary":"  Survival prediction is a major concern for cancer management. Deep survival\nmodels based on deep learning have been widely adopted to perform end-to-end\nsurvival prediction from medical images. Recent deep survival models achieved\npromising performance by jointly performing tumor segmentation with survival\nprediction, where the models were guided to extract tumor-related information\nthrough Multi-Task Learning (MTL). However, these deep survival models have\ndifficulties in exploring out-of-tumor prognostic information. In addition,\nexisting deep survival models are unable to effectively leverage multi-modality\nimages. Empirically-designed fusion strategies were commonly adopted to fuse\nmulti-modality information via task-specific manually-designed networks, thus\nlimiting the adaptability to different scenarios. In this study, we propose an\nAdaptive Multi-modality Segmentation-to-Survival model (AdaMSS) for survival\nprediction from PET/CT images. Instead of adopting MTL, we propose a novel\nSegmentation-to-Survival Learning (SSL) strategy, where our AdaMSS is trained\nfor tumor segmentation and survival prediction sequentially in two stages. This\nstrategy enables the AdaMSS to focus on tumor regions in the first stage and\ngradually expand its focus to include other prognosis-related regions in the\nsecond stage. We also propose a data-driven strategy to fuse multi-modality\ninformation, which realizes adaptive optimization of fusion strategies based on\ntraining data during training. With the SSL and data-driven fusion strategies,\nour AdaMSS is designed as an adaptive model that can self-adapt its focus\nregions and fusion strategy for different training stages. Extensive\nexperiments with two large clinical datasets show that our AdaMSS outperforms\nstate-of-the-art survival prediction methods.\n","authors":["Mingyuan Meng","Bingxin Gu","Michael Fulham","Shaoli Song","Dagan Feng","Lei Bi","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2305.09946v3.pdf","comment":"The extended version of this paper has been published at npj\n  Precision Oncology as \"Adaptive segmentation-to-survival learning for\n  survival prediction from multi-modality medical images\""},{"id":"http://arxiv.org/abs/2410.12143v1","updated":"2024-10-16T01:06:12Z","published":"2024-10-16T01:06:12Z","title":"Unveiling the Limits of Alignment: Multi-modal Dynamic Local Fusion\n  Network and A Benchmark for Unaligned RGBT Video Object Detection","summary":"  Current RGB-Thermal Video Object Detection (RGBT VOD) methods still depend on\nmanually aligning data at the image level, which hampers its practical\napplication in real-world scenarios since image pairs captured by multispectral\nsensors often differ in both fields of view and resolution. To address this\nlimitation, we propose a Multi-modal Dynamic Local fusion Network (MDLNet)\ndesigned to handle unaligned RGBT image pairs. Specifically, our proposed\nMulti-modal Dynamic Local Fusion (MDLF) module includes a set of predefined\nboxes, each enhanced with random Gaussian noise to generate a dynamic box. Each\nbox selects a local region from the original high-resolution RGB image. This\nregion is then fused with the corresponding information from another modality\nand reinserted into the RGB. This method adapts to various data alignment\nscenarios by interacting with local features across different ranges.\nSimultaneously, we introduce a Cascaded Temporal Scrambler (CTS) within an\nend-to-end architecture. This module leverages consistent spatiotemporal\ninformation from consecutive frames to enhance the representation capability of\nthe current frame while maintaining network efficiency. We have curated an open\ndataset called UVT-VOD2024 for unaligned RGBT VOD. It consists of 30,494 pairs\nof unaligned RGBT images captured directly from a multispectral camera. We\nconduct a comprehensive evaluation and comparison with MDLNet and\nstate-of-the-art (SOTA) models, demonstrating the superior effectiveness of\nMDLNet. We will release our code and UVT-VOD2024 to the public for further\nresearch.\n","authors":["Qishun Wang","Zhengzheng Tu","Kunpeng Wang","Le Gu","Chuanwang Guo"],"pdf_url":"https://arxiv.org/pdf/2410.12143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01680v3","updated":"2024-10-16T01:06:11Z","published":"2024-03-04T02:25:41Z","title":"Zero-shot Generalizable Incremental Learning for Vision-Language Object\n  Detection","summary":"  This paper presents Incremental Vision-Language Object Detection (IVLOD), a\nnovel learning task designed to incrementally adapt pre-trained Vision-Language\nObject Detection Models (VLODMs) to various specialized domains, while\nsimultaneously preserving their zero-shot generalization capabilities for the\ngeneralized domain. To address this new challenge, we present the\nZero-interference Reparameterizable Adaptation (ZiRa), a novel method that\nintroduces Zero-interference Loss and reparameterization techniques to tackle\nIVLOD without incurring additional inference costs or a significant increase in\nmemory usage. Comprehensive experiments on COCO and ODinW-13 datasets\ndemonstrate that ZiRa effectively safeguards the zero-shot generalization\nability of VLODMs while continuously adapting to new tasks. Specifically, after\ntraining on ODinW-13 datasets, ZiRa exhibits superior performance compared to\nCL-DETR and iDETR, boosting zero-shot generalizability by substantial 13.91 and\n8.74 AP, respectively.Our code is available at\nhttps://github.com/JarintotionDin/ZiRaGroundingDINO.\n","authors":["Jieren Deng","Haojian Zhang","Kun Ding","Jianhua Hu","Xingxuan Zhang","Yunkuan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.01680v3.pdf","comment":"This paper has been accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2208.04464v3","updated":"2024-10-16T00:47:58Z","published":"2022-08-08T23:25:05Z","title":"In the Eye of Transformer: Global-Local Correlation for Egocentric Gaze\n  Estimation","summary":"  In this paper, we present the first transformer-based model to address the\nchallenging problem of egocentric gaze estimation. We observe that the\nconnection between the global scene context and local visual information is\nvital for localizing the gaze fixation from egocentric video frames. To this\nend, we design the transformer encoder to embed the global context as one\nadditional visual token and further propose a novel Global-Local Correlation\n(GLC) module to explicitly model the correlation of the global token and each\nlocal token. We validate our model on two egocentric video datasets - EGTEA\nGaze+ and Ego4D. Our detailed ablation studies demonstrate the benefits of our\nmethod. In addition, our approach exceeds previous state-of-the-arts by a large\nmargin. We also provide additional visualizations to support our claim that\nglobal-local correlation serves a key representation for predicting gaze\nfixation from egocentric videos. More details can be found in our website\n(https://bolinlai.github.io/GLC-EgoGazeEst).\n","authors":["Bolin Lai","Miao Liu","Fiona Ryan","James M. Rehg"],"pdf_url":"https://arxiv.org/pdf/2208.04464v3.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2409.18124v3","updated":"2024-10-16T00:36:13Z","published":"2024-09-26T17:58:55Z","title":"Lotus: Diffusion-based Visual Foundation Model for High-quality Dense\n  Prediction","summary":"  Leveraging the visual priors of pre-trained text-to-image diffusion models\noffers a promising solution to enhance zero-shot generalization in dense\nprediction tasks. However, existing methods often uncritically use the original\ndiffusion formulation, which may not be optimal due to the fundamental\ndifferences between dense prediction and image generation. In this paper, we\nprovide a systemic analysis of the diffusion formulation for the dense\nprediction, focusing on both quality and efficiency. And we find that the\noriginal parameterization type for image generation, which learns to predict\nnoise, is harmful for dense prediction; the multi-step noising/denoising\ndiffusion process is also unnecessary and challenging to optimize. Based on\nthese insights, we introduce Lotus, a diffusion-based visual foundation model\nwith a simple yet effective adaptation protocol for dense prediction.\nSpecifically, Lotus is trained to directly predict annotations instead of\nnoise, thereby avoiding harmful variance. We also reformulate the diffusion\nprocess into a single-step procedure, simplifying optimization and\nsignificantly boosting inference speed. Additionally, we introduce a novel\ntuning strategy called detail preserver, which achieves more accurate and\nfine-grained predictions. Without scaling up the training data or model\ncapacity, Lotus achieves SoTA performance in zero-shot depth and normal\nestimation across various datasets. It also enhances efficiency, being\nsignificantly faster than most existing diffusion-based methods. Lotus'\nsuperior quality and efficiency also enable a wide range of practical\napplications, such as joint estimation, single/multi-view 3D reconstruction,\netc. Project page: https://lotus3d.github.io/.\n","authors":["Jing He","Haodong Li","Wei Yin","Yixun Liang","Leheng Li","Kaiqiang Zhou","Hongbo Zhang","Bingbing Liu","Ying-Cong Chen"],"pdf_url":"https://arxiv.org/pdf/2409.18124v3.pdf","comment":"The first two authors contributed equally. Project page:\n  https://lotus3d.github.io/"},{"id":"http://arxiv.org/abs/2402.02263v5","updated":"2024-10-16T00:15:51Z","published":"2024-02-03T21:12:36Z","title":"MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly\n  Mixed Classifiers","summary":"  Adversarial robustness often comes at the cost of degraded accuracy, impeding\nreal-life applications of robust classification models. Training-based\nsolutions for better trade-offs are limited by incompatibilities with\nalready-trained high-performance large models, necessitating the exploration of\ntraining-free ensemble approaches. Observing that robust models are more\nconfident in correct predictions than in incorrect ones on clean and\nadversarial data alike, we speculate amplifying this \"benign confidence\nproperty\" can reconcile accuracy and robustness in an ensemble setting. To\nachieve so, we propose \"MixedNUTS\", a training-free method where the output\nlogits of a robust classifier and a standard non-robust classifier are\nprocessed by nonlinear transformations with only three parameters, which are\noptimized through an efficient algorithm. MixedNUTS then converts the\ntransformed logits into probabilities and mixes them as the overall output. On\nCIFAR-10, CIFAR-100, and ImageNet datasets, experimental results with custom\nstrong adaptive attacks demonstrate MixedNUTS's vastly improved accuracy and\nnear-SOTA robustness -- it boosts CIFAR-100 clean accuracy by 7.86 points,\nsacrificing merely 0.87 points in robust accuracy.\n","authors":["Yatong Bai","Mo Zhou","Vishal M. Patel","Somayeh Sojoudi"],"pdf_url":"https://arxiv.org/pdf/2402.02263v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06949v2","updated":"2024-10-16T23:55:27Z","published":"2023-07-13T17:59:47Z","title":"HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image\n  Models","summary":"  Personalization has emerged as a prominent aspect within the field of\ngenerative AI, enabling the synthesis of individuals in diverse contexts and\nstyles, while retaining high-fidelity to their identities. However, the process\nof personalization presents inherent challenges in terms of time and memory\nrequirements. Fine-tuning each personalized model needs considerable GPU time\ninvestment, and storing a personalized model per subject can be demanding in\nterms of storage capacity. To overcome these challenges, we propose\nHyperDreamBooth - a hypernetwork capable of efficiently generating a small set\nof personalized weights from a single image of a person. By composing these\nweights into the diffusion model, coupled with fast finetuning, HyperDreamBooth\ncan generate a person's face in various contexts and styles, with high subject\ndetails while also preserving the model's crucial knowledge of diverse styles\nand semantic modifications. Our method achieves personalization on faces in\nroughly 20 seconds, 25x faster than DreamBooth and 125x faster than Textual\nInversion, using as few as one reference image, with the same quality and style\ndiversity as DreamBooth. Also our method yields a model that is 10,000x smaller\nthan a normal DreamBooth model. Project page: https://hyperdreambooth.github.io\n","authors":["Nataniel Ruiz","Yuanzhen Li","Varun Jampani","Wei Wei","Tingbo Hou","Yael Pritch","Neal Wadhwa","Michael Rubinstein","Kfir Aberman"],"pdf_url":"https://arxiv.org/pdf/2307.06949v2.pdf","comment":"project page: https://hyperdreambooth.github.io"},{"id":"http://arxiv.org/abs/2410.13094v1","updated":"2024-10-16T23:42:27Z","published":"2024-10-16T23:42:27Z","title":"Task Consistent Prototype Learning for Incremental Few-shot Semantic\n  Segmentation","summary":"  Incremental Few-Shot Semantic Segmentation (iFSS) tackles a task that\nrequires a model to continually expand its segmentation capability on novel\nclasses using only a few annotated examples. Typical incremental approaches\nencounter a challenge that the objective of the base training phase (fitting\nbase classes with sufficient instances) does not align with the incremental\nlearning phase (rapidly adapting to new classes with less forgetting). This\ndisconnect can result in suboptimal performance in the incremental setting.\nThis study introduces a meta-learning-based prototype approach that encourages\nthe model to learn how to adapt quickly while preserving previous knowledge.\nConcretely, we mimic the incremental evaluation protocol during the base\ntraining session by sampling a sequence of pseudo-incremental tasks. Each task\nin the simulated sequence is trained using a meta-objective to enable rapid\nadaptation without forgetting. To enhance discrimination among class\nprototypes, we introduce prototype space redistribution learning, which\ndynamically updates class prototypes to establish optimal inter-prototype\nboundaries within the prototype space. Extensive experiments on iFSS datasets\nbuilt upon PASCAL and COCO benchmarks show the advanced performance of the\nproposed approach, offering valuable insights for addressing iFSS challenges.\n","authors":["Wenbo Xu","Yanan Wu","Haoran Jiang","Yang Wang","Qiang Wu","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.13094v1.pdf","comment":"conference"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2406.12593v2","updated":"2024-10-16T13:45:54Z","published":"2024-06-18T13:25:18Z","title":"PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval","summary":"  Differentiable Search Index (DSI) utilizes Pre-trained Language Models (PLMs)\nfor efficient document retrieval without relying on external indexes. However,\nDSI needs full re-training to handle updates in dynamic corpora, causing\nsignificant computational inefficiencies. We introduce PromptDSI, a\nprompt-based rehearsal-free approach for instance-wise incremental learning\ndocument retrieval. PromptDSI attaches prompts to the frozen PLM's encoder of\nDSI, leveraging its powerful representation to efficiently index new corpora\nwhile maintaining a balance between stability and plasticity. We eliminate the\ninitial forward pass of prompt-based continual learning methods that doubles\ntraining and inference time. Moreover, we propose a topic-aware prompt pool\nthat employs neural topic embeddings as fixed keys. This strategy ensures\ndiverse and effective prompt usage, addressing the challenge of parameter\nunderutilization caused by the collapse of the query-key matching mechanism.\nOur empirical evaluations demonstrate that BERT-based PromptDSI matches IncDSI\nin managing forgetting while improving new corpora performance by more than 4%\nHits@10 on NQ320k and upto 3% MRR@10 on MS MARCO 300k.\n","authors":["Tuan-Luc Huynh","Thuy-Trang Vu","Weiqing Wang","Yinwei Wei","Trung Le","Dragan Gasevic","Yuan-Fang Li","Thanh-Toan Do"],"pdf_url":"https://arxiv.org/pdf/2406.12593v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2406.14162v3","updated":"2024-10-16T13:16:25Z","published":"2024-06-20T10:04:09Z","title":"DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval\n  Augmented Generation","summary":"  Retrieval Augmented Generation (RAG) is widely employed to ground responses\nto queries on domain-specific documents. But do RAG implementations leave out\nimportant information when answering queries that need an integrated analysis\nof information (e.g., Tell me good news in the stock market today.)? To address\nthese concerns, RAG developers need to annotate information retrieval (IR) data\nfor their domain of interest, which is challenging because (1) domain-specific\nqueries usually need nuanced definitions of relevance beyond shallow semantic\nrelevance; and (2) human or GPT-4 annotation is costly and cannot cover all\n(query, document) pairs (i.e., annotation selection bias), thus harming the\neffectiveness in evaluating IR recall. To address these challenges, we propose\nDIRAS (Domain-specific Information Retrieval Annotation with Scalability), a\nmanual-annotation-free schema that fine-tunes open-sourced LLMs to consider\nnuanced relevance definition and annotate (partial) relevance labels with\ncalibrated relevance scores. Extensive evaluation shows that DIRAS enables\nsmaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking\nunseen (query, document) pairs, and is helpful for real-world RAG development.\nAll code, LLM generations, and human annotations can be found in\n\\url{https://github.com/EdisonNi-hku/DIRAS}.\n","authors":["Jingwei Ni","Tobias Schimanski","Meihong Lin","Mrinmaya Sachan","Elliott Ash","Markus Leippold"],"pdf_url":"https://arxiv.org/pdf/2406.14162v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19014v3","updated":"2024-10-16T12:55:55Z","published":"2024-09-24T01:40:50Z","title":"FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark","summary":"  Text-to-SQL systems have become crucial for translating natural language into\nSQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, the Execution Accuracy\n(EX), the most prevalent evaluation metric, still shows many false positives\nand negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our metric improves\nagreement with human experts (from 62 to 87.04 in Cohen's kappa) with\ncomprehensive context and sophisticated criteria. Our extensive experiments\nyield several key insights: (1) Models' performance increases by over 2.6\npoints on average, substantially affecting rankings on Spider and BIRD\nbenchmarks; (2) The underestimation of models in EX primarily stems from\nannotation quality issues; and (3) Model performance on particularly\nchallenging questions tends to be overestimated. This work contributes to a\nmore accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field.\n","authors":["Heegyu Kim","Taeyang Jeon","Seunghwan Choi","Seungtaek Choi","Hyunsouk Cho"],"pdf_url":"https://arxiv.org/pdf/2409.19014v3.pdf","comment":"preprint, under review"},{"id":"http://arxiv.org/abs/2410.12519v1","updated":"2024-10-16T12:54:34Z","published":"2024-10-16T12:54:34Z","title":"RosePO: Aligning LLM-based Recommenders with Human Values","summary":"  Recently, there has been a growing interest in leveraging Large Language\nModels (LLMs) for recommendation systems, which usually adapt a pre-trained LLM\nto the recommendation scenario through supervised fine-tuning (SFT). However,\nboth the pre-training and SFT stages fail to explicitly model the comparative\nrelationships of a user's preferences on different items. To construct a\n\"helpful and harmless\" LLM-based recommender, we propose a general framework --\nRecommendation with smoothing personalized Preference Optimization (RosePO),\nwhich better aligns with customized human values during the post-training\nstage. Specifically, in addition to the input and chosen response that\nnaturally align with SFT data, we design a rejected sampling strategy tailored\nfor enhancing helpfulness, along with two strategies aimed at mitigating biases\nto promote harmlessness. To ensure robustness against uncertain labels present\nin automatically constructed preference data, we introduce a personalized\nsmoothing factor predicted by a preference oracle into the optimization\nobjective. Evaluation on three real-world datasets demonstrates the\neffectiveness of our method, showcasing not only improved recommendation\nperformance but also mitigation of semantic hallucination and popularity bias.\n","authors":["Jiayi Liao","Xiangnan He","Ruobing Xie","Jiancan Wu","Yancheng Yuan","Xingwu Sun","Zhanhui Kang","Xiang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.12519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12473v1","updated":"2024-10-16T11:41:24Z","published":"2024-10-16T11:41:24Z","title":"Unifying Economic and Language Models for Enhanced Sentiment Analysis of\n  the Oil Market","summary":"  Crude oil, a critical component of the global economy, has its prices\ninfluenced by various factors such as economic trends, political events, and\nnatural disasters. Traditional prediction methods based on historical data have\ntheir limits in forecasting, but recent advancements in natural language\nprocessing bring new possibilities for event-based analysis. In particular,\nLanguage Models (LM) and their advancement, the Generative Pre-trained\nTransformer (GPT), have shown potential in classifying vast amounts of natural\nlanguage. However, these LMs often have difficulty with domain-specific\nterminology, limiting their effectiveness in the crude oil sector. Addressing\nthis gap, we introduce CrudeBERT, a fine-tuned LM specifically for the crude\noil market. The results indicate that CrudeBERT's sentiment scores align more\nclosely with the WTI Futures curve and significantly enhance price predictions,\nunderscoring the crucial role of integrating economic principles into LMs.\n","authors":["Himmet Kaplan","Ralf-Peter Mundani","Heiko Rölke","Albert Weichselbraun","Martin Tschudy"],"pdf_url":"https://arxiv.org/pdf/2410.12473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12451v1","updated":"2024-10-16T10:58:53Z","published":"2024-10-16T10:58:53Z","title":"Mitigating Dual Latent Confounding Biases in Recommender Systems","summary":"  Recommender systems are extensively utilised across various areas to predict\nuser preferences for personalised experiences and enhanced user engagement and\nsatisfaction. Traditional recommender systems, however, are complicated by\nconfounding bias, particularly in the presence of latent confounders that\naffect both item exposure and user feedback. Existing debiasing methods often\nfail to capture the complex interactions caused by latent confounders in\ninteraction data, especially when dual latent confounders affect both the user\nand item sides. To address this, we propose a novel debiasing method that\njointly integrates the Instrumental Variables (IV) approach and identifiable\nVariational Auto-Encoder (iVAE) for Debiased representation learning in\nRecommendation systems, referred to as IViDR. Specifically, IViDR leverages the\nembeddings of user features as IVs to address confounding bias caused by latent\nconfounders between items and user feedback, and reconstructs the embedding of\nitems to obtain debiased interaction data. Moreover, IViDR employs an\nIdentifiable Variational Auto-Encoder (iVAE) to infer identifiable\nrepresentations of latent confounders between item exposure and user feedback\nfrom both the original and debiased interaction data. Additionally, we provide\ntheoretical analyses of the soundness of using IV and the identifiability of\nthe latent representations. Extensive experiments on both synthetic and\nreal-world datasets demonstrate that IViDR outperforms state-of-the-art models\nin reducing bias and providing reliable recommendations.\n","authors":["Jianfeng Deng","Qingfeng Chen","Debo Cheng","Jiuyong Li","Lin Liu","Xiaojing Du"],"pdf_url":"https://arxiv.org/pdf/2410.12451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12400v1","updated":"2024-10-16T09:28:58Z","published":"2024-10-16T09:28:58Z","title":"QUIDS: Query Intent Generation via Dual Space Modeling","summary":"  Query understanding is a crucial component of Information Retrieval (IR),\naimed at identifying the underlying search intent of textual queries. However,\nmost existing approaches oversimplify this task into query classification or\nclustering, which fails to fully capture the nuanced intent behind the query.\nIn this paper, we address the task of query intent generation: to automatically\ngenerate detailed and precise intent descriptions for search queries using\nrelevant and irrelevant documents given a query. These intent descriptions can\nhelp users understand why the search engine considered the top-ranked documents\nrelevant, and provide more transparency to the retrieval process. We propose a\ndual-space model that uses semantic relevance and irrelevance information in\nthe returned documents to explain the understanding of the query intent.\nSpecifically, in the encoding process, we project, separate, and distinguish\nrelevant and irrelevant documents in the representation space. Then, we\nintroduce a semantic decoupling model in the novel disentangling space, where\nthe semantics of irrelevant information are removed from the relevant space,\nensuring that only the essential and relevant intent is captured. This process\nrefines the understanding of the query and provides more accurate explanations\nfor the search results. Experiments on benchmark data demonstrate that our\nmethods produce high-quality query intent descriptions, outperforming existing\nmethods for this task, as well as state-of-the-art query-based summarization\nmethods. A token-level visualization of attention scores reveals that our model\neffectively reduces the focus on irrelevant intent topics. Our findings open up\npromising research and application directions for query intent generation,\nparticularly in exploratory search.\n","authors":["Yumeng Wang","Xiuying Chen","Suzan Verberne"],"pdf_url":"https://arxiv.org/pdf/2410.12400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12366v1","updated":"2024-10-16T08:37:39Z","published":"2024-10-16T08:37:39Z","title":"Multi-Cause Deconfounding for Recommender Systems with Latent\n  Confounders","summary":"  In recommender systems, various latent confounding factors (e.g., user social\nenvironment and item public attractiveness) can affect user behavior, item\nexposure, and feedback in distinct ways. These factors may directly or\nindirectly impact user feedback and are often shared across items or users,\nmaking them multi-cause latent confounders. However, existing methods typically\nfail to account for latent confounders between users and their feedback, as\nwell as those between items and user feedback simultaneously. To address the\nproblem of multi-cause latent confounders, we propose a multi-cause\ndeconfounding method for recommender systems with latent confounders (MCDCF).\nMCDCF leverages multi-cause causal effect estimation to learn substitutes for\nlatent confounders associated with both users and items, using user behaviour\ndata. Specifically, MCDCF treats the multiple items that users interact with\nand the multiple users that interact with items as treatment variables,\nenabling it to learn substitutes for the latent confounders that influence the\nestimation of causality between users and their feedback, as well as between\nitems and user feedback. Additionally, we theoretically demonstrate the\nsoundness of our MCDCF method. Extensive experiments on three real-world\ndatasets demonstrate that our MCDCF method effectively recovers latent\nconfounders related to users and items, reducing bias and thereby improving\nrecommendation accuracy.\n","authors":["Zhirong Huang","Shichao Zhang","Debo Cheng","Jiuyong Li","Lin Liu","Guixian Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00333v2","updated":"2024-10-16T05:21:47Z","published":"2024-06-01T07:18:56Z","title":"A Practice-Friendly LLM-Enhanced Paradigm with Preference Parsing for\n  Sequential Recommendation","summary":"  The training paradigm integrating large language models (LLM) is gradually\nreshaping sequential recommender systems (SRS) and has shown promising results.\nHowever, most existing LLM-enhanced methods rely on rich textual information on\nthe item side and instance-level supervised fine-tuning (SFT) to inject\ncollaborative information into LLM, which is inefficient and limited in many\napplications. To alleviate these problems, this paper proposes a\npractice-friendly LLM-enhanced paradigm with preference parsing (P2Rec) for\nSRS. Specifically, in the information reconstruction stage, we design a new\nuser-level SFT task for collaborative information injection with the assistance\nof a pre-trained SRS model, which is more efficient and compatible with limited\ntext information. Our goal is to let LLM learn to reconstruct a corresponding\nprior preference distribution from each user's interaction sequence, where LLM\nneeds to effectively parse the latent category of each item and the\nrelationship between different items to accomplish this task. In the\ninformation augmentation stage, we feed each item into LLM to obtain a set of\nenhanced embeddings that combine collaborative information and LLM inference\ncapabilities. These embeddings can then be used to help train various future\nSRS models. Finally, we verify the effectiveness and efficiency of our TSLRec\non three SRS benchmark datasets.\n","authors":["Dugang Liu","Shenxian Xian","Xiaolin Lin","Xiaolian Zhang","Hong Zhu","Yuan Fang","Zhen Chen","Zhong Ming"],"pdf_url":"https://arxiv.org/pdf/2406.00333v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12229v1","updated":"2024-10-16T04:44:34Z","published":"2024-10-16T04:44:34Z","title":"Comprehending Knowledge Graphs with Large Language Models for\n  Recommender Systems","summary":"  Recently, the introduction of knowledge graphs (KGs) has significantly\nadvanced recommender systems by facilitating the discovery of potential\nassociations between items. However, existing methods still face several\nlimitations. First, most KGs suffer from missing facts or limited scopes. This\ncan lead to biased knowledge representations, thereby constraining the model's\nperformance. Second, existing methods typically convert textual information\ninto IDs, resulting in the loss of natural semantic connections between\ndifferent items. Third, existing methods struggle to capture high-order\nrelationships in global KGs due to their inefficient layer-by-layer information\npropagation mechanisms, which are prone to introducing significant noise. To\naddress these limitations, we propose a novel method called CoLaKG, which\nleverages large language models (LLMs) for knowledge-aware recommendation. The\nextensive world knowledge and remarkable reasoning capabilities of LLMs enable\nthem to supplement KGs. Additionally, the strong text comprehension abilities\nof LLMs allow for a better understanding of semantic information. Based on\nthis, we first extract subgraphs centered on each item from the KG and convert\nthem into textual inputs for the LLM. The LLM then outputs its comprehension of\nthese item-centered subgraphs, which are subsequently transformed into semantic\nembeddings. Furthermore, to utilize the global information of the KG, we\nconstruct an item-item graph using these semantic embeddings, which can\ndirectly capture higher-order associations between items. Both the semantic\nembeddings and the structural information from the item-item graph are\neffectively integrated into the recommendation model through our designed\nrepresentation alignment and neighbor augmentation modules. Extensive\nexperiments on four real-world datasets demonstrate the superiority of our\nmethod.\n","authors":["Ziqiang Cui","Yunpeng Weng","Xing Tang","Fuyuan Lyu","Dugang Liu","Xiuqiang He","Chen Ma"],"pdf_url":"https://arxiv.org/pdf/2410.12229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12228v1","updated":"2024-10-16T04:44:15Z","published":"2024-10-16T04:44:15Z","title":"Triple Modality Fusion: Aligning Visual, Textual, and Graph Data with\n  Large Language Models for Multi-Behavior Recommendations","summary":"  Integrating diverse data modalities is crucial for enhancing the performance\nof personalized recommendation systems. Traditional models, which often rely on\nsingular data sources, lack the depth needed to accurately capture the\nmultifaceted nature of item features and user behaviors. This paper introduces\na novel framework for multi-behavior recommendations, leveraging the fusion of\ntriple-modality, which is visual, textual, and graph data through alignment\nwith large language models (LLMs). By incorporating visual information, we\ncapture contextual and aesthetic item characteristics; textual data provides\ninsights into user interests and item features in detail; and graph data\nelucidates relationships within the item-behavior heterogeneous graphs. Our\nproposed model called Triple Modality Fusion (TMF) utilizes the power of LLMs\nto align and integrate these three modalities, achieving a comprehensive\nrepresentation of user behaviors. The LLM models the user's interactions\nincluding behaviors and item features in natural languages. Initially, the LLM\nis warmed up using only natural language-based prompts. We then devise the\nmodality fusion module based on cross-attention and self-attention mechanisms\nto integrate different modalities from other models into the same embedding\nspace and incorporate them into an LLM. Extensive experiments demonstrate the\neffectiveness of our approach in improving recommendation accuracy. Further\nablation studies validate the effectiveness of our model design and benefits of\nthe TMF.\n","authors":["Luyi Ma","Xiaohan Li","Zezhong Fan","Jianpeng Xu","Jason Cho","Praveen Kanumala","Kaushiki Nag","Sushant Kumar","Kannan Achan"],"pdf_url":"https://arxiv.org/pdf/2410.12228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17969v2","updated":"2024-10-16T03:35:22Z","published":"2024-05-28T08:56:33Z","title":"Knowledge Circuits in Pretrained Transformers","summary":"  The remarkable capabilities of modern large language models are rooted in\ntheir vast repositories of knowledge encoded within their parameters, enabling\nthem to perceive the world and engage in reasoning. The inner workings of how\nthese models store knowledge have long been a subject of intense interest and\ninvestigation among researchers. To date, most studies have concentrated on\nisolated components within these models, such as the Multilayer Perceptrons and\nattention head. In this paper, we delve into the computation graph of the\nlanguage model to uncover the knowledge circuits that are instrumental in\narticulating specific knowledge. The experiments, conducted with GPT2 and\nTinyLLAMA, have allowed us to observe how certain information heads, relation\nheads, and Multilayer Perceptrons collaboratively encode knowledge within the\nmodel. Moreover, we evaluate the impact of current knowledge editing techniques\non these knowledge circuits, providing deeper insights into the functioning and\nconstraints of these editing methodologies. Finally, we utilize knowledge\ncircuits to analyze and interpret language model behaviors such as\nhallucinations and in-context learning. We believe the knowledge circuits hold\npotential for advancing our understanding of Transformers and guiding the\nimproved design of knowledge editing. Code and data are available in\nhttps://github.com/zjunlp/KnowledgeCircuits.\n","authors":["Yunzhi Yao","Ningyu Zhang","Zekun Xi","Mengru Wang","Ziwen Xu","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.17969v2.pdf","comment":"NeurIPS 2024, 32 pages"},{"id":"http://arxiv.org/abs/2410.09119v2","updated":"2024-10-16T03:15:12Z","published":"2024-10-10T21:13:56Z","title":"$\\textit{lucie}$: An Improved Python Package for Loading Datasets from\n  the UCI Machine Learning Repository","summary":"  The University of California--Irvine (UCI) Machine Learning (ML) Repository\n(UCIMLR) is consistently cited as one of the most popular dataset repositories,\nhosting hundreds of high-impact datasets. However, a significant portion,\nincluding 28.4% of the top 250, cannot be imported via the $\\textit{ucimlrepo}$\npackage that is provided and recommended by the UCIMLR website. Instead, they\nare hosted as .zip files, containing nonstandard formats that are difficult to\nimport without additional ad hoc processing. To address this issue, here we\npresent $\\textit{lucie}$ -- $\\underline{l}oad$ $\\underline{U}niversity$\n$\\underline{C}alifornia$ $\\underline{I}rvine$ $\\underline{e}xamples$ -- a\nutility that automatically determines the data format and imports many of these\npreviously non-importable datasets, while preserving as much of a tabular data\nstructure as possible. $\\textit{lucie}$ was designed using the top 100 most\npopular datasets and benchmarked on the next 130, where it resulted in a\nsuccess rate of 95.4% vs. 73.1% for $\\textit{ucimlrepo}$. $\\textit{lucie}$ is\navailable as a Python package on PyPI with 98% code coverage.\n","authors":["Kenneth Ge","Phuc Nguyen","Ramy Arnaout"],"pdf_url":"https://arxiv.org/pdf/2410.09119v2.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.07427v2","updated":"2024-10-16T02:37:50Z","published":"2024-08-14T10:03:40Z","title":"Beyond Inter-Item Relations: Dynamic Adaption for Enhancing LLM-Based\n  Sequential Recommendation","summary":"  Sequential recommender systems (SRS) predict the next items that users may\nprefer based on user historical interaction sequences. Inspired by the rise of\nlarge language models (LLMs) in various AI applications, there is a surge of\nwork on LLM-based SRS. Despite their attractive performance, existing LLM-based\nSRS still exhibit some limitations, including neglecting intra-item relations,\nignoring long-term collaborative knowledge and using inflexible architecture\ndesigns for adaption. To alleviate these issues, we propose an LLM-based\nsequential recommendation model named DARec. Built on top of coarse-grained\nadaption for capturing inter-item relations, DARec is further enhanced with (1)\ncontext masking that models intra-item relations to help LLM better understand\ntoken and item semantics in the context of SRS, (2) collaborative knowledge\ninjection that helps LLM incorporate long-term collaborative knowledge, and (3)\na dynamic adaption mechanism that uses Bayesian optimization to flexibly choose\nlayer-wise adapter architectures in order to better incorporate different\nsequential information. Extensive experiments demonstrate that DARec can\neffectively handle sequential recommendation in a dynamic and adaptive manner.\n","authors":["CanYi Liu","Wei Li"," Youchen"," Zhang","Hui Li","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2408.07427v2.pdf","comment":"11 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.13070v1","updated":"2024-10-16T21:53:48Z","published":"2024-10-16T21:53:48Z","title":"Is Semantic Chunking Worth the Computational Cost?","summary":"  Recent advances in Retrieval-Augmented Generation (RAG) systems have\npopularized semantic chunking, which aims to improve retrieval performance by\ndividing documents into semantically coherent segments. Despite its growing\nadoption, the actual benefits over simpler fixed-size chunking, where documents\nare split into consecutive, fixed-size segments, remain unclear. This study\nsystematically evaluates the effectiveness of semantic chunking using three\ncommon retrieval-related tasks: document retrieval, evidence retrieval, and\nretrieval-based answer generation. The results show that the computational\ncosts associated with semantic chunking are not justified by consistent\nperformance gains. These findings challenge the previous assumptions about\nsemantic chunking and highlight the need for more efficient chunking strategies\nin RAG systems.\n","authors":["Renyi Qu","Ruixuan Tu","Forrest Bao"],"pdf_url":"https://arxiv.org/pdf/2410.13070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13051v1","updated":"2024-10-16T21:24:13Z","published":"2024-10-16T21:24:13Z","title":"Supply Chain Network Extraction and Entity Classification Leveraging\n  Large Language Models","summary":"  Supply chain networks are critical to the operational efficiency of\nindustries, yet their increasing complexity presents significant challenges in\nmapping relationships and identifying the roles of various entities.\nTraditional methods for constructing supply chain networks rely heavily on\nstructured datasets and manual data collection, limiting their scope and\nefficiency. In contrast, recent advancements in Natural Language Processing\n(NLP) and large language models (LLMs) offer new opportunities for discovering\nand analyzing supply chain networks using unstructured text data. This paper\nproposes a novel approach that leverages LLMs to extract and process raw\ntextual information from publicly available sources to construct a\ncomprehensive supply chain graph. We focus on the civil engineering sector as a\ncase study, demonstrating how LLMs can uncover hidden relationships among\ncompanies, projects, and other entities. Additionally, we fine-tune an LLM to\nclassify entities within the supply chain graph, providing detailed insights\ninto their roles and relationships. The results show that domain-specific\nfine-tuning improves classification accuracy, highlighting the potential of\nLLMs for industry-specific supply chain analysis. Our contributions include the\ndevelopment of a supply chain graph for the civil engineering sector, as well\nas a fine-tuned LLM model that enhances entity classification and understanding\nof supply chain networks.\n","authors":["Tong Liu","Hadi Meidani"],"pdf_url":"https://arxiv.org/pdf/2410.13051v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.13047v1","updated":"2024-10-16T21:17:18Z","published":"2024-10-16T21:17:18Z","title":"LLM Confidence Evaluation Measures in Zero-Shot CSS Classification","summary":"  Assessing classification confidence is critical for leveraging large language\nmodels (LLMs) in automated labeling tasks, especially in the sensitive domains\npresented by Computational Social Science (CSS) tasks. In this paper, we make\nthree key contributions: (1) we propose an uncertainty quantification (UQ)\nperformance measure tailored for data annotation tasks, (2) we compare, for the\nfirst time, five different UQ strategies across three distinct LLMs and CSS\ndata annotation tasks, (3) we introduce a novel UQ aggregation strategy that\neffectively identifies low-confidence LLM annotations and disproportionately\nuncovers data incorrectly labeled by the LLMs. Our results demonstrate that our\nproposed UQ aggregation strategy improves upon existing methods andcan be used\nto significantly improve human-in-the-loop data annotation processes.\n","authors":["David Farr","Iain Cruickshank","Nico Manzonelli","Nicholas Clark","Kate Starbird","Jevin West"],"pdf_url":"https://arxiv.org/pdf/2410.13047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13037v1","updated":"2024-10-16T20:52:39Z","published":"2024-10-16T20:52:39Z","title":"LFOSum: Summarizing Long-form Opinions with Large Language Models","summary":"  Online reviews play a pivotal role in influencing consumer decisions across\nvarious domains, from purchasing products to selecting hotels or restaurants.\nHowever, the sheer volume of reviews -- often containing repetitive or\nirrelevant content -- leads to information overload, making it challenging for\nusers to extract meaningful insights. Traditional opinion summarization models\nface challenges in handling long inputs and large volumes of reviews, while\nnewer Large Language Model (LLM) approaches often fail to generate accurate and\nfaithful summaries. To address those challenges, this paper introduces (1) a\nnew dataset of long-form user reviews, each entity comprising over a thousand\nreviews, (2) two training-free LLM-based summarization approaches that scale to\nlong inputs, and (3) automatic evaluation metrics. Our dataset of user reviews\nis paired with in-depth and unbiased critical summaries by domain experts,\nserving as a reference for evaluation. Additionally, our novel reference-free\nevaluation metrics provide a more granular, context-sensitive assessment of\nsummary faithfulness. We benchmark several open-source and closed-source LLMs\nusing our methods. Our evaluation reveals that LLMs still face challenges in\nbalancing sentiment and format adherence in long-form summaries, though\nopen-source models can narrow the gap when relevant information is retrieved in\na focused manner.\n","authors":["Mir Tafseer Nayeem","Davood Rafiei"],"pdf_url":"https://arxiv.org/pdf/2410.13037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12956v1","updated":"2024-10-16T18:44:28Z","published":"2024-10-16T18:44:28Z","title":"Towards Computational Analysis of Pansori Singing","summary":"  Pansori is one of the most representative vocal genres of Korean traditional\nmusic, which has an elaborated vocal melody line with strong vibrato. Although\nthe music is transmitted orally without any music notation, transcribing\npansori music in Western staff notation has been introduced for several\npurposes, such as documentation of music, education, or research. In this\npaper, we introduce computational analysis of pansori based on both audio and\ncorresponding transcription, how modern Music Information Retrieval tasks can\nbe used in analyzing traditional music and how it revealed different audio\ncharacteristics of what pansori contains.\n","authors":["Sangheon Park","Danbinaerin Han","Dasaem Jeong"],"pdf_url":"https://arxiv.org/pdf/2410.12956v1.pdf","comment":"Late-Breaking Demo Session of the 25th International Society for\n  Music Information Retrieval (ISMIR) Conference, 2024"},{"id":"http://arxiv.org/abs/2407.18940v2","updated":"2024-10-16T18:37:15Z","published":"2024-07-10T18:00:03Z","title":"LitSearch: A Retrieval Benchmark for Scientific Literature Search","summary":"  Literature search questions, such as \"Where can I find research on the\nevaluation of consistency in generated summaries?\" pose significant challenges\nfor modern search engines and retrieval systems. These questions often require\na deep understanding of research concepts and the ability to reason across\nentire articles. In this work, we introduce LitSearch, a retrieval benchmark\ncomprising 597 realistic literature search queries about recent ML and NLP\npapers. LitSearch is constructed using a combination of (1) questions generated\nby GPT-4 based on paragraphs containing inline citations from research papers\nand (2) questions manually written by authors about their recently published\npapers. All LitSearch questions were manually examined or edited by experts to\nensure high quality. We extensively benchmark state-of-the-art retrieval models\nand also evaluate two LLM-based reranking pipelines. We find a significant\nperformance gap between BM25 and state-of-the-art dense retrievers, with a\n24.8% absolute difference in recall@5. The LLM-based reranking strategies\nfurther improve the best-performing dense retriever by 4.4%. Additionally,\ncommercial search engines and research tools like Google Search perform poorly\non LitSearch, lagging behind the best dense retriever by up to 32 recall\npoints. Taken together, these results show that LitSearch is an informative new\ntestbed for retrieval systems while catering to a real-world use case.\n","authors":["Anirudh Ajith","Mengzhou Xia","Alexis Chevalier","Tanya Goyal","Danqi Chen","Tianyu Gao"],"pdf_url":"https://arxiv.org/pdf/2407.18940v2.pdf","comment":"Accepted by EMNLP 2024. Dataset and code are available at\n  https://github.com/princeton-nlp/LitSearch"},{"id":"http://arxiv.org/abs/2410.12890v1","updated":"2024-10-16T08:43:39Z","published":"2024-10-16T08:43:39Z","title":"REFINE on Scarce Data: Retrieval Enhancement through Fine-Tuning via\n  Model Fusion of Embedding Models","summary":"  Retrieval augmented generation (RAG) pipelines are commonly used in tasks\nsuch as question-answering (QA), relying on retrieving relevant documents from\na vector store computed using a pretrained embedding model. However, if the\nretrieved context is inaccurate, the answers generated using the large language\nmodel (LLM) may contain errors or hallucinations. Although pretrained embedding\nmodels have advanced, adapting them to new domains remains challenging.\nFine-tuning is a potential solution, but industry settings often lack the\nnecessary fine-tuning data. To address these challenges, we propose REFINE, a\nnovel technique that generates synthetic data from available documents and then\nuses a model fusion approach to fine-tune embeddings for improved retrieval\nperformance in new domains, while preserving out-of-domain capability. We\nconducted experiments on the two public datasets: SQUAD and RAG-12000 and a\nproprietary TOURISM dataset. Results demonstrate that even the standard\nfine-tuning with the proposed data augmentation technique outperforms the\nvanilla pretrained model. Furthermore, when combined with model fusion, the\nproposed approach achieves superior performance, with a 5.76% improvement in\nrecall on the TOURISM dataset, and 6.58 % and 0.32% enhancement on SQUAD and\nRAG-12000 respectively.\n","authors":["Ambuje Gupta","Mrinal Rawat","Andreas Stolcke","Roberto Pieraccini"],"pdf_url":"https://arxiv.org/pdf/2410.12890v1.pdf","comment":"Accepted in AJCAI'24"},{"id":"http://arxiv.org/abs/2410.12886v1","updated":"2024-10-16T01:57:56Z","published":"2024-10-16T01:57:56Z","title":"AT-RAG: An Adaptive RAG Model Enhancing Query Efficiency with Topic\n  Filtering and Iterative Reasoning","summary":"  Recent advancements in QA with LLM, like GPT-4, have shown limitations in\nhandling complex multi-hop queries. We propose AT-RAG, a novel multistep RAG\nincorporating topic modeling for efficient document retrieval and reasoning.\nUsing BERTopic, our model dynamically assigns topics to queries, improving\nretrieval accuracy and efficiency. We evaluated AT-RAG on multihop benchmark\ndatasets QA and a medical case study QA. Results show significant improvements\nin correctness, completeness, and relevance compared to existing methods.\nAT-RAG reduces retrieval time while maintaining high precision, making it\nsuitable for general tasks QA and complex domain-specific challenges such as\nmedical QA. The integration of topic filtering and iterative reasoning enables\nour model to handle intricate queries efficiently, which makes it suitable for\napplications that require nuanced information retrieval and decision-making.\n","authors":["Mohammad Reza Rezaei","Maziar Hafezi","Amit Satpathy","Lovell Hodge","Ebrahim Pourjafari"],"pdf_url":"https://arxiv.org/pdf/2410.12886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13905v1","updated":"2024-10-16T12:29:22Z","published":"2024-10-16T12:29:22Z","title":"P4GCN: Vertical Federated Social Recommendation with Privacy-Preserving\n  Two-Party Graph Convolution Networks","summary":"  In recent years, graph neural networks (GNNs) have been commonly utilized for\nsocial recommendation systems. However, real-world scenarios often present\nchallenges related to user privacy and business constraints, inhibiting direct\naccess to valuable social information from other platforms. While many existing\nmethods have tackled matrix factorization-based social recommendations without\ndirect social data access, developing GNN-based federated social recommendation\nmodels under similar conditions remains largely unexplored. To address this\nissue, we propose a novel vertical federated social recommendation method\nleveraging privacy-preserving two-party graph convolution networks (P4GCN) to\nenhance recommendation accuracy without requiring direct access to sensitive\nsocial information. First, we introduce a Sandwich-Encryption module to ensure\ncomprehensive data privacy during the collaborative computing process. Second,\nwe provide a thorough theoretical analysis of the privacy guarantees,\nconsidering the participation of both curious and honest parties. Extensive\nexperiments on four real-world datasets demonstrate that P4GCN outperforms\nstate-of-the-art methods in terms of recommendation accuracy. The code is\navailable at https://github.com/WwZzz/P4GCN.\n","authors":["Zheng Wang","Wanwan Wang","Yimin Huang","Zhaopeng Peng","Ziqi Yang","Cheng Wang","Xiaoliang Fan"],"pdf_url":"https://arxiv.org/pdf/2410.13905v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2410.12790v1","updated":"2024-10-16T17:59:49Z","published":"2024-10-16T17:59:49Z","title":"Dual Prototype Evolving for Test-Time Generalization of Vision-Language\n  Models","summary":"  Test-time adaptation, which enables models to generalize to diverse data with\nunlabeled test samples, holds significant value in real-world scenarios.\nRecently, researchers have applied this setting to advanced pre-trained\nvision-language models (VLMs), developing approaches such as test-time prompt\ntuning to further extend their practical applicability. However, these methods\ntypically focus solely on adapting VLMs from a single modality and fail to\naccumulate task-specific knowledge as more samples are processed. To address\nthis, we introduce Dual Prototype Evolving (DPE), a novel test-time adaptation\napproach for VLMs that effectively accumulates task-specific knowledge from\nmulti-modalities. Specifically, we create and evolve two sets of\nprototypes--textual and visual--to progressively capture more accurate\nmulti-modal representations for target classes during test time. Moreover, to\npromote consistent multi-modal representations, we introduce and optimize\nlearnable residuals for each test sample to align the prototypes from both\nmodalities. Extensive experimental results on 15 benchmark datasets demonstrate\nthat our proposed DPE consistently outperforms previous state-of-the-art\nmethods while also exhibiting competitive computational efficiency. Code is\navailable at https://github.com/zhangce01/DPE-CLIP.\n","authors":["Ce Zhang","Simon Stepputtis","Katia Sycara","Yaqi Xie"],"pdf_url":"https://arxiv.org/pdf/2410.12790v1.pdf","comment":"Accepted by NeurIPS 2024. Project page:\n  https://zhangce01.github.io/DPE-CLIP"},{"id":"http://arxiv.org/abs/2410.12785v1","updated":"2024-10-16T17:58:34Z","published":"2024-10-16T17:58:34Z","title":"Metal Price Spike Prediction via a Neurosymbolic Ensemble Approach","summary":"  Predicting price spikes in critical metals such as Cobalt, Copper, Magnesium,\nand Nickel is crucial for mitigating economic risks associated with global\ntrends like the energy transition and reshoring of manufacturing. While\ntraditional models have focused on regression-based approaches, our work\nintroduces a neurosymbolic ensemble framework that integrates multiple neural\nmodels with symbolic error detection and correction rules. This framework is\ndesigned to enhance predictive accuracy by correcting individual model errors\nand offering interpretability through rule-based explanations. We show that our\nmethod provides up to 6.42% improvement in precision, 29.41% increase in recall\nat 13.24% increase in F1 over the best performing neural models. Further, our\nmethod, as it is based on logical rules, has the benefit of affording an\nexplanation as to which combination of neural models directly contribute to a\ngiven prediction.\n","authors":["Nathaniel Lee","Noel Ngu","Harshdeep Singh Sahdev","Pramod Motaganahall","Al Mehdi Saadat Chowdhury","Bowen Xi","Paulo Shakarian"],"pdf_url":"https://arxiv.org/pdf/2410.12785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12784v1","updated":"2024-10-16T17:58:19Z","published":"2024-10-16T17:58:19Z","title":"JudgeBench: A Benchmark for Evaluating LLM-based Judges","summary":"  LLM-based judges have emerged as a scalable alternative to human evaluation\nand are increasingly used to assess, compare, and improve models. However, the\nreliability of LLM-based judges themselves is rarely scrutinized. As LLMs\nbecome more advanced, their responses grow more sophisticated, requiring\nstronger judges to evaluate them. Existing benchmarks primarily focus on a\njudge's alignment with human preferences, but often fail to account for more\nchallenging tasks where crowdsourced human preference is a poor indicator of\nfactual and logical correctness. To address this, we propose a novel evaluation\nframework to objectively evaluate LLM-based judges. Based on this framework, we\npropose JudgeBench, a benchmark for evaluating LLM-based judges on challenging\nresponse pairs spanning knowledge, reasoning, math, and coding. JudgeBench\nleverages a novel pipeline for converting existing difficult datasets into\nchallenging response pairs with preference labels reflecting objective\ncorrectness. Our comprehensive evaluation on a collection of prompted judges,\nfine-tuned judges, multi-agent judges, and reward models shows that JudgeBench\nposes a significantly greater challenge than previous benchmarks, with many\nstrong models (e.g., GPT-4o) performing just slightly better than random\nguessing. Overall, JudgeBench offers a reliable platform for assessing\nincreasingly advanced LLM-based judges. Data and code are available at\nhttps://github.com/ScalerLab/JudgeBench .\n","authors":["Sijun Tan","Siyuan Zhuang","Kyle Montgomery","William Y. Tang","Alejandro Cuadron","Chenguang Wang","Raluca Ada Popa","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2410.12784v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.12783v1","updated":"2024-10-16T17:58:08Z","published":"2024-10-16T17:58:08Z","title":"Context-Scaling versus Task-Scaling in In-Context Learning","summary":"  Transformers exhibit In-Context Learning (ICL), where these models solve new\ntasks by using examples in the prompt without additional training. In our work,\nwe identify and analyze two key components of ICL: (1) context-scaling, where\nmodel performance improves as the number of in-context examples increases and\n(2) task-scaling, where model performance improves as the number of\npre-training tasks increases. While transformers are capable of both\ncontext-scaling and task-scaling, we empirically show that standard Multi-Layer\nPerceptrons (MLPs) with vectorized input are only capable of task-scaling. To\nunderstand how transformers are capable of context-scaling, we first propose a\nsignificantly simplified transformer architecture without key, query, value\nweights. We show that it performs ICL comparably to the original GPT-2 model in\nvarious statistical learning tasks including linear regression, teacher-student\nsettings. Furthermore, a single block of our simplified transformer can be\nviewed as data dependent feature map followed by an MLP. This feature map on\nits own is a powerful predictor that is capable of context-scaling but is not\ncapable of task-scaling. We show empirically that concatenating the output of\nthis feature map with vectorized data as an input to MLPs enables both\ncontext-scaling and task-scaling. This finding provides a simple setting to\nstudy context and task-scaling for ICL.\n","authors":["Amirhesam Abedsoltan","Adityanarayanan Radhakrishnan","Jingfeng Wu","Mikhail Belkin"],"pdf_url":"https://arxiv.org/pdf/2410.12783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16257v2","updated":"2024-10-16T17:57:24Z","published":"2024-06-24T01:45:13Z","title":"Towards Scalable Exact Machine Unlearning Using Parameter-Efficient\n  Fine-Tuning","summary":"  Machine unlearning is the process of efficiently removing the influence of a\ntraining data instance from a trained machine learning model without retraining\nit from scratch. A popular subclass of unlearning approaches is exact machine\nunlearning, which focuses on techniques that explicitly guarantee the removal\nof the influence of a data instance from a model. Exact unlearning approaches\nuse a machine learning model in which individual components are trained on\ndisjoint subsets of the data. During deletion, exact unlearning approaches only\nretrain the affected components rather than the entire model. While existing\napproaches reduce retraining costs, it can still be expensive for an\norganization to retrain a model component as it requires halting a system in\nproduction, which leads to service failure and adversely impacts customers. To\naddress these challenges, we introduce an exact unlearning framework --\nSequence-aware Sharded Sliced Training (S3T), which is designed to enhance the\ndeletion capabilities of an exact unlearning system while minimizing the impact\non model's performance. At the core of S3T, we utilize a lightweight\nparameter-efficient fine-tuning approach that enables parameter isolation by\nsequentially training layers with disjoint data slices. This enables efficient\nunlearning by simply deactivating the layers affected by data deletion.\nFurthermore, to reduce the retraining cost and improve model performance, we\ntrain the model on multiple data sequences, which allows S3T to handle an\nincreased number of deletion requests. Both theoretically and empirically, we\ndemonstrate that S3T attains superior deletion capabilities and enhanced\nperformance compared to baselines across a wide range of settings.\n","authors":["Somnath Basu Roy Chowdhury","Krzysztof Choromanski","Arijit Sehanobish","Avinava Dubey","Snigdha Chaturvedi"],"pdf_url":"https://arxiv.org/pdf/2406.16257v2.pdf","comment":"Preliminary version accepted at the SafeGenAi Workshop, NeurIPS, 2024"},{"id":"http://arxiv.org/abs/2409.06953v2","updated":"2024-10-16T17:56:20Z","published":"2024-09-11T02:29:53Z","title":"Neural Algorithmic Reasoning with Multiple Correct Solutions","summary":"  Neural Algorithmic Reasoning (NAR) aims to optimize classical algorithms.\nHowever, canonical implementations of NAR train neural networks to return only\na single solution, even when there are multiple correct solutions to a problem,\nsuch as single-source shortest paths. For some applications, it is desirable to\nrecover more than one correct solution. To that end, we give the first method\nfor NAR with multiple solutions. We demonstrate our method on two classical\nalgorithms: Bellman-Ford (BF) and Depth-First Search (DFS), favouring deeper\ninsight into two algorithms over a broader survey of algorithms. This method\ninvolves generating appropriate training data as well as sampling and\nvalidating solutions from model output. Each step of our method, which can\nserve as a framework for neural algorithmic reasoning beyond the tasks\npresented in this paper, might be of independent interest to the field and our\nresults represent the first attempt at this task in the NAR literature.\n","authors":["Zeno Kujawa","John Poole","Dobrik Georgiev","Danilo Numeroso","Pietro Liò"],"pdf_url":"https://arxiv.org/pdf/2409.06953v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08895v3","updated":"2024-10-16T17:54:15Z","published":"2024-01-17T00:36:58Z","title":"cedar: Optimized and Unified Machine Learning Input Data Pipelines","summary":"  The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.\n","authors":["Mark Zhao","Emanuel Adamiak","Christos Kozyrakis"],"pdf_url":"https://arxiv.org/pdf/2401.08895v3.pdf","comment":"Accepted to PVLDB Volume 18"},{"id":"http://arxiv.org/abs/2410.12779v1","updated":"2024-10-16T17:53:26Z","published":"2024-10-16T17:53:26Z","title":"Geometry-Aware Generative Autoencoders for Warped Riemannian Metric\n  Learning and Generative Modeling on Data Manifolds","summary":"  Rapid growth of high-dimensional datasets in fields such as single-cell RNA\nsequencing and spatial genomics has led to unprecedented opportunities for\nscientific discovery, but it also presents unique computational and statistical\nchallenges. Traditional methods struggle with geometry-aware data generation,\ninterpolation along meaningful trajectories, and transporting populations via\nfeasible paths. To address these issues, we introduce Geometry-Aware Generative\nAutoencoder (GAGA), a novel framework that combines extensible manifold\nlearning with generative modeling. GAGA constructs a neural network embedding\nspace that respects the intrinsic geometries discovered by manifold learning\nand learns a novel warped Riemannian metric on the data space. This warped\nmetric is derived from both the points on the data manifold and negative\nsamples off the manifold, allowing it to characterize a meaningful geometry\nacross the entire latent space. Using this metric, GAGA can uniformly sample\npoints on the manifold, generate points along geodesics, and interpolate\nbetween populations across the learned manifold. GAGA shows competitive\nperformance in simulated and real world datasets, including a 30% improvement\nover the state-of-the-art methods in single-cell population-level trajectory\ninference.\n","authors":["Xingzhi Sun","Danqi Liao","Kincaid MacDonald","Yanlei Zhang","Chen Liu","Guillaume Huguet","Guy Wolf","Ian Adelstein","Tim G. J. Rudner","Smita Krishnaswamy"],"pdf_url":"https://arxiv.org/pdf/2410.12779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14033v2","updated":"2024-10-16T17:52:37Z","published":"2024-05-22T22:08:13Z","title":"Adversarial Training of Two-Layer Polynomial and ReLU Activation\n  Networks via Convex Optimization","summary":"  Training neural networks which are robust to adversarial attacks remains an\nimportant problem in deep learning, especially as heavily overparameterized\nmodels are adopted in safety-critical settings. Drawing from recent work which\nreformulates the training problems for two-layer ReLU and polynomial activation\nnetworks as convex programs, we devise a convex semidefinite program (SDP) for\nadversarial training of two-layer polynomial activation networks and prove that\nthe convex SDP achieves the same globally optimal solution as its nonconvex\ncounterpart. The convex SDP is observed to improve robust test accuracy against\n$\\ell_\\infty$ attacks relative to the original convex training formulation on\nmultiple datasets. Additionally, we present scalable implementations of\nadversarial training for two-layer polynomial and ReLU networks which are\ncompatible with standard machine learning libraries and GPU acceleration.\nLeveraging these implementations, we retrain the final two fully connected\nlayers of a Pre-Activation ResNet-18 model on the CIFAR-10 dataset with both\npolynomial and ReLU activations. The two `robustified' models achieve\nsignificantly higher robust test accuracies against $\\ell_\\infty$ attacks than\na Pre-Activation ResNet-18 model trained with sharpness-aware minimization,\ndemonstrating the practical utility of convex adversarial training on\nlarge-scale problems.\n","authors":["Daniel Kuelbs","Sanjay Lall","Mert Pilanci"],"pdf_url":"https://arxiv.org/pdf/2405.14033v2.pdf","comment":"17 pages, 2 figures. Added a proof of the main theorem in the\n  appendix. Expanded numerical results section. Added references"},{"id":"http://arxiv.org/abs/2410.12777v1","updated":"2024-10-16T17:51:25Z","published":"2024-10-16T17:51:25Z","title":"Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned\n  Concepts","summary":"  With the rapid progress of diffusion-based content generation, significant\nefforts are being made to unlearn harmful or copyrighted concepts from\npretrained diffusion models (DMs) to prevent potential model misuse. However,\nit is observed that even when DMs are properly unlearned before release,\nmalicious finetuning can compromise this process, causing DMs to relearn the\nunlearned concepts. This occurs partly because certain benign concepts (e.g.,\n\"skin\") retained in DMs are related to the unlearned ones (e.g., \"nudity\"),\nfacilitating their relearning via finetuning. To address this, we propose\nmeta-unlearning on DMs. Intuitively, a meta-unlearned DM should behave like an\nunlearned DM when used as is; moreover, if the meta-unlearned DM undergoes\nmalicious finetuning on unlearned concepts, the related benign concepts\nretained within it will be triggered to self-destruct, hindering the relearning\nof unlearned concepts. Our meta-unlearning framework is compatible with most\nexisting unlearning methods, requiring only the addition of an\neasy-to-implement meta objective. We validate our approach through empirical\nexperiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4\nand SDXL), supported by extensive ablation studies. Our code is available at\nhttps://github.com/sail-sg/Meta-Unlearning.\n","authors":["Hongcheng Gao","Tianyu Pang","Chao Du","Taihang Hu","Zhijie Deng","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.12777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12766v1","updated":"2024-10-16T17:41:59Z","published":"2024-10-16T17:41:59Z","title":"The Non-Local Model Merging Problem: Permutation Symmetries and Variance\n  Collapse","summary":"  Model merging aims to efficiently combine the weights of multiple expert\nmodels, each trained on a specific task, into a single multi-task model, with\nstrong performance across all tasks. When applied to all but the last layer of\nweights, existing methods -- such as Task Arithmetic, TIES-merging, and TALL\nmask merging -- work well to combine expert models obtained by fine-tuning a\ncommon foundation model, operating within a \"local\" neighborhood of the\nfoundation model. This work explores the more challenging scenario of\n\"non-local\" merging, which we find arises when an expert model changes\nsignificantly during pretraining or where the expert models do not even share a\ncommon foundation model.\n  We observe that standard merging techniques often fail to generalize\neffectively in this non-local setting, even when accounting for permutation\nsymmetries using standard techniques. We identify that this failure is, in\npart, due to \"variance collapse\", a phenomenon identified also in the setting\nof linear mode connectivity by Jordan et al. (2023). To address this, we\npropose a multi-task technique to re-scale and shift the output activations of\nthe merged model for each task, aligning its output statistics with those of\nthe corresponding task-specific expert models. Our experiments demonstrate that\nthis correction significantly improves the performance of various model merging\napproaches in non-local settings, providing a strong baseline for future\nresearch on this problem.\n","authors":["Ekansh Sharma","Daniel M. Roy","Gintare Karolina Dziugaite"],"pdf_url":"https://arxiv.org/pdf/2410.12766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17563v3","updated":"2024-10-16T17:36:49Z","published":"2024-04-26T17:45:32Z","title":"An exactly solvable model for emergence and scaling laws in the\n  multitask sparse parity problem","summary":"  Deep learning models can exhibit what appears to be a sudden ability to solve\na new problem as training time, training data, or model size increases, a\nphenomenon known as emergence. In this paper, we present a framework where each\nnew ability (a skill) is represented as a basis function. We solve a simple\nmulti-linear model in this skill-basis, finding analytic expressions for the\nemergence of new skills, as well as for scaling laws of the loss with training\ntime, data size, model size, and optimal compute. We compare our detailed\ncalculations to direct simulations of a two-layer neural network trained on\nmultitask sparse parity, where the tasks in the dataset are distributed\naccording to a power-law. Our simple model captures, using a single fit\nparameter, the sigmoidal emergence of multiple new skills as training time,\ndata size or model size increases in the neural network.\n","authors":["Yoonsoo Nam","Nayara Fonseca","Seok Hyeong Lee","Chris Mingard","Ard A. Louis"],"pdf_url":"https://arxiv.org/pdf/2404.17563v3.pdf","comment":"Accepted at NeurIPS 2024 Conference"},{"id":"http://arxiv.org/abs/2410.12761v1","updated":"2024-10-16T17:32:23Z","published":"2024-10-16T17:32:23Z","title":"SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And\n  Video Generation","summary":"  Recent advances in diffusion models have significantly enhanced their ability\nto generate high-quality images and videos, but they have also increased the\nrisk of producing unsafe content. Existing unlearning/editing-based methods for\nsafe generation remove harmful concepts from models but face several\nchallenges: (1) They cannot instantly remove harmful concepts without training.\n(2) Their safe generation capabilities depend on collected training data. (3)\nThey alter model weights, risking degradation in quality for content unrelated\nto toxic concepts. To address these, we propose SAFREE, a novel, training-free\napproach for safe T2I and T2V, that does not alter the model's weights.\nSpecifically, we detect a subspace corresponding to a set of toxic concepts in\nthe text embedding space and steer prompt embeddings away from this subspace,\nthereby filtering out harmful content while preserving intended semantics. To\nbalance the trade-off between filtering toxicity and preserving safe concepts,\nSAFREE incorporates a novel self-validating filtering mechanism that\ndynamically adjusts the denoising steps when applying the filtered embeddings.\nAdditionally, we incorporate adaptive re-attention mechanisms within the\ndiffusion latent space to selectively diminish the influence of features\nrelated to toxic concepts at the pixel level. In the end, SAFREE ensures\ncoherent safety checking, preserving the fidelity, quality, and safety of the\noutput. SAFREE achieves SOTA performance in suppressing unsafe content in T2I\ngeneration compared to training-free baselines and effectively filters targeted\nconcepts while maintaining high-quality images. It also shows competitive\nresults against training-based methods. We extend SAFREE to various T2I\nbackbones and T2V tasks, showcasing its flexibility and generalization. SAFREE\nprovides a robust and adaptable safeguard for ensuring safe visual generation.\n","authors":["Jaehong Yoon","Shoubin Yu","Vaidehi Patil","Huaxiu Yao","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2410.12761v1.pdf","comment":"The first two authors contributed equally; Project page:\n  https://safree-safe-t2i-t2v.github.io/"},{"id":"http://arxiv.org/abs/2410.12757v1","updated":"2024-10-16T17:25:25Z","published":"2024-10-16T17:25:25Z","title":"StyleDistance: Stronger Content-Independent Style Embeddings with\n  Synthetic Parallel Examples","summary":"  Style representations aim to embed texts with similar writing styles closely\nand texts with different styles far apart, regardless of content. However, the\ncontrastive triplets often used for training these representations may vary in\nboth style and content, leading to potential content leakage in the\nrepresentations. We introduce StyleDistance, a novel approach to training\nstronger content-independent style embeddings. We use a large language model to\ncreate a synthetic dataset of near-exact paraphrases with controlled style\nvariations, and produce positive and negative examples across 40 distinct style\nfeatures for precise contrastive learning. We assess the quality of our\nsynthetic data and embeddings through human and automatic evaluations.\nStyleDistance enhances the content-independence of style embeddings, which\ngeneralize to real-world benchmarks and outperform leading style\nrepresentations in downstream applications. Our model can be found at\nhttps://huggingface.co/StyleDistance/styledistance .\n","authors":["Ajay Patel","Jiacheng Zhu","Justin Qiu","Zachary Horvitz","Marianna Apidianaki","Kathleen McKeown","Chris Callison-Burch"],"pdf_url":"https://arxiv.org/pdf/2410.12757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10267v2","updated":"2024-10-16T17:22:54Z","published":"2023-11-17T01:27:01Z","title":"Energy and Carbon Considerations of Fine-Tuning BERT","summary":"  Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP\ncommunity, existing work quantifying energy costs and associated carbon\nemissions has largely focused on language model pre-training. Although a single\npre-training run draws substantially more energy than fine-tuning, fine-tuning\nis performed more frequently by many more individual actors, and thus must be\naccounted for when considering the energy and carbon footprint of NLP. In order\nto better characterize the role of fine-tuning in the landscape of energy and\ncarbon emissions in NLP, we perform a careful empirical study of the\ncomputational costs of fine-tuning across tasks, datasets, hardware\ninfrastructure and measurement modalities. Our experimental results allow us to\nplace fine-tuning energy and carbon costs into perspective with respect to\npre-training and inference, and outline recommendations to NLP researchers and\npractitioners who wish to improve their fine-tuning energy efficiency.\n","authors":["Xiaorong Wang","Clara Na","Emma Strubell","Sorelle Friedler","Sasha Luccioni"],"pdf_url":"https://arxiv.org/pdf/2311.10267v2.pdf","comment":"EMNLP 2023 Findings; First two authors contributed equally; 12 pages"},{"id":"http://arxiv.org/abs/2410.12747v1","updated":"2024-10-16T17:06:55Z","published":"2024-10-16T17:06:55Z","title":"Initialization Method for Factorization Machine Based on Low-Rank\n  Approximation for Constructing a Corrected Approximate Ising Model","summary":"  This paper presents an initialization method that can approximate a given\napproximate Ising model with a high degree of accuracy using the Factorization\nMachine (FM), a machine learning model. The construction of Ising models using\nFM is applied to the combinatorial optimization problem using the factorization\nmachine with quantum annealing. It is anticipated that the optimization\nperformance of FMQA will be enhanced through the implementation of the\nwarm-start method. Nevertheless, the optimal initialization method for\nleveraging the warm-start approach in FMQA remains undetermined. Consequently,\nthe present study compares a number of initialization methods and identifies\nthe most appropriate for use with a warm-start in FMQA through numerical\nexperimentation. Furthermore, the properties of the proposed FM initialization\nmethod are analyzed using random matrix theory, demonstrating that the\napproximation accuracy of the proposed method is not significantly influenced\nby the specific Ising model under consideration. The findings of this study\nwill facilitate the advancement of combinatorial optimization problem-solving\nthrough the use of Ising machines.\n","authors":["Yuya Seki","Hyakka Nakada","Shu Tanaka"],"pdf_url":"https://arxiv.org/pdf/2410.12747v1.pdf","comment":"25 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.08710v2","updated":"2024-10-16T17:06:41Z","published":"2024-10-11T10:53:38Z","title":"Preferential Normalizing Flows","summary":"  Eliciting a high-dimensional probability distribution from an expert via\nnoisy judgments is notoriously challenging, yet useful for many applications,\nsuch as prior elicitation and reward modeling. We introduce a method for\neliciting the expert's belief density as a normalizing flow based solely on\npreferential questions such as comparing or ranking alternatives. This allows\neliciting in principle arbitrarily flexible densities, but flow estimation is\nsusceptible to the challenge of collapsing or diverging probability mass that\nmakes it difficult in practice. We tackle this problem by introducing a novel\nfunctional prior for the flow, motivated by a decision-theoretic argument, and\nshow empirically that the belief density can be inferred as the function-space\nmaximum a posteriori estimate. We demonstrate our method by eliciting\nmultivariate belief densities of simulated experts, including the prior belief\nof a general-purpose large language model over a real-world dataset.\n","authors":["Petrus Mikkola","Luigi Acerbi","Arto Klami"],"pdf_url":"https://arxiv.org/pdf/2410.08710v2.pdf","comment":"29 pages, 18 figures, Accepted at NeurIPS2024"},{"id":"http://arxiv.org/abs/2307.06541v2","updated":"2024-10-16T16:59:58Z","published":"2023-07-13T03:06:36Z","title":"On the Effective Horizon of Inverse Reinforcement Learning","summary":"  Inverse reinforcement learning (IRL) algorithms often rely on (forward)\nreinforcement learning or planning over a given time horizon to compute an\napproximately optimal policy for a hypothesized reward function and then match\nthis policy with expert demonstrations. The time horizon plays a critical role\nin determining both the accuracy of reward estimates and the computational\nefficiency of IRL algorithms. Interestingly, an \\emph{effective time horizon}\nshorter than the ground-truth value often produces better results faster. This\nwork formally analyzes this phenomenon and provides an explanation: the time\nhorizon controls the complexity of an induced policy class and mitigates\noverfitting with limited data. This analysis serves as a guide for the\nprincipled choice of the effective horizon for IRL. It also prompts us to\nre-examine the classic IRL formulation: it is more natural to learn jointly the\nreward and the effective horizon rather than the reward alone with a given\nhorizon. To validate our findings, we implement a cross-validation extension\nand the experimental results confirm the theoretical analysis.\n","authors":["Yiqing Xu","Finale Doshi-Velez","David Hsu"],"pdf_url":"https://arxiv.org/pdf/2307.06541v2.pdf","comment":"9 pages, under review"},{"id":"http://arxiv.org/abs/2406.14003v3","updated":"2024-10-16T16:51:13Z","published":"2024-06-20T05:13:33Z","title":"Deep Optimal Experimental Design for Parameter Estimation Problems","summary":"  Optimal experimental design is a well studied field in applied science and\nengineering. Techniques for estimating such a design are commonly used within\nthe framework of parameter estimation. Nonetheless, in recent years parameter\nestimation techniques are changing rapidly with the introduction of deep\nlearning techniques to replace traditional estimation methods. This in turn\nrequires the adaptation of optimal experimental design that is associated with\nthese new techniques. In this paper we investigate a new experimental design\nmethodology that uses deep learning. We show that the training of a network as\na Likelihood Free Estimator can be used to significantly simplify the design\nprocess and circumvent the need for the computationally expensive bi-level\noptimization problem that is inherent in optimal experimental design for\nnon-linear systems. Furthermore, deep design improves the quality of the\nrecovery process for parameter estimation problems. As proof of concept we\napply our methodology to two different systems of Ordinary Differential\nEquations.\n","authors":["Md Shahriar Rahim Siddiqui","Arman Rahmim","Eldad Haber"],"pdf_url":"https://arxiv.org/pdf/2406.14003v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12735v1","updated":"2024-10-16T16:51:01Z","published":"2024-10-16T16:51:01Z","title":"CREAM: Consistency Regularized Self-Rewarding Language Models","summary":"  Recent self-rewarding large language models (LLM) have successfully applied\nLLM-as-a-Judge to iteratively improve the alignment performance without the\nneed of human annotations for preference data. These methods commonly utilize\nthe same LLM to act as both the policy model (which generates responses) and\nthe reward model (which scores and ranks those responses). The ranked responses\nare then used as preference pairs to train the LLM via direct alignment\ntechnologies (e.g. DPO). However, it is noteworthy that throughout this\nprocess, there is no guarantee of accuracy in the rewarding and ranking, which\nis critical for ensuring accurate rewards and high-quality preference data.\nEmpirical results from relatively small LLMs (e.g., 7B parameters) also\nindicate that improvements from self-rewarding may diminish after several\niterations in certain situations, which we hypothesize is due to accumulated\nbias in the reward system. This bias can lead to unreliable preference data for\ntraining the LLM. To address this issue, we first formulate and analyze the\ngeneralized iterative preference fine-tuning framework for self-rewarding\nlanguage model. We then introduce the regularization to this generalized\nframework to mitigate the overconfident preference labeling in the\nself-rewarding process. Based on this theoretical insight, we propose a\nConsistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages\nthe rewarding consistency across different iterations to regularize the\nself-rewarding training, helping the model to learn from more reliable\npreference data. With this explicit regularization, our empirical results\ndemonstrate the superiority of CREAM in improving both reward consistency and\nalignment performance. The code is publicly available at\nhttps://github.com/Raibows/CREAM.\n","authors":["Zhaoyang Wang","Weilei He","Zhiyuan Liang","Xuchao Zhang","Chetan Bansal","Ying Wei","Weitong Zhang","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2410.12735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12730v1","updated":"2024-10-16T16:44:12Z","published":"2024-10-16T16:44:12Z","title":"Counterfactual Generative Modeling with Variational Causal Inference","summary":"  Estimating an individual's potential outcomes under counterfactual treatments\nis a challenging task for traditional causal inference and supervised learning\napproaches when the outcome is high-dimensional (e.g. gene expressions, facial\nimages) and covariates are relatively limited. In this case, to predict one's\noutcomes under counterfactual treatments, it is crucial to leverage individual\ninformation contained in its high-dimensional observed outcome in addition to\nthe covariates. Prior works using variational inference in counterfactual\ngenerative modeling have been focusing on neural adaptations and model variants\nwithin the conditional variational autoencoder formulation, which we argue is\nfundamentally ill-suited to the notion of counterfactual in causal inference.\nIn this work, we present a novel variational Bayesian causal inference\nframework and its theoretical backings to properly handle counterfactual\ngenerative modeling tasks, through which we are able to conduct counterfactual\nsupervision end-to-end during training without any counterfactual samples, and\nencourage latent disentanglement that aids the correct identification of causal\neffect in counterfactual generations. In experiments, we demonstrate the\nadvantage of our framework compared to state-of-the-art models in\ncounterfactual generative modeling on multiple benchmarks.\n","authors":["Yulun Wu","Louie McConnell","Claudia Iriondo"],"pdf_url":"https://arxiv.org/pdf/2410.12730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12728v1","updated":"2024-10-16T16:42:20Z","published":"2024-10-16T16:42:20Z","title":"Transformer based super-resolution downscaling for regional reanalysis:\n  Full domain vs tiling approaches","summary":"  Super-resolution (SR) is a promising cost-effective downscaling methodology\nfor producing high-resolution climate information from coarser counterparts. A\nparticular application is downscaling regional reanalysis outputs (predictand)\nfrom the driving global counterparts (predictor). This study conducts an\nintercomparison of various SR downscaling methods focusing on temperature and\nusing the CERRA reanalysis (5.5 km resolution, produced with a regional\natmospheric model driven by ERA5) as example. The method proposed in this work\nis the Swin transformer and two alternative methods are used as benchmark\n(fully convolutional U-Net and convolutional and dense DeepESD) as well as the\nsimple bicubic interpolation. We compare two approaches, the standard one using\nthe full domain as input and a more scalable tiling approach, dividing the full\ndomain into tiles that are used as input. The methods are trained to downscale\nCERRA surface temperature, based on temperature information from the driving\nERA5; in addition, the tiling approach includes static orographic information.\nWe show that the tiling approach, which requires spatial transferability, comes\nat the cost of a lower performance (although it outperforms some full-domain\nbenchmarks), but provides an efficient scalable solution that allows SR\nreduction on a pan-European scale and is valuable for real-time applications.\n","authors":["Antonio Pérez","Mario Santa Cruz","Daniel San Martín","José Manuel Gutiérrez"],"pdf_url":"https://arxiv.org/pdf/2410.12728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06163v2","updated":"2024-10-16T16:40:00Z","published":"2024-10-08T16:08:24Z","title":"Likelihood-based Differentiable Structure Learning","summary":"  Existing approaches to differentiable structure learning of directed acyclic\ngraphs (DAGs) rely on strong identifiability assumptions in order to guarantee\nthat global minimizers of the acyclicity-constrained optimization problem\nidentifies the true DAG. Moreover, it has been observed empirically that the\noptimizer may exploit undesirable artifacts in the loss function. We explain\nand remedy these issues by studying the behavior of differentiable\nacyclicity-constrained programs under general likelihoods with multiple global\nminimizers. By carefully regularizing the likelihood, it is possible to\nidentify the sparsest model in the Markov equivalence class, even in the\nabsence of an identifiable parametrization. We first study the Gaussian case in\ndetail, showing how proper regularization of the likelihood defines a score\nthat identifies the sparsest model. Assuming faithfulness, it also recovers the\nMarkov equivalence class. These results are then generalized to general models\nand likelihoods, where the same claims hold. These theoretical results are\nvalidated empirically, showing how this can be done using standard\ngradient-based optimizers, thus paving the way for differentiable structure\nlearning under general models and losses.\n","authors":["Chang Deng","Kevin Bello","Pradeep Ravikumar","Bryon Aragam"],"pdf_url":"https://arxiv.org/pdf/2410.06163v2.pdf","comment":"38 pages, 14 figures, to appear at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.12725v1","updated":"2024-10-16T16:36:23Z","published":"2024-10-16T16:36:23Z","title":"Optimizing 3D Geometry Reconstruction from Implicit Neural\n  Representations","summary":"  Implicit neural representations have emerged as a powerful tool in learning\n3D geometry, offering unparalleled advantages over conventional representations\nlike mesh-based methods. A common type of INR implicitly encodes a shape's\nboundary as the zero-level set of the learned continuous function and learns a\nmapping from a low-dimensional latent space to the space of all possible shapes\nrepresented by its signed distance function. However, most INRs struggle to\nretain high-frequency details, which are crucial for accurate geometric\ndepiction, and they are computationally expensive. To address these\nlimitations, we present a novel approach that both reduces computational\nexpenses and enhances the capture of fine details. Our method integrates\nperiodic activation functions, positional encodings, and normals into the\nneural network architecture. This integration significantly enhances the\nmodel's ability to learn the entire space of 3D shapes while preserving\nintricate details and sharp features, areas where conventional representations\noften fall short.\n","authors":["Shen Fan","Przemyslaw Musialski"],"pdf_url":"https://arxiv.org/pdf/2410.12725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10759v2","updated":"2024-10-16T16:31:37Z","published":"2024-10-14T17:38:41Z","title":"SplitLLM: Collaborative Inference of LLMs for Model Placement and\n  Throughput Optimization","summary":"  Large language models (LLMs) have been a disruptive innovation in recent\nyears, and they play a crucial role in our daily lives due to their ability to\nunderstand and generate human-like text. Their capabilities include natural\nlanguage understanding, information retrieval and search, translation,\nchatbots, virtual assistance, and many more. However, it is well known that\nLLMs are massive in terms of the number of parameters. Additionally, the\nself-attention mechanism in the underlying architecture of LLMs, Transformers,\nhas quadratic complexity in terms of both computation and memory with respect\nto the input sequence length. For these reasons, LLM inference is\nresource-intensive, and thus, the throughput of LLM inference is limited,\nespecially for the longer sequences. In this report, we design a collaborative\ninference architecture between a server and its clients to alleviate the\nthroughput limit. In this design, we consider the available resources on both\nsides, i.e., the computation and communication costs. We develop a dynamic\nprogramming-based algorithm to optimally allocate computation between the\nserver and the client device to increase the server throughput, while not\nviolating the service level agreement (SLA). We show in the experiments that we\nare able to efficiently distribute the workload allowing for roughly 1/3\nreduction in the server workload, while achieving 19 percent improvement over a\ngreedy method. As a result, we are able to demonstrate that, in an environment\nwith different types of LLM inference requests, the throughput of the server is\nimproved.\n","authors":["Akrit Mudvari","Yuang Jiang","Leandros Tassiulas"],"pdf_url":"https://arxiv.org/pdf/2410.10759v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18567v2","updated":"2024-10-16T16:26:16Z","published":"2024-02-28T18:57:56Z","title":"Diffusion Language Models Are Versatile Protein Learners","summary":"  This paper introduces diffusion protein language model (DPLM), a versatile\nprotein language model that demonstrates strong generative and predictive\ncapabilities for protein sequences. We first pre-train scalable DPLMs from\nevolutionary-scale protein sequences within a generative self-supervised\ndiscrete diffusion probabilistic framework, which generalizes language modeling\nfor proteins in a principled way. After pre-training, DPLM exhibits the ability\nto generate structurally plausible, novel, and diverse protein sequences for\nunconditional generation. We further demonstrate the proposed diffusion\ngenerative pre-training makes DPLM possess a better understanding of proteins,\nmaking it a superior representation learner, which can be fine-tuned for\nvarious predictive tasks, comparing favorably to ESM2 (Lin et al., 2022).\nMoreover, DPLM can be tailored for various needs, which showcases its prowess\nof conditional generation in several ways: (1) conditioning on partial peptide\nsequences, e.g., generating scaffolds for functional motifs with high success\nrate; (2) incorporating other modalities as conditioner, e.g.,\nstructure-conditioned generation for inverse folding; and (3) steering sequence\ngeneration towards desired properties, e.g., satisfying specified secondary\nstructures, through a plug-and-play classifier guidance. Code is released at\n\\url{https://github.com/bytedance/dplm}.\n","authors":["Xinyou Wang","Zaixiang Zheng","Fei Ye","Dongyu Xue","Shujian Huang","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2402.18567v2.pdf","comment":"ICML 2024 camera-ready version"},{"id":"http://arxiv.org/abs/2407.20126v2","updated":"2024-10-16T16:20:44Z","published":"2024-07-29T15:55:52Z","title":"Extreme time extrapolation capabilities and thermodynamic consistency of\n  physics-inspired Neural Networks for the 3D microstructure evolution of\n  materials via Cahn-Hilliard flow","summary":"  A Convolutional Recurrent Neural Network (CRNN) is trained to reproduce the\nevolution of the spinodal decomposition process in three dimensions as\ndescribed by the Cahn-Hilliard equation. A specialized, physics-inspired\narchitecture is proven to provide close accordance between the predicted\nevolutions and the ground truth ones obtained via conventional integration\nschemes. The method can accurately reproduce the evolution of microstructures\nnot represented in the training set at a fraction of the computational costs.\nExtremely long-time extrapolation capabilities are achieved, up to reaching the\ntheoretically expected equilibrium state of the system, consisting of a\nlayered, phase-separated morphology, despite the training set containing only\nrelatively-short, initial phases of the evolution. Quantitative accordance with\nthe decay rate of the Free energy is also demonstrated up to the late\ncoarsening stages, proving that this class of Machine Learning approaches can\nbecome a new and powerful tool for the long timescale and high throughput\nsimulation of materials, while retaining thermodynamic consistency and\nhigh-accuracy.\n","authors":["Daniele Lanzoni","Andrea Fantasia","Roberto Bergamaschini","Olivier Pierre-Louis","Francesco Montalenti"],"pdf_url":"https://arxiv.org/pdf/2407.20126v2.pdf","comment":"12 pages, 6 main text figures, 2 appendix figures, 1 supplementary\n  material figure"},{"id":"http://arxiv.org/abs/2410.12713v1","updated":"2024-10-16T16:20:07Z","published":"2024-10-16T16:20:07Z","title":"How Does Variance Shape the Regret in Contextual Bandits?","summary":"  We consider realizable contextual bandits with general function\napproximation, investigating how small reward variance can lead to\nbetter-than-minimax regret bounds. Unlike in minimax bounds, we show that the\neluder dimension $d_\\text{elu}$$-$a complexity measure of the function\nclass$-$plays a crucial role in variance-dependent bounds. We consider two\ntypes of adversary:\n  (1) Weak adversary: The adversary sets the reward variance before observing\nthe learner's action. In this setting, we prove that a regret of\n$\\Omega(\\sqrt{\\min\\{A,d_\\text{elu}\\}\\Lambda}+d_\\text{elu})$ is unavoidable when\n$d_{\\text{elu}}\\leq\\sqrt{AT}$, where $A$ is the number of actions, $T$ is the\ntotal number of rounds, and $\\Lambda$ is the total variance over $T$ rounds.\nFor the $A\\leq d_\\text{elu}$ regime, we derive a nearly matching upper bound\n$\\tilde{O}(\\sqrt{A\\Lambda}+d_\\text{elu})$ for the special case where the\nvariance is revealed at the beginning of each round.\n  (2) Strong adversary: The adversary sets the reward variance after observing\nthe learner's action. We show that a regret of\n$\\Omega(\\sqrt{d_\\text{elu}\\Lambda}+d_\\text{elu})$ is unavoidable when\n$\\sqrt{d_\\text{elu}\\Lambda}+d_\\text{elu}\\leq\\sqrt{AT}$. In this setting, we\nprovide an upper bound of order\n$\\tilde{O}(d_\\text{elu}\\sqrt{\\Lambda}+d_\\text{elu})$.\n  Furthermore, we examine the setting where the function class additionally\nprovides distributional information of the reward, as studied by Wang et al.\n(2024). We demonstrate that the regret bound\n$\\tilde{O}(\\sqrt{d_\\text{elu}\\Lambda}+d_\\text{elu})$ established in their work\nis unimprovable when $\\sqrt{d_{\\text{elu}}\\Lambda}+d_\\text{elu}\\leq\\sqrt{AT}$.\nHowever, with a slightly different definition of the total variance and with\nthe assumption that the reward follows a Gaussian distribution, one can achieve\na regret of $\\tilde{O}(\\sqrt{A\\Lambda}+d_\\text{elu})$.\n","authors":["Zeyu Jia","Jian Qian","Alexander Rakhlin","Chen-Yu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.12713v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.12712v1","updated":"2024-10-16T16:17:21Z","published":"2024-10-16T16:17:21Z","title":"On the sample complexity of purity and inner product estimation","summary":"  We study the sample complexity of the prototypical tasks quantum purity\nestimation and quantum inner product estimation. In purity estimation, we are\nto estimate $tr(\\rho^2)$ of an unknown quantum state $\\rho$ to additive error\n$\\epsilon$. Meanwhile, for quantum inner product estimation, Alice and Bob are\nto estimate $tr(\\rho\\sigma)$ to additive error $\\epsilon$ given copies of\nunknown quantum state $\\rho$ and $\\sigma$ using classical communication and\nrestricted quantum communication.\n  In this paper, we show a strong connection between the sample complexity of\npurity estimation with bounded quantum memory and inner product estimation with\nbounded quantum communication and unentangled measurements. We propose a\nprotocol that solves quantum inner product estimation with $k$-qubit one-way\nquantum communication and unentangled local measurements using\n$O(median\\{1/\\epsilon^2,2^{n/2}/\\epsilon,2^{n-k}/\\epsilon^2\\})$ copies of\n$\\rho$ and $\\sigma$. Our protocol can be modified to estimate the purity of an\nunknown quantum state $\\rho$ using $k$-qubit quantum memory with the same\ncomplexity. We prove that arbitrary protocols with $k$-qubit quantum memory\nthat estimate purity to error $\\epsilon$ require\n$\\Omega(median\\{1/\\epsilon^2,2^{n/2}/\\sqrt{\\epsilon},2^{n-k}/\\epsilon^2\\})$\ncopies of $\\rho$. This indicates the same lower bound for quantum inner product\nestimation with one-way $k$-qubit quantum communication and classical\ncommunication, and unentangled local measurements. For purity estimation, we\nfurther improve the lower bound to\n$\\Omega(\\max\\{1/\\epsilon^2,2^{n/2}/\\epsilon\\})$ for any protocols using an\nidentical single-copy projection-valued measurement.\n  Additionally, we investigate a decisional variant of quantum distributed\ninner product estimation without quantum communication for mixed state and\nprovide a lower bound on the sample complexity.\n","authors":["Weiyuan Gong","Jonas Haferkamp","Qi Ye","Zhihan Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12712v1.pdf","comment":"33 pages, 1 figure"},{"id":"http://arxiv.org/abs/2407.00463v5","updated":"2024-10-16T16:13:32Z","published":"2024-06-29T15:20:11Z","title":"Open-Source Conversational AI with SpeechBrain 1.0","summary":"  SpeechBrain is an open-source Conversational AI toolkit based on PyTorch,\nfocused particularly on speech processing tasks such as speech recognition,\nspeech enhancement, speaker recognition, text-to-speech, and much more. It\npromotes transparency and replicability by releasing both the pre-trained\nmodels and the complete \"recipes\" of code and algorithms required for training\nthem. This paper presents SpeechBrain 1.0, a significant milestone in the\nevolution of the toolkit, which now has over 200 recipes for speech, audio, and\nlanguage processing tasks, and more than 100 models available on Hugging Face.\nSpeechBrain 1.0 introduces new technologies to support diverse learning\nmodalities, Large Language Model (LLM) integration, and advanced decoding\nstrategies, along with novel models, tasks, and modalities. It also includes a\nnew benchmark repository, offering researchers a unified platform for\nevaluating models across diverse tasks.\n","authors":["Mirco Ravanelli","Titouan Parcollet","Adel Moumen","Sylvain de Langen","Cem Subakan","Peter Plantinga","Yingzhi Wang","Pooneh Mousavi","Luca Della Libera","Artem Ploujnikov","Francesco Paissan","Davide Borra","Salah Zaiem","Zeyu Zhao","Shucong Zhang","Georgios Karakasidis","Sung-Lin Yeh","Pierre Champion","Aku Rouhe","Rudolf Braun","Florian Mai","Juan Zuluaga-Gomez","Seyed Mahed Mousavi","Andreas Nautsch","Xuechen Liu","Sangeet Sagar","Jarod Duret","Salima Mdhaffar","Gaelle Laperriere","Mickael Rouvier","Renato De Mori","Yannick Esteve"],"pdf_url":"https://arxiv.org/pdf/2407.00463v5.pdf","comment":"Accepted to the Journal of Machine Learning research (JMLR), Machine\n  Learning Open Source Software"},{"id":"http://arxiv.org/abs/2410.12707v1","updated":"2024-10-16T16:13:19Z","published":"2024-10-16T16:13:19Z","title":"FusionLLM: A Decentralized LLM Training System on Geo-distributed GPUs\n  with Adaptive Compression","summary":"  To alleviate hardware scarcity in training large deep neural networks (DNNs),\nparticularly large language models (LLMs), we present FusionLLM, a\ndecentralized training system designed and implemented for training DNNs using\ngeo-distributed GPUs across different computing clusters or individual devices.\nDecentralized training faces significant challenges regarding system design and\nefficiency, including: 1) the need for remote automatic differentiation (RAD),\n2) support for flexible model definitions and heterogeneous software, 3)\nheterogeneous hardware leading to low resource utilization or the straggler\nproblem, and 4) slow network communication. To address these challenges, in the\nsystem design, we represent the model as a directed acyclic graph of operators\n(OP-DAG). Each node in the DAG represents the operator in the DNNs, while the\nedge represents the data dependency between operators. Based on this design, 1)\nusers are allowed to customize any DNN without caring low-level operator\nimplementation; 2) we enable the task scheduling with the more fine-grained\nsub-tasks, offering more optimization space; 3) a DAG runtime executor can\nimplement RAD withour requiring the consistent low-level ML framework versions.\n  To enhance system efficiency, we implement a workload estimator and design an\nOP-Fence scheduler to cluster devices with similar bandwidths together and\npartition the DAG to increase throughput. Additionally, we propose an AdaTopK\ncompressor to adaptively compress intermediate activations and gradients at the\nslowest communication links. To evaluate the convergence and efficiency of our\nsystem and algorithms, we train ResNet-101 and GPT-2 on three real-world\ntestbeds using 48 GPUs connected with 8 Mbps~10 Gbps networks. Experimental\nresults demonstrate that our system and method can achieve 1.45 - 9.39x speedup\ncompared to baseline methods while ensuring convergence.\n","authors":["Zhenheng Tang","Xueze Kang","Yiming Yin","Xinglin Pan","Yuxin Wang","Xin He","Qiang Wang","Rongfei Zeng","Kaiyong Zhao","Shaohuai Shi","Amelie Chi Zhou","Bo Li","Bingsheng He","Xiaowen Chu"],"pdf_url":"https://arxiv.org/pdf/2410.12707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14106v2","updated":"2024-10-16T16:13:01Z","published":"2024-05-23T02:24:52Z","title":"Nearly Tight Black-Box Auditing of Differentially Private Machine\n  Learning","summary":"  This paper presents an auditing procedure for the Differentially Private\nStochastic Gradient Descent (DP-SGD) algorithm in the black-box threat model\nthat is substantially tighter than prior work. The main intuition is to craft\nworst-case initial model parameters, as DP-SGD's privacy analysis is agnostic\nto the choice of the initial model parameters. For models trained on MNIST and\nCIFAR-10 at theoretical $\\varepsilon=10.0$, our auditing procedure yields\nempirical estimates of $\\varepsilon_{emp} = 7.21$ and $6.95$, respectively, on\na 1,000-record sample and $\\varepsilon_{emp}= 6.48$ and $4.96$ on the full\ndatasets. By contrast, previous audits were only (relatively) tight in stronger\nwhite-box models, where the adversary can access the model's inner parameters\nand insert arbitrary gradients. Overall, our auditing procedure can offer\nvaluable insight into how the privacy analysis of DP-SGD could be improved and\ndetect bugs and DP violations in real-world implementations. The source code\nneeded to reproduce our experiments is available at\nhttps://github.com/spalabucr/bb-audit-dpsgd.\n","authors":["Meenatchi Sundaram Muthu Selva Annamalai","Emiliano De Cristofaro"],"pdf_url":"https://arxiv.org/pdf/2405.14106v2.pdf","comment":"To appear in the Proceedings of the Thirty-eighth Annual Conference\n  on Neural Information Processing Systems (NeurIPS 2024). Please cite\n  accordingly"},{"id":"http://arxiv.org/abs/2410.12704v1","updated":"2024-10-16T16:10:59Z","published":"2024-10-16T16:10:59Z","title":"Sarcasm Detection in a Less-Resourced Language","summary":"  The sarcasm detection task in natural language processing tries to classify\nwhether an utterance is sarcastic or not. It is related to sentiment analysis\nsince it often inverts surface sentiment. Because sarcastic sentences are\nhighly dependent on context, and they are often accompanied by various\nnon-verbal cues, the task is challenging. Most of related work focuses on\nhigh-resourced languages like English. To build a sarcasm detection dataset for\na less-resourced language, such as Slovenian, we leverage two modern\ntechniques: a machine translation specific medium-size transformer model, and a\nvery large generative language model. We explore the viability of translated\ndatasets and how the size of a pretrained transformer affects its ability to\ndetect sarcasm. We train ensembles of detection models and evaluate models'\nperformance. The results show that larger models generally outperform smaller\nones and that ensembling can slightly improve sarcasm detection performance.\nOur best ensemble approach achieves an $\\text{F}_1$-score of 0.765 which is\nclose to annotators' agreement in the source language.\n","authors":["Lazar Đoković","Marko Robnik-Šikonja"],"pdf_url":"https://arxiv.org/pdf/2410.12704v1.pdf","comment":"4 pages, published in the Slovenian Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2310.04727v2","updated":"2024-10-16T16:06:17Z","published":"2023-10-07T07:55:22Z","title":"Task Aware Modulation using Representation Learning: An Approach for Few\n  Shot Learning in Environmental Systems","summary":"  We introduce TAM-RL (Task Aware Modulation using Representation Learning), a\nnovel multimodal meta-learning framework for few-shot learning in heterogeneous\nsystems, designed for science and engineering problems where entities share a\ncommon underlying forward model but exhibit heterogeneity due to\nentity-specific characteristics. TAM-RL leverages an amortized training process\nwith a modulation network and a base network to learn task-specific modulation\nparameters, enabling efficient adaptation to new tasks with limited data. We\nevaluate TAM-RL on two real-world environmental datasets: Gross Primary Product\n(GPP) prediction and streamflow forecasting, demonstrating significant\nimprovements over existing meta-learning methods. On the FLUXNET dataset,\nTAM-RL improves RMSE by 18.9\\% over MMAML with just one month of few-shot data,\nwhile for streamflow prediction, it achieves an 8.21\\% improvement with one\nyear of data. Synthetic data experiments further validate TAM-RL's superior\nperformance in heterogeneous task distributions, outperforming the baselines in\nthe most heterogeneous setting. Notably, TAM-RL offers substantial\ncomputational efficiency, with at least 3x faster training times compared to\ngradient-based meta-learning approaches while being much simpler to train due\nto reduced complexity. Ablation studies highlight the importance of pretraining\nand adaptation mechanisms in TAM-RL's performance.\n","authors":["Arvind Renganathan","Rahul Ghosh","Ankush Khandelwal","Vipin Kumar"],"pdf_url":"https://arxiv.org/pdf/2310.04727v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12703v1","updated":"2024-10-16T16:05:46Z","published":"2024-10-16T16:05:46Z","title":"Neural-based Control for CubeSat Docking Maneuvers","summary":"  Autonomous Rendezvous and Docking (RVD) have been extensively studied in\nrecent years, addressing the stringent requirements of spacecraft dynamics\nvariations and the limitations of GNC systems. This paper presents an\ninnovative approach employing Artificial Neural Networks (ANN) trained through\nReinforcement Learning (RL) for autonomous spacecraft guidance and control\nduring the final phase of the rendezvous maneuver. The proposed strategy is\neasily implementable onboard and offers fast adaptability and robustness to\ndisturbances by learning control policies from experience rather than relying\non predefined models. Extensive Monte Carlo simulations within a relevant\nenvironment are conducted in 6DoF settings to validate our approach, along with\nhardware tests that demonstrate deployment feasibility. Our findings highlight\nthe efficacy of RL in assuring the adaptability and efficiency of spacecraft\nRVD, offering insights into future mission expectations.\n","authors":["Matteo Stoisa","Federica Paganelli Azza","Luca Romanelli","Mattia Varile"],"pdf_url":"https://arxiv.org/pdf/2410.12703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12700v1","updated":"2024-10-16T16:03:42Z","published":"2024-10-16T16:03:42Z","title":"Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via\n  Lightweight Value Optimization","summary":"  Recent advancements in diffusion models trained on large-scale data have\nenabled the generation of indistinguishable human-level images, yet they often\nproduce harmful content misaligned with human values, e.g., social bias, and\noffensive content. Despite extensive research on Large Language Models (LLMs),\nthe challenge of Text-to-Image (T2I) model alignment remains largely\nunexplored. Addressing this problem, we propose LiVO (Lightweight Value\nOptimization), a novel lightweight method for aligning T2I models with human\nvalues. LiVO only optimizes a plug-and-play value encoder to integrate a\nspecified value principle with the input prompt, allowing the control of\ngenerated images over both semantics and values. Specifically, we design a\ndiffusion model-tailored preference optimization loss, which theoretically\napproximates the Bradley-Terry model used in LLM alignment but provides a more\nflexible trade-off between image quality and value conformity. To optimize the\nvalue encoder, we also develop a framework to automatically construct a\ntext-image preference dataset of 86k (prompt, aligned image, violating image,\nvalue principle) samples. Without updating most model parameters and through\nadaptive value selection from the input prompt, LiVO significantly reduces\nharmful outputs and achieves faster convergence, surpassing several strong\nbaselines and taking an initial step towards ethically aligned T2I models.\n","authors":["Xingqi Wang","Xiaoyuan Yi","Xing Xie","Jia Jia"],"pdf_url":"https://arxiv.org/pdf/2410.12700v1.pdf","comment":"Accepted by ACM Multimedia 2024. The dataset and code can be found at\n  https://github.com/achernarwang/LiVO"},{"id":"http://arxiv.org/abs/2410.09838v2","updated":"2024-10-16T15:59:19Z","published":"2024-10-13T13:37:36Z","title":"Uncovering, Explaining, and Mitigating the Superficial Safety of\n  Backdoor Defense","summary":"  Backdoor attacks pose a significant threat to Deep Neural Networks (DNNs) as\nthey allow attackers to manipulate model predictions with backdoor triggers. To\naddress these security vulnerabilities, various backdoor purification methods\nhave been proposed to purify compromised models. Typically, these purified\nmodels exhibit low Attack Success Rates (ASR), rendering them resistant to\nbackdoored inputs. However, Does achieving a low ASR through current safety\npurification methods truly eliminate learned backdoor features from the\npretraining phase? In this paper, we provide an affirmative answer to this\nquestion by thoroughly investigating the Post-Purification Robustness of\ncurrent backdoor purification methods. We find that current safety purification\nmethods are vulnerable to the rapid re-learning of backdoor behavior, even when\nfurther fine-tuning of purified models is performed using a very small number\nof poisoned samples. Based on this, we further propose the practical\nQuery-based Reactivation Attack (QRA) which could effectively reactivate the\nbackdoor by merely querying purified models. We find the failure to achieve\nsatisfactory post-purification robustness stems from the insufficient deviation\nof purified models from the backdoored model along the backdoor-connected path.\nTo improve the post-purification robustness, we propose a straightforward\ntuning defense, Path-Aware Minimization (PAM), which promotes deviation along\nbackdoor-connected paths with extra model updates. Extensive experiments\ndemonstrate that PAM significantly improves post-purification robustness while\nmaintaining a good clean accuracy and low ASR. Our work provides a new\nperspective on understanding the effectiveness of backdoor safety tuning and\nhighlights the importance of faithfully assessing the model's safety.\n","authors":["Rui Min","Zeyu Qin","Nevin L. Zhang","Li Shen","Minhao Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.09838v2.pdf","comment":"NeurIPS 2024 Spotlight paper. The first two authors contributed\n  equally"},{"id":"http://arxiv.org/abs/2405.16012v2","updated":"2024-10-16T15:57:03Z","published":"2024-05-25T02:30:46Z","title":"Pessimistic Backward Policy for GFlowNets","summary":"  This paper studies Generative Flow Networks (GFlowNets), which learn to\nsample objects proportionally to a given reward function through the trajectory\nof state transitions. In this work, we observe that GFlowNets tend to\nunder-exploit the high-reward objects due to training on insufficient number of\ntrajectories, which may lead to a large gap between the estimated flow and the\n(known) reward value. In response to this challenge, we propose a pessimistic\nbackward policy for GFlowNets (PBP-GFN), which maximizes the observed flow to\nalign closely with the true reward for the object. We extensively evaluate\nPBP-GFN across eight benchmarks, including hyper-grid environment, bag\ngeneration, structured set generation, molecular generation, and four RNA\nsequence generation tasks. In particular, PBP-GFN enhances the discovery of\nhigh-reward objects, maintains the diversity of the objects, and consistently\noutperforms existing methods.\n","authors":["Hyosoon Jang","Yunhui Jang","Minsu Kim","Jinkyoo Park","Sungsoo Ahn"],"pdf_url":"https://arxiv.org/pdf/2405.16012v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12692v1","updated":"2024-10-16T15:52:32Z","published":"2024-10-16T15:52:32Z","title":"Machine Learning Approach to Brain Tumor Detection and Classification","summary":"  Brain tumor detection and classification are critical tasks in medical image\nanalysis, particularly in early-stage diagnosis, where accurate and timely\ndetection can significantly improve treatment outcomes. In this study, we apply\nvarious statistical and machine learning models to detect and classify brain\ntumors using brain MRI images. We explore a variety of statistical models\nincluding linear, logistic, and Bayesian regressions, and the machine learning\nmodels including decision tree, random forest, single-layer perceptron,\nmulti-layer perceptron, convolutional neural network (CNN), recurrent neural\nnetwork, and long short-term memory. Our findings show that CNN outperforms\nother models, achieving the best performance. Additionally, we confirm that the\nCNN model can also work for multi-class classification, distinguishing between\nfour categories of brain MRI images such as normal, glioma, meningioma, and\npituitary tumor images. This study demonstrates that machine learning\napproaches are suitable for brain tumor detection and classification,\nfacilitating real-world medical applications in assisting radiologists with\nearly and accurate diagnosis.\n","authors":["Alice Oh","Inyoung Noh","Jian Choo","Jihoo Lee","Justin Park","Kate Hwang","Sanghyeon Kim","Soo Min Oh"],"pdf_url":"https://arxiv.org/pdf/2410.12692v1.pdf","comment":"7 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.12690v1","updated":"2024-10-16T15:50:57Z","published":"2024-10-16T15:50:57Z","title":"Local transfer learning Gaussian process modeling, with applications to\n  surrogate modeling of expensive computer simulators","summary":"  A critical bottleneck for scientific progress is the costly nature of\ncomputer simulations for complex systems. Surrogate models provide an appealing\nsolution: such models are trained on simulator evaluations, then used to\nemulate and quantify uncertainty on the expensive simulator at unexplored\ninputs. In many applications, one often has available data on related systems.\nFor example, in designing a new jet turbine, there may be existing studies on\nturbines with similar configurations. A key question is how information from\nsuch \"source\" systems can be transferred for effective surrogate training on\nthe \"target\" system of interest. We thus propose a new LOcal transfer Learning\nGaussian Process (LOL-GP) model, which leverages a carefully-designed Gaussian\nprocess to transfer such information for surrogate modeling. The key novelty of\nthe LOL-GP is a latent regularization model, which identifies regions where\ntransfer should be performed and regions where it should be avoided. This\n\"local transfer\" property is desirable in scientific systems: at certain\nparameters, such systems may behave similarly and thus transfer is beneficial;\nat other parameters, they may behave differently and thus transfer is\ndetrimental. By accounting for local transfer, the LOL-GP can rectify a\ncritical limitation of \"negative transfer\" in existing transfer learning\nmodels, where the transfer of information worsens predictive performance. We\nderive a Gibbs sampling algorithm for efficient posterior predictive sampling\non the LOL-GP, for both the multi-source and multi-fidelity transfer settings.\nWe then show, via a suite of numerical experiments and an application for jet\nturbine design, the improved surrogate performance of the LOL-GP over existing\nmethods.\n","authors":["Xinming Wang","Simon Mak","John Miller","Jianguo Wu"],"pdf_url":"https://arxiv.org/pdf/2410.12690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12689v1","updated":"2024-10-16T15:49:25Z","published":"2024-10-16T15:49:25Z","title":"A distance function for stochastic matrices","summary":"  Motivated by information geometry, a distance function on the space of\nstochastic matrices is advocated. Starting with sequences of Markov chains the\nBhattacharyya angle is advocated as the natural tool for comparing both short\nand long term Markov chain runs. Bounds on the convergence of the distance and\nmixing times are derived. Guided by the desire to compare different Markov\nchain models, especially in the setting of healthcare processes, a new distance\nfunction on the space of stochastic matrices is presented. It is a true\ndistance measure which has a closed form and is efficient to implement for\nnumerical evaluation. In the case of ergodic Markov chains, it is shown that\nconsidering either the Bhattacharyya angle on Markov sequences or the new\nstochastic matrix distance leads to the same distance between models.\n","authors":["Antony Lee","Peter Tino","Iain Bruce Styles"],"pdf_url":"https://arxiv.org/pdf/2410.12689v1.pdf","comment":"9 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.12686v1","updated":"2024-10-16T15:48:28Z","published":"2024-10-16T15:48:28Z","title":"Automatic Mapping of Anatomical Landmarks from Free-Text Using Large\n  Language Models: Insights from Llama-2","summary":"  Anatomical landmarks are vital in medical imaging for navigation and anomaly\ndetection. Modern large language models (LLMs), like Llama-2, offer promise for\nautomating the mapping of these landmarks in free-text radiology reports to\ncorresponding positions in image data. Recent studies propose LLMs may develop\ncoherent representations of generative processes. Motivated by these insights,\nwe investigated whether LLMs accurately represent the spatial positions of\nanatomical landmarks. Through experiments with Llama-2 models, we found that\nthey can linearly represent anatomical landmarks in space with considerable\nrobustness to different prompts. These results underscore the potential of LLMs\nto enhance the efficiency and accuracy of medical imaging workflows.\n","authors":["Mohamad Abdi","Gerardo Hemosillo Valadez","Halid Ziya Yerebakan"],"pdf_url":"https://arxiv.org/pdf/2410.12686v1.pdf","comment":"6 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2410.12683v1","updated":"2024-10-16T15:46:48Z","published":"2024-10-16T15:46:48Z","title":"Generative Neural Reparameterization for Differentiable PDE-constrained\n  Optimization","summary":"  Partial-differential-equation (PDE)-constrained optimization is a well-worn\ntechnique for acquiring optimal parameters of systems governed by PDEs.\nHowever, this approach is limited to providing a single set of optimal\nparameters per optimization. Given a differentiable PDE solver, if the free\nparameters are reparameterized as the output of a neural network, that neural\nnetwork can be trained to learn a map from a probability distribution to the\ndistribution of optimal parameters. This proves useful in the case where there\nare many well performing local minima for the PDE. We apply this technique to\ntrain a neural network that generates optimal parameters that minimize\nlaser-plasma instabilities relevant to laser fusion and show that the neural\nnetwork generates many well performing and diverse minima.\n","authors":["Archis S. Joglekar"],"pdf_url":"https://arxiv.org/pdf/2410.12683v1.pdf","comment":"Accepted to D3S3: Data-driven and Differentiable Simulations,\n  Surrogates, and Solvers - Workshop @ NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.12679v1","updated":"2024-10-16T15:44:15Z","published":"2024-10-16T15:44:15Z","title":"Optimizing Multi-Task Learning for Accurate Spacecraft Pose Estimation","summary":"  Accurate satellite pose estimation is crucial for autonomous guidance,\nnavigation, and control (GNC) systems in in-orbit servicing (IOS) missions.\nThis paper explores the impact of different tasks within a multi-task learning\n(MTL) framework for satellite pose estimation using monocular images. By\nintegrating tasks such as direct pose estimation, keypoint prediction, object\nlocalization, and segmentation into a single network, the study aims to\nevaluate the reciprocal influence between tasks by testing different multi-task\nconfigurations thanks to the modularity of the convolutional neural network\n(CNN) used in this work. The trends of mutual bias between the analyzed tasks\nare found by employing different weighting strategies to further test the\nrobustness of the findings. A synthetic dataset was developed to train and test\nthe MTL network. Results indicate that direct pose estimation and heatmap-based\npose estimation positively influence each other in general, while both the\nbounding box and segmentation tasks do not provide significant contributions\nand tend to degrade the overall estimation accuracy.\n","authors":["Francesco Evangelisti","Francesco Rossi","Tobia Giani","Ilaria Bloise","Mattia Varile"],"pdf_url":"https://arxiv.org/pdf/2410.12679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12677v1","updated":"2024-10-16T15:41:08Z","published":"2024-10-16T15:41:08Z","title":"Efficient Optimization Algorithms for Linear Adversarial Training","summary":"  Adversarial training can be used to learn models that are robust against\nperturbations. For linear models, it can be formulated as a convex optimization\nproblem. Compared to methods proposed in the context of deep learning,\nleveraging the optimization structure allows significantly faster convergence\nrates. Still, the use of generic convex solvers can be inefficient for\nlarge-scale problems. Here, we propose tailored optimization algorithms for the\nadversarial training of linear models, which render large-scale regression and\nclassification problems more tractable. For regression problems, we propose a\nfamily of solvers based on iterative ridge regression and, for classification,\na family of solvers based on projected gradient descent. The methods are based\non extended variable reformulations of the original problem. We illustrate\ntheir efficiency in numerical examples.\n","authors":["Antônio H. RIbeiro","Thomas B. Schön","Dave Zahariah","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2410.12677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12672v1","updated":"2024-10-16T15:36:13Z","published":"2024-10-16T15:36:13Z","title":"Context Matters: Leveraging Contextual Features for Time Series\n  Forecasting","summary":"  Time series forecasts are often influenced by exogenous contextual features\nin addition to their corresponding history. For example, in financial settings,\nit is hard to accurately predict a stock price without considering public\nsentiments and policy decisions in the form of news articles, tweets, etc.\nThough this is common knowledge, the current state-of-the-art (SOTA)\nforecasting models fail to incorporate such contextual information, owing to\nits heterogeneity and multimodal nature. To address this, we introduce\nContextFormer, a novel plug-and-play method to surgically integrate multimodal\ncontextual information into existing pre-trained forecasting models.\nContextFormer effectively distills forecast-specific information from rich\nmultimodal contexts, including categorical, continuous, time-varying, and even\ntextual information, to significantly enhance the performance of existing base\nforecasters. ContextFormer outperforms SOTA forecasting models by up to 30% on\na range of real-world datasets spanning energy, traffic, environmental, and\nfinancial domains.\n","authors":["Sameep Chattopadhyay","Pulkit Paliwal","Sai Shankar Narasimhan","Shubhankar Agarwal","Sandeep P. Chinchali"],"pdf_url":"https://arxiv.org/pdf/2410.12672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12671v1","updated":"2024-10-16T15:36:10Z","published":"2024-10-16T15:36:10Z","title":"New Paradigm of Adversarial Training: Breaking Inherent Trade-Off\n  between Accuracy and Robustness via Dummy Classes","summary":"  Adversarial Training (AT) is one of the most effective methods to enhance the\nrobustness of DNNs. However, existing AT methods suffer from an inherent\ntrade-off between adversarial robustness and clean accuracy, which seriously\nhinders their real-world deployment. While this problem has been widely studied\nwithin the current AT paradigm, existing AT methods still typically experience\na reduction in clean accuracy by over 10% to date, without significant\nimprovements in robustness compared with simple baselines like PGD-AT. This\ninherent trade-off raises a question: whether the current AT paradigm, which\nassumes to learn the corresponding benign and adversarial samples as the same\nclass, inappropriately combines clean and robust objectives that may be\nessentially inconsistent. In this work, we surprisingly reveal that up to 40%\nof CIFAR-10 adversarial samples always fail to satisfy such an assumption\nacross various AT methods and robust models, explicitly indicating the\nimprovement room for the current AT paradigm. Accordingly, to relax the tension\nbetween clean and robust learning derived from this overstrict assumption, we\npropose a new AT paradigm by introducing an additional dummy class for each\noriginal class, aiming to accommodate the hard adversarial samples with shifted\ndistribution after perturbation. The robustness w.r.t. these adversarial\nsamples can be achieved by runtime recovery from the predicted dummy classes to\ntheir corresponding original ones, eliminating the compromise with clean\nlearning. Building on this new paradigm, we propose a novel plug-and-play AT\ntechnology named DUmmy Classes-based Adversarial Training (DUCAT). Extensive\nexperiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that the\nDUCAT concurrently improves clean accuracy and adversarial robustness compared\nwith state-of-the-art benchmarks, effectively breaking the existing inherent\ntrade-off.\n","authors":["Yanyun Wang","Li Liu","Zi Liang","Qingqing Ye","Haibo Hu"],"pdf_url":"https://arxiv.org/pdf/2410.12671v1.pdf","comment":"Preprint. Work in progress. The code is available at\n  https://github.com/FlaAI/DUCAT"},{"id":"http://arxiv.org/abs/2402.07204v4","updated":"2024-10-16T15:28:18Z","published":"2024-02-11T13:30:53Z","title":"ITINERA: Integrating Spatial Optimization with Large Language Models for\n  Open-domain Urban Itinerary Planning","summary":"  Citywalk, a recently popular form of urban travel, requires genuine\npersonalization and understanding of fine-grained requests compared to\ntraditional itinerary planning. In this paper, we introduce the novel task of\nOpen-domain Urban Itinerary Planning (OUIP), which generates personalized urban\nitineraries from user requests in natural language. We then present ITINERA, an\nOUIP system that integrates spatial optimization with large language models to\nprovide customized urban itineraries based on user needs. This involves\ndecomposing user requests, selecting candidate points of interest (POIs),\nordering the POIs based on cluster-aware spatial optimization, and generating\nthe itinerary. Experiments on real-world datasets and the performance of the\ndeployed system demonstrate our system's capacity to deliver personalized and\nspatially coherent itineraries compared to current solutions. Source codes of\nITINERA are available at https://github.com/YihongT/ITINERA.\n","authors":["Yihong Tang","Zhaokai Wang","Ao Qu","Yihao Yan","Zhaofeng Wu","Dingyi Zhuang","Jushi Kai","Kebing Hou","Xiaotong Guo","Jinhua Zhao","Zhan Zhao","Wei Ma"],"pdf_url":"https://arxiv.org/pdf/2402.07204v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12657v1","updated":"2024-10-16T15:18:03Z","published":"2024-10-16T15:18:03Z","title":"Explanation-Preserving Augmentation for Semi-Supervised Graph\n  Representation Learning","summary":"  Graph representation learning (GRL), enhanced by graph augmentation methods,\nhas emerged as an effective technique achieving performance improvements in\nwide tasks such as node classification and graph classification. In\nself-supervised GRL, paired graph augmentations are generated from each graph.\nIts objective is to infer similar representations for augmentations of the same\ngraph, but maximally distinguishable representations for augmentations of\ndifferent graphs. Analogous to image and language domains, the desiderata of an\nideal augmentation method include both (1) semantics-preservation; and (2)\ndata-perturbation; i.e., an augmented graph should preserve the semantics of\nits original graph while carrying sufficient variance. However, most existing\n(un-)/self-supervised GRL methods focus on data perturbation but largely\nneglect semantics preservation. To address this challenge, in this paper, we\npropose a novel method, Explanation-Preserving Augmentation (EPA), that\nleverages graph explanation techniques for generating augmented graphs that can\nbridge the gap between semantics-preservation and data-perturbation. EPA first\nuses a small number of labels to train a graph explainer to infer the\nsub-structures (explanations) that are most relevant to a graph's semantics.\nThese explanations are then used to generate semantics-preserving augmentations\nfor self-supervised GRL, namely EPA-GRL. We demonstrate theoretically, using an\nanalytical example, and through extensive experiments on a variety of benchmark\ndatasets that EPA-GRL outperforms the state-of-the-art (SOTA) GRL methods,\nwhich are built upon semantics-agnostic data augmentations.\n","authors":["Zhuomin Chen","Jingchao Ni","Hojat Allah Salehi","Xu Zheng","Esteban Schafir","Farhad Shirani","Dongsheng Luo"],"pdf_url":"https://arxiv.org/pdf/2410.12657v1.pdf","comment":"16 pages, 7 figures, 7 tables"},{"id":"http://arxiv.org/abs/2410.12655v1","updated":"2024-10-16T15:16:50Z","published":"2024-10-16T15:16:50Z","title":"Position Specific Scoring Is All You Need? Revisiting Protein Sequence\n  Classification Tasks","summary":"  Understanding the structural and functional characteristics of proteins are\ncrucial for developing preventative and curative strategies that impact fields\nfrom drug discovery to policy development. An important and popular technique\nfor examining how amino acids make up these characteristics of the protein\nsequences with position-specific scoring (PSS). While the string kernel is\ncrucial in natural language processing (NLP), it is unclear if string kernels\ncan extract biologically meaningful information from protein sequences, despite\nthe fact that they have been shown to be effective in the general sequence\nanalysis tasks. In this work, we propose a weighted PSS kernel matrix (or\nW-PSSKM), that combines a PSS representation of protein sequences, which\nencodes the frequency information of each amino acid in a sequence, with the\nnotion of the string kernel. This results in a novel kernel function that\noutperforms many other approaches for protein sequence classification. We\nperform extensive experimentation to evaluate the proposed method. Our findings\ndemonstrate that the W-PSSKM significantly outperforms existing baselines and\nstate-of-the-art methods and achieves up to 45.1\\% improvement in\nclassification accuracy.\n","authors":["Sarwan Ali","Taslim Murad","Prakash Chourasia","Haris Mansoor","Imdad Ullah Khan","Pin-Yu Chen","Murray Patterson"],"pdf_url":"https://arxiv.org/pdf/2410.12655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12652v1","updated":"2024-10-16T15:16:04Z","published":"2024-10-16T15:16:04Z","title":"Constrained Posterior Sampling: Time Series Generation with Hard\n  Constraints","summary":"  Generating realistic time series samples is crucial for stress-testing models\nand protecting user privacy by using synthetic data. In engineering and\nsafety-critical applications, these samples must meet certain hard constraints\nthat are domain-specific or naturally imposed by physics or nature. Consider,\nfor example, generating electricity demand patterns with constraints on peak\ndemand times. This can be used to stress-test the functioning of power grids\nduring adverse weather conditions. Existing approaches for generating\nconstrained time series are either not scalable or degrade sample quality. To\naddress these challenges, we introduce Constrained Posterior Sampling (CPS), a\ndiffusion-based sampling algorithm that aims to project the posterior mean\nestimate into the constraint set after each denoising update. Notably, CPS\nscales to a large number of constraints (~100) without requiring additional\ntraining. We provide theoretical justifications highlighting the impact of our\nprojection step on sampling. Empirically, CPS outperforms state-of-the-art\nmethods in sample quality and similarity to real time series by around 10% and\n42%, respectively, on real-world stocks, traffic, and air quality datasets.\n","authors":["Sai Shankar Narasimhan","Shubhankar Agarwal","Litu Rout","Sanjay Shakkottai","Sandeep P. Chinchali"],"pdf_url":"https://arxiv.org/pdf/2410.12652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11785v2","updated":"2024-10-16T15:15:44Z","published":"2024-06-17T17:39:10Z","title":"CELL your Model: Contrastive Explanations for Large Language Models","summary":"  The advent of black-box deep neural network classification models has sparked\nthe need to explain their decisions. However, in the case of generative AI,\nsuch as large language models (LLMs), there is no class prediction to explain.\nRather, one can ask why an LLM output a particular response to a given prompt.\nIn this paper, we answer this question by proposing, to the best of our\nknowledge, the first contrastive explanation methods requiring simply\nblack-box/query access. Our explanations suggest that an LLM outputs a reply to\na given prompt because if the prompt was slightly modified, the LLM would have\ngiven a different response that is either less preferable or contradicts the\noriginal response. The key insight is that contrastive explanations simply\nrequire a scoring function that has meaning to the user and not necessarily a\nspecific real valued quantity (viz. class label). We offer two algorithms for\nfinding contrastive explanations: i) A myopic algorithm, which although\neffective in creating contrasts, requires many model calls and ii) A budgeted\nalgorithm, our main algorithmic contribution, which intelligently creates\ncontrasts adhering to a query budget, necessary for longer contexts. We show\nthe efficacy of these methods on diverse natural language tasks such as\nopen-text generation, automated red teaming, and explaining conversational\ndegradation.\n","authors":["Ronny Luss","Erik Miehling","Amit Dhurandhar"],"pdf_url":"https://arxiv.org/pdf/2406.11785v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11720v2","updated":"2024-10-16T15:10:58Z","published":"2024-10-15T15:52:45Z","title":"Light-Weight Fault Tolerant Attention for Large Language Model Training","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious natural language processing tasks. However, the training of these\nmodels is computationally intensive and susceptible to faults, particularly in\nthe attention mechanism, which is a critical component of transformer-based\nLLMs. In this paper, we investigate the impact of faults on LLM training,\nfocusing on INF, NaN, and near-INF values in the computation results with\nsystematic fault injection experiments. We observe the propagation patterns of\nthese errors, which can trigger non-trainable states in the model and disrupt\ntraining, forcing the procedure to load from checkpoints. To mitigate the\nimpact of these faults, we propose ATTNChecker, the first Algorithm-Based Fault\nTolerance (ABFT) technique tailored for the attention mechanism in LLMs.\nATTNChecker is designed based on fault propagation patterns of LLM and\nincorporates performance optimization to adapt to both system reliability and\nmodel vulnerability while providing lightweight protection for fast LLM\ntraining. Evaluations on four LLMs show that ATTNChecker on average incurs on\naverage 7% overhead on training while detecting and correcting all extreme\nerrors. Compared with the state-of-the-art checkpoint/restore approach,\nATTNChecker reduces recovery overhead by up to 49x.\n","authors":["Yuhang Liang","Xinyi Li","Jie Ren","Ang Li","Bo Fang","Jieyang Chen"],"pdf_url":"https://arxiv.org/pdf/2410.11720v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12642v1","updated":"2024-10-16T15:03:28Z","published":"2024-10-16T15:03:28Z","title":"Optimization and Application of Cloud-based Deep Learning Architecture\n  for Multi-Source Data Prediction","summary":"  This study develops a cloud-based deep learning system for early prediction\nof diabetes, leveraging the distributed computing capabilities of the AWS cloud\nplatform and deep learning technologies to achieve efficient and accurate risk\nassessment. The system utilizes EC2 p3.8xlarge GPU instances to accelerate\nmodel training, reducing training time by 93.2% while maintaining a prediction\naccuracy of 94.2%. With an automated data processing and model training\npipeline built using Apache Airflow, the system can complete end-to-end updates\nwithin 18.7 hours. In clinical applications, the system demonstrates a\nprediction accuracy of 89.8%, sensitivity of 92.3%, and specificity of 95.1%.\nEarly interventions based on predictions lead to a 37.5% reduction in diabetes\nincidence among the target population. The system's high performance and\nscalability provide strong support for large-scale diabetes prevention and\nmanagement, showcasing significant public health value.\n","authors":["Yang Zhang","Fa Wang","Xin Huang","Xintao Li","Sibei Liu","Hansong Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12642v1.pdf","comment":"6 Pages, 5 Figures, 3 Tables. The final version will be published in\n  the proceedings of the IEEE conference"},{"id":"http://arxiv.org/abs/2202.03023v4","updated":"2024-10-16T15:02:27Z","published":"2022-02-07T09:27:34Z","title":"CECILIA: Comprehensive Secure Machine Learning Framework","summary":"  Since ML algorithms have proven their success in many different applications,\nthere is also a big interest in privacy preserving (PP) ML methods for building\nmodels on sensitive data. Moreover, the increase in the number of data sources\nand the high computational power required by those algorithms force individuals\nto outsource the training and/or the inference of a ML model to the clouds\nproviding such services. To address this, we propose a secure 3-party\ncomputation framework, CECILIA, offering PP building blocks to enable complex\noperations privately. In addition to the adapted and common operations like\naddition and multiplication, it offers multiplexer, most significant bit and\nmodulus conversion. The first two are novel in terms of methodology and the\nlast one is novel in terms of both functionality and methodology. CECILIA also\nhas two complex novel methods, which are the exact exponential of a public base\nraised to the power of a secret value and the inverse square root of a secret\nGram matrix. We use CECILIA to realize the private inference on pre-trained\nRKNs, which require more complex operations than most other DNNs, on the\nstructural classification of proteins as the first study ever accomplishing the\nPP inference on RKNs. In addition to the successful private computation of\nbasic building blocks, the results demonstrate that we perform the exact and\nfully private exponential computation, which is done by approximation in the\nliterature so far. Moreover, they also show that we compute the exact inverse\nsquare root of a secret Gram matrix up to a certain privacy level, which has\nnot been addressed in the literature at all. We also analyze the scalability of\nCECILIA to various settings on a synthetic dataset. The framework shows a great\npromise to make other ML algorithms as well as further computations privately\ncomputable by the building blocks of the framework.\n","authors":["Ali Burak Ünal","Nico Pfeifer","Mete Akgün"],"pdf_url":"https://arxiv.org/pdf/2202.03023v4.pdf","comment":"Preprint version of \"A privacy-preserving approach for cloud-based\n  protein fold recognition\" paper published in Patterns, ~8 pages of the main\n  paper, ~5 pages of Supplement"},{"id":"http://arxiv.org/abs/2408.04969v2","updated":"2024-10-16T14:57:33Z","published":"2024-08-09T09:43:10Z","title":"Towards aerodynamic surrogate modeling based on $β$-variational\n  autoencoders","summary":"  Surrogate models that combine dimensionality reduction and regression\ntechniques are essential to reduce the need for costly high-fidelity\ncomputational fluid dynamics data. New approaches using $\\beta$-Variational\nAutoencoder ($\\beta$-VAE) architectures have shown promise in obtaining\nhigh-quality low-dimensional representations of high-dimensional flow data\nwhile enabling physical interpretation of their latent spaces. We propose a\nsurrogate model based on latent space regression to predict pressure\ndistributions on a transonic wing given the flight conditions: Mach number and\nangle of attack. The $\\beta$-VAE model, enhanced with Principal Component\nAnalysis (PCA), maps high-dimensional data to a low-dimensional latent space,\nshowing a direct correlation with flight conditions. Regularization through\n$\\beta$ requires careful tuning to improve overall performance, while PCA\npreprocessing helps to construct an effective latent space, improving\nautoencoder training and performance. Gaussian Process Regression is used to\npredict latent space variables from flight conditions, showing robust behavior\nindependent of $\\beta$, and the decoder reconstructs the high-dimensional\npressure field data. This pipeline provides insight into unexplored flight\nconditions. Furthermore, a fine-tuning process of the decoder further refines\nthe model, reducing the dependence on $\\beta$ and enhancing accuracy.\nStructured latent space, robust regression performance, and significant\nimprovements in fine-tuning collectively create a highly accurate and efficient\nsurrogate model. Our methodology demonstrates the effectiveness of $\\beta$-VAEs\nfor aerodynamic surrogate modeling, offering a rapid, cost-effective, and\nreliable alternative for aerodynamic data prediction.\n","authors":["Víctor Francés-Belda","Alberto Solera-Rico","Javier Nieto-Centenero","Esther Andrés","Carlos Sanmiguel Vila","Rodrigo Castellanos"],"pdf_url":"https://arxiv.org/pdf/2408.04969v2.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2409.15360v3","updated":"2024-10-16T14:56:15Z","published":"2024-09-18T02:35:41Z","title":"Reward-Robust RLHF in LLMs","summary":"  As Large Language Models (LLMs) continue to progress toward more advanced\nforms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is\nincreasingly seen as a key pathway toward achieving Artificial General\nIntelligence (AGI). However, the reliance on reward-model-based (RM-based)\nalignment methods introduces significant challenges due to the inherent\ninstability and imperfections of Reward Models (RMs), which can lead to\ncritical issues such as reward hacking and misalignment with human intentions.\nIn this paper, we introduce a reward-robust RLHF framework aimed at addressing\nthese fundamental challenges, paving the way for more reliable and resilient\nlearning in LLMs. Our approach introduces a novel optimization objective that\ncarefully balances performance and robustness by incorporating Bayesian Reward\nModel Ensembles (BRME) to model the uncertainty set of reward functions. This\nallows the framework to integrate both nominal performance and minimum reward\nsignals, ensuring more stable learning even with imperfect RMs. Empirical\nresults demonstrate that our framework consistently outperforms baselines\nacross diverse benchmarks, showing improved accuracy and long-term stability.\nWe also provide a theoretical analysis, demonstrating that reward-robust RLHF\napproaches the stability of constant reward settings, which proves to be\nacceptable even in a stochastic-case analysis. Together, these contributions\nhighlight the framework potential to enhance both the performance and stability\nof LLM alignment.\n","authors":["Yuzi Yan","Xingzhou Lou","Jialian Li","Yiping Zhang","Jian Xie","Chao Yu","Yu Wang","Dong Yan","Yuan Shen"],"pdf_url":"https://arxiv.org/pdf/2409.15360v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12636v1","updated":"2024-10-16T14:55:16Z","published":"2024-10-16T14:55:16Z","title":"Towards Arbitrary QUBO Optimization: Analysis of Classical and\n  Quantum-Activated Feedforward Neural Networks","summary":"  Quadratic Unconstrained Binary Optimization (QUBO) sits at the heart of many\nindustries and academic fields such as logistics, supply chain, finance,\npharmaceutical science, chemistry, IT, and energy sectors, among others. These\nproblems typically involve optimizing a large number of binary variables, which\nmakes finding exact solutions exponentially more difficult. Consequently, most\nQUBO problems are classified as NP-hard. To address this challenge, we\ndeveloped a powerful feedforward neural network (FNN) optimizer for arbitrary\nQUBO problems. In this work, we demonstrate that the FNN optimizer can provide\nhigh-quality approximate solutions for large problems, including dense\n80-variable weighted MaxCut and random QUBOs, achieving an average accuracy of\nover 99% in less than 1.1 seconds on an 8-core CPU. Additionally, the FNN\noptimizer outperformed the Gurobi optimizer by 72% on 200-variable random QUBO\nproblems within a 100-second computation time limit, exhibiting strong\npotential for real-time optimization tasks. Building on this model, we explored\nthe novel approach of integrating FNNs with a quantum annealer-based activation\nfunction to create a quantum-classical encoder-decoder (QCED) optimizer, aiming\nto further enhance the performance of FNNs in QUBO optimization.\n","authors":["Chia-Tso Lai","Carsten Blank","Peter Schmelcher","Rick Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2410.12636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12635v1","updated":"2024-10-16T14:55:11Z","published":"2024-10-16T14:55:11Z","title":"An Exact Finite-dimensional Explicit Feature Map for Kernel Functions","summary":"  Kernel methods in machine learning use a kernel function that takes two data\npoints as input and returns their inner product after mapping them to a Hilbert\nspace, implicitly and without actually computing the mapping. For many kernel\nfunctions, such as Gaussian and Laplacian kernels, the feature space is known\nto be infinite-dimensional, making operations in this space possible only\nimplicitly. This implicit nature necessitates algorithms to be expressed using\ndual representations and the kernel trick. In this paper, given an arbitrary\nkernel function, we introduce an explicit, finite-dimensional feature map for\nany arbitrary kernel function that ensures the inner product of data points in\nthe feature space equals the kernel function value, during both training and\ntesting. The existence of this explicit mapping allows for kernelized\nalgorithms to be formulated in their primal form, without the need for the\nkernel trick or the dual representation. As a first application, we demonstrate\nhow to derive kernelized machine learning algorithms directly, without\nresorting to the dual representation, and apply this method specifically to\nPCA. As another application, without any changes to the t-SNE algorithm and its\nimplementation, we use it for visualizing the feature space of kernel\nfunctions.\n","authors":["Kamaledin Ghiasi-Shirazi","Mohammadreza Qaraei"],"pdf_url":"https://arxiv.org/pdf/2410.12635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12631v1","updated":"2024-10-16T14:53:13Z","published":"2024-10-16T14:53:13Z","title":"Explainable Moral Values: a neuro-symbolic approach to value\n  classification","summary":"  This work explores the integration of ontology-based reasoning and Machine\nLearning techniques for explainable value classification. By relying on an\nontological formalization of moral values as in the Moral Foundations Theory,\nrelying on the DnS Ontology Design Pattern, the \\textit{sandra} neuro-symbolic\nreasoner is used to infer values (fomalized as descriptions) that are\n\\emph{satisfied by} a certain sentence. Sentences, alongside their structured\nrepresentation, are automatically generated using an open-source Large Language\nModel. The inferred descriptions are used to automatically detect the value\nassociated with a sentence. We show that only relying on the reasoner's\ninference results in explainable classification comparable to other more\ncomplex approaches. We show that combining the reasoner's inferences with\ndistributional semantics methods largely outperforms all the baselines,\nincluding complex models based on neural network architectures. Finally, we\nbuild a visualization tool to explore the potential of theory-based values\nclassification, which is publicly available at http://xmv.geomeaning.com/.\n","authors":["Nicolas Lazzari","Stefano De Giorgis","Aldo Gangemi","Valentina Presutti"],"pdf_url":"https://arxiv.org/pdf/2410.12631v1.pdf","comment":"Published at ESWC24 Satellite Event"},{"id":"http://arxiv.org/abs/2402.01632v3","updated":"2024-10-16T14:46:34Z","published":"2024-02-02T18:52:16Z","title":"Time-Varying Gaussian Process Bandits with Unknown Prior","summary":"  Bayesian optimisation requires fitting a Gaussian process model, which in\nturn requires specifying prior on the unknown black-box function -- most of the\ntheoretical literature assumes this prior is known. However, it is common to\nhave more than one possible prior for a given black-box function, for example\nsuggested by domain experts with differing opinions. In some cases, the type-II\nmaximum likelihood estimator for selecting prior enjoys the consistency\nguarantee, but it does not universally apply to all types of priors. If the\nproblem is stationary, one could rely on the Regret Balancing scheme to conduct\nthe optimisation, but in the case of time-varying problems, such a scheme\ncannot be used. To address this gap in existing research, we propose a novel\nalgorithm, PE-GP-UCB, which is capable of solving time-varying Bayesian\noptimisation problems even without the exact knowledge of the function's prior.\nThe algorithm relies on the fact that either the observed function values are\nconsistent with some of the priors, in which case it is easy to reject the\nwrong priors, or the observations are consistent with all candidate priors, in\nwhich case it does not matter which prior our model relies on. We provide a\nregret bound on the proposed algorithm. Finally, we empirically evaluate our\nalgorithm on toy and real-world time-varying problems and show that it\noutperforms the maximum likelihood estimator, fully Bayesian treatment of\nunknown prior and Regret Balancing.\n","authors":["Juliusz Ziomek","Masaki Adachi","Michael A. Osborne"],"pdf_url":"https://arxiv.org/pdf/2402.01632v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12621v1","updated":"2024-10-16T14:40:32Z","published":"2024-10-16T14:40:32Z","title":"Weak-to-Strong Generalization beyond Accuracy: a Pilot Study in Safety,\n  Toxicity, and Legal Reasoning","summary":"  As large language models (LLMs) continue to advance, ensuring their alignment\nwith human values becomes increasingly critical. Traditional alignment methods\nheavily rely on human feedback to fine-tune models. With the emergence of\nsuperhuman models whose outputs may surpass human understanding, evaluating and\naligning these models using human judgments poses significant challenges. To\naddress the challenges, recent works use weak supervisors to elicit knowledge\nfrom much stronger models. However, there are important disanalogies between\nthe empirical setup in the existing works and the genuine goal of alignment. We\nremark that existing works investigate the phenomenon of weak-to-strong\ngeneration in analogous setup (i.e., binary classification), rather than\npractical alignment-relevant tasks (e.g., safety). In this paper, we bridge\nthis gap by extending weak-to-strong generation to the context of practical\nalignment. We empirically demonstrate the widespread phenomenon of\nweak-to-strong generation in three complicated alignment tasks: safety,\ntoxicity, and legal reasoning}. Furthermore, we explore efficient strategies\nfor improving alignment performance to enhance the quality of model outcomes.\nLastly, we summarize and analyze the challenges and potential solutions in\nregard to specific alignment tasks, which we hope to catalyze the research\nprogress on the topic of weak-to-strong generalization. Our code is released at\nhttps://github.com/yeruimeng/WTS.git.\n","authors":["Ruimeng Ye","Yang Xiao","Bo Hui"],"pdf_url":"https://arxiv.org/pdf/2410.12621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12613v1","updated":"2024-10-16T14:29:29Z","published":"2024-10-16T14:29:29Z","title":"Exploring Model Kinship for Merging Large Language Models","summary":"  Model merging has become one of the key technologies for enhancing the\ncapabilities and efficiency of Large Language Models (LLMs). However, our\nunderstanding of the expected performance gains and principles when merging any\ntwo models remains limited. In this work, we introduce model kinship, the\ndegree of similarity or relatedness between LLMs, analogous to biological\nevolution. With comprehensive empirical analysis, we find that there is a\ncertain relationship between model kinship and the performance gains after\nmodel merging, which can help guide our selection of candidate models. Inspired\nby this, we propose a new model merging strategy: Top-k Greedy Merging with\nModel Kinship, which can yield better performance on benchmark datasets.\nSpecifically, we discover that using model kinship as a criterion can assist us\nin continuously performing model merging, alleviating the degradation (local\noptima) in model evolution, whereas model kinship can serve as a guide to\nescape these traps. Code is available at\nhttps://github.com/zjunlp/ModelKinship.\n","authors":["Yedi Hu","Yunzhi Yao","Ningyu Zhang","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12613v1.pdf","comment":"Ongoing work"},{"id":"http://arxiv.org/abs/2409.19434v2","updated":"2024-10-16T14:28:32Z","published":"2024-09-28T18:44:39Z","title":"Energy-Efficient Computation with DVFS using Deep Reinforcement Learning\n  for Multi-Task Systems in Edge Computing","summary":"  Periodic soft real-time systems have broad applications in many areas, such\nas IoT. Finding an optimal energy-efficient policy that is adaptable to\nunderlying edge devices while meeting deadlines for tasks has always been\nchallenging. This research studies generalized systems with multi-task,\nmulti-deadline scenarios with reinforcement learning-based DVFS for energy\nsaving. This work addresses the limitation of previous work that models a\nperiodic system as a single task and single-deadline scenario, which is too\nsimplified to cope with complex situations. The method encodes time series\ninformation in the Linux kernel into information that is easy to use for\nreinforcement learning, allowing the system to generate DVFS policies to adapt\nsystem patterns based on the general workload. For encoding, we present two\ndifferent methods for comparison. Both methods use only one performance\ncounter: system utilization and the kernel only needs minimal information from\nthe userspace. Our method is implemented on Jetson Nano Board (2GB) and is\ntested with three fixed multitask workloads, which are three, five, and eight\ntasks in the workload, respectively. For randomness and generalization, we also\ndesigned a random workload generator to build different multitask workloads to\ntest. Based on the test results, our method could save 3%-10% power compared to\nLinux built-in governors.\n","authors":["Xinyi Li","Ti Zhou","Haoyu Wang","Man Lin"],"pdf_url":"https://arxiv.org/pdf/2409.19434v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14463v4","updated":"2024-10-16T14:27:49Z","published":"2023-05-23T18:37:30Z","title":"ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain\n  Readability Assessment","summary":"  We present a comprehensive evaluation of large language models for\nmultilingual readability assessment. Existing evaluation resources lack domain\nand language diversity, limiting the ability for cross-domain and cross-lingual\nanalyses. This paper introduces ReadMe++, a multilingual multi-domain dataset\nwith human annotations of 9757 sentences in Arabic, English, French, Hindi, and\nRussian, collected from 112 different data sources. This benchmark will\nencourage research on developing robust multilingual readability assessment\nmethods. Using ReadMe++, we benchmark multilingual and monolingual language\nmodels in the supervised, unsupervised, and few-shot prompting settings. The\ndomain and language diversity in ReadMe++ enable us to test more effective\nfew-shot prompting, and identify shortcomings in state-of-the-art unsupervised\nmethods. Our experiments also reveal exciting results of superior domain\ngeneralization and enhanced cross-lingual transfer capabilities by models\ntrained on ReadMe++. We will make our data publicly available and release a\npython package tool for multilingual sentence readability prediction using our\ntrained models at: https://github.com/tareknaous/readme\n","authors":["Tarek Naous","Michael J. Ryan","Anton Lavrouk","Mohit Chandra","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2305.14463v4.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.12609v1","updated":"2024-10-16T14:26:08Z","published":"2024-10-16T14:26:08Z","title":"Towards Graph Foundation Models: The Perspective of Zero-shot Reasoning\n  on Knowledge Graphs","summary":"  Inspired by the success of artificial general intelligence, there is a trend\ntowards developing Graph Foundation Models that excel in generalization across\nvarious graph tasks and domains. However, current models often require\nextensive training or fine-tuning to capture structural and semantic insights\non new graphs, which limits their versatility. In this work, we explore graph\nfoundation models from the perspective of zero-shot reasoning on Knowledge\nGraphs (KGs). Our focus is on utilizing KGs as a unified topological structure\nto tackle diverse tasks, while addressing semantic isolation challenges in KG\nreasoning to effectively integrate diverse semantic and structural features.\nThis brings us new methodological insights into KG reasoning, as well as high\ngeneralizability towards foundation models in practice. Methodologically, we\nintroduce SCORE, a unified graph reasoning framework that effectively\ngeneralizes diverse graph tasks using zero-shot learning. At the core of SCORE\nis semantic conditional message passing, a technique designed to capture both\nstructural and semantic invariances in graphs, with theoretical backing for its\nexpressive power. Practically, we evaluate the zero-shot reasoning capability\nof SCORE using 38 diverse graph datasets, covering node-level, link-level, and\ngraph-level tasks across multiple domains. Our experiments reveal a substantial\nperformance improvement over prior foundation models and supervised baselines,\nhighlighting the efficacy and adaptability of our approach.\n","authors":["Kai Wang","Siqiang Luo"],"pdf_url":"https://arxiv.org/pdf/2410.12609v1.pdf","comment":"17 Pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.12607v1","updated":"2024-10-16T14:24:51Z","published":"2024-10-16T14:24:51Z","title":"Low-Rank Adversarial PGD Attack","summary":"  Adversarial attacks on deep neural network models have seen rapid development\nand are extensively used to study the stability of these networks. Among\nvarious adversarial strategies, Projected Gradient Descent (PGD) is a widely\nadopted method in computer vision due to its effectiveness and quick\nimplementation, making it suitable for adversarial training. In this work, we\nobserve that in many cases, the perturbations computed using PGD predominantly\naffect only a portion of the singular value spectrum of the original image,\nsuggesting that these perturbations are approximately low-rank. Motivated by\nthis observation, we propose a variation of PGD that efficiently computes a\nlow-rank attack. We extensively validate our method on a range of standard\nmodels as well as robust models that have undergone adversarial training. Our\nanalysis indicates that the proposed low-rank PGD can be effectively used in\nadversarial training due to its straightforward and fast implementation coupled\nwith competitive performance. Notably, we find that low-rank PGD often performs\ncomparably to, and sometimes even outperforms, the traditional full-rank PGD\nattack, while using significantly less memory.\n","authors":["Dayana Savostianova","Emanuele Zangrando","Francesco Tudisco"],"pdf_url":"https://arxiv.org/pdf/2410.12607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12606v1","updated":"2024-10-16T14:24:44Z","published":"2024-10-16T14:24:44Z","title":"Self-Supervised Learning of Disentangled Representations for\n  Multivariate Time-Series","summary":"  Multivariate time-series data in fields like healthcare and industry are\ninformative but challenging due to high dimensionality and lack of labels.\nRecent self-supervised learning methods excel in learning rich representations\nwithout labels but struggle with disentangled embeddings and inductive bias\nissues like transformation-invariance. To address these challenges, we\nintroduce TimeDRL, a framework for multivariate time-series representation\nlearning with dual-level disentangled embeddings. TimeDRL features: (i)\ndisentangled timestamp-level and instance-level embeddings using a [CLS] token\nstrategy; (ii) timestamp-predictive and instance-contrastive tasks for\nrepresentation learning; and (iii) avoidance of augmentation methods to\neliminate inductive biases. Experiments on forecasting and classification\ndatasets show TimeDRL outperforms existing methods, with further validation in\nsemi-supervised settings with limited labeled data.\n","authors":["Ching Chang","Chiao-Tung Chan","Wei-Yao Wang","Wen-Chih Peng","Tien-Fu Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12606v1.pdf","comment":"NeurIPS 2024 Workshop: Self-Supervised Learning - Theory and Practice"},{"id":"http://arxiv.org/abs/2410.12604v1","updated":"2024-10-16T14:23:36Z","published":"2024-10-16T14:23:36Z","title":"The Bayesian Confidence (BACON) Estimator for Deep Neural Networks","summary":"  This paper introduces the Bayesian Confidence Estimator (BACON) for deep\nneural networks. Current practice of interpreting Softmax values in the output\nlayer as probabilities of outcomes is prone to extreme predictions of class\nprobability. In this work we extend Waagen's method of representing the\nterminal layers with a geometric model, where the probability associated with\nan output vector is estimated with Bayes' Rule using validation data to provide\nlikelihood and normalization values. This estimator provides superior ECE and\nACE calibration error compared to Softmax for ResNet-18 at 85% network\naccuracy, and EfficientNet-B0 at 95% network accuracy, on the CIFAR-10 dataset\nwith an imbalanced test set, except for very high accuracy edge cases. In\naddition, when using the ACE metric, BACON demonstrated improved calibration\nerror when estimating probabilities for the imbalanced test set when using\nactual class distribution fractions.\n","authors":["Patrick D. Kee","Max J. Brown","Jonathan C. Rice","Christian A. Howell"],"pdf_url":"https://arxiv.org/pdf/2410.12604v1.pdf","comment":"14 pages, 15 figures (10 of which include sub-figures)"},{"id":"http://arxiv.org/abs/2410.12598v1","updated":"2024-10-16T14:15:28Z","published":"2024-10-16T14:15:28Z","title":"Dynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach","summary":"  In Deep Reinforcement Learning models trained using gradient-based\ntechniques, the choice of optimizer and its learning rate are crucial to\nachieving good performance: higher learning rates can prevent the model from\nlearning effectively, while lower ones might slow convergence. Additionally,\ndue to the non-stationarity of the objective function, the best-performing\nlearning rate can change over the training steps. To adapt the learning rate, a\nstandard technique consists of using decay schedulers. However, these\nschedulers assume that the model is progressively approaching convergence,\nwhich may not always be true, leading to delayed or premature adjustments. In\nthis work, we propose dynamic Learning Rate for deep Reinforcement Learning\n(LRRL), a meta-learning approach that selects the learning rate based on the\nagent's performance during training. LRRL is based on a multi-armed bandit\nalgorithm, where each arm represents a different learning rate, and the bandit\nfeedback is provided by the cumulative returns of the RL policy to update the\narms' probability distribution. Our empirical results demonstrate that LRRL can\nsubstantially improve the performance of deep RL algorithms.\n","authors":["Henrique Donâncio","Antoine Barrier","Leah F. South","Florence Forbes"],"pdf_url":"https://arxiv.org/pdf/2410.12598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12597v1","updated":"2024-10-16T14:15:01Z","published":"2024-10-16T14:15:01Z","title":"Personalized Prediction Models for Changes in Knee Pain among Patients\n  with Osteoarthritis Participating in Supervised Exercise and Education","summary":"  Knee osteoarthritis (OA) is a widespread chronic condition that impairs\nmobility and diminishes quality of life. Despite the proven benefits of\nexercise therapy and patient education in managing the OA symptoms pain and\nfunctional limitations, these strategies are often underutilized. Personalized\noutcome prediction models can help motivate and engage patients, but the\naccuracy of existing models in predicting changes in knee pain remains\ninsufficiently examined. To validate existing models and introduce a concise\npersonalized model predicting changes in knee pain before to after\nparticipating in a supervised education and exercise therapy program (GLA:D)\nfor knee OA patients. Our models use self-reported patient information and\nfunctional measures. To refine the number of variables, we evaluated the\nvariable importance and applied clinical reasoning. We trained random forest\nregression models and compared the rate of true predictions of our models with\nthose utilizing average values. We evaluated the performance of a full,\ncontinuous, and concise model including all 34, all 11 continuous, and the six\nmost predictive variables respectively. All three models performed similarly\nand were comparable to the existing model, with R-squares of 0.31-0.32 and\nRMSEs of 18.65-18.85 - despite our increased sample size. Allowing a deviation\nof 15 VAS points from the true change in pain, our concise model and utilizing\nthe average values estimated the change in pain at 58% and 51% correctly,\nrespectively. Our supplementary analysis led to similar outcomes. Our concise\npersonalized prediction model more accurately predicts changes in knee pain\nfollowing the GLA:D program compared to average pain improvement values.\nNeither the increase in sample size nor the inclusion of additional variables\nimproved previous models. To improve predictions, new variables beyond those in\nthe GLA:D are required.\n","authors":["M. Rafiei","S. Das","M. Bakhtiari","E. M. Roos","S. T. Skou","D. T. Grønne","J. Baumbach","L. Baumbach"],"pdf_url":"https://arxiv.org/pdf/2410.12597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08770v3","updated":"2024-10-16T14:14:41Z","published":"2024-09-13T12:24:12Z","title":"Increasing Both Batch Size and Learning Rate Accelerates Stochastic\n  Gradient Descent","summary":"  The performance of mini-batch stochastic gradient descent (SGD) strongly\ndepends on setting the batch size and learning rate to minimize the empirical\nloss in training the deep neural network. In this paper, we present theoretical\nanalyses of mini-batch SGD with four schedulers: (i) constant batch size and\ndecaying learning rate scheduler, (ii) increasing batch size and decaying\nlearning rate scheduler, (iii) increasing batch size and increasing learning\nrate scheduler, and (iv) increasing batch size and warm-up decaying learning\nrate scheduler. We show that mini-batch SGD using scheduler (i) does not always\nminimize the expectation of the full gradient norm of the empirical loss,\nwhereas it does using any of schedulers (ii), (iii), and (iv). Furthermore,\nschedulers (iii) and (iv) accelerate mini-batch SGD. The paper also provides\nnumerical results of supporting analyses showing that using scheduler (iii) or\n(iv) minimizes the full gradient norm of the empirical loss faster than using\nscheduler (i) or (ii).\n","authors":["Hikaru Umeda","Hideaki Iiduka"],"pdf_url":"https://arxiv.org/pdf/2409.08770v3.pdf","comment":"28 pages, 18 figures"},{"id":"http://arxiv.org/abs/2403.11894v4","updated":"2024-10-16T14:14:27Z","published":"2024-03-18T15:53:33Z","title":"From Explainable to Interpretable Deep Learning for Natural Language\n  Processing in Healthcare: How Far from Reality?","summary":"  Deep learning (DL) has substantially enhanced natural language processing\n(NLP) in healthcare research. However, the increasing complexity of DL-based\nNLP necessitates transparent model interpretability, or at least\nexplainability, for reliable decision-making. This work presents a thorough\nscoping review of explainable and interpretable DL in healthcare NLP. The term\n\"eXplainable and Interpretable Artificial Intelligence\" (XIAI) is introduced to\ndistinguish XAI from IAI. Different models are further categorized based on\ntheir functionality (model-, input-, output-based) and scope (local, global).\nOur analysis shows that attention mechanisms are the most prevalent emerging\nIAI technique. The use of IAI is growing, distinguishing it from XAI. The major\nchallenges identified are that most XIAI does not explore \"global\" modelling\nprocesses, the lack of best practices, and the lack of systematic evaluation\nand benchmarks. One important opportunity is to use attention mechanisms to\nenhance multi-modal XIAI for personalized medicine. Additionally, combining DL\nwith causal logic holds promise. Our discussion encourages the integration of\nXIAI in Large Language Models (LLMs) and domain-specific smaller models. In\nconclusion, XIAI adoption in healthcare requires dedicated in-house expertise.\nCollaboration with domain experts, end-users, and policymakers can lead to\nready-to-use XIAI methods across NLP and medical tasks. While challenges exist,\nXIAI techniques offer a valuable foundation for interpretable NLP algorithms in\nhealthcare.\n","authors":["Guangming Huang","Yingya Li","Shoaib Jameel","Yunfei Long","Giorgos Papanastasiou"],"pdf_url":"https://arxiv.org/pdf/2403.11894v4.pdf","comment":"This paper has been accepted by Computational and Structural\n  Biotechnology Journal"},{"id":"http://arxiv.org/abs/2410.12593v1","updated":"2024-10-16T14:12:11Z","published":"2024-10-16T14:12:11Z","title":"Expand and Compress: Exploring Tuning Principles for Continual\n  Spatio-Temporal Graph Forecasting","summary":"  The widespread deployment of sensing devices leads to a surge in data for\nspatio-temporal forecasting applications such as traffic flow, air quality, and\nwind energy. Although spatio-temporal graph neural networks have achieved\nsuccess in modeling various static spatio-temporal forecasting scenarios,\nreal-world spatio-temporal data are typically received in a streaming manner,\nand the network continuously expands with the installation of new sensors.\nThus, spatio-temporal forecasting in streaming scenarios faces dual challenges:\nthe inefficiency of retraining models over newly arrived data and the\ndetrimental effects of catastrophic forgetting over long-term history. To\naddress these challenges, we propose a novel prompt tuning-based continuous\nforecasting method, following two fundamental tuning principles guided by\nempirical and theoretical analysis: expand and compress, which effectively\nresolve the aforementioned problems with lightweight tuning parameters.\nSpecifically, we integrate the base spatio-temporal graph neural network with a\ncontinuous prompt pool, utilizing stored prompts (i.e., few learnable\nparameters) in memory, and jointly optimize them with the base spatio-temporal\ngraph neural network. This method ensures that the model sequentially learns\nfrom the spatio-temporal data stream to accomplish tasks for corresponding\nperiods. Extensive experimental results on multiple real-world datasets\ndemonstrate the multi-faceted superiority of our method over the\nstate-of-the-art baselines, including effectiveness, efficiency, universality,\netc.\n","authors":["Wei Chen","Yuxuan Liang"],"pdf_url":"https://arxiv.org/pdf/2410.12593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03293v3","updated":"2024-10-16T14:11:21Z","published":"2024-10-04T10:06:55Z","title":"Five Years of COVID-19 Discourse on Instagram: A Labeled Instagram\n  Dataset of Over Half a Million Posts for Multilingual Sentiment Analysis","summary":"  The work presented in this paper makes three scientific contributions with a\nspecific focus on mining and analysis of COVID-19-related posts on Instagram.\nFirst, it presents a multilingual dataset of 500,153 Instagram posts about\nCOVID-19 published between January 2020 and September 2024. This dataset,\navailable at https://dx.doi.org/10.21227/d46p-v480, contains Instagram posts in\n161 different languages as well as 535,021 distinct hashtags. After the\ndevelopment of this dataset, multilingual sentiment analysis was performed,\nwhich involved classifying each post as positive, negative, or neutral. The\nresults of sentiment analysis are presented as a separate attribute in this\ndataset. Second, it presents the results of performing sentiment analysis per\nyear from 2020 to 2024. The findings revealed the trends in sentiment related\nto COVID-19 on Instagram since the beginning of the pandemic. For instance,\nbetween 2020 and 2024, the sentiment trends show a notable shift, with positive\nsentiment decreasing from 38.35% to 28.69%, while neutral sentiment rising from\n44.19% to 58.34%. Finally, the paper also presents findings of\nlanguage-specific sentiment analysis. This analysis highlighted similar and\ncontrasting trends of sentiment across posts published in different languages\non Instagram. For instance, out of all English posts, 49.68% were positive,\n14.84% were negative, and 35.48% were neutral. In contrast, among Hindi posts,\n4.40% were positive, 57.04% were negative, and 38.56% were neutral, reflecting\ndistinct differences in the sentiment distribution between these two languages.\n","authors":["Nirmalya Thakur"],"pdf_url":"https://arxiv.org/pdf/2410.03293v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12592v1","updated":"2024-10-16T14:10:53Z","published":"2024-10-16T14:10:53Z","title":"Cocoon: Robust Multi-Modal Perception with Uncertainty-Aware Sensor\n  Fusion","summary":"  An important paradigm in 3D object detection is the use of multiple\nmodalities to enhance accuracy in both normal and challenging conditions,\nparticularly for long-tail scenarios. To address this, recent studies have\nexplored two directions of adaptive approaches: MoE-based adaptive fusion,\nwhich struggles with uncertainties arising from distinct object configurations,\nand late fusion for output-level adaptive fusion, which relies on separate\ndetection pipelines and limits comprehensive understanding. In this work, we\nintroduce Cocoon, an object- and feature-level uncertainty-aware fusion\nframework. The key innovation lies in uncertainty quantification for\nheterogeneous representations, enabling fair comparison across modalities\nthrough the introduction of a feature aligner and a learnable surrogate ground\ntruth, termed feature impression. We also define a training objective to ensure\nthat their relationship provides a valid metric for uncertainty quantification.\nCocoon consistently outperforms existing static and adaptive methods in both\nnormal and challenging conditions, including those with natural and artificial\ncorruptions. Furthermore, we show the validity and efficacy of our uncertainty\nmetric across diverse datasets.\n","authors":["Minkyoung Cho","Yulong Cao","Jiachen Sun","Qingzhao Zhang","Marco Pavone","Jeong Joon Park","Heng Yang","Z. Morley Mao"],"pdf_url":"https://arxiv.org/pdf/2410.12592v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2410.12589v1","updated":"2024-10-16T14:10:15Z","published":"2024-10-16T14:10:15Z","title":"From Lab to Pocket: A Novel Continual Learning-based Mobile Application\n  for Screening COVID-19","summary":"  Artificial intelligence (AI) has emerged as a promising tool for predicting\nCOVID-19 from medical images. In this paper, we propose a novel continual\nlearning-based approach and present the design and implementation of a mobile\napplication for screening COVID-19. Our approach demonstrates the ability to\nadapt to evolving datasets, including data collected from different locations\nor hospitals, varying virus strains, and diverse clinical presentations,\nwithout retraining from scratch. We have evaluated state-of-the-art continual\nlearning methods for detecting COVID-19 from chest X-rays and selected the\nbest-performing model for our mobile app. We evaluated various deep learning\narchitectures to select the best-performing one as a foundation model for\ncontinual learning. Both regularization and memory-based methods for continual\nlearning were tested, using different memory sizes to develop the optimal\ncontinual learning model for our app. DenseNet161 emerged as the best\nfoundation model with 96.87\\% accuracy, and Learning without Forgetting (LwF)\nwas the top continual learning method with an overall performance of 71.99\\%.\nThe mobile app design considers both patient and doctor perspectives. It\nincorporates the continual learning DenseNet161 LwF model on a cloud server,\nenabling the model to learn from new instances of chest X-rays and their\nclassifications as they are submitted. The app is designed, implemented, and\nevaluated to ensure it provides an efficient tool for COVID-19 screening. The\napp is available to download from\nhttps://github.com/DannyFGitHub/COVID-19PneumoCheckApp.\n","authors":["Danny Falero","Muhammad Ashad Kabir","Nusrat Homaira"],"pdf_url":"https://arxiv.org/pdf/2410.12589v1.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2410.08469v2","updated":"2024-10-16T14:09:14Z","published":"2024-10-11T02:42:13Z","title":"Semantic Token Reweighting for Interpretable and Controllable Text\n  Embeddings in CLIP","summary":"  A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial\nrole in translating textual input into an embedding space shared with images,\nthereby facilitating the interpretative analysis of vision tasks through\nnatural language. Despite the varying significance of different textual\nelements within a sentence depending on the context, efforts to account for\nvariation of importance in constructing text embeddings have been lacking. We\npropose a framework of Semantic Token Reweighting to build Interpretable text\nembeddings (SToRI), which incorporates controllability as well. SToRI refines\nthe text encoding process in CLIP by differentially weighting semantic elements\nbased on contextual importance, enabling finer control over emphasis responsive\nto data-driven insights and user preferences. The efficacy of SToRI is\ndemonstrated through comprehensive experiments on few-shot image classification\nand image retrieval tailored to user preferences.\n","authors":["Eunji Kim","Kyuhong Shim","Simyung Chang","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2410.08469v2.pdf","comment":"Accepted at EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.11031v2","updated":"2024-10-16T14:05:57Z","published":"2024-10-14T19:33:46Z","title":"NAR-*ICP: Neural Execution of Classical ICP-based Pointcloud\n  Registration Algorithms","summary":"  This study explores the intersection of neural networks and classical\nrobotics algorithms through the Neural Algorithmic Reasoning (NAR) framework,\nallowing to train neural networks to effectively reason like classical robotics\nalgorithms by learning to execute them. Algorithms are integral to robotics and\nsafety-critical applications due to their predictable and consistent\nperformance through logical and mathematical principles. In contrast, while\nneural networks are highly adaptable, handling complex, high-dimensional data\nand generalising across tasks, they often lack interpretability and\ntransparency in their internal computations. We propose a Graph Neural Network\n(GNN)-based learning framework, NAR-*ICP, which learns the intermediate\nalgorithmic steps of classical ICP-based pointcloud registration algorithms,\nand extend the CLRS Algorithmic Reasoning Benchmark with classical robotics\nperception algorithms. We evaluate our approach across diverse datasets, from\nreal-world to synthetic, demonstrating its flexibility in handling complex and\nnoisy inputs, along with its potential to be used as part of a larger learning\nsystem. Our results indicate that our method achieves superior performance\nacross all benchmarks and datasets, consistently surpassing even the algorithms\nit has been trained on, further demonstrating its ability to generalise beyond\nthe capabilities of traditional algorithms.\n","authors":["Efimia Panagiotaki","Daniele De Martini","Lars Kunze","Petar Veličković"],"pdf_url":"https://arxiv.org/pdf/2410.11031v2.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.12584v1","updated":"2024-10-16T14:04:06Z","published":"2024-10-16T14:04:06Z","title":"Self-DenseMobileNet: A Robust Framework for Lung Nodule Classification\n  using Self-ONN and Stacking-based Meta-Classifier","summary":"  In this study, we propose a novel and robust framework, Self-DenseMobileNet,\ndesigned to enhance the classification of nodules and non-nodules in chest\nradiographs (CXRs). Our approach integrates advanced image standardization and\nenhancement techniques to optimize the input quality, thereby improving\nclassification accuracy. To enhance predictive accuracy and leverage the\nstrengths of multiple models, the prediction probabilities from\nSelf-DenseMobileNet were transformed into tabular data and used to train eight\nclassical machine learning (ML) models; the top three performers were then\ncombined via a stacking algorithm, creating a robust meta-classifier that\nintegrates their collective insights for superior classification performance.\nTo enhance the interpretability of our results, we employed class activation\nmapping (CAM) to visualize the decision-making process of the best-performing\nmodel. Our proposed framework demonstrated remarkable performance on internal\nvalidation data, achieving an accuracy of 99.28\\% using a Meta-Random Forest\nClassifier. When tested on an external dataset, the framework maintained strong\ngeneralizability with an accuracy of 89.40\\%. These results highlight a\nsignificant improvement in the classification of CXRs with lung nodules.\n","authors":["Md. Sohanur Rahman","Muhammad E. H. Chowdhury","Hasib Ryan Rahman","Mosabber Uddin Ahmed","Muhammad Ashad Kabir","Sanjiban Sekhar Roy","Rusab Sarmun"],"pdf_url":"https://arxiv.org/pdf/2410.12584v1.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2308.06686v4","updated":"2024-10-16T13:55:36Z","published":"2023-08-13T05:22:49Z","title":"TorchQL: A Programming Framework for Integrity Constraints in Machine\n  Learning","summary":"  Finding errors in machine learning applications requires a thorough\nexploration of their behavior over data. Existing approaches used by\npractitioners are often ad-hoc and lack the abstractions needed to scale this\nprocess. We present TorchQL, a programming framework to evaluate and improve\nthe correctness of machine learning applications. TorchQL allows users to write\nqueries to specify and check integrity constraints over machine learning models\nand datasets. It seamlessly integrates relational algebra with functional\nprogramming to allow for highly expressive queries using only eight intuitive\noperators. We evaluate TorchQL on diverse use-cases including finding critical\ntemporal inconsistencies in objects detected across video frames in autonomous\ndriving, finding data imputation errors in time-series medical records, finding\ndata labeling errors in real-world images, and evaluating biases and\nconstraining outputs of language models. Our experiments show that TorchQL\nenables up to 13x faster query executions than baselines like Pandas and\nMongoDB, and up to 40% shorter queries than native Python. We also conduct a\nuser study and find that TorchQL is natural enough for developers familiar with\nPython to specify complex integrity constraints.\n","authors":["Aaditya Naik","Adam Stein","Yinjun Wu","Mayur Naik","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2308.06686v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12572v1","updated":"2024-10-16T13:50:04Z","published":"2024-10-16T13:50:04Z","title":"On the Role of Activation Functions in EEG-To-Text Decoder","summary":"  In recent years, much interdisciplinary research has been conducted exploring\npotential use cases of neuroscience to advance the field of information\nretrieval. Initial research concentrated on the use of fMRI data, but fMRI was\ndeemed to be not suitable for real-world applications, and soon, research\nshifted towards using EEG data. In this paper, we try to improve the original\nperformance of a first attempt at generating text using EEG by focusing on the\nless explored area of optimising neural network performance. We test a set of\ndifferent activation functions and compare their performance. Our results show\nthat introducing a higher degree polynomial activation function can enhance\nmodel performance without changing the model architecture. We also show that\nthe learnable 3rd-degree activation function performs better on the 1-gram\nevaluation compared to a 3rd-degree non-learnable function. However, when\nevaluating the model on 2-grams and above, the polynomial function lacks in\nperformance, whilst the leaky ReLU activation function outperforms the\nbaseline.\n","authors":["Zenon Lamprou","Iakovos Tenedios","Yashar Moshfeghi"],"pdf_url":"https://arxiv.org/pdf/2410.12572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12593v2","updated":"2024-10-16T13:45:54Z","published":"2024-06-18T13:25:18Z","title":"PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval","summary":"  Differentiable Search Index (DSI) utilizes Pre-trained Language Models (PLMs)\nfor efficient document retrieval without relying on external indexes. However,\nDSI needs full re-training to handle updates in dynamic corpora, causing\nsignificant computational inefficiencies. We introduce PromptDSI, a\nprompt-based rehearsal-free approach for instance-wise incremental learning\ndocument retrieval. PromptDSI attaches prompts to the frozen PLM's encoder of\nDSI, leveraging its powerful representation to efficiently index new corpora\nwhile maintaining a balance between stability and plasticity. We eliminate the\ninitial forward pass of prompt-based continual learning methods that doubles\ntraining and inference time. Moreover, we propose a topic-aware prompt pool\nthat employs neural topic embeddings as fixed keys. This strategy ensures\ndiverse and effective prompt usage, addressing the challenge of parameter\nunderutilization caused by the collapse of the query-key matching mechanism.\nOur empirical evaluations demonstrate that BERT-based PromptDSI matches IncDSI\nin managing forgetting while improving new corpora performance by more than 4%\nHits@10 on NQ320k and upto 3% MRR@10 on MS MARCO 300k.\n","authors":["Tuan-Luc Huynh","Thuy-Trang Vu","Weiqing Wang","Yinwei Wei","Trung Le","Dragan Gasevic","Yuan-Fang Li","Thanh-Toan Do"],"pdf_url":"https://arxiv.org/pdf/2406.12593v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2408.13452v3","updated":"2024-10-16T13:43:08Z","published":"2024-08-24T03:43:35Z","title":"Data Augmentation for Continual RL via Adversarial Gradient Episodic\n  Memory","summary":"  Data efficiency of learning, which plays a key role in the Reinforcement\nLearning (RL) training process, becomes even more important in continual RL\nwith sequential environments. In continual RL, the learner interacts with\nnon-stationary, sequential tasks and is required to learn new tasks without\nforgetting previous knowledge. However, there is little work on implementing\ndata augmentation for continual RL. In this paper, we investigate the efficacy\nof data augmentation for continual RL. Specifically, we provide benchmarking\ndata augmentations for continual RL, by (1) summarising existing data\naugmentation methods and (2) including a new augmentation method for continual\nRL: Adversarial Augmentation with Gradient Episodic Memory (Adv-GEM). Extensive\nexperiments show that data augmentations, such as random amplitude scaling,\nstate-switch, mixup, adversarial augmentation, and Adv-GEM, can improve\nexisting continual RL algorithms in terms of their average performance,\ncatastrophic forgetting, and forward transfer, on robot control tasks. All data\naugmentation methods are implemented as plug-in modules for trivial integration\ninto continual RL methods.\n","authors":["Sihao Wu","Xingyu Zhao","Xiaowei Huang"],"pdf_url":"https://arxiv.org/pdf/2408.13452v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07762v2","updated":"2024-10-16T13:42:33Z","published":"2024-02-12T16:28:52Z","title":"Scalable Structure Learning for Sparse Context-Specific Systems","summary":"  Several approaches to graphically representing context-specific relations\namong jointly distributed categorical variables have been proposed, along with\nstructure learning algorithms. While existing optimization-based methods have\nlimited scalability due to the large number of context-specific models, the\nconstraint-based methods are more prone to error than even constraint-based\ndirected acyclic graph learning algorithms since more relations must be tested.\nWe present an algorithm for learning context-specific models that scales to\nhundreds of variables. Scalable learning is achieved through a combination of\nan order-based Markov chain Monte-Carlo search and a novel, context-specific\nsparsity assumption that is analogous to those typically invoked for directed\nacyclic graphical models. Unlike previous Markov chain Monte-Carlo search\nmethods, our Markov chain is guaranteed to have the true posterior of the\nvariable orderings as the stationary distribution. To implement the method, we\nsolve a first case of an open problem recently posed by Alon and Balogh. Future\nwork solving increasingly general instances of this problem would allow our\nmethods to learn increasingly dense models. The method is shown to perform well\non synthetic data and real world examples, in terms of both accuracy and\nscalability.\n","authors":["Felix Leopoldo Rios","Alex Markham","Liam Solus"],"pdf_url":"https://arxiv.org/pdf/2402.07762v2.pdf","comment":"34 pages, 6 figures; for associated code, see\n  https://cstrees.readthedocs.io"},{"id":"http://arxiv.org/abs/2410.12557v1","updated":"2024-10-16T13:34:40Z","published":"2024-10-16T13:34:40Z","title":"One Step Diffusion via Shortcut Models","summary":"  Diffusion models and flow-matching models have enabled generating diverse and\nrealistic images by learning to transfer noise to data. However, sampling from\nthese models involves iterative denoising over many neural network passes,\nmaking generation slow and expensive. Previous approaches for speeding up\nsampling require complex training regimes, such as multiple training phases,\nmultiple networks, or fragile scheduling. We introduce shortcut models, a\nfamily of generative models that use a single network and training phase to\nproduce high-quality samples in a single or multiple sampling steps. Shortcut\nmodels condition the network not only on the current noise level but also on\nthe desired step size, allowing the model to skip ahead in the generation\nprocess. Across a wide range of sampling step budgets, shortcut models\nconsistently produce higher quality samples than previous approaches, such as\nconsistency models and reflow. Compared to distillation, shortcut models reduce\ncomplexity to a single network and training phase and additionally allow\nvarying step budgets at inference time.\n","authors":["Kevin Frans","Danijar Hafner","Sergey Levine","Pieter Abbeel"],"pdf_url":"https://arxiv.org/pdf/2410.12557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12555v1","updated":"2024-10-16T13:32:35Z","published":"2024-10-16T13:32:35Z","title":"Investigating Sensitive Directions in GPT-2: An Improved Baseline and\n  Comparative Analysis of SAEs","summary":"  Sensitive directions experiments attempt to understand the computational\nfeatures of Language Models (LMs) by measuring how much the next token\nprediction probabilities change by perturbing activations along specific\ndirections. We extend the sensitive directions work by introducing an improved\nbaseline for perturbation directions. We demonstrate that KL divergence for\nSparse Autoencoder (SAE) reconstruction errors are no longer pathologically\nhigh compared to the improved baseline. We also show that feature directions\nuncovered by SAEs have varying impacts on model outputs depending on the SAE's\nsparsity, with lower L0 SAE feature directions exerting a greater influence.\nAdditionally, we find that end-to-end SAE features do not exhibit stronger\neffects on model outputs compared to traditional SAEs.\n","authors":["Daniel J. Lee","Stefan Heimersheim"],"pdf_url":"https://arxiv.org/pdf/2410.12555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12542v1","updated":"2024-10-16T13:20:57Z","published":"2024-10-16T13:20:57Z","title":"Evaluating Utility of Memory Efficient Medical Image Generation: A Study\n  on Lung Nodule Segmentation","summary":"  The scarcity of publicly available medical imaging data limits the\ndevelopment of effective AI models. This work proposes a memory-efficient\npatch-wise denoising diffusion probabilistic model (DDPM) for generating\nsynthetic medical images, focusing on CT scans with lung nodules. Our approach\ngenerates high-utility synthetic images with nodule segmentation while\nefficiently managing memory constraints, enabling the creation of training\ndatasets. We evaluate the method in two scenarios: training a segmentation\nmodel exclusively on synthetic data, and augmenting real-world training data\nwith synthetic images. In the first case, models trained solely on synthetic\ndata achieve Dice scores comparable to those trained on real-world data\nbenchmarks. In the second case, augmenting real-world data with synthetic\nimages significantly improves segmentation performance. The generated images\ndemonstrate their potential to enhance medical image datasets in scenarios with\nlimited real-world data.\n","authors":["Kathrin Khadra","Utku Türkbey"],"pdf_url":"https://arxiv.org/pdf/2410.12542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12537v1","updated":"2024-10-16T13:19:03Z","published":"2024-10-16T13:19:03Z","title":"Is Complex Query Answering Really Complex?","summary":"  Complex query answering (CQA) on knowledge graphs (KGs) is gaining momentum\nas a challenging reasoning task. In this paper, we show that the current\nbenchmarks for CQA are not really complex, and the way they are built distorts\nour perception of progress in this field. For example, we find that in these\nbenchmarks, most queries (up to 98% for some query types) can be reduced to\nsimpler problems, e.g., link prediction, where only one link needs to be\npredicted. The performance of state-of-the-art CQA models drops significantly\nwhen such models are evaluated on queries that cannot be reduced to easier\ntypes. Thus, we propose a set of more challenging benchmarks, composed of\nqueries that require models to reason over multiple hops and better reflect the\nconstruction of real-world KGs. In a systematic empirical investigation, the\nnew benchmarks show that current methods leave much to be desired from current\nCQA methods.\n","authors":["Cosimo Gregucci","Bo Xiong","Daniel Hernandez","Lorenzo Loconte","Pasquale Minervini","Steffen Staab","Antonio Vergari"],"pdf_url":"https://arxiv.org/pdf/2410.12537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12536v1","updated":"2024-10-16T13:18:45Z","published":"2024-10-16T13:18:45Z","title":"SiFiSinger: A High-Fidelity End-to-End Singing Voice Synthesizer based\n  on Source-filter Model","summary":"  This paper presents an advanced end-to-end singing voice synthesis (SVS)\nsystem based on the source-filter mechanism that directly translates lyrical\nand melodic cues into expressive and high-fidelity human-like singing.\nSimilarly to VISinger 2, the proposed system also utilizes training paradigms\nevolved from VITS and incorporates elements like the fundamental pitch (F0)\npredictor and waveform generation decoder. To address the issue that the\ncoupling of mel-spectrogram features with F0 information may introduce errors\nduring F0 prediction, we consider two strategies. Firstly, we leverage\nmel-cepstrum (mcep) features to decouple the intertwined mel-spectrogram and F0\ncharacteristics. Secondly, inspired by the neural source-filter models, we\nintroduce source excitation signals as the representation of F0 in the SVS\nsystem, aiming to capture pitch nuances more accurately. Meanwhile,\ndifferentiable mcep and F0 losses are employed as the waveform decoder\nsupervision to fortify the prediction accuracy of speech envelope and pitch in\nthe generated speech. Experiments on the Opencpop dataset demonstrate efficacy\nof the proposed model in synthesis quality and intonation accuracy.\n","authors":["Jianwei Cui","Yu Gu","Chao Weng","Jie Zhang","Liping Chen","Lirong Dai"],"pdf_url":"https://arxiv.org/pdf/2410.12536v1.pdf","comment":"Accepted by ICASSP 2024, Synthesized audio samples are available at:\n  https://sounddemos.github.io/sifisinger"},{"id":"http://arxiv.org/abs/2410.12530v1","updated":"2024-10-16T13:10:04Z","published":"2024-10-16T13:10:04Z","title":"Disentangling data distribution for Federated Learning","summary":"  Federated Learning (FL) facilitates collaborative training of a global model\nwhose performance is boosted by private data owned by distributed clients,\nwithout compromising data privacy. Yet the wide applicability of FL is hindered\nby entanglement of data distributions across different clients. This paper\ndemonstrates for the first time that by disentangling data distributions FL can\nin principle achieve efficiencies comparable to those of distributed systems,\nrequiring only one round of communication. To this end, we propose a novel\nFedDistr algorithm, which employs stable diffusion models to decouple and\nrecover data distributions. Empirical results on the CIFAR100 and DomainNet\ndatasets show that FedDistr significantly enhances model utility and efficiency\nin both disentangled and near-disentangled scenarios while ensuring privacy,\noutperforming traditional federated learning methods.\n","authors":["Xinyuan Zhao","Hanlin Gu","Lixin Fan","Qiang Yang","Yuxing Han"],"pdf_url":"https://arxiv.org/pdf/2410.12530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12522v1","updated":"2024-10-16T13:02:02Z","published":"2024-10-16T13:02:02Z","title":"MING: A Functional Approach to Learning Molecular Generative Models","summary":"  Traditional molecule generation methods often rely on sequence or graph-based\nrepresentations, which can limit their expressive power or require complex\npermutation-equivariant architectures. This paper introduces a novel paradigm\nfor learning molecule generative models based on functional representations.\nSpecifically, we propose Molecular Implicit Neural Generation (MING), a\ndiffusion-based model that learns molecular distributions in function space.\nUnlike standard diffusion processes in data space, MING employs a novel\nfunctional denoising probabilistic process, which jointly denoises the\ninformation in both the function's input and output spaces by leveraging an\nexpectation-maximization procedure for latent implicit neural representations\nof data. This approach allows for a simple yet effective model design that\naccurately captures underlying function distributions. Experimental results on\nmolecule-related datasets demonstrate MING's superior performance and ability\nto generate plausible molecular samples, surpassing state-of-the-art data-space\nmethods while offering a more streamlined architecture and significantly faster\ngeneration times.\n","authors":["Van Khoa Nguyen","Maciej Falkiewicz","Giangiacomo Mercatali","Alexandros Kalousis"],"pdf_url":"https://arxiv.org/pdf/2410.12522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19014v3","updated":"2024-10-16T12:55:55Z","published":"2024-09-24T01:40:50Z","title":"FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark","summary":"  Text-to-SQL systems have become crucial for translating natural language into\nSQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, the Execution Accuracy\n(EX), the most prevalent evaluation metric, still shows many false positives\nand negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our metric improves\nagreement with human experts (from 62 to 87.04 in Cohen's kappa) with\ncomprehensive context and sophisticated criteria. Our extensive experiments\nyield several key insights: (1) Models' performance increases by over 2.6\npoints on average, substantially affecting rankings on Spider and BIRD\nbenchmarks; (2) The underestimation of models in EX primarily stems from\nannotation quality issues; and (3) Model performance on particularly\nchallenging questions tends to be overestimated. This work contributes to a\nmore accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field.\n","authors":["Heegyu Kim","Taeyang Jeon","Seunghwan Choi","Seungtaek Choi","Hyunsouk Cho"],"pdf_url":"https://arxiv.org/pdf/2409.19014v3.pdf","comment":"preprint, under review"},{"id":"http://arxiv.org/abs/2410.10114v2","updated":"2024-10-16T12:30:53Z","published":"2024-10-14T03:05:12Z","title":"Mixture of Experts Made Personalized: Federated Prompt Learning for\n  Vision-Language Models","summary":"  Prompt learning for pre-trained Vision-Language Models (VLMs) like CLIP has\ndemonstrated potent applicability across diverse downstream tasks. This\nlightweight approach has quickly gained traction from federated learning (FL)\nresearchers who seek to efficiently adapt VLMs to heterogeneous scenarios.\nHowever, current federated prompt learning methods are habitually restricted to\nthe traditional FL paradigm, where the participating clients are generally only\nallowed to download a single globally aggregated model from the server. While\njustifiable for training full-sized models under federated settings, in this\nwork, we argue that this paradigm is ill-suited for lightweight prompts. By\nfacilitating the clients to download multiple pre-aggregated prompts as fixed\nnon-local experts, we propose Personalized Federated Mixture of Adaptive\nPrompts (pFedMoAP), a novel FL framework that personalizes the prompt learning\nprocess through the lens of Mixture of Experts (MoE). pFedMoAP implements a\nlocal attention-based gating network that learns to generate enhanced text\nfeatures for better alignment with local image data on the client, benefiting\nfrom both local and downloaded non-local adaptive prompt experts. The non-local\nexperts are sparsely selected from a server-maintained pool, fostering\ncollaborative learning across clients. To evaluate the proposed algorithm, we\nconduct extensive experiments across 9 datasets under various heterogeneous\nfederated settings. The results show that pFedMoAP consistently outperforms the\nstate-of-the-art alternatives, underscoring its efficacy in personalizing\nprompt learning for CLIP within the federated learning paradigm.\n","authors":["Jun Luo","Chen Chen","Shandong Wu"],"pdf_url":"https://arxiv.org/pdf/2410.10114v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2311.07454v5","updated":"2024-10-16T12:28:25Z","published":"2023-11-13T16:35:34Z","title":"Causal Discovery under Latent Class Confounding","summary":"  An acyclic causal structure can be described with directed acyclic graph\n(DAG), where arrows indicate the possibility of direct causation. The task of\nlearning this structure from data is known as \"causal discovery.\" Diverse\npopulations or changing environments can sometimes give rise to data that is\nheterogeneous in the following sense: each population/environment is a \"source\"\nwhich idiosyncratically determines the forms of those direct causal effects.\nFrom this perspective, the source is a latent common cause for every observed\nvariable. While some methods for causal discovery are able to work around\nlatent confounding in special cases, especially when only few observables are\nconfounded, a global confounder is a difficult challenge. The only known ways\nto deal with latent global confounding involve assumptions that limit the\nstructural equations and/or noise functions. We demonstrate that globally\nconfounded causal structures can still be identifiable with arbitrary\nstructural equations and noise functions, so long as the number of latent\nclasses remains small relative to the size and sparsity of the underlying DAG.\n","authors":["Bijan Mazaheri","Spencer Gordon","Yuval Rabani","Leonard Schulman"],"pdf_url":"https://arxiv.org/pdf/2311.07454v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11929v4","updated":"2024-10-16T12:20:35Z","published":"2024-01-22T13:15:40Z","title":"Parsimony or Capability? Decomposition Delivers Both in Long-term Time\n  Series Forecasting","summary":"  Long-term time series forecasting (LTSF) represents a critical frontier in\ntime series analysis, characterized by extensive input sequences, as opposed to\nthe shorter spans typical of traditional approaches. While longer sequences\ninherently offer richer information for enhanced predictive precision,\nprevailing studies often respond by escalating model complexity. These\nintricate models can inflate into millions of parameters, resulting in\nprohibitive parameter scales. Our study demonstrates, through both analytical\nand empirical evidence, that decomposition is key to containing excessive model\ninflation while achieving uniformly superior and robust results across various\ndatasets. Remarkably, by tailoring decomposition to the intrinsic dynamics of\ntime series data, our proposed model outperforms existing benchmarks, using\nover 99 \\% fewer parameters than the majority of competing methods. Through\nthis work, we aim to unleash the power of a restricted set of parameters by\ncapitalizing on domain characteristics--a timely reminder that in the realm of\nLTSF, bigger is not invariably better.\n","authors":["Jinliang Deng","Feiyang Ye","Du Yin","Xuan Song","Ivor W. Tsang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2401.11929v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12492v1","updated":"2024-10-16T12:14:29Z","published":"2024-10-16T12:14:29Z","title":"End-to-end Planner Training for Language Modeling","summary":"  Through end-to-end training to predict the next token, LLMs have become\nvaluable tools for various tasks. Enhancing their core training in language\nmodeling can improve numerous downstream applications. A successful approach to\nenhance language modeling uses a separate planning module to predict abstract\nlabels of future sentences and conditions the LM on these predictions. However,\nthis method is non-differentiable, preventing joint end-to-end tuning of the\nplanner with the LM. We propose an effective method to improve this approach by\nenabling joint fine-tuning of the planner and the LM. We show that a naive way\nof approximating the gradient of selecting a label via the straight-through\nestimator is not effective. Instead, we propose to use the predicted label\nprobabilities as mixing weights to condition the LM on a weighted average of\nlabel embeddings in a differentiable manner. This not only enables joint\nfine-tuning of the planner and the LM, but also allows the LM to draw on the\nfull label distribution predicted by the planner, retaining more information.\nOur experimental results show consistent improvements in perplexity.\n","authors":["Nathan Cornille","Florian Mai","Jingyuan Sun","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2410.12492v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2410.04492v3","updated":"2024-10-16T12:07:27Z","published":"2024-10-06T14:11:39Z","title":"Interpret Your Decision: Logical Reasoning Regularization for\n  Generalization in Visual Classification","summary":"  Vision models excel in image classification but struggle to generalize to\nunseen data, such as classifying images from unseen domains or discovering\nnovel categories. In this paper, we explore the relationship between logical\nreasoning and deep learning generalization in visual classification. A logical\nregularization termed L-Reg is derived which bridges a logical analysis\nframework to image classification. Our work reveals that L-Reg reduces the\ncomplexity of the model in terms of the feature distribution and classifier\nweights. Specifically, we unveil the interpretability brought by L-Reg, as it\nenables the model to extract the salient features, such as faces to persons,\nfor classification. Theoretical analysis and experiments demonstrate that L-Reg\nenhances generalization across various scenarios, including multi-domain\ngeneralization and generalized category discovery. In complex real-world\nscenarios where images span unknown classes and unseen domains, L-Reg\nconsistently improves generalization, highlighting its practical efficacy.\n","authors":["Zhaorui Tan","Xi Yang","Qiufeng Wang","Anh Nguyen","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2410.04492v3.pdf","comment":"Accepted by NeurIPS2024 as Spotlight"},{"id":"http://arxiv.org/abs/2410.12485v1","updated":"2024-10-16T12:03:37Z","published":"2024-10-16T12:03:37Z","title":"Data-Driven Gyroscope Calibration","summary":"  Gyroscopes are inertial sensors that measure the angular velocity of the\nplatforms to which they are attached. To estimate the gyroscope deterministic\nerror terms prior mission start, a calibration procedure is performed. When\nconsidering low-cost gyroscopes, the calibration requires a turntable as the\ngyros are incapable of sensing the Earth turn rate. In this paper, we propose a\ndata-driven framework to estimate the scale factor and bias of a gyroscope. To\ntrain and validate our approach, a dataset of 56 minutes was recorded using a\nturntable. We demonstrated that our proposed approach outperforms the\nmodel-based approach, in terms of accuracy and convergence time. Specifically,\nwe improved the scale factor and bias estimation by an average of 72% during\nsix seconds of calibration time, demonstrating an average of 75% calibration\ntime improvement. That is, instead of minutes, our approach requires only\nseveral seconds for the calibration.\n","authors":["Zeev Yampolsky","Itzik Klein"],"pdf_url":"https://arxiv.org/pdf/2410.12485v1.pdf","comment":"19 Pages, 5 Figures, 3 Tables"},{"id":"http://arxiv.org/abs/2406.16535v2","updated":"2024-10-16T12:00:46Z","published":"2024-06-24T11:16:26Z","title":"Token-based Decision Criteria Are Suboptimal in In-context Learning","summary":"  In-Context Learning (ICL) typically utilizes classification criteria from\noutput probabilities of manually selected label tokens. However, we argue that\nsuch token-based classification criteria lead to suboptimal decision\nboundaries, despite delicate calibrations through translation and constrained\nrotation applied. To address this problem, we propose Hidden Calibration, which\nrenounces token probabilities and uses the nearest centroid classifier on the\nLM's last hidden states. In detail, we assign the label of the nearest centroid\npreviously estimated from a calibration set to the test sample as the predicted\nlabel. Our experiments on 6 models and 10 classification datasets indicate that\nHidden Calibration consistently outperforms current token-based baselines by\nabout 20%~50%, achieving a strong state-of-the-art in ICL. Our further analysis\ndemonstrates that Hidden Calibration finds better classification criteria with\nless inter-class overlap, and LMs provide linearly separable intra-class\nclusters with the help of demonstrations, which supports Hidden Calibration and\ngives new insights into the principle of ICL.\n","authors":["Hakaze Cho","Yoshihiro Sakai","Mariko Kato","Kenshiro Tanaka","Akira Ishii","Naoya Inoue"],"pdf_url":"https://arxiv.org/pdf/2406.16535v2.pdf","comment":"24 pages, 15 figures, 13 tables"},{"id":"http://arxiv.org/abs/2410.12481v1","updated":"2024-10-16T11:59:27Z","published":"2024-10-16T11:59:27Z","title":"SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and\n  Hindsight Relabeling","summary":"  The past years have seen Large Language Models (LLMs) strive not only as\ngenerative models but also as agents solving textual sequential decision-making\ntasks. When facing complex environments where their zero-shot abilities are\ninsufficient, recent work showed online Reinforcement Learning (RL) could be\nused for the LLM agent to discover and learn efficient strategies\ninteractively. However, most prior work sticks to on-policy algorithms, which\ngreatly reduces the scope of methods such agents could use for both exploration\nand exploitation, such as experience replay and hindsight relabeling. Yet, such\nmethods may be key for LLM learning agents, and in particular when designing\nautonomous intrinsically motivated agents sampling and pursuing their own goals\n(i.e. autotelic agents). This paper presents and studies an adaptation of Soft\nActor-Critic and hindsight relabeling to LLM agents. Our method not only paves\nthe path towards autotelic LLM agents that learn online but can also outperform\non-policy methods in more classic multi-goal RL environments.\n","authors":["Loris Gaven","Clement Romac","Thomas Carta","Sylvain Lamprier","Olivier Sigaud","Pierre-Yves Oudeyer"],"pdf_url":"https://arxiv.org/pdf/2410.12481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11224v2","updated":"2024-10-16T11:56:57Z","published":"2024-10-15T03:09:06Z","title":"DeltaDock: A Unified Framework for Accurate, Efficient, and Physically\n  Reliable Molecular Docking","summary":"  Molecular docking, a technique for predicting ligand binding poses, is\ncrucial in structure-based drug design for understanding protein-ligand\ninteractions. Recent advancements in docking methods, particularly those\nleveraging geometric deep learning (GDL), have demonstrated significant\nefficiency and accuracy advantages over traditional sampling methods. Despite\nthese advancements, current methods are often tailored for specific docking\nsettings, and limitations such as the neglect of protein side-chain structures,\ndifficulties in handling large binding pockets, and challenges in predicting\nphysically valid structures exist. To accommodate various docking settings and\nachieve accurate, efficient, and physically reliable docking, we propose a\nnovel two-stage docking framework, DeltaDock, consisting of pocket prediction\nand site-specific docking. We innovatively reframe the pocket prediction task\nas a pocket-ligand alignment problem rather than direct prediction in the first\nstage. Then we follow a bi-level coarse-to-fine iterative refinement process to\nperform site-specific docking. Comprehensive experiments demonstrate the\nsuperior performance of DeltaDock. Notably, in the blind docking setting,\nDeltaDock achieves a 31\\% relative improvement over the docking success rate\ncompared with the previous state-of-the-art GDL model. With the consideration\nof physical validity, this improvement increases to about 300\\%.\n","authors":["Jiaxian Yan","Zaixi Zhang","Jintao Zhu","Kai Zhang","Jianfeng Pei","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2410.11224v2.pdf","comment":"Accepted by NeurIPS'24"},{"id":"http://arxiv.org/abs/2410.12480v1","updated":"2024-10-16T11:50:02Z","published":"2024-10-16T11:50:02Z","title":"KcMF: A Knowledge-compliant Framework for Schema and Entity Matching\n  with Fine-tuning-free LLMs","summary":"  Schema and entity matching tasks are crucial for data integration and\nmanagement. While large language models (LLMs) have shown promising results in\nthese tasks, they suffer from hallucinations and confusion about task\ninstructions. In this paper, we present the Knowledge-Compliant Matching\nFramework (KcMF), an LLM-based approach that addresses these issues without the\nneed for domain-specific fine-tuning. KcMF employs a pseudo-code-based task\ndecomposition strategy to adopt task-specific natural language statements that\nguide LLM reasoning and reduce confusion. We also propose two mechanisms,\nDataset as Knowledge (DaK) and Example as Knowledge (EaK), to build domain\nknowledge sets when unstructured domain knowledge is lacking. Additionally, we\nintroduce a result-ensembling strategy to leverage multiple knowledge sources\nand suppress poorly formatted outputs. Comprehensive evaluations on schema and\nentity matching tasks demonstrate that KcMF outperforms previous non-LLM\nstate-of-the-art (SOTA) methods by an average F1 score of 22.9% and competes\neffectively with SOTA fine-tuned LLMs. Moreover, KcMF generalizes well across\ndifferent LLMs.\n","authors":["Yongqin Xu","Huan Li","Ke Chen","Lidan Shou"],"pdf_url":"https://arxiv.org/pdf/2410.12480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12476v1","updated":"2024-10-16T11:46:32Z","published":"2024-10-16T11:46:32Z","title":"Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial\n  Generation","summary":"  Machine learning (ML) exhibits promise in the clinical domain. However, it is\nconstrained by data scarcity and ethical considerations, as the generation of\nclinical trials presents significant challenges due to stringent privacy\nregulations, high costs, and the extended duration required for conducting\nstudies with human participants. Despite the advancements of large language\nmodels (LLMs) in general generation tasks, their potential in facilitating the\ngeneration of synthetic clinical trials is under-explored. To address this gap,\nwe introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs\nto generate artificial yet realistic and diverse clinical trials with binary\nsuccess/failure labels. Experiments conducted on real clinical trials from the\n\\url{ClinicalTrials.gov} database demonstrate that our synthetic data can\neffectively augment real datasets. Furthermore, by fine-tuning a pre-trained\nmodel as a binary classifier on synthetic clinical trial datasets, we\ndemonstrate that this augmentation enhances model training for downstream tasks\nsuch as trial outcome prediction. Our findings suggest that LLMs for synthetic\nclinical trial generation hold promise for accelerating clinical research and\nupholding ethical standards for patient privacy. The code is publicly available\nat\nhttps://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.\n","authors":["Zerui Xu","Fang Wu","Tianfan Fu","Yue Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.12476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.12146v8","updated":"2024-10-16T11:45:22Z","published":"2021-11-23T20:41:06Z","title":"Sharing to learn and learning to share; Fitting together Meta-Learning,\n  Multi-Task Learning, and Transfer Learning: A meta review","summary":"  Integrating knowledge across different domains is an essential feature of\nhuman learning. Learning paradigms such as transfer learning, meta-learning,\nand multi-task learning reflect the human learning process by exploiting the\nprior knowledge for new tasks, encouraging faster learning and good\ngeneralization for new tasks. This article gives a detailed view of these\nlearning paradigms and their comparative analysis. The weakness of one learning\nalgorithm turns out to be a strength of another, and thus, merging them is a\nprevalent trait in the literature. Numerous research papers focus on each of\nthese learning paradigms separately and provide a comprehensive overview of\nthem. However, this article reviews research studies that combine (two of)\nthese learning algorithms. This survey describes how these techniques are\ncombined to solve problems in many different fields of research, including\ncomputer vision, natural language processing, hyper-spectral imaging, and many\nmore, in a supervised setting only. Based on the knowledge accumulated from the\nliterature, we hypothesize a generic task-agnostic and model-agnostic learning\nnetwork - an ensemble of meta-learning, transfer learning, and multi-task\nlearning, termed Multi-modal Multi-task Meta Transfer Learning. We also present\nsome open research questions, limitations, and future research directions for\nthis proposed network. The aim of this article is to spark interest among\nscholars in effectively merging existing learning algorithms with the intention\nof advancing research in this field. Instead of presenting experimental\nresults, we invite readers to explore and contemplate techniques for merging\nalgorithms while navigating through their limitations.\n","authors":["Richa Upadhyay","Ronald Phlypo","Rajkumar Saini","Marcus Liwicki"],"pdf_url":"https://arxiv.org/pdf/2111.12146v8.pdf","comment":"This article has been accepted for publication in IEEE Access. This\n  is the author's version which has not been fully edited and content may\n  slightly change prior to final publication. Citation information: DOI\n  10.1109/ACCESS.2024.3478805"},{"id":"http://arxiv.org/abs/2410.12474v1","updated":"2024-10-16T11:42:11Z","published":"2024-10-16T11:42:11Z","title":"Mind the Gap Between Prototypes and Images in Cross-domain Finetuning","summary":"  In cross-domain few-shot classification (CFC), recent works mainly focus on\nadapting a simple transformation head on top of a frozen pre-trained backbone\nwith few labeled data to project embeddings into a task-specific metric space\nwhere classification can be performed by measuring similarities between image\ninstance and prototype representations. Technically, an assumption implicitly\nadopted in such a framework is that the prototype and image instance embeddings\nshare the same representation transformation. However, in this paper, we find\nthat there naturally exists a gap, which resembles the modality gap, between\nthe prototype and image instance embeddings extracted from the frozen\npre-trained backbone, and simply applying the same transformation during the\nadaptation phase constrains exploring the optimal representations and shrinks\nthe gap between prototype and image representations. To solve this problem, we\npropose a simple yet effective method, contrastive prototype-image adaptation\n(CoPA), to adapt different transformations respectively for prototypes and\nimages similarly to CLIP by treating prototypes as text prompts. Extensive\nexperiments on Meta-Dataset demonstrate that CoPA achieves the state-of-the-art\nperformance more efficiently. Meanwhile, further analyses also indicate that\nCoPA can learn better representation clusters, enlarge the gap, and achieve\nminimal validation loss at the enlarged gap.\n","authors":["Hongduan Tian","Feng Liu","Zhanke Zhou","Tongliang Liu","Chengqi Zhang","Bo Han"],"pdf_url":"https://arxiv.org/pdf/2410.12474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12461v1","updated":"2024-10-16T11:21:07Z","published":"2024-10-16T11:21:07Z","title":"Challenges, Methods, Data -- a Survey of Machine Learning in Water\n  Distribution Networks","summary":"  Research on methods for planning and controlling water distribution networks\ngains increasing relevance as the availability of drinking water will decrease\nas a consequence of climate change. So far, the majority of approaches is based\non hydraulics and engineering expertise. However, with the increasing\navailability of sensors, machine learning techniques constitute a promising\ntool. This work presents the main tasks in water distribution networks,\ndiscusses how they relate to machine learning and analyses how the\nparticularities of the domain pose challenges to and can be leveraged by\nmachine learning approaches. Besides, it provides a technical toolkit by\npresenting evaluation benchmarks and a structured survey of the exemplary task\nof leakage detection and localization.\n","authors":["Valerie Vaquet","Fabian Hinder","André Artelt","Inaam Ashraf","Janine Strotherm","Jonas Vaquet","Johannes Brinkrolf","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2410.12461v1.pdf","comment":"This preprint has not undergone any post-submission improvements or\n  corrections. The Version of Record of this contribution is published in\n  Artificial Neural Networks and Machine Learning -- ICANN 2024"},{"id":"http://arxiv.org/abs/2410.11190v2","updated":"2024-10-16T11:19:56Z","published":"2024-10-15T02:10:45Z","title":"Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex\n  Capabilities","summary":"  GPT-4o, an all-encompassing model, represents a milestone in the development\nof large multi-modal language models. It can understand visual, auditory, and\ntextual modalities, directly output audio, and support flexible duplex\ninteraction. Models from the open-source community often achieve some\nfunctionalities of GPT-4o, such as visual understanding and voice chat.\nNevertheless, training a unified model that incorporates all modalities is\nchallenging due to the complexities of multi-modal data, intricate model\narchitectures, and training processes. In this paper, we introduce Mini-Omni2,\na visual-audio assistant capable of providing real-time, end-to-end voice\nresponses to visoin and audio queries. By integrating pretrained visual and\nauditory encoders, Mini-Omni2 maintains performance in individual modalities.\nWe propose a three-stage training process to align modalities, allowing the\nlanguage model to handle multi-modal inputs and outputs after training on a\nlimited dataset. For interaction, we introduce a command-based interruption\nmechanism, enabling more flexible interaction with users. To the best of our\nknowledge, Mini-Omni2 is one of the closest reproductions of GPT-4o, which have\nsimilar form of functionality, and we hope it can offer valuable insights for\nsubsequent research.\n","authors":["Zhifei Xie","Changqiao Wu"],"pdf_url":"https://arxiv.org/pdf/2410.11190v2.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.12459v1","updated":"2024-10-16T11:16:47Z","published":"2024-10-16T11:16:47Z","title":"HELM: Hierarchical Encoding for mRNA Language Modeling","summary":"  Messenger RNA (mRNA) plays a crucial role in protein synthesis, with its\ncodon structure directly impacting biological properties. While Language Models\n(LMs) have shown promise in analyzing biological sequences, existing approaches\nfail to account for the hierarchical nature of mRNA's codon structure. We\nintroduce Hierarchical Encoding for mRNA Language Modeling (HELM), a novel\npre-training strategy that incorporates codon-level hierarchical structure into\nlanguage model training. HELM modulates the loss function based on codon\nsynonymity, aligning the model's learning process with the biological reality\nof mRNA sequences. We evaluate HELM on diverse mRNA datasets and tasks,\ndemonstrating that HELM outperforms standard language model pre-training as\nwell as existing foundation model baselines on six diverse downstream property\nprediction tasks and an antibody region annotation tasks on average by around\n8\\%. Additionally, HELM enhances the generative capabilities of language model,\nproducing diverse mRNA sequences that better align with the underlying true\ndata distribution compared to non-hierarchical baselines.\n","authors":["Mehdi Yazdani-Jahromi","Mangal Prakash","Tommaso Mansi","Artem Moskalev","Rui Liao"],"pdf_url":"https://arxiv.org/pdf/2410.12459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12457v1","updated":"2024-10-16T11:08:06Z","published":"2024-10-16T11:08:06Z","title":"Sharpness-Aware Black-Box Optimization","summary":"  Black-box optimization algorithms have been widely used in various machine\nlearning problems, including reinforcement learning and prompt fine-tuning.\nHowever, directly optimizing the training loss value, as commonly done in\nexisting black-box optimization methods, could lead to suboptimal model quality\nand generalization performance. To address those problems in black-box\noptimization, we propose a novel Sharpness-Aware Black-box Optimization (SABO)\nalgorithm, which applies a sharpness-aware minimization strategy to improve the\nmodel generalization. Specifically, the proposed SABO method first\nreparameterizes the objective function by its expectation over a Gaussian\ndistribution. Then it iteratively updates the parameterized distribution by\napproximated stochastic gradients of the maximum objective value within a small\nneighborhood around the current solution in the Gaussian distribution space.\nTheoretically, we prove the convergence rate and generalization bound of the\nproposed SABO algorithm. Empirically, extensive experiments on the black-box\nprompt fine-tuning tasks demonstrate the effectiveness of the proposed SABO\nmethod in improving model generalization performance.\n","authors":["Feiyang Ye","Yueming Lyu","Xuehao Wang","Masashi Sugiyama","Yu Zhang","Ivor Tsang"],"pdf_url":"https://arxiv.org/pdf/2410.12457v1.pdf","comment":"27 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.12456v1","updated":"2024-10-16T11:08:02Z","published":"2024-10-16T11:08:02Z","title":"Training Neural Samplers with Reverse Diffusive KL Divergence","summary":"  Training generative models to sample from unnormalized density functions is\nan important and challenging task in machine learning. Traditional training\nmethods often rely on the reverse Kullback-Leibler (KL) divergence due to its\ntractability. However, the mode-seeking behavior of reverse KL hinders\neffective approximation of multi-modal target distributions. To address this,\nwe propose to minimize the reverse KL along diffusion trajectories of both\nmodel and target densities. We refer to this objective as the reverse diffusive\nKL divergence, which allows the model to capture multiple modes. Leveraging\nthis objective, we train neural samplers that can efficiently generate samples\nfrom the target distribution in one step. We demonstrate that our method\nenhances sampling performance across various Boltzmann distributions, including\nboth synthetic multi-modal densities and n-body particle systems.\n","authors":["Jiajun He","Wenlin Chen","Mingtian Zhang","David Barber","José Miguel Hernández-Lobato"],"pdf_url":"https://arxiv.org/pdf/2410.12456v1.pdf","comment":"23 pages, 6 figures, 3 tables, 1 algorithm"},{"id":"http://arxiv.org/abs/2410.12455v1","updated":"2024-10-16T11:05:43Z","published":"2024-10-16T11:05:43Z","title":"Loss Landscape Characterization of Neural Networks without\n  Over-Parametrziation","summary":"  Optimization methods play a crucial role in modern machine learning, powering\nthe remarkable empirical achievements of deep learning models. These successes\nare even more remarkable given the complex non-convex nature of the loss\nlandscape of these models. Yet, ensuring the convergence of optimization\nmethods requires specific structural conditions on the objective function that\nare rarely satisfied in practice. One prominent example is the widely\nrecognized Polyak-Lojasiewicz (PL) inequality, which has gained considerable\nattention in recent years. However, validating such assumptions for deep neural\nnetworks entails substantial and often impractical levels of\nover-parametrization. In order to address this limitation, we propose a novel\nclass of functions that can characterize the loss landscape of modern deep\nmodels without requiring extensive over-parametrization and can also include\nsaddle points. Crucially, we prove that gradient-based optimizers possess\ntheoretical guarantees of convergence under this assumption. Finally, we\nvalidate the soundness of our new function class through both theoretical\nanalysis and empirical experimentation across a diverse range of deep learning\nmodels.\n","authors":["Rustem Islamov","Niccolò Ajroldi","Antonio Orvieto","Aurelien Lucchi"],"pdf_url":"https://arxiv.org/pdf/2410.12455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12452v1","updated":"2024-10-16T11:00:25Z","published":"2024-10-16T11:00:25Z","title":"FairGLVQ: Fairness in Partition-Based Classification","summary":"  Fairness is an important objective throughout society. From the distribution\nof limited goods such as education, over hiring and payment, to taxes,\nlegislation, and jurisprudence. Due to the increasing importance of machine\nlearning approaches in all areas of daily life including those related to\nhealth, security, and equity, an increasing amount of research focuses on fair\nmachine learning. In this work, we focus on the fairness of partition- and\nprototype-based models. The contribution of this work is twofold: 1) we develop\na general framework for fair machine learning of partition-based models that\ndoes not depend on a specific fairness definition, and 2) we derive a fair\nversion of learning vector quantization (LVQ) as a specific instantiation. We\ncompare the resulting algorithm against other algorithms from the literature on\ntheoretical and real-world data showing its practical relevance.\n","authors":["Felix Störck","Fabian Hinder","Johannes Brinkrolf","Benjamin Paassen","Valerie Vaquet","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2410.12452v1.pdf","comment":"This preprint has not undergone any post-submission improvements or\n  corrections. The Version of Record of this contribution is published in\n  Advances in Self-Organizing Maps, Learning Vector Quantization, Interpretable\n  Machine Learning, and Beyond"},{"id":"http://arxiv.org/abs/2410.11267v2","updated":"2024-10-16T11:00:09Z","published":"2024-10-15T04:44:21Z","title":"FedCCRL: Federated Domain Generalization with Cross-Client\n  Representation Learning","summary":"  Domain Generalization (DG) aims to train models that can effectively\ngeneralize to unseen domains. However, in the context of Federated Learning\n(FL), where clients collaboratively train a model without directly sharing\ntheir data, most existing DG algorithms are not directly applicable to the FL\nsetting due to privacy constraints, as well as the limited data quantity and\ndomain diversity at each client. To tackle these challenges, we propose\nFedCCRL, a novel federated domain generalization method that significantly\nimproves the model's ability to generalize to unseen domains without\ncompromising privacy or incurring excessive computational and communication\ncosts. Specifically, we adapt MixStyle to the federated setting to transfer\ndomain-specific features while AugMix is employed to perturb domain-invariant\nfeatures. Furthermore, we leverage supervised contrastive loss for\nrepresentation alignment and utilize Jensen-Shannon divergence to ensure\nconsistent predictions between original and augmented samples. Extensive\nexperimental results demonstrate that FedCCRL achieves the state-of-the-art\nperformances on the PACS, OfficeHome and miniDomainNet datasets across varying\nnumbers of clients. Code is available at\nhttps://github.com/SanphouWang/FedCCRL.\n","authors":["Xinpeng Wang","Xiaoying Tang"],"pdf_url":"https://arxiv.org/pdf/2410.11267v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12443v1","updated":"2024-10-16T10:41:17Z","published":"2024-10-16T10:41:17Z","title":"Reconstruction of Differentially Private Text Sanitization via Large\n  Language Models","summary":"  Differential privacy (DP) is the de facto privacy standard against privacy\nleakage attacks, including many recently discovered ones against large language\nmodels (LLMs). However, we discovered that LLMs could reconstruct the\naltered/removed privacy from given DP-sanitized prompts. We propose two attacks\n(black-box and white-box) based on the accessibility to LLMs and show that LLMs\ncould connect the pair of DP-sanitized text and the corresponding private\ntraining data of LLMs by giving sample text pairs as instructions (in the\nblack-box attacks) or fine-tuning data (in the white-box attacks). To\nillustrate our findings, we conduct comprehensive experiments on modern LLMs\n(e.g., LLaMA-2, LLaMA-3, ChatGPT-3.5, ChatGPT-4, ChatGPT-4o, Claude-3,\nClaude-3.5, OPT, GPT-Neo, GPT-J, Gemma-2, and Pythia) using commonly used\ndatasets (such as WikiMIA, Pile-CC, and Pile-Wiki) against both word-level and\nsentence-level DP. The experimental results show promising recovery rates,\ne.g., the black-box attacks against the word-level DP over WikiMIA dataset gave\n72.18% on LLaMA-2 (70B), 82.39% on LLaMA-3 (70B), 75.35% on Gemma-2, 91.2% on\nChatGPT-4o, and 94.01% on Claude-3.5 (Sonnet). More urgently, this study\nindicates that these well-known LLMs have emerged as a new security risk for\nexisting DP text sanitization approaches in the current environment.\n","authors":["Shuchao Pang","Zhigang Lu","Haichen Wang","Peng Fu","Yongbin Zhou","Minhui Xue","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2410.12443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12439v1","updated":"2024-10-16T10:34:11Z","published":"2024-10-16T10:34:11Z","title":"ConLUX: Concept-Based Local Unified Explanations","summary":"  With the rapid advancements of various machine learning models, there is a\nsignificant demand for model-agnostic explanation techniques, which can explain\nthese models across different architectures. Mainstream model-agnostic\nexplanation techniques generate local explanations based on basic features\n(e.g., words for text models and (super-)pixels for image models). However,\nthese explanations often do not align with the decision-making processes of the\ntarget models and end-users, resulting in explanations that are unfaithful and\ndifficult for users to understand. On the other hand, concept-based techniques\nprovide explanations based on high-level features (e.g., topics for text models\nand objects for image models), but most are model-specific or require\nadditional pre-defined external concept knowledge. To address this limitation,\nwe propose \\toolname, a general framework to provide concept-based local\nexplanations for any machine learning models. Our key insight is that we can\nautomatically extract high-level concepts from large pre-trained models, and\nuniformly extend existing local model-agnostic techniques to provide unified\nconcept-based explanations. We have instantiated \\toolname on four different\ntypes of explanation techniques: LIME, Kernel SHAP, Anchor, and LORE, and\napplied these techniques to text and image models. Our evaluation results\ndemonstrate that 1) compared to the vanilla versions, \\toolname offers more\nfaithful explanations and makes them more understandable to users, and 2) by\noffering multiple forms of explanations, \\toolname outperforms state-of-the-art\nconcept-based explanation techniques specifically designed for text and image\nmodels, respectively.\n","authors":["Junhao Liu","Haonan Yu","Xin Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12435v1","updated":"2024-10-16T10:28:22Z","published":"2024-10-16T10:28:22Z","title":"Approaching Metaheuristic Deep Learning Combos for Automated Data Mining","summary":"  Lack of data on which to perform experimentation is a recurring issue in many\nareas of research, particularly in machine learning. The inability of most\nautomated data mining techniques to be generalized to all types of data is\ninherently related with their dependency on those types which deems them\nineffective against anything slightly different. Meta-heuristics are algorithms\nwhich attempt to optimize some solution independently of the type of data used,\nwhilst classifiers or neural networks focus on feature extrapolation and\ndimensionality reduction to fit some model onto data arranged in a particular\nway. These two algorithmic fields encompass a group of characteristics which\nwhen combined are seemingly capable of achieving data mining regardless of how\nit is arranged. To this end, this work proposes a means of combining\nmeta-heuristic methods with conventional classifiers and neural networks in\norder to perform automated data mining. Experiments on the MNIST dataset for\nhandwritten digit recognition were performed and it was empirically observed\nthat using a ground truth labeled dataset's validation accuracy is inadequate\nfor correcting labels of other previously unseen data instances.\n","authors":["Gustavo Assunção","Paulo Menezes"],"pdf_url":"https://arxiv.org/pdf/2410.12435v1.pdf","comment":"Tentative submission for data mining and knowledge discovery"},{"id":"http://arxiv.org/abs/2406.03402v2","updated":"2024-10-16T10:14:36Z","published":"2024-06-04T09:07:45Z","title":"Mixed-Precision Federated Learning via Multi-Precision Over-The-Air\n  Aggregation","summary":"  Over-the-Air Federated Learning (OTA-FL) is a privacy-preserving distributed\nlearning mechanism, by aggregating updates in the electromagnetic channel\nrather than at the server. A critical research gap in existing OTA-FL research\nis the assumption of homogeneous client computational bit precision. While in\nreal world application, clients with varying hardware resources may exploit\napproximate computing (AxC) to operate at different bit precisions optimized\nfor energy and computational efficiency. And model updates of various\nprecisions amongst clients poses an open challenge for OTA-FL, as it is\nincompatible in the wireless modulation superposition. Here, we propose an\nmixed-precision OTA-FL framework of clients with multiple bit precisions,\ndemonstrating the following innovations: (i) the superior trade-off for both\nserver and clients within the constraints of varying edge computing\ncapabilities, energy efficiency, and learning accuracy requirements comparing\nto homogeneous client bit precision, and (ii) a multi-precision gradient\nmodulation scheme to ensure compatibility with OTA aggregation and eliminate\nthe overheads of precision conversion. Through case study with real world data,\nwe validate our modulation scheme that enables AxC based mixed-precision\nOTA-FL. In comparison to homogeneous standard precision of 32-bit and 16-bit,\nour framework presents more than 10% in 4-bit ultra low precision client\nperformance and over 65%and 13% of energy savings respectively. This\ndemonstrates the great potential of our mixed-precision OTA-FL approach in\nheterogeneous edge computing environments.\n","authors":["Jinsheng Yuan","Zhuangkun Wei","Weisi Guo"],"pdf_url":"https://arxiv.org/pdf/2406.03402v2.pdf","comment":"Submitted to WCNC 2025"},{"id":"http://arxiv.org/abs/2410.12425v1","updated":"2024-10-16T10:08:02Z","published":"2024-10-16T10:08:02Z","title":"Perseus: Leveraging Common Data Patterns with Curriculum Learning for\n  More Robust Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) excel at handling graph data but remain\nvulnerable to adversarial attacks. Existing defense methods typically rely on\nassumptions like graph sparsity and homophily to either preprocess the graph or\nguide structure learning. However, preprocessing methods often struggle to\naccurately distinguish between normal edges and adversarial perturbations,\nleading to suboptimal results due to the loss of valuable edge information.\nRobust graph neural network models train directly on graph data affected by\nadversarial perturbations, without preprocessing. This can cause the model to\nget stuck in poor local optima, negatively affecting its performance. To\naddress these challenges, we propose Perseus, a novel adversarial defense\nmethod based on curriculum learning. Perseus assesses edge difficulty using\nglobal homophily and applies a curriculum learning strategy to adjust the\nlearning order, guiding the model to learn the full graph structure while\nadaptively focusing on common data patterns. This approach mitigates the impact\nof adversarial perturbations. Experiments show that models trained with Perseus\nachieve superior performance and are significantly more robust to adversarial\nattacks.\n","authors":["Kaiwen Xia","Huijun Wu","Duanyu Li","Min Xie","Ruibo Wang","Wenzhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12424v1","updated":"2024-10-16T10:07:07Z","published":"2024-10-16T10:07:07Z","title":"Nonlinear bayesian tomography of ion temperature and velocity for\n  Doppler coherence imaging spectroscopy in RT-1","summary":"  We present a novel Bayesian tomography approach for Coherence Imaging\nSpectroscopy (CIS) that simultaneously reconstructs ion temperature and\nvelocity distributions in plasmas. Utilizing nonlinear Gaussian Process\nTomography (GPT) with the Laplace approximation, we model prior distributions\nof log-emissivity, temperature, and velocity as Gaussian processes. This\nframework rigorously incorporates nonlinear effects and temperature\ndependencies often neglected in conventional CIS tomography, enabling robust\nreconstruction even in the region of high temperature and velocity. By applying\na log-Gaussian process, we also address issues like velocity divergence in\nlow-emissivity regions. Validated with phantom simulations and experimental\ndata from the RT-1 device, our method reveals detailed spatial structures of\nion temperature and toroidal ion flow characteristic of magnetospheric plasma.\nThis work significantly broadens the scope of CIS tomography, offering a robust\ntool for plasma diagnostics and facilitating integration with complementary\nmeasurement techniques.\n","authors":["Kenji Ueda","Masaki. Nishiura"],"pdf_url":"https://arxiv.org/pdf/2410.12424v1.pdf","comment":"13 page, 9 figures"},{"id":"http://arxiv.org/abs/2403.00853v2","updated":"2024-10-16T09:39:02Z","published":"2024-02-29T18:03:03Z","title":"Parallel Momentum Methods Under Biased Gradient Estimations","summary":"  Parallel stochastic gradient methods are gaining prominence in solving\nlarge-scale machine learning problems that involve data distributed across\nmultiple nodes. However, obtaining unbiased stochastic gradients, which have\nbeen the focus of most theoretical research, is challenging in many distributed\nmachine learning applications. The gradient estimations easily become biased,\nfor example, when gradients are compressed or clipped, when data is shuffled,\nand in meta-learning and reinforcement learning. In this work, we establish\nworst-case bounds on parallel momentum methods under biased gradient estimation\non both general non-convex and $\\mu$-PL problems. Our analysis covers general\ndistributed optimization problems, and we work out the implications for special\ncases where gradient estimates are biased, i.e. in meta-learning and when the\ngradients are compressed or clipped. Our numerical experiments verify our\ntheoretical findings and show faster convergence performance of momentum\nmethods than traditional biased gradient descent.\n","authors":["Ali Beikmohammadi","Sarit Khirirat","Sindri Magnússon"],"pdf_url":"https://arxiv.org/pdf/2403.00853v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2308.01472v2","updated":"2024-10-16T09:37:11Z","published":"2023-08-02T23:39:29Z","title":"Reverse Stable Diffusion: What prompt was used to generate this image?","summary":"  Text-to-image diffusion models have recently attracted the interest of many\nresearchers, and inverting the diffusion process can play an important role in\nbetter understanding the generative process and how to engineer prompts in\norder to obtain the desired images. To this end, we study the task of\npredicting the prompt embedding given an image generated by a generative\ndiffusion model. We consider a series of white-box and black-box models (with\nand without access to the weights of the diffusion network) to deal with the\nproposed task. We propose a novel learning framework comprising a joint prompt\nregression and multi-label vocabulary classification objective that generates\nimproved prompts. To further improve our method, we employ a curriculum\nlearning procedure that promotes the learning of image-prompt pairs with lower\nlabeling noise (i.e. that are better aligned). We conduct experiments on the\nDiffusionDB data set, predicting text prompts from images generated by Stable\nDiffusion. In addition, we make an interesting discovery: training a diffusion\nmodel on the prompt generation task can make the model generate images that are\nmuch better aligned with the input prompts, when the model is directly reused\nfor text-to-image generation. Our code is publicly available for download at\nhttps://github.com/CroitoruAlin/Reverse-Stable-Diffusion.\n","authors":["Florinel-Alin Croitoru","Vlad Hondru","Radu Tudor Ionescu","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2308.01472v2.pdf","comment":"Accepted for publication in Computer Vision and Image Understanding"},{"id":"http://arxiv.org/abs/2405.19783v2","updated":"2024-10-16T09:28:22Z","published":"2024-05-30T07:48:32Z","title":"Instruction-Guided Visual Masking","summary":"  Instruction following is crucial in contemporary LLM. However, when extended\nto multimodal setting, it often suffers from misalignment between specific\ntextual instruction and targeted local region of an image. To achieve more\naccurate and nuanced multimodal instruction following, we introduce\nInstruction-guided Visual Masking (IVM), a new versatile visual grounding model\nthat is compatible with diverse multimodal models, such as LMM and robot model.\nBy constructing visual masks for instruction-irrelevant regions, IVM-enhanced\nmultimodal models can effectively focus on task-relevant image regions to\nbetter align with complex instructions. Specifically, we design a visual\nmasking data generation pipeline and create an IVM-Mix-1M dataset with 1\nmillion image-instruction pairs. We further introduce a new learning technique,\nDiscriminator Weighted Supervised Learning (DWSL) for preferential IVM training\nthat prioritizes high-quality data samples. Experimental results on generic\nmultimodal tasks such as VQA and embodied robotic control demonstrate the\nversatility of IVM, which as a plug-and-play tool, significantly boosts the\nperformance of diverse multimodal models, yielding new state-of-the-art results\nacross challenging multimodal benchmarks. Code, model and data are available at\nhttps://github.com/2toinf/IVM.\n","authors":["Jinliang Zheng","Jianxiong Li","Sijie Cheng","Yinan Zheng","Jiaming Li","Jihao Liu","Yu Liu","Jingjing Liu","Xianyuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2405.19783v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2401.02300v3","updated":"2024-10-16T09:20:47Z","published":"2024-01-04T14:42:29Z","title":"Collocation-based Robust Variational Physics-Informed Neural Networks\n  (CRVPINN)","summary":"  Physics-Informed Neural Networks (PINNs) have been successfully applied to\nsolve Partial Differential Equations (PDEs). Their loss function is founded on\na strong residual minimization scheme. Variational Physics-Informed Neural\nNetworks (VPINNs) are their natural extension to weak variational settings. In\nthis context, the recent work of Robust Variational Physics-Informed Neural\nNetworks (RVPINNs) highlights the importance of conveniently translating the\nnorms of the underlying continuum-level spaces to the discrete level.\nOtherwise, VPINNs might become unrobust, implying that residual minimization\nmight be highly uncorrelated with a desired minimization of the error in the\nenergy norm. However, applying this robustness to VPINNs typically entails\ndealing with the inverse of a Gram matrix, usually producing slow convergence\nspeeds during training. In this work, we accelerate the implementation of\nRVPINN, establishing a LU factorization of sparse Gram matrix in a kind of\npoint-collocation scheme with the same spirit as original PINNs. We call out\nmethod the Collocation-based Robust Variational Physics Informed Neural\nNetworks (CRVPINN). We test our efficient CRVPINN algorithm on Laplace,\nadvection-diffusion, and Stokes problems in two spatial dimensions.\n","authors":["Marcin Łoś","Tomasz Służalec","Paweł Maczuga","Askold Vilkha","Carlos Uriarte","Maciej Paszyński"],"pdf_url":"https://arxiv.org/pdf/2401.02300v3.pdf","comment":"39 pages, 16 figures"},{"id":"http://arxiv.org/abs/2410.12391v1","updated":"2024-10-16T09:18:39Z","published":"2024-10-16T09:18:39Z","title":"Tracking Universal Features Through Fine-Tuning and Model Merging","summary":"  We study how features emerge, disappear, and persist across models fine-tuned\non different domains of text. More specifically, we start from a base one-layer\nTransformer language model that is trained on a combination of the BabyLM\ncorpus, and a collection of Python code from The Stack. This base model is\nadapted to two new domains of text: TinyStories, and the Lua programming\nlanguage, respectively; and then these two models are merged using these two\nmodels using spherical linear interpolation. Our exploration aims to provide\ndeeper insights into the stability and transformation of features across\ntypical transfer-learning scenarios using small-scale models and sparse\nauto-encoders.\n","authors":["Niels Horn","Desmond Elliott"],"pdf_url":"https://arxiv.org/pdf/2410.12391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05898v3","updated":"2024-10-16T09:10:54Z","published":"2024-10-08T10:55:40Z","title":"Manifolds, Random Matrices and Spectral Gaps: The geometric phases of\n  generative diffusion","summary":"  In this paper, we investigate the latent geometry of generative diffusion\nmodels under the manifold hypothesis. To this purpose, we analyze the spectrum\nof eigenvalues (and singular values) of the Jacobian of the score function,\nwhose discontinuities (gaps) reveal the presence and dimensionality of distinct\nsub-manifolds. Using a statistical physics approach, we derive the spectral\ndistributions and formulas for the spectral gaps under several distributional\nassumptions and we compare these theoretical predictions with the spectra\nestimated from trained networks. Our analysis reveals the existence of three\ndistinct qualitative phases during the generative process: a trivial phase; a\nmanifold coverage phase where the diffusion process fits the distribution\ninternal to the manifold; a consolidation phase where the score becomes\northogonal to the manifold and all particles are projected on the support of\nthe data. This `division of labor' between different timescales provides an\nelegant explanation on why generative diffusion models are not affected by the\nmanifold overfitting phenomenon that plagues likelihood-based models, since the\ninternal distribution and the manifold geometry are produced at different time\npoints during generation.\n","authors":["Enrico Ventura","Beatrice Achilli","Gianluigi Silvestri","Carlo Lucibello","Luca Ambrogioni"],"pdf_url":"https://arxiv.org/pdf/2410.05898v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12367v1","updated":"2024-10-16T08:39:40Z","published":"2024-10-16T08:39:40Z","title":"Adaptive and Stratified Subsampling Techniques for High Dimensional\n  Non-Standard Data Environments","summary":"  This paper addresses the challenge of estimating high-dimensional parameters\nin non-standard data environments, where traditional methods often falter due\nto issues such as heavy-tailed distributions, data contamination, and dependent\nobservations. We propose robust subsampling techniques, specifically Adaptive\nImportance Sampling (AIS) and Stratified Subsampling, designed to enhance the\nreliability and efficiency of parameter estimation. Under some clearly outlined\nconditions, we establish consistency and asymptotic normality for the proposed\nestimators, providing non-asymptotic error bounds that quantify their\nperformance. Our theoretical foundations are complemented by controlled\nexperiments demonstrating the superiority of our methods over conventional\napproaches. By bridging the gap between theory and practice, this work offers\nsignificant contributions to robust statistical estimation, paving the way for\nadvancements in various applied domains.\n","authors":["Prateek Mittal","Jai Dalmotra","Joohi Chauhan"],"pdf_url":"https://arxiv.org/pdf/2410.12367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11769v2","updated":"2024-10-16T08:30:57Z","published":"2024-10-15T16:44:40Z","title":"Can Search-Based Testing with Pareto Optimization Effectively Cover\n  Failure-Revealing Test Inputs?","summary":"  Search-based software testing (SBST) is a widely adopted technique for\ntesting complex systems with large input spaces, such as Deep Learning-enabled\n(DL-enabled) systems. Many SBST techniques focus on Pareto-based optimization,\nwhere multiple objectives are optimized in parallel to reveal failures.\nHowever, it is important to ensure that identified failures are spread\nthroughout the entire failure-inducing area of a search domain and not\nclustered in a sub-region. This ensures that identified failures are\nsemantically diverse and reveal a wide range of underlying causes. In this\npaper, we present a theoretical argument explaining why testing based on Pareto\noptimization is inadequate for covering failure-inducing areas within a search\ndomain. We support our argument with empirical results obtained by applying two\nwidely used types of Pareto-based optimization techniques, namely NSGA-II (an\nevolutionary algorithm) and OMOPSO (a swarm-based Pareto-optimization\nalgorithm), to two DL-enabled systems: an industrial Automated Valet Parking\n(AVP) system and a system for classifying handwritten digits. We measure the\ncoverage of failure-revealing test inputs in the input space using a metric\nthat we refer to as the Coverage Inverted Distance quality indicator. Our\nresults show that NSGA-II-based search and OMOPSO are not more effective than a\nna\\\"ive random search baseline in covering test inputs that reveal failures.\nThe replication package for this study is available in a GitHub repository.\n","authors":["Lev Sorokin","Damir Safin","Shiva Nejati"],"pdf_url":"https://arxiv.org/pdf/2410.11769v2.pdf","comment":"Accepted for publication by Empirical Software Engineering Journal\n  (EMSE) (in October 2024)"},{"id":"http://arxiv.org/abs/2410.12360v1","updated":"2024-10-16T08:23:39Z","published":"2024-10-16T08:23:39Z","title":"Towards Neural Scaling Laws for Time Series Foundation Models","summary":"  Scaling laws offer valuable insights into the design of time series\nfoundation models (TSFMs). However, previous research has largely focused on\nthe scaling laws of TSFMs for in-distribution (ID) data, leaving their\nout-of-distribution (OOD) scaling behavior and the influence of model\narchitectures less explored. In this work, we examine two common TSFM\narchitectures, encoder-only and decoder-only Transformers, and investigate\ntheir scaling behavior on both ID and OOD data. These models are trained and\nevaluated across varying parameter counts, compute budgets, and dataset sizes.\nOur experiments reveal that the log-likelihood loss of TSFMs exhibits similar\nscaling behavior in both OOD and ID settings. We further compare the scaling\nproperties across different architectures, incorporating two state-of-the-art\nTSFMs as case studies, showing that model architecture plays a significant role\nin scaling. The encoder-only Transformers demonstrate better scalability than\nthe decoder-only Transformers, while the architectural enhancements in the two\nadvanced TSFMs primarily improve ID performance but reduce OOD scalability.\nWhile scaling up TSFMs is expected to drive performance breakthroughs, the lack\nof a comprehensive understanding of TSFM scaling laws has hindered the\ndevelopment of a robust framework to guide model scaling. We fill this gap in\nthis work by synthesizing our findings and providing practical guidelines for\ndesigning and scaling larger TSFMs with enhanced model capabilities.\n","authors":["Qingren Yao","Chao-Han Huck Yang","Renhe Jiang","Yuxuan Liang","Ming Jin","Shirui Pan"],"pdf_url":"https://arxiv.org/pdf/2410.12360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09858v3","updated":"2024-10-16T08:23:11Z","published":"2024-09-15T20:41:18Z","title":"A Survey of Out-of-distribution Generalization for Graph Machine\n  Learning from a Causal View","summary":"  Graph machine learning (GML) has been successfully applied across a wide\nrange of tasks. Nonetheless, GML faces significant challenges in generalizing\nover out-of-distribution (OOD) data, which raises concerns about its wider\napplicability. Recent advancements have underscored the crucial role of\ncausality-driven approaches in overcoming these generalization challenges.\nDistinct from traditional GML methods that primarily rely on statistical\ndependencies, causality-focused strategies delve into the underlying causal\nmechanisms of data generation and model prediction, thus significantly\nimproving the generalization of GML across different environments. This paper\noffers a thorough review of recent progress in causality-involved GML\ngeneralization. We elucidate the fundamental concepts of employing causality to\nenhance graph model generalization and categorize the various approaches,\nproviding detailed descriptions of their methodologies and the connections\namong them. Furthermore, we explore the incorporation of causality in other\nrelated important areas of trustworthy GML, such as explanation, fairness, and\nrobustness. Concluding with a discussion on potential future research\ndirections, this review seeks to articulate the continuing development and\nfuture potential of causality in enhancing the trustworthiness of graph machine\nlearning.\n","authors":["Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2409.09858v3.pdf","comment":"15 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2410.10394v2","updated":"2024-10-16T08:20:44Z","published":"2024-10-14T11:30:18Z","title":"PIVOT-R: Primitive-Driven Waypoint-Aware World Model for Robotic\n  Manipulation","summary":"  Language-guided robotic manipulation is a challenging task that requires an\nembodied agent to follow abstract user instructions to accomplish various\ncomplex manipulation tasks. Previous work trivially fitting the data without\nrevealing the relation between instruction and low-level executable actions,\nthese models are prone to memorizing the surficial pattern of the data instead\nof acquiring the transferable knowledge, and thus are fragile to dynamic\nenvironment changes. To address this issue, we propose a PrIrmitive-driVen\nwaypOinT-aware world model for Robotic manipulation (PIVOT-R) that focuses\nsolely on the prediction of task-relevant waypoints. Specifically, PIVOT-R\nconsists of a Waypoint-aware World Model (WAWM) and a lightweight action\nprediction module. The former performs primitive action parsing and\nprimitive-driven waypoint prediction, while the latter focuses on decoding\nlow-level actions. Additionally, we also design an asynchronous hierarchical\nexecutor (AHE), which can use different execution frequencies for different\nmodules of the model, thereby helping the model reduce computational redundancy\nand improve model execution efficiency. Our PIVOT-R outperforms\nstate-of-the-art (SoTA) open-source models on the SeaWave benchmark, achieving\nan average relative improvement of 19.45% across four levels of instruction\ntasks. Moreover, compared to the synchronously executed PIVOT-R, the execution\nefficiency of PIVOT-R with AHE is increased by 28-fold, with only a 2.9% drop\nin performance. These results provide compelling evidence that our PIVOT-R can\nsignificantly improve both the performance and efficiency of robotic\nmanipulation.\n","authors":["Kaidong Zhang","Pengzhen Ren","Bingqian Lin","Junfan Lin","Shikui Ma","Hang Xu","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2410.10394v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2402.02701v2","updated":"2024-10-16T08:12:42Z","published":"2024-02-05T03:27:52Z","title":"Understanding What Affects the Generalization Gap in Visual\n  Reinforcement Learning: Theory and Empirical Evidence","summary":"  Recently, there are many efforts attempting to learn useful policies for\ncontinuous control in visual reinforcement learning (RL). In this scenario, it\nis important to learn a generalizable policy, as the testing environment may\ndiffer from the training environment, e.g., there exist distractors during\ndeployment. Many practical algorithms are proposed to handle this problem.\nHowever, to the best of our knowledge, none of them provide a theoretical\nunderstanding of what affects the generalization gap and why their proposed\nmethods work. In this paper, we bridge this issue by theoretically answering\nthe key factors that contribute to the generalization gap when the testing\nenvironment has distractors. Our theories indicate that minimizing the\nrepresentation distance between training and testing environments, which aligns\nwith human intuition, is the most critical for the benefit of reducing the\ngeneralization gap. Our theoretical results are supported by the empirical\nevidence in the DMControl Generalization Benchmark (DMC-GB).\n","authors":["Jiafei Lyu","Le Wan","Xiu Li","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2402.02701v2.pdf","comment":"Accepted by Journal of Artificial Intelligence Research (JAIR)"},{"id":"http://arxiv.org/abs/2410.12343v1","updated":"2024-10-16T08:04:57Z","published":"2024-10-16T08:04:57Z","title":"Federated Temporal Graph Clustering","summary":"  Temporal graph clustering is a complex task that involves discovering\nmeaningful structures in dynamic graphs where relationships and entities change\nover time. Existing methods typically require centralized data collection,\nwhich poses significant privacy and communication challenges. In this work, we\nintroduce a novel Federated Temporal Graph Clustering (FTGC) framework that\nenables decentralized training of graph neural networks (GNNs) across multiple\nclients, ensuring data privacy throughout the process. Our approach\nincorporates a temporal aggregation mechanism to effectively capture the\nevolution of graph structures over time and a federated optimization strategy\nto collaboratively learn high-quality clustering representations. By preserving\ndata privacy and reducing communication overhead, our framework achieves\ncompetitive performance on temporal graph datasets, making it a promising\nsolution for privacy-sensitive, real-world applications involving dynamic data.\n","authors":["Yang Liu","Zihao Zhou","Xianghong Xu","Qian Li"],"pdf_url":"https://arxiv.org/pdf/2410.12343v1.pdf","comment":"8 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.12330v1","updated":"2024-10-16T07:52:26Z","published":"2024-10-16T07:52:26Z","title":"MAX: Masked Autoencoder for X-ray Fluorescence in Geological\n  Investigation","summary":"  Pre-training foundation models has become the de-facto procedure for deep\nlearning approaches, yet its application remains limited in the geological\nstudies, where in needs of the model transferability to break the shackle of\ndata scarcity. Here we target on the X-ray fluorescence (XRF) scanning data, a\nstandard high-resolution measurement in extensive scientific drilling projects.\nWe propose a scalable self-supervised learner, masked autoencoders on XRF\nspectra (MAX), to pre-train a foundation model covering geological records from\nmultiple regions of the Pacific and Southern Ocean. In pre-training, we find\nthat masking a high proportion of the input spectrum (50\\%) yields a nontrivial\nand meaningful self-supervisory task. For downstream tasks, we select the\nquantification of XRF spectra into two costly geochemical measurements,\nCaCO$_3$ and total organic carbon, due to their importance in understanding the\npaleo-oceanic carbon system. Our results show that MAX, requiring only\none-third of the data, outperforms models without pre-training in terms of\nquantification accuracy. Additionally, the model's generalizability improves by\nmore than 60\\% in zero-shot tests on new materials, with explainability further\nensuring its robustness. Thus, our approach offers a promising pathway to\novercome data scarcity in geological discovery by leveraging the\nself-supervised foundation model and fast-acquired XRF scanning data.\n","authors":["An-Sheng Lee","Yu-Wen Pao","Hsuan-Tien Lin","Sofia Ya Hsuan Liou"],"pdf_url":"https://arxiv.org/pdf/2410.12330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12328v1","updated":"2024-10-16T07:48:53Z","published":"2024-10-16T07:48:53Z","title":"Improved Anomaly Detection through Conditional Latent Space VAE\n  Ensembles","summary":"  We propose a novel Conditional Latent space Variational Autoencoder (CL-VAE)\nto perform improved pre-processing for anomaly detection on data with known\ninlier classes and unknown outlier classes. This proposed variational\nautoencoder (VAE) improves latent space separation by conditioning on\ninformation within the data. The method fits a unique prior distribution to\neach class in the dataset, effectively expanding the classic prior distribution\nfor VAEs to include a Gaussian mixture model. An ensemble of these VAEs are\nmerged in the latent spaces to form a group consensus that greatly improves the\naccuracy of anomaly detection across data sets. Our approach is compared\nagainst the capabilities of a typical VAE, a CNN, and a PCA, with regards AUC\nfor anomaly detection. The proposed model shows increased accuracy in anomaly\ndetection, achieving an AUC of 97.4% on the MNIST dataset compared to 95.7% for\nthe second best model. In addition, the CL-VAE shows increased benefits from\nensembling, a more interpretable latent space, and an increased ability to\nlearn patterns in complex data with limited model sizes.\n","authors":["Oskar Åström","Alexandros Sopasakis"],"pdf_url":"https://arxiv.org/pdf/2410.12328v1.pdf","comment":"13 pages of main article, 19 pages including references and appendix,\n  4 figures"},{"id":"http://arxiv.org/abs/2410.12326v1","updated":"2024-10-16T07:47:31Z","published":"2024-10-16T07:47:31Z","title":"Revisited Large Language Model for Time Series Analysis through Modality\n  Alignment","summary":"  Large Language Models have demonstrated impressive performance in many\npivotal web applications such as sensor data analysis. However, since LLMs are\nnot designed for time series tasks, simpler models like linear regressions can\noften achieve comparable performance with far less complexity. In this study,\nwe perform extensive experiments to assess the effectiveness of applying LLMs\nto key time series tasks, including forecasting, classification, imputation,\nand anomaly detection. We compare the performance of LLMs against simpler\nbaseline models, such as single-layer linear models and randomly initialized\nLLMs. Our results reveal that LLMs offer minimal advantages for these core time\nseries tasks and may even distort the temporal structure of the data. In\ncontrast, simpler models consistently outperform LLMs while requiring far fewer\nparameters. Furthermore, we analyze existing reprogramming techniques and show,\nthrough data manifold analysis, that these methods fail to effectively align\ntime series data with language and display pseudo-alignment behaviour in\nembedding space. Our findings suggest that the performance of LLM-based methods\nin time series tasks arises from the intrinsic characteristics and structure of\ntime series data, rather than any meaningful alignment with the language model\narchitecture.\n","authors":["Liangwei Nathan Zheng","Chang George Dong","Wei Emma Zhang","Lin Yue","Miao Xu","Olaf Maennel","Weitong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.14522v3","updated":"2024-10-16T07:44:04Z","published":"2023-06-26T08:54:46Z","title":"Nonconvex Stochastic Bregman Proximal Gradient Method with Application\n  to Deep Learning","summary":"  Stochastic gradient methods for minimizing nonconvex composite objective\nfunctions typically rely on the Lipschitz smoothness of the differentiable\npart, but this assumption fails in many important problem classes like\nquadratic inverse problems and neural network training, leading to instability\nof the algorithms in both theory and practice. To address this, we propose a\nfamily of stochastic Bregman proximal gradient (SBPG) methods that only require\nsmooth adaptivity. SBPG replaces the quadratic approximation in SGD with a\nBregman proximity measure, offering a better approximation model that handles\nnon-Lipschitz gradients in nonconvex objectives. We establish the convergence\nproperties of vanilla SBPG and show it achieves optimal sample complexity in\nthe nonconvex setting. Experimental results on quadratic inverse problems\ndemonstrate SBPG's robustness in terms of stepsize selection and sensitivity to\nthe initial point. Furthermore, we introduce a momentum-based variant, MSBPG,\nwhich enhances convergence by relaxing the mini-batch size requirement while\npreserving the optimal oracle complexity. We apply MSBPG to the training of\ndeep neural networks, utilizing a polynomial kernel function to ensure smooth\nadaptivity of the loss function. Experimental results on benchmark datasets\nconfirm the effectiveness and robustness of MSBPG in training neural networks.\nGiven its negligible additional computational cost compared to SGD in\nlarge-scale optimization, MSBPG shows promise as a universal open-source\noptimizer for future applications.\n","authors":["Kuangyu Ding","Jingyang Li","Kim-Chuan Toh"],"pdf_url":"https://arxiv.org/pdf/2306.14522v3.pdf","comment":"44 pages"},{"id":"http://arxiv.org/abs/2405.18749v3","updated":"2024-10-16T07:35:31Z","published":"2024-05-29T04:22:18Z","title":"A SARS-CoV-2 Interaction Dataset and VHH Sequence Corpus for Antibody\n  Language Models","summary":"  Antibodies are crucial proteins produced by the immune system to eliminate\nharmful foreign substances and have become pivotal therapeutic agents for\ntreating human diseases. To accelerate the discovery of antibody therapeutics,\nthere is growing interest in constructing language models using antibody\nsequences. However, the applicability of pre-trained language models for\nantibody discovery has not been thoroughly evaluated due to the scarcity of\nlabeled datasets. To overcome these limitations, we introduce AVIDa-SARS-CoV-2,\na dataset featuring the antigen-variable domain of heavy chain of heavy chain\nantibody (VHH) interactions obtained from two alpacas immunized with severe\nacute respiratory syndrome coronavirus 2 (SARS-CoV-2) spike proteins.\nAVIDa-SARS-CoV-2 includes binary labels indicating the binding or non-binding\nof diverse VHH sequences to 12 SARS-CoV-2 mutants, such as the Delta and\nOmicron variants. Furthermore, we release VHHCorpus-2M, a pre-training dataset\nfor antibody language models, containing over two million VHH sequences. We\nreport benchmark results for predicting SARS-CoV-2-VHH binding using VHHBERT\npre-trained on VHHCorpus-2M and existing general protein and antibody-specific\npre-trained language models. These results confirm that AVIDa-SARS-CoV-2\nprovides valuable benchmarks for evaluating the representation capabilities of\nantibody language models for binding prediction, thereby facilitating the\ndevelopment of AI-driven antibody discovery. The datasets are available at\nhttps://datasets.cognanous.com.\n","authors":["Hirofumi Tsuruta","Hiroyuki Yamazaki","Ryota Maeda","Ryotaro Tamura","Akihiro Imura"],"pdf_url":"https://arxiv.org/pdf/2405.18749v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12316v1","updated":"2024-10-16T07:33:29Z","published":"2024-10-16T07:33:29Z","title":"TPFL: A Trustworthy Personalized Federated Learning Framework via\n  Subjective Logic","summary":"  Federated learning (FL) enables collaborative model training across\ndistributed clients while preserving data privacy. Despite its widespread\nadoption, most FL approaches focusing solely on privacy protection fall short\nin scenarios where trustworthiness is crucial, necessitating advancements in\nsecure training, dependable decision-making mechanisms, robustness on\ncorruptions, and enhanced performance with Non-IID data. To bridge this gap, we\nintroduce Trustworthy Personalized Federated Learning (TPFL) framework designed\nfor classification tasks via subjective logic in this paper. Specifically, TPFL\nadopts a unique approach by employing subjective logic to construct federated\nmodels, providing probabilistic decisions coupled with an assessment of\nuncertainty rather than mere probability assignments. By incorporating a\ntrainable heterogeneity prior to the local training phase, TPFL effectively\nmitigates the adverse effects of data heterogeneity. Model uncertainty and\ninstance uncertainty are further utilized to ensure the safety and reliability\nof the training and inference stages. Through extensive experiments on widely\nrecognized federated learning benchmarks, we demonstrate that TPFL not only\nachieves competitive performance compared with advanced methods but also\nexhibits resilience against prevalent malicious attacks, robustness on domain\nshifts, and reliability in high-stake scenarios.\n","authors":["Jinqian Chen","Jihua Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.12316v1.pdf","comment":"17 Pages with Appendix"},{"id":"http://arxiv.org/abs/2410.12307v1","updated":"2024-10-16T07:18:36Z","published":"2024-10-16T07:18:36Z","title":"DAT: Improving Adversarial Robustness via Generative Amplitude Mix-up in\n  Frequency Domain","summary":"  To protect deep neural networks (DNNs) from adversarial attacks, adversarial\ntraining (AT) is developed by incorporating adversarial examples (AEs) into\nmodel training. Recent studies show that adversarial attacks disproportionately\nimpact the patterns within the phase of the sample's frequency spectrum --\ntypically containing crucial semantic information -- more than those in the\namplitude, resulting in the model's erroneous categorization of AEs. We find\nthat, by mixing the amplitude of training samples' frequency spectrum with\nthose of distractor images for AT, the model can be guided to focus on phase\npatterns unaffected by adversarial perturbations. As a result, the model's\nrobustness can be improved. Unfortunately, it is still challenging to select\nappropriate distractor images, which should mix the amplitude without affecting\nthe phase patterns. To this end, in this paper, we propose an optimized\nAdversarial Amplitude Generator (AAG) to achieve a better tradeoff between\nimproving the model's robustness and retaining phase patterns. Based on this\ngenerator, together with an efficient AE production procedure, we design a new\nDual Adversarial Training (DAT) strategy. Experiments on various datasets show\nthat our proposed DAT leads to significantly improved robustness against\ndiverse adversarial attacks.\n","authors":["Fengpeng Li","Kemou Li","Haiwei Wu","Jinyu Tian","Jiantao Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.12307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00733v2","updated":"2024-10-16T07:14:57Z","published":"2024-09-01T14:49:48Z","title":"Benign Overfitting under Learning Rate Conditions for $α$\n  Sub-exponential Input","summary":"  This paper investigates the phenomenon of benign overfitting in binary\nclassification problems with heavy-tailed input distributions, extending the\nanalysis of maximum margin classifiers to $\\alpha$ sub-exponential\ndistributions ($\\alpha \\in (0, 2]$). This generalizes previous work focused on\nsub-gaussian inputs. We provide generalization error bounds for linear\nclassifiers trained using gradient descent on unregularized logistic loss in\nthis heavy-tailed setting. Our results show that, under certain conditions on\nthe dimensionality $p$ and the distance between the centers of the\ndistributions, the misclassification error of the maximum margin classifier\nasymptotically approaches the noise level, the theoretical optimal value.\nMoreover, we derive an upper bound on the learning rate $\\beta$ for benign\noverfitting to occur and show that as the tail heaviness of the input\ndistribution $\\alpha$ increases, the upper bound on the learning rate\ndecreases. These results demonstrate that benign overfitting persists even in\nsettings with heavier-tailed inputs than previously studied, contributing to a\ndeeper understanding of the phenomenon in more realistic data environments.\n","authors":["Kota Okudo","Kei Kobayashi"],"pdf_url":"https://arxiv.org/pdf/2409.00733v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13993v2","updated":"2024-10-16T07:14:20Z","published":"2024-06-20T04:44:20Z","title":"Exploring Changes in Nation Perception with Nationality-Assigned\n  Personas in LLMs","summary":"  Persona assignment has become a common strategy for customizing LLM use to\nparticular tasks and contexts. In this study, we explore how evaluation of\ndifferent nations change when LLMs are assigned specific nationality personas.\nWe assign 193 different nationality personas (e.g., an American person) to four\nLLMs and examine how the LLM evaluations (or ''perceptions'')of countries\nchange. We find that all LLM-persona combinations tend to favor Western\nEuropean nations, though nation-personas push LLM behaviors to focus more on\nand treat the nation-persona's own region more favorably. Eastern European,\nLatin American, and African nations are treated more negatively by different\nnationality personas. We additionally find that evaluations by nation-persona\nLLMs of other nations correlate with human survey responses but fail to match\nthe values closely. Our study provides insight into how biases and stereotypes\nare realized within LLMs when adopting different national personas. In line\nwith the ''Blueprint for an AI Bill of Rights'', our findings underscore the\ncritical need for developing mechanisms to ensure that LLM outputs promote\nfairness and avoid over-generalization.\n","authors":["Mahammed Kamruzzaman","Gene Louis Kim"],"pdf_url":"https://arxiv.org/pdf/2406.13993v2.pdf","comment":"Pre-print, Under review"},{"id":"http://arxiv.org/abs/2311.17137v3","updated":"2024-10-16T07:08:57Z","published":"2023-11-28T18:59:02Z","title":"Generative Models: What Do They Know? Do They Know Things? Let's Find\n  Out!","summary":"  Generative models excel at mimicking real scenes, suggesting they might\ninherently encode important intrinsic scene properties. In this paper, we aim\nto explore the following key questions: (1) What intrinsic knowledge do\ngenerative models like GANs, Autoregressive models, and Diffusion models\nencode? (2) Can we establish a general framework to recover intrinsic\nrepresentations from these models, regardless of their architecture or model\ntype? (3) How minimal can the required learnable parameters and labeled data be\nto successfully recover this knowledge? (4) Is there a direct link between the\nquality of a generative model and the accuracy of the recovered scene\nintrinsics?\n  Our findings indicate that a small Low-Rank Adaptators (LoRA) can recover\nintrinsic images-depth, normals, albedo and shading-across different generators\n(Autoregressive, GANs and Diffusion) while using the same decoder head that\ngenerates the image. As LoRA is lightweight, we introduce very few learnable\nparameters (as few as 0.04% of Stable Diffusion model weights for a rank of 2),\nand we find that as few as 250 labeled images are enough to generate intrinsic\nimages with these LoRA modules. Finally, we also show a positive correlation\nbetween the generative model's quality and the accuracy of the recovered\nintrinsics through control experiments.\n","authors":["Xiaodan Du","Nicholas Kolkin","Greg Shakhnarovich","Anand Bhattad"],"pdf_url":"https://arxiv.org/pdf/2311.17137v3.pdf","comment":"https://intrinsic-lora.github.io/"},{"id":"http://arxiv.org/abs/2410.05581v2","updated":"2024-10-16T07:07:20Z","published":"2024-10-08T00:37:16Z","title":"Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes\n  Fail to Improve?","summary":"  In the last decade, the generalization and adaptation abilities of deep\nlearning models were typically evaluated on fixed training and test\ndistributions. Contrary to traditional deep learning, large language models\n(LLMs) are (i) even more overparameterized, (ii) trained on unlabeled text\ncorpora curated from the Internet with minimal human intervention, and (iii)\ntrained in an online fashion. These stark contrasts prevent researchers from\ntransferring lessons learned on model generalization and adaptation in deep\nlearning contexts to LLMs. To this end, our short paper introduces empirical\nobservations that aim to shed light on further training of already pretrained\nlanguage models. Specifically, we demonstrate that training a model on a text\ndomain could degrade its perplexity on the test portion of the same domain. We\nobserve with our subsequent analysis that the performance degradation is\npositively correlated with the similarity between the additional and the\noriginal pretraining dataset of the LLM. Our further token-level perplexity\nobservations reveals that the perplexity degradation is due to a handful of\ntokens that are not informative about the domain. We hope these findings will\nguide us in determining when to adapt a model vs when to rely on its\nfoundational capabilities.\n","authors":["Fırat Öncel","Matthias Bethge","Beyza Ermis","Mirco Ravanelli","Cem Subakan","Çağatay Yıldız"],"pdf_url":"https://arxiv.org/pdf/2410.05581v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.12303v1","updated":"2024-10-16T07:05:06Z","published":"2024-10-16T07:05:06Z","title":"Continuous Pupillography: A Case for Visual Health Ecosystem","summary":"  This article aims to cover pupillography, and its potential use in a number\nof ophthalmological diagnostic applications in biomedical space. With the\never-increasing incorporation of technology within our daily lives and an\never-growing active research into smart devices and technologies, we try to\nmake a case for a health ecosystem that revolves around continuous eye\nmonitoring. We tend to summarize the design constraints & requirements for an\nIoT-based continuous pupil detection system, with an attempt at developing a\npipeline for wearable pupillographic device, while comparing two compact\nmini-camera modules currently available in the market. We use a light algorithm\nthat can be directly adopted to current micro-controllers, and share our\nresults for different lighting conditions, and scenarios. Lastly, we present\nour findings, along with an analysis on the challenges faced and a way ahead\ntowards successfully building this ecosystem.\n","authors":["Usama Younus","Nirupam Roy"],"pdf_url":"https://arxiv.org/pdf/2410.12303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12302v1","updated":"2024-10-16T07:02:51Z","published":"2024-10-16T07:02:51Z","title":"Two Birds with One Stone: Multi-Task Semantic Communications Systems\n  over Relay Channel","summary":"  In this paper, we propose a novel multi-task, multi-link relay semantic\ncommunications (MTML-RSC) scheme that enables the destination node to\nsimultaneously perform image reconstruction and classification with one\ntransmission from the source node. In the MTML-RSC scheme, the source node\nbroadcasts a signal using semantic communications, and the relay node forwards\nthe signal to the destination. We analyze the coupling relationship between the\ntwo tasks and the two links (source-to-relay and source-to-destination) and\ndesign a semantic-focused forward method for the relay node, where it\nselectively forwards only the semantics of the relevant class while ignoring\nothers. At the destination, the node combines signals from both the source node\nand the relay node to perform classification, and then uses the classification\nresult to assist in decoding the signal from the relay node for image\nreconstructing. Experimental results demonstrate that the proposed MTML-RSC\nscheme achieves significant performance gains, e.g., $1.73$ dB improvement in\npeak-signal-to-noise ratio (PSNR) for image reconstruction and increasing the\naccuracy from $64.89\\%$ to $70.31\\%$ for classification.\n","authors":["Yujie Cao","Tong Wu","Zhiyong Chen","Yin Xu","Meixia Tao","Wenjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12302v1.pdf","comment":"submitted to IEEE WCNC"},{"id":"http://arxiv.org/abs/2410.12297v1","updated":"2024-10-16T06:56:53Z","published":"2024-10-16T06:56:53Z","title":"Conjunction Subspaces Test for Conformal and Selective Classification","summary":"  In this paper, we present a new classifier, which integrates significance\ntesting results over different random subspaces to yield consensus p-values for\nquantifying the uncertainty of classification decision. The null hypothesis is\nthat the test sample has no association with the target class on a randomly\nchosen subspace, and hence the classification problem can be formulated as a\nproblem of testing for the conjunction of hypotheses. The proposed classifier\ncan be easily deployed for the purpose of conformal prediction and selective\nclassification with reject and refine options by simply thresholding the\nconsensus p-values. The theoretical analysis on the generalization error bound\nof the proposed classifier is provided and empirical studies on real data sets\nare conducted as well to demonstrate its effectiveness.\n","authors":["Zengyou He","Zerun Li","Junjie Dong","Xinying Liu","Mudi Jiang","Lianyu Hu"],"pdf_url":"https://arxiv.org/pdf/2410.12297v1.pdf","comment":"36 pages, 9 figures"},{"id":"http://arxiv.org/abs/2405.05219v2","updated":"2024-10-16T06:55:47Z","published":"2024-05-08T17:11:38Z","title":"Conv-Basis: A New Paradigm for Efficient Attention Inference and\n  Gradient Computation in Transformers","summary":"  The self-attention mechanism is the key to the success of transformers in\nrecent Large Language Models (LLMs). However, the quadratic computational cost\n$O(n^2)$ in the input sequence length $n$ is a notorious obstacle for further\nimprovement and scalability in longer contexts. In this work, we leverage the\nconvolution-like structure of attention matrices to develop an efficient\napproximation method for attention computation using convolution matrices. We\npropose a $\\mathsf{conv}$ basis system, analogous to the rank basis, and show\nthat any lower triangular matrix can always be decomposed as a sum of\nstructured convolution matrices in this basis. We then design a fast algorithm\nto approximate the attention matrix via a sum of such $k$ convolution matrices.\nThis allows us to compute the attention {\\it inference} via Fast Fourier\nTransforms (FFT) in $O(knd \\log n)$ time, where $d$ is the hidden dimension,\nand thus achieve almost linear time $n^{1+o(1)}$ in the practical scenario\nwhere $kd = n^{o(1)}$. Furthermore, the attention {\\it training forward} and\n{\\it backward gradient} can be computed in $n^{1+o(1)}$ as well. We provide\ntheoretical guarantees on the run time and approximation error and conduct\npreliminary experiments to evaluate its effectiveness. We hope our new paradigm\nfor accelerating attention computation in transformer models can help their\napplication to longer contexts.\n","authors":["Yingyu Liang","Heshan Liu","Zhenmei Shi","Zhao Song","Zhuoyan Xu","Junze Yin"],"pdf_url":"https://arxiv.org/pdf/2405.05219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12295v1","updated":"2024-10-16T06:55:02Z","published":"2024-10-16T06:55:02Z","title":"Consistency Calibration: Improving Uncertainty Calibration via\n  Consistency among Perturbed Neighbors","summary":"  Calibration is crucial in deep learning applications, especially in fields\nlike healthcare and autonomous driving, where accurate confidence estimates are\nvital for decision-making. However, deep neural networks often suffer from\nmiscalibration, with reliability diagrams and Expected Calibration Error (ECE)\nbeing the only standard perspective for evaluating calibration performance. In\nthis paper, we introduce the concept of consistency as an alternative\nperspective on model calibration, inspired by uncertainty estimation literature\nin large language models (LLMs). We highlight its advantages over the\ntraditional reliability-based view. Building on this concept, we propose a\npost-hoc calibration method called Consistency Calibration (CC), which adjusts\nconfidence based on the model's consistency across perturbed inputs. CC is\nparticularly effective in locally uncertainty estimation, as it requires no\nadditional data samples or label information, instead generating input\nperturbations directly from the source data. Moreover, we show that performing\nperturbations at the logit level significantly improves computational\nefficiency. We validate the effectiveness of CC through extensive comparisons\nwith various post-hoc and training-time calibration methods, demonstrating\nstate-of-the-art performance on standard datasets such as CIFAR-10, CIFAR-100,\nand ImageNet, as well as on long-tailed datasets like ImageNet-LT.\n","authors":["Linwei Tao","Haolan Guo","Minjing Dong","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2410.12295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03920v4","updated":"2024-10-16T06:51:39Z","published":"2024-06-06T10:02:49Z","title":"Towards Physically Consistent Deep Learning For Climate Model\n  Parameterizations","summary":"  Climate models play a critical role in understanding and projecting climate\nchange. Due to their complexity, their horizontal resolution of about 40-100 km\nremains too coarse to resolve processes such as clouds and convection, which\nneed to be approximated via parameterizations. These parameterizations are a\nmajor source of systematic errors and large uncertainties in climate\nprojections. Deep learning (DL)-based parameterizations, trained on data from\ncomputationally expensive short, high-resolution simulations, have shown great\npromise for improving climate models in that regard. However, their lack of\ninterpretability and tendency to learn spurious non-physical correlations\nresult in reduced trust in the climate simulation. We propose an efficient\nsupervised learning framework for DL-based parameterizations that leads to\nphysically consistent models with improved interpretability and negligible\ncomputational overhead compared to standard supervised training. First, key\nfeatures determining the target physical processes are uncovered. Subsequently,\nthe neural network is fine-tuned using only those relevant features. We show\nempirically that our method robustly identifies a small subset of the inputs as\nactual physical drivers, therefore removing spurious non-physical\nrelationships. This results in by design physically consistent and\ninterpretable neural networks while maintaining the predictive performance of\nunconstrained black-box DL-based parameterizations.\n","authors":["Birgit Kühbacher","Fernando Iglesias-Suarez","Niki Kilbertus","Veronika Eyring"],"pdf_url":"https://arxiv.org/pdf/2406.03920v4.pdf","comment":"Accepted at ICMLA 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.12700v1","updated":"2024-10-16T16:03:42Z","published":"2024-10-16T16:03:42Z","title":"Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via\n  Lightweight Value Optimization","summary":"  Recent advancements in diffusion models trained on large-scale data have\nenabled the generation of indistinguishable human-level images, yet they often\nproduce harmful content misaligned with human values, e.g., social bias, and\noffensive content. Despite extensive research on Large Language Models (LLMs),\nthe challenge of Text-to-Image (T2I) model alignment remains largely\nunexplored. Addressing this problem, we propose LiVO (Lightweight Value\nOptimization), a novel lightweight method for aligning T2I models with human\nvalues. LiVO only optimizes a plug-and-play value encoder to integrate a\nspecified value principle with the input prompt, allowing the control of\ngenerated images over both semantics and values. Specifically, we design a\ndiffusion model-tailored preference optimization loss, which theoretically\napproximates the Bradley-Terry model used in LLM alignment but provides a more\nflexible trade-off between image quality and value conformity. To optimize the\nvalue encoder, we also develop a framework to automatically construct a\ntext-image preference dataset of 86k (prompt, aligned image, violating image,\nvalue principle) samples. Without updating most model parameters and through\nadaptive value selection from the input prompt, LiVO significantly reduces\nharmful outputs and achieves faster convergence, surpassing several strong\nbaselines and taking an initial step towards ethically aligned T2I models.\n","authors":["Xingqi Wang","Xiaoyuan Yi","Xing Xie","Jia Jia"],"pdf_url":"https://arxiv.org/pdf/2410.12700v1.pdf","comment":"Accepted by ACM Multimedia 2024. The dataset and code can be found at\n  https://github.com/achernarwang/LiVO"},{"id":"http://arxiv.org/abs/2410.12407v1","updated":"2024-10-16T09:42:29Z","published":"2024-10-16T09:42:29Z","title":"Beyond Coarse-Grained Matching in Video-Text Retrieval","summary":"  Video-text retrieval has seen significant advancements, yet the ability of\nmodels to discern subtle differences in captions still requires verification.\nIn this paper, we introduce a new approach for fine-grained evaluation. Our\napproach can be applied to existing datasets by automatically generating hard\nnegative test captions with subtle single-word variations across nouns, verbs,\nadjectives, adverbs, and prepositions. We perform comprehensive experiments\nusing four state-of-the-art models across two standard benchmarks (MSR-VTT and\nVATEX) and two specially curated datasets enriched with detailed descriptions\n(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our\nanalyses show that the current evaluation benchmarks fall short in detecting a\nmodel's ability to perceive subtle single-word differences, 2) our fine-grained\nevaluation highlights the difficulty models face in distinguishing such subtle\nvariations. To enhance fine-grained understanding, we propose a new baseline\nthat can be easily combined with current methods. Experiments on our\nfine-grained evaluations demonstrate that this approach enhances a model's\nability to understand fine-grained differences.\n","authors":["Aozhu Chen","Hazel Doughty","Xirong Li","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12407v1.pdf","comment":"Accepted to ACCV 2024"},{"id":"http://arxiv.org/abs/2410.12220v1","updated":"2024-10-16T04:31:25Z","published":"2024-10-16T04:31:25Z","title":"Rethinking Bjøntegaard Delta for Compression Efficiency Evaluation:\n  Are We Calculating It Precisely and Reliably?","summary":"  For decades, the Bj{\\o}ntegaard Delta (BD) has been the metric for evaluating\ncodec Rate-Distortion (R-D) performance. Yet, in most studies, BD is determined\nusing just 4-5 R-D data points, could this be sufficient? As codecs and quality\nmetrics advance, does the conventional BD estimation still hold up? Crucially,\nare the performance improvements of new codecs and tools genuine, or merely\nartifacts of estimation flaws? This paper addresses these concerns by\nreevaluating BD estimation. We present a novel approach employing a\nparameterized deep neural network to model R-D curves with high precision\nacross various metrics, accompanied by a comprehensive R-D dataset. This\napproach both assesses the reliability of BD calculations and serves as a\nprecise BD estimator. Our findings advocate for the adoption of rigorous R-D\nsampling and reliability metrics in future compression research to ensure the\nvalidity and reliability of results.\n","authors":["Xinyu Hang","Shenpeng Song","Zhimeng Huang","Chuanmin Jia","Siwei Ma","Wen Gao"],"pdf_url":"https://arxiv.org/pdf/2410.12220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12219v1","updated":"2024-10-16T04:29:46Z","published":"2024-10-16T04:29:46Z","title":"OmnixR: Evaluating Omni-modality Language Models on Reasoning across\n  Modalities","summary":"  We introduce OmnixR, an evaluation suite designed to benchmark SoTA\nOmni-modality Language Models, such as GPT-4o and Gemini. Evaluating OLMs,\nwhich integrate multiple modalities such as text, vision, and audio, presents\nunique challenges. Particularly, the user message might often consist of\nmultiple modalities, such that OLMs have to establish holistic understanding\nand reasoning across modalities to accomplish the task. Existing benchmarks are\nlimited to single modality or dual-modality tasks, overlooking comprehensive\nmulti-modal assessments of model reasoning. To address this, OmnixR offers two\nevaluation variants: (1)synthetic subset: a synthetic dataset generated\nautomatically by translating text into multiple modalities--audio, images,\nvideo, and hybrids (Omnify). (2)realistic subset: a real-world dataset,\nmanually curated and annotated by experts, for evaluating cross-modal reasoning\nin natural settings. OmnixR presents a unique evaluation towards assessing OLMs\nover a diverse mix of modalities, such as a question that involves video,\naudio, and text, providing a rigorous cross-modal reasoning testbed unlike any\nexisting benchmarks. Our experiments find that all state-of-the-art OLMs\nstruggle with OmnixR questions that require integrating information from\nmultiple modalities to answer. Further analysis highlights differences in\nreasoning behavior, underscoring the challenges of omni-modal AI alignment.\n","authors":["Lichang Chen","Hexiang Hu","Mingda Zhang","Yiwen Chen","Zifeng Wang","Yandong Li","Pranav Shyam","Tianyi Zhou","Heng Huang","Ming-Hsuan Yang","Boqing Gong"],"pdf_url":"https://arxiv.org/pdf/2410.12219v1.pdf","comment":"19 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2407.07464v2","updated":"2024-10-16T03:44:41Z","published":"2024-07-10T08:40:39Z","title":"Video-to-Audio Generation with Hidden Alignment","summary":"  Generating semantically and temporally aligned audio content in accordance\nwith video input has become a focal point for researchers, particularly\nfollowing the remarkable breakthrough in text-to-video generation. In this\nwork, we aim to offer insights into the video-to-audio generation paradigm,\nfocusing on three crucial aspects: vision encoders, auxiliary embeddings, and\ndata augmentation techniques. Beginning with a foundational model built on a\nsimple yet surprisingly effective intuition, we explore various vision encoders\nand auxiliary embeddings through ablation studies. Employing a comprehensive\nevaluation pipeline that emphasizes generation quality and video-audio\nsynchronization alignment, we demonstrate that our model exhibits\nstate-of-the-art video-to-audio generation capabilities. Furthermore, we\nprovide critical insights into the impact of different data augmentation\nmethods on enhancing the generation framework's overall capacity. We showcase\npossibilities to advance the challenge of generating synchronized audio from\nsemantic and temporal perspectives. We hope these insights will serve as a\nstepping stone toward developing more realistic and accurate audio-visual\ngeneration models.\n","authors":["Manjie Xu","Chenxing Li","Xinyi Tu","Yong Ren","Rilin Chen","Yu Gu","Wei Liang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2407.07464v2.pdf","comment":"https://sites.google.com/view/vta-ldm"},{"id":"http://arxiv.org/abs/2410.12191v1","updated":"2024-10-16T03:25:16Z","published":"2024-10-16T03:25:16Z","title":"Test-time adaptation for image compression with distribution\n  regularization","summary":"  Current test- or compression-time adaptation image compression (TTA-IC)\napproaches, which leverage both latent and decoder refinements as a two-step\nadaptation scheme, have potentially enhanced the rate-distortion (R-D)\nperformance of learned image compression models on cross-domain compression\ntasks, \\textit{e.g.,} from natural to screen content images. However, compared\nwith the emergence of various decoder refinement variants, the latent\nrefinement, as an inseparable ingredient, is barely tailored to cross-domain\nscenarios. To this end, we aim to develop an advanced latent refinement method\nby extending the effective hybrid latent refinement (HLR) method, which is\ndesigned for \\textit{in-domain} inference improvement but shows noticeable\ndegradation of the rate cost in \\textit{cross-domain} tasks. Specifically, we\nfirst provide theoretical analyses, in a cue of marginalization approximation\nfrom in- to cross-domain scenarios, to uncover that the vanilla HLR suffers\nfrom an underlying mismatch between refined Gaussian conditional and hyperprior\ndistributions, leading to deteriorated joint probability approximation of\nmarginal distribution with increased rate consumption. To remedy this issue, we\nintroduce a simple Bayesian approximation-endowed \\textit{distribution\nregularization} to encourage learning a better joint probability approximation\nin a plug-and-play manner. Extensive experiments on six in- and cross-domain\ndatasets demonstrate that our proposed method not only improves the R-D\nperformance compared with other latent refinement counterparts, but also can be\nflexibly integrated into existing TTA-IC methods with incremental benefits.\n","authors":["Kecheng Chen","Pingping Zhang","Tiexin Qin","Shiqi Wang","Hong Yan","Haoliang Li"],"pdf_url":"https://arxiv.org/pdf/2410.12191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13088v1","updated":"2024-10-16T23:05:59Z","published":"2024-10-16T23:05:59Z","title":"Self-Comparison for Dataset-Level Membership Inference in Large\n  (Vision-)Language Models","summary":"  Large Language Models (LLMs) and Vision-Language Models (VLMs) have made\nsignificant advancements in a wide range of natural language processing and\nvision-language tasks. Access to large web-scale datasets has been a key factor\nin their success. However, concerns have been raised about the unauthorized use\nof copyrighted materials and potential copyright infringement. Existing\nmethods, such as sample-level Membership Inference Attacks (MIA) and\ndistribution-based dataset inference, distinguish member data (data used for\ntraining) and non-member data by leveraging the common observation that models\ntend to memorize and show greater confidence in member data. Nevertheless,\nthese methods face challenges when applied to LLMs and VLMs, such as the\nrequirement for ground-truth member data or non-member data that shares the\nsame distribution as the test data. In this paper, we propose a novel\ndataset-level membership inference method based on Self-Comparison. We find\nthat a member prefix followed by a non-member suffix (paraphrased from a member\nsuffix) can further trigger the model's memorization on training data. Instead\nof directly comparing member and non-member data, we introduce paraphrasing to\nthe second half of the sequence and evaluate how the likelihood changes before\nand after paraphrasing. Unlike prior approaches, our method does not require\naccess to ground-truth member data or non-member data in identical\ndistribution, making it more practical. Extensive experiments demonstrate that\nour proposed method outperforms traditional MIA and dataset inference\ntechniques across various datasets and models, including including public\nmodels, fine-tuned models, and API-based commercial models.\n","authors":["Jie Ren","Kangrui Chen","Chen Chen","Vikash Sehwag","Yue Xing","Jiliang Tang","Lingjuan Lyu"],"pdf_url":"https://arxiv.org/pdf/2410.13088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12957v1","updated":"2024-10-16T18:44:56Z","published":"2024-10-16T18:44:56Z","title":"MuVi: Video-to-Music Generation with Semantic Alignment and Rhythmic\n  Synchronization","summary":"  Generating music that aligns with the visual content of a video has been a\nchallenging task, as it requires a deep understanding of visual semantics and\ninvolves generating music whose melody, rhythm, and dynamics harmonize with the\nvisual narratives. This paper presents MuVi, a novel framework that effectively\naddresses these challenges to enhance the cohesion and immersive experience of\naudio-visual content. MuVi analyzes video content through a specially designed\nvisual adaptor to extract contextually and temporally relevant features. These\nfeatures are used to generate music that not only matches the video's mood and\ntheme but also its rhythm and pacing. We also introduce a contrastive\nmusic-visual pre-training scheme to ensure synchronization, based on the\nperiodicity nature of music phrases. In addition, we demonstrate that our\nflow-matching-based music generator has in-context learning ability, allowing\nus to control the style and genre of the generated music. Experimental results\nshow that MuVi demonstrates superior performance in both audio quality and\ntemporal synchronization. The generated music video samples are available at\nhttps://muvi-v2m.github.io.\n","authors":["Ruiqi Li","Siqi Zheng","Xize Cheng","Ziang Zhang","Shengpeng Ji","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.12957v1.pdf","comment":"Working in progress"}]},"2024-10-15T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2410.12109v1","updated":"2024-10-15T23:16:28Z","published":"2024-10-15T23:16:28Z","title":"OMCAT: Omni Context Aware Transformer","summary":"  Large Language Models (LLMs) have made significant strides in text generation\nand comprehension, with recent advancements extending into multimodal LLMs that\nintegrate visual and audio inputs. However, these models continue to struggle\nwith fine-grained, cross-modal temporal understanding, particularly when\ncorrelating events across audio and video streams. We address these challenges\nwith two key contributions: a new dataset and model, called OCTAV and OMCAT\nrespectively. OCTAV (Omni Context and Temporal Audio Video) is a novel dataset\ndesigned to capture event transitions across audio and video. Second, OMCAT\n(Omni Context Aware Transformer) is a powerful model that leverages RoTE\n(Rotary Time Embeddings), an innovative extension of RoPE, to enhance temporal\ngrounding and computational efficiency in time-anchored tasks. Through a robust\nthree-stage training pipeline-feature alignment, instruction tuning, and\nOCTAV-specific training-OMCAT excels in cross-modal temporal understanding. Our\nmodel demonstrates state-of-the-art performance on Audio-Visual Question\nAnswering (AVQA) tasks and the OCTAV benchmark, showcasing significant gains in\ntemporal reasoning and cross-modal alignment, as validated through\ncomprehensive experiments and ablation studies. Our dataset and code will be\nmade publicly available. The link to our demo page is https://om-cat.github.io.\n","authors":["Arushi Goel","Karan Sapra","Matthieu Le","Rafael Valle","Andrew Tao","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2410.12109v1.pdf","comment":"Demo page: https://om-cat.github.io"},{"id":"http://arxiv.org/abs/2310.12976v2","updated":"2024-10-15T22:26:58Z","published":"2023-10-19T17:59:37Z","title":"Exploring Invariance in Images through One-way Wave Equations","summary":"  In this paper, we empirically reveal an invariance over images-images share a\nset of one-way wave equations with latent speeds. Each image is uniquely\nassociated with a solution to these wave equations, allowing for its\nreconstruction with high fidelity from an initial condition. We demonstrate it\nusing an intuitive encoder-decoder framework where each image is encoded into\nits corresponding initial condition (a single vector). Subsequently, the\ninitial condition undergoes a specialized decoder, transforming the one-way\nwave equations into a first-order norm + linear autoregressive process. This\nprocess propagates the initial condition along the x and y directions,\ngenerating a high-resolution feature map (up to the image resolution), followed\nby a few convolutional layers to reconstruct image pixels. The revealed\ninvariance, rooted in the shared wave equations, offers a fresh perspective for\ncomprehending images, establishing a promising avenue for further exploration.\n","authors":["Yinpeng Chen","Dongdong Chen","Xiyang Dai","Mengchen Liu","Yinan Feng","Youzuo Lin","Lu Yuan","Zicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2310.12976v2.pdf","comment":"This is an improvement version, which fuses some parts from the\n  preliminary work arXiv:2305.16319"},{"id":"http://arxiv.org/abs/2410.12080v1","updated":"2024-10-15T21:52:24Z","published":"2024-10-15T21:52:24Z","title":"SplatPose+: Real-time Image-Based Pose-Agnostic 3D Anomaly Detection","summary":"  Image-based Pose-Agnostic 3D Anomaly Detection is an important task that has\nemerged in industrial quality control. This task seeks to find anomalies from\nquery images of a tested object given a set of reference images of an\nanomaly-free object. The challenge is that the query views (a.k.a poses) are\nunknown and can be different from the reference views. Currently, new methods\nsuch as OmniposeAD and SplatPose have emerged to bridge the gap by synthesizing\npseudo reference images at the query views for pixel-to-pixel comparison.\nHowever, none of these methods can infer in real-time, which is critical in\nindustrial quality control for massive production. For this reason, we propose\nSplatPose+, which employs a hybrid representation consisting of a Structure\nfrom Motion (SfM) model for localization and a 3D Gaussian Splatting (3DGS)\nmodel for Novel View Synthesis. Although our proposed pipeline requires the\ncomputation of an additional SfM model, it offers real-time inference speeds\nand faster training compared to SplatPose. Quality-wise, we achieved a new SOTA\non the Pose-agnostic Anomaly Detection benchmark with the Multi-Pose Anomaly\nDetection (MAD-SIM) dataset.\n","authors":["Yizhe Liu","Yan Song Hu","Yuhao Chen","John Zelek"],"pdf_url":"https://arxiv.org/pdf/2410.12080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12075v1","updated":"2024-10-15T21:29:26Z","published":"2024-10-15T21:29:26Z","title":"WeatherDG: LLM-assisted Procedural Weather Generation for\n  Domain-Generalized Semantic Segmentation","summary":"  In this work, we propose a novel approach, namely WeatherDG, that can\ngenerate realistic, weather-diverse, and driving-screen images based on the\ncooperation of two foundation models, i.e, Stable Diffusion (SD) and Large\nLanguage Model (LLM). Specifically, we first fine-tune the SD with source data,\naligning the content and layout of generated samples with real-world driving\nscenarios. Then, we propose a procedural prompt generation method based on LLM,\nwhich can enrich scenario descriptions and help SD automatically generate more\ndiverse, detailed images. In addition, we introduce a balanced generation\nstrategy, which encourages the SD to generate high-quality objects of tailed\nclasses under various weather conditions, such as riders and motorcycles. This\nsegmentation-model-agnostic method can improve the generalization ability of\nexisting models by additionally adapting them with the generated synthetic\ndata. Experiments on three challenging datasets show that our method can\nsignificantly improve the segmentation performance of different\nstate-of-the-art models on target domains. Notably, in the setting of\n''Cityscapes to ACDC'', our method improves the baseline HRDA by 13.9% in mIoU.\n","authors":["Chenghao Qian","Yuhu Guo","Yuhong Mo","Wenjing Li"],"pdf_url":"https://arxiv.org/pdf/2410.12075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12074v1","updated":"2024-10-15T21:24:31Z","published":"2024-10-15T21:24:31Z","title":"nvTorchCam: An Open-source Library for Camera-Agnostic Differentiable\n  Geometric Vision","summary":"  We introduce nvTorchCam, an open-source library under the Apache 2.0 license,\ndesigned to make deep learning algorithms camera model-independent. nvTorchCam\nabstracts critical camera operations such as projection and unprojection,\nallowing developers to implement algorithms once and apply them across diverse\ncamera models--including pinhole, fisheye, and 360 equirectangular panoramas,\nwhich are commonly used in automotive and real estate capture applications.\nBuilt on PyTorch, nvTorchCam is fully differentiable and supports GPU\nacceleration and batching for efficient computation. Furthermore, deep learning\nmodels trained for one camera type can be directly transferred to other camera\ntypes without requiring additional modification. In this paper, we provide an\noverview of nvTorchCam, its functionality, and present various code examples\nand diagrams to demonstrate its usage. Source code and installation\ninstructions can be found on the nvTorchCam GitHub page at\nhttps://github.com/NVlabs/nvTorchCam.\n","authors":["Daniel Lichy","Hang Su","Abhishek Badki","Jan Kautz","Orazio Gallo"],"pdf_url":"https://arxiv.org/pdf/2410.12074v1.pdf","comment":"Source code and installation instructions are available at\n  https://github.com/NVlabs/nvTorchCam"},{"id":"http://arxiv.org/abs/2410.12068v1","updated":"2024-10-15T21:08:08Z","published":"2024-10-15T21:08:08Z","title":"V3D-SLAM: Robust RGB-D SLAM in Dynamic Environments with 3D Semantic\n  Geometry Voting","summary":"  Simultaneous localization and mapping (SLAM) in highly dynamic environments\nis challenging due to the correlation complexity between moving objects and the\ncamera pose. Many methods have been proposed to deal with this problem;\nhowever, the moving properties of dynamic objects with a moving camera remain\nunclear. Therefore, to improve SLAM's performance, minimizing disruptive events\nof moving objects with a physical understanding of 3D shapes and dynamics of\nobjects is needed. In this paper, we propose a robust method, V3D-SLAM, to\nremove moving objects via two lightweight re-evaluation stages, including\nidentifying potentially moving and static objects using a spatial-reasoned\nHough voting mechanism and refining static objects by detecting dynamic noise\ncaused by intra-object motions using Chamfer distances as similarity\nmeasurements. Our experiment on the TUM RGB-D benchmark on dynamic sequences\nwith ground-truth camera trajectories showed that our methods outperform the\nmost recent state-of-the-art SLAM methods. Our source code is available at\nhttps://github.com/tuantdang/v3d-slam.\n","authors":["Tuan Dang","Khang Nguyen","Mandfred Huber"],"pdf_url":"https://arxiv.org/pdf/2410.12068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12053v1","updated":"2024-10-15T20:47:48Z","published":"2024-10-15T20:47:48Z","title":"SOE: SO(3)-Equivariant 3D MRI Encoding","summary":"  Representation learning has become increasingly important, especially as\npowerful models have shifted towards learning latent representations before\nfine-tuning for downstream tasks. This approach is particularly valuable in\nleveraging the structural information within brain anatomy. However, a common\nlimitation of recent models developed for MRIs is their tendency to ignore or\nremove geometric information, such as translation and rotation, thereby\ncreating invariance with respect to geometric operations. We contend that\nincorporating knowledge about these geometric transformations into the model\ncan significantly enhance its ability to learn more detailed anatomical\ninformation within brain structures. As a result, we propose a novel method for\nencoding 3D MRIs that enforces equivariance with respect to all rotations in 3D\nspace, in other words, SO(3)-equivariance (SOE). By explicitly modeling this\ngeometric equivariance in the representation space, we ensure that any\nrotational operation applied to the input image space is also reflected in the\nembedding representation space. This approach requires moving beyond\ntraditional representation learning methods, as we need a representation vector\nspace that allows for the application of the same SO(3) operation in that\nspace. To facilitate this, we leverage the concept of vector neurons. The\nrepresentation space formed by our method captures the brain's structural and\nanatomical information more effectively. We evaluate SOE pretrained on the\nstructural MRIs of two public data sets with respect to the downstream task of\npredicting age and diagnosing Alzheimer's Disease from T1-weighted brain scans\nof the ADNI data set. We demonstrate that our approach not only outperforms\nother methods but is also robust against various degrees of rotation along\ndifferent axes. The code is available at\nhttps://github.com/shizhehe/SOE-representation-learning.\n","authors":["Shizhe He","Magdalini Paschali","Jiahong Ouyang","Adnan Masood","Akshay Chaudhari","Ehsan Adeli"],"pdf_url":"https://arxiv.org/pdf/2410.12053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00252v4","updated":"2024-10-15T20:11:42Z","published":"2024-06-01T01:17:25Z","title":"Towards Rationality in Language and Multimodal Agents: A Survey","summary":"  Rationality is the quality of being guided by reason, characterized by\ndecision-making that aligns with evidence and logical principles. It plays a\ncrucial role in reliable problem-solving by ensuring well-grounded and\nconsistent solutions. While large language models (LLMs) have made significant\nprogress in generating human-like text, they still exhibit limitations such as\nbounded knowledge space and inconsistent outputs. In response, recent efforts\nhave shifted toward developing multimodal and multi-agent systems, as well as\nintegrating modules like external tools, programming codes, symbolic reasoners,\nutility function, and conformal risk controls rather than relying solely on a\nsingle LLM for decision-making. This paper surveys the state-of-the-art\nadvancements in language and multimodal agents, evaluates how they contribute\nto make intelligent agents more rational, and identifies open challenges and\nfuture research directions. We maintain an open repository at\nhttps://github.com/bowen-upenn/Agent_Rationality.\n","authors":["Bowen Jiang","Yangxinyu Xie","Xiaomeng Wang","Yuan Yuan","Zhuoqun Hao","Xinyi Bai","Weijie J. Su","Camillo J. Taylor","Tanwi Mallick"],"pdf_url":"https://arxiv.org/pdf/2406.00252v4.pdf","comment":"We maintain an open repository at\n  https://github.com/bowen-upenn/Agent_Rationality"},{"id":"http://arxiv.org/abs/2407.05357v3","updated":"2024-10-15T20:11:07Z","published":"2024-07-07T13:19:51Z","title":"On the power of data augmentation for head pose estimation","summary":"  Deep learning has been impressively successful in the last decade in\npredicting human head poses from monocular images. However, for in-the-wild\ninputs the research community relies predominantly on a single training set,\n300W-LP, of semisynthetic nature without many alternatives. This paper focuses\non gradual extension and improvement of the data to explore the performance\nachievable with augmentation and synthesis strategies further. Modeling-wise a\nnovel multitask head/loss design which includes uncertainty estimation is\nproposed. Overall, the thus obtained models are small, efficient, suitable for\nfull 6 DoF pose estimation, and exhibit very competitive accuracy.\n","authors":["Michael Welter"],"pdf_url":"https://arxiv.org/pdf/2407.05357v3.pdf","comment":"CVPR version. Added evaluation on BIWI. Plenty of writing changes"},{"id":"http://arxiv.org/abs/2404.04452v2","updated":"2024-10-15T19:49:31Z","published":"2024-04-05T23:38:57Z","title":"Vision transformers in domain adaptation and domain generalization: a\n  study of robustness","summary":"  Deep learning models are often evaluated in scenarios where the data\ndistribution is different from those used in the training and validation\nphases. The discrepancy presents a challenge for accurately predicting the\nperformance of models once deployed on the target distribution. Domain\nadaptation and generalization are widely recognized as effective strategies for\naddressing such shifts, thereby ensuring reliable performance. The recent\npromising results in applying vision transformers in computer vision tasks,\ncoupled with advancements in self-attention mechanisms, have demonstrated their\nsignificant potential for robustness and generalization in handling\ndistribution shifts. Motivated by the increased interest from the research\ncommunity, our paper investigates the deployment of vision transformers in\ndomain adaptation and domain generalization scenarios. For domain adaptation\nmethods, we categorize research into feature-level, instance-level, model-level\nadaptations, and hybrid approaches, along with other categorizations with\nrespect to diverse strategies for enhancing domain adaptation. Similarly, for\ndomain generalization, we categorize research into multi-domain learning,\nmeta-learning, regularization techniques, and data augmentation strategies. We\nfurther classify diverse strategies in research, underscoring the various\napproaches researchers have taken to address distribution shifts by integrating\nvision transformers. The inclusion of comprehensive tables summarizing these\ncategories is a distinct feature of our work, offering valuable insights for\nresearchers. These findings highlight the versatility of vision transformers in\nmanaging distribution shifts, crucial for real-world applications, especially\nin critical safety and decision-making scenarios.\n","authors":["Shadi Alijani","Jamil Fayyad","Homayoun Najjaran"],"pdf_url":"https://arxiv.org/pdf/2404.04452v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12023v1","updated":"2024-10-15T19:42:45Z","published":"2024-10-15T19:42:45Z","title":"Learned Neural Physics Simulation for Articulated 3D Human Pose\n  Reconstruction","summary":"  We propose a novel neural network approach, LARP (Learned Articulated Rigid\nbody Physics), to model the dynamics of articulated human motion with contact.\nOur goal is to develop a faster and more convenient methodological alternative\nto traditional physics simulators for use in computer vision tasks such as\nhuman motion reconstruction from video. To that end we introduce a training\nprocedure and model components that support the construction of a recurrent\nneural architecture to accurately simulate articulated rigid body dynamics. Our\nneural architecture supports features typically found in traditional physics\nsimulators, such as modeling of joint motors, variable dimensions of body\nparts, contact between body parts and objects, and is an order of magnitude\nfaster than traditional systems when multiple simulations are run in parallel.\nTo demonstrate the value of LARP we use it as a drop-in replacement for a state\nof the art classical non-differentiable simulator in an existing video-based\nreconstruction framework and show comparative or better 3D human pose\nreconstruction accuracy.\n","authors":["Mykhaylo Andriluka","Baruch Tabanpour","C. Daniel Freeman","Cristian Sminchisescu"],"pdf_url":"https://arxiv.org/pdf/2410.12023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12018v1","updated":"2024-10-15T19:33:57Z","published":"2024-10-15T19:33:57Z","title":"LocoMotion: Learning Motion-Focused Video-Language Representations","summary":"  This paper strives for motion-focused video-language representations.\nExisting methods to learn video-language representations use spatial-focused\ndata, where identifying the objects and scene is often enough to distinguish\nthe relevant caption. We instead propose LocoMotion to learn from\nmotion-focused captions that describe the movement and temporal progression of\nlocal object motions. We achieve this by adding synthetic motions to videos and\nusing the parameters of these motions to generate corresponding captions.\nFurthermore, we propose verb-variation paraphrasing to increase the caption\nvariety and learn the link between primitive motions and high-level verbs. With\nthis, we are able to learn a motion-focused video-language representation.\nExperiments demonstrate our approach is effective for a variety of downstream\ntasks, particularly when limited data is available for fine-tuning. Code is\navailable: https://hazeldoughty.github.io/Papers/LocoMotion/\n","authors":["Hazel Doughty","Fida Mohammad Thoker","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12018v1.pdf","comment":"ACCV 2024"},{"id":"http://arxiv.org/abs/2410.12006v1","updated":"2024-10-15T19:13:05Z","published":"2024-10-15T19:13:05Z","title":"Beyond Labels: A Self-Supervised Framework with Masked Autoencoders and\n  Random Cropping for Breast Cancer Subtype Classification","summary":"  This work contributes to breast cancer sub-type classification using\nhistopathological images. We utilize masked autoencoders (MAEs) to learn a\nself-supervised embedding tailored for computer vision tasks in this domain.\nThis embedding captures informative representations of histopathological data,\nfacilitating feature learning without extensive labeled datasets. During\npre-training, we investigate employing a random crop technique to generate a\nlarge dataset from WSIs automatically. Additionally, we assess the performance\nof linear probes for multi-class classification tasks of cancer sub-types using\nthe representations learnt by the MAE. Our approach aims to achieve strong\nperformance on downstream tasks by leveraging the complementary strengths of\nViTs and autoencoders. We evaluate our model's performance on the BRACS dataset\nand compare it with existing benchmarks.\n","authors":["Annalisa Chiocchetti","Marco Dossena","Christopher Irwin","Luigi Portinale"],"pdf_url":"https://arxiv.org/pdf/2410.12006v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.12123v1","updated":"2024-10-15T23:51:04Z","published":"2024-10-15T23:51:04Z","title":"The Moral Case for Using Language Model Agents for Recommendation","summary":"  Our information and communication environment has fallen short of the ideals\nthat networked global communication might have served. Identifying all the\ncauses of its pathologies is difficult, but existing recommender systems very\nlikely play a contributing role. In this paper, which draws on the normative\ntools of philosophy of computing, informed by empirical and technical insights\nfrom natural language processing and recommender systems, we make the moral\ncase for an alternative approach. We argue that existing recommenders\nincentivise mass surveillance, concentrate power, fall prey to narrow\nbehaviourism, and compromise user agency. Rather than just trying to avoid\nalgorithms entirely, or to make incremental improvements to the current\nparadigm, researchers and engineers should explore an alternative paradigm: the\nuse of language model (LM) agents to source and curate content that matches\nusers' preferences and values, expressed in natural language. The use of LM\nagents for recommendation poses its own challenges, including those related to\ncandidate generation, computational efficiency, preference modelling, and\nprompt injection. Nonetheless, if implemented successfully LM agents could:\nguide us through the digital public sphere without relying on mass\nsurveillance; shift power away from platforms towards users; optimise for what\nmatters instead of just for behavioural proxies; and scaffold our agency\ninstead of undermining it.\n","authors":["Seth Lazar","Luke Thorburn","Tian Jin","Luca Belli"],"pdf_url":"https://arxiv.org/pdf/2410.12123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07722v2","updated":"2024-10-15T19:58:37Z","published":"2024-10-10T08:41:34Z","title":"DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities","summary":"  Learned Sparse Retrieval (LSR) models use vocabularies from pre-trained\ntransformers, which often split entities into nonsensical fragments. Splitting\nentities can reduce retrieval accuracy and limits the model's ability to\nincorporate up-to-date world knowledge not included in the training data. In\nthis work, we enhance the LSR vocabulary with Wikipedia concepts and entities,\nenabling the model to resolve ambiguities more effectively and stay current\nwith evolving knowledge. Central to our approach is a Dynamic Vocabulary (DyVo)\nhead, which leverages existing entity embeddings and an entity retrieval\ncomponent that identifies entities relevant to a query or document. We use the\nDyVo head to generate entity weights, which are then merged with word piece\nweights to create joint representations for efficient indexing and retrieval\nusing an inverted index. In experiments across three entity-rich document\nranking datasets, the resulting DyVo model substantially outperforms\nstate-of-the-art baselines.\n","authors":["Thong Nguyen","Shubham Chatterjee","Sean MacAvaney","Iain Mackie","Jeff Dalton","Andrew Yates"],"pdf_url":"https://arxiv.org/pdf/2410.07722v2.pdf","comment":"https://github.com/thongnt99/DyVo"},{"id":"http://arxiv.org/abs/2410.11841v1","updated":"2024-10-15T17:59:30Z","published":"2024-10-15T17:59:30Z","title":"GaVaMoE: Gaussian-Variational Gated Mixture of Experts for Explainable\n  Recommendation","summary":"  Large language model-based explainable recommendation (LLM-based ER) systems\nshow promise in generating human-like explanations for recommendations.\nHowever, they face challenges in modeling user-item collaborative preferences,\npersonalizing explanations, and handling sparse user-item interactions. To\naddress these issues, we propose GaVaMoE, a novel Gaussian-Variational Gated\nMixture of Experts framework for explainable recommendation. GaVaMoE introduces\ntwo key components: (1) a rating reconstruction module that employs Variational\nAutoencoder (VAE) with a Gaussian Mixture Model (GMM) to capture complex\nuser-item collaborative preferences, serving as a pre-trained multi-gating\nmechanism; and (2) a set of fine-grained expert models coupled with the\nmulti-gating mechanism for generating highly personalized explanations. The VAE\ncomponent models latent factors in user-item interactions, while the GMM\nclusters users with similar behaviors. Each cluster corresponds to a gate in\nthe multi-gating mechanism, routing user-item pairs to appropriate expert\nmodels. This architecture enables GaVaMoE to generate tailored explanations for\nspecific user types and preferences, mitigating data sparsity by leveraging\nuser similarities. Extensive experiments on three real-world datasets\ndemonstrate that GaVaMoE significantly outperforms existing methods in\nexplanation quality, personalization, and consistency. Notably, GaVaMoE\nexhibits robust performance in scenarios with sparse user-item interactions,\nmaintaining high-quality explanations even for users with limited historical\ndata.\n","authors":["Fei Tang","Yongliang Shen","Hang Zhang","Zeqi Tan","Wenqi Zhang","Guiyang Hou","Kaitao Song","Weiming Lu","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2410.11841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.03764v2","updated":"2024-10-15T16:01:11Z","published":"2024-05-06T18:02:00Z","title":"GOVERN: Gradient Orientation Vote Ensemble for Multi-Teacher Reinforced\n  Distillation","summary":"  Pre-trained language models have become an integral component of\nquestion-answering systems, achieving remarkable performance. However, for\npractical deployment, it is crucial to perform knowledge distillation to\nmaintain high performance while operating under computational constraints. In\nthis paper, we address a key question: given the importance of unsupervised\ndistillation for student model performance, how can knowledge from multiple\nteacher models be effectively ensemble during this stage without the guidance\nof labels? We propose a novel algorithm, GOVERN, to tackle this issue. GOVERN\nhas demonstrated significant improvements in both offline and online\nexperiments, enabling the student model to achieve results comparable to that\nof teacher ensembles. Our experiments show that GOVERN remarkably requires a\nmere 1\\% of the ensemble method's inference budget to achieve 99.5\\% of\nperformance. The proposed algorithm has been successfully deployed in a\nreal-world commercial question-answering system, demonstrating its real-world\napplicability.\n","authors":["Wenjie Zhou","Zhenxin Ding","Xiaodong Zhang","Haibo Shi","Junfeng Wang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2405.03764v2.pdf","comment":"Accepted by EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.11719v1","updated":"2024-10-15T15:50:53Z","published":"2024-10-15T15:50:53Z","title":"Adaptive Coordinators and Prompts on Heterogeneous Graphs for\n  Cross-Domain Recommendations","summary":"  In the online digital world, users frequently engage with diverse items\nacross multiple domains (e.g., e-commerce platforms, streaming services, and\nsocial media networks), forming complex heterogeneous interaction graphs.\nLeveraging this multi-domain information can undoubtedly enhance the\nperformance of recommendation systems by providing more comprehensive user\ninsights and alleviating data sparsity in individual domains. However,\nintegrating multi-domain knowledge for the cross-domain recommendation is very\nhard due to inherent disparities in user behavior and item characteristics and\nthe risk of negative transfer, where irrelevant or conflicting information from\nthe source domains adversely impacts the target domain's performance. To\naddress these challenges, we offer HAGO, a novel framework with\n$\\textbf{H}$eterogeneous $\\textbf{A}$daptive $\\textbf{G}$raph\nco$\\textbf{O}$rdinators, which dynamically integrate multi-domain graphs into a\ncohesive structure by adaptively adjusting the connections between coordinators\nand multi-domain graph nodes, thereby enhancing beneficial inter-domain\ninteractions while mitigating negative transfer effects. Additionally, we\ndevelop a universal multi-domain graph pre-training strategy alongside HAGO to\ncollaboratively learn high-quality node representations across domains. To\neffectively transfer the learned multi-domain knowledge to the target domain,\nwe design an effective graph prompting method, which incorporates pre-trained\nembeddings with learnable prompts for the recommendation task. Our framework is\ncompatible with various graph-based models and pre-training techniques,\ndemonstrating broad applicability and effectiveness. Further experimental\nresults show that our solutions outperform state-of-the-art methods in\nmulti-domain recommendation scenarios and highlight their potential for\nreal-world applications.\n","authors":["Hengyu Zhang","Chunxu Shen","Xiangguo Sun","Jie Tan","Yu Rong","Chengzhi Piao","Hong Cheng","Lingling Yi"],"pdf_url":"https://arxiv.org/pdf/2410.11719v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.07671v2","updated":"2024-10-15T15:29:51Z","published":"2024-10-10T07:29:31Z","title":"DISCO: A Hierarchical Disentangled Cognitive Diagnosis Framework for\n  Interpretable Job Recommendation","summary":"  The rapid development of online recruitment platforms has created\nunprecedented opportunities for job seekers while concurrently posing the\nsignificant challenge of quickly and accurately pinpointing positions that\nalign with their skills and preferences. Job recommendation systems have\nsignificantly alleviated the extensive search burden for job seekers by\noptimizing user engagement metrics, such as clicks and applications, thus\nachieving notable success. In recent years, a substantial amount of research\nhas been devoted to developing effective job recommendation models, primarily\nfocusing on text-matching based and behavior modeling based methods. While\nthese approaches have realized impressive outcomes, it is imperative to note\nthat research on the explainability of recruitment recommendations remains\nprofoundly unexplored. To this end, in this paper, we propose DISCO, a\nhierarchical Disentanglement based Cognitive diagnosis framework, aimed at\nflexibly accommodating the underlying representation learning model for\neffective and interpretable job recommendations. Specifically, we first design\na hierarchical representation disentangling module to explicitly mine the\nhierarchical skill-related factors implied in hidden representations of job\nseekers and jobs. Subsequently, we propose level-aware association modeling to\nenhance information communication and robust representation learning both\ninter- and intra-level, which consists of the interlevel knowledge influence\nmodule and the level-wise contrastive learning. Finally, we devise an\ninteraction diagnosis module incorporating a neural diagnosis function for\neffectively modeling the multi-level recruitment interaction process between\njob seekers and jobs, which introduces the cognitive measurement theory.\n","authors":["Xiaoshan Yu","Chuan Qin","Qi Zhang","Chen Zhu","Haiping Ma","Xingyi Zhang","Hengshu Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.07671v2.pdf","comment":"Accepted by ICDM 2024. 10 pages"},{"id":"http://arxiv.org/abs/2401.00797v2","updated":"2024-10-15T12:37:40Z","published":"2024-01-01T15:57:15Z","title":"Curriculum-scheduled Knowledge Distillation from Multiple Pre-trained\n  Teachers for Multi-domain Sequential Recommendation","summary":"  Pre-trained recommendation models (PRMs) have received increasing interest\nrecently. However, their intrinsically heterogeneous model structure, huge\nmodel size and computation cost hinder their adoptions in practical recommender\nsystems. Hence, it is highly essential to explore how to use different\npre-trained recommendation models efficiently in real-world systems. In this\npaper, we propose a novel curriculum-scheduled knowledge distillation from\nmultiple pre-trained teachers for multi-domain sequential recommendation,\ncalled CKD-MDSR, which takes full advantages of different PRMs as multiple\nteacher models to boost a small student recommendation model, integrating the\nknowledge across multiple domains from PRMs. Specifically, CKD-MDSR first\nadopts curriculum-scheduled user behavior sequence sampling and distills\ninformative knowledge jointly from the representative PRMs such as UniSRec and\nRecformer. Then, the knowledge from the above PRMs are selectively integrated\ninto the student model in consideration of their confidence and consistency.\nFinally, we verify the proposed method on multi-domain sequential\nrecommendation and further demonstrate its universality with multiple types of\nstudent models, including feature interaction and graph based recommendation\nmodels. Extensive experiments on five real-world datasets demonstrate the\neffectiveness and efficiency of CKD-MDSR, which can be viewed as an efficient\nshortcut using PRMs in real-world systems.\n","authors":["Wenqi Sun","Ruobing Xie","Junjie Zhang","Wayne Xin Zhao","Leyu Lin","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2401.00797v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10645v2","updated":"2024-10-15T11:01:33Z","published":"2024-08-20T08:36:59Z","title":"CoRA: Collaborative Information Perception by Large Language Model's\n  Weights for Recommendation","summary":"  Involving collaborative information in Large Language Models (LLMs) is a\npromising technique for adapting LLMs for recommendation. Existing methods\nachieve this by concatenating collaborative features with text tokens into a\nunified sequence input and then fine-tuning to align these features with LLM's\ninput space. Although effective, in this work, we identify two limitations when\nadapting LLMs to recommendation tasks, which hinder the integration of general\nknowledge and collaborative information, resulting in sub-optimal\nrecommendation performance. (1) Fine-tuning LLM with recommendation data can\nundermine its inherent world knowledge and fundamental competencies, which are\ncrucial for interpreting and inferring recommendation text. (2) Incorporating\ncollaborative features into textual prompts disrupts the semantics of the\noriginal prompts, preventing LLM from generating appropriate outputs. In this\npaper, we propose a new paradigm, CoRA (an acronym for Collaborative LoRA),\nwith a collaborative weights generator. Rather than input space alignment, this\nmethod aligns collaborative information with LLM's parameter space,\nrepresenting them as incremental weights to update LLM's output. This way, LLM\nperceives collaborative information without altering its general knowledge and\ntext inference capabilities. Specifically, we employ a collaborative filtering\nmodel to extract user and item embeddings, converting them into collaborative\nweights with low-rank properties through the collaborative weights generator.\nWe then merge the collaborative weights into LLM's weights, enabling LLM to\nperceive the collaborative signals and generate personalized recommendations\nwithout fine-tuning or extra collaborative tokens in prompts. Extensive\nexperiments confirm that CoRA effectively integrates collaborative information\ninto LLM, enhancing recommendation performance.\n","authors":["Yuting Liu","Jinghao Zhang","Yizhou Dang","Yuliang Liang","Qiang Liu","Guibing Guo","Jianzhe Zhao","Xingwei Wang"],"pdf_url":"https://arxiv.org/pdf/2408.10645v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11464v1","updated":"2024-10-15T10:11:18Z","published":"2024-10-15T10:11:18Z","title":"CoActionGraphRec: Sequential Multi-Interest Recommendations Using\n  Co-Action Graphs","summary":"  There are unique challenges to developing item recommender systems for\ne-commerce platforms like eBay due to sparse data and diverse user interests.\nWhile rich user-item interactions are important, eBay's data sparsity exceeds\nother e-commerce sites by an order of magnitude. To address this challenge, we\npropose CoActionGraphRec (CAGR), a text based two-tower deep learning model\n(Item Tower and User Tower) utilizing co-action graph layers. In order to\nenhance user and item representations, a graph-based solution tailored to\neBay's environment is utilized. For the Item Tower, we represent each item\nusing its co-action items to capture collaborative signals in a co-action graph\nthat is fully leveraged by the graph neural network component. For the User\nTower, we build a fully connected graph of each user's behavior sequence, with\nedges encoding pairwise relationships. Furthermore, an explicit interaction\nmodule learns representations capturing behavior interactions. Extensive\noffline and online A/B test experiments demonstrate the effectiveness of our\nproposed approach and results show improved performance over state-of-the-art\nmethods on key metrics.\n","authors":["Yi Sun","Yuri M. Brovman"],"pdf_url":"https://arxiv.org/pdf/2410.11464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11457v1","updated":"2024-10-15T10:02:55Z","published":"2024-10-15T10:02:55Z","title":"LR-SQL: A Supervised Fine-Tuning Method for Text2SQL Tasks under\n  Low-Resource Scenarios","summary":"  Large language models revolutionize Text2SQL through supervised fine-tuning,\nyet a crucial limitation is overlooked: the complexity of databases leads to an\nincreased context length, consequently resulting in higher GPU memory demands\nfor model fine-tuning. To address this issue, we propose LR-SQL. LR-SQL\ncomprises two supervised fine-tuning models: the schema\\_link model and the\nSQL\\_generation model, with the schema\\_link model serving as the focal point\nfor streamlining the overall process. During the fine-tuning of the\nschema\\_link model, LR-SQL breaks down the complete database into flexible\ncombinations of tables with adjustable quantities, enabling the model to learn\nthe relationships within the entire database from these dispersed slices.\nFurthermore, to enhance the model's ability to perceive the relationships among\nvarious discrete slices during inference, LR-SQL trains the model's\nChain-of-Thought capability for this task. Experimental results demonstrate\nthat LR-SQL can reduce the total GPU memory usage by 40\\% compared to existing\nfine-tuning methods, while only losing 2\\% of table prediction accuracy in\nschema\\_link task. For the overall Text2SQL task, the Execution Accuracy\ndecrease by 0.6\\%.Our project is now available on\nhttps://github.com/hongWin/LR-SQL\n","authors":["Wen Wuzhenghong","Zhang Yongpan","Pan Su","Sun Yuwei","Lu Pengwei","Ding Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.11457v1.pdf","comment":"12pages, 4 figures,submitting to a journal"},{"id":"http://arxiv.org/abs/2410.11370v1","updated":"2024-10-15T07:50:34Z","published":"2024-10-15T07:50:34Z","title":"Enhance Graph Alignment for Large Language Models","summary":"  Graph-structured data is prevalent in the real world. Recently, due to the\npowerful emergent capabilities, Large Language Models (LLMs) have shown\npromising performance in modeling graphs. The key to effectively applying LLMs\non graphs is converting graph data into a format LLMs can comprehend.\nGraph-to-token approaches are popular in enabling LLMs to process graph\ninformation. They transform graphs into sequences of tokens and align them with\ntext tokens through instruction tuning, where self-supervised instruction\ntuning helps LLMs acquire general knowledge about graphs, and supervised\nfine-tuning specializes LLMs for the downstream tasks on graphs. Despite their\ninitial success, we find that existing methods have a misalignment between\nself-supervised tasks and supervised downstream tasks, resulting in negative\ntransfer from self-supervised fine-tuning to downstream tasks. To address these\nissues, we propose Graph Alignment Large Language Models (GALLM) to benefit\nfrom aligned task templates. In the self-supervised tuning stage, we introduce\na novel text matching task using templates aligned with downstream tasks. In\nthe task-specific tuning stage, we propose two category prompt methods that\nlearn supervision information from additional explanation with further aligned\ntemplates. Experimental evaluations on four datasets demonstrate substantial\nimprovements in supervised learning, multi-dataset generalizability, and\nparticularly in zero-shot capability, highlighting the model's potential as a\ngraph foundation model.\n","authors":["Haitong Luo","Xuying Meng","Suhang Wang","Tianxiang Zhao","Fali Wang","Hanyun Cao","Yujun Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.11370v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.11355v1","updated":"2024-10-15T07:25:33Z","published":"2024-10-15T07:25:33Z","title":"Reducing Labeling Costs in Sentiment Analysis via Semi-Supervised\n  Learning","summary":"  Labeling datasets is a noteworthy challenge in machine learning, both in\nterms of cost and time. This research, however, leverages an efficient answer.\nBy exploring label propagation in semi-supervised learning, we can\nsignificantly reduce the number of labels required compared to traditional\nmethods. We employ a transductive label propagation method based on the\nmanifold assumption for text classification. Our approach utilizes a\ngraph-based method to generate pseudo-labels for unlabeled data for the text\nclassification task, which are then used to train deep neural networks. By\nextending labels based on cosine proximity within a nearest neighbor graph from\nnetwork embeddings, we combine unlabeled data into supervised learning, thereby\nreducing labeling costs. Based on previous successes in other domains, this\nstudy builds and evaluates this approach's effectiveness in sentiment analysis,\npresenting insights into semi-supervised learning.\n","authors":["Minoo Jafarlou","Mario M. Kubek"],"pdf_url":"https://arxiv.org/pdf/2410.11355v1.pdf","comment":"12 pages, 7 figures, accepted at the 2024 8th International\n  Conference on Natural Language Processing and Information Retrieval (NLPIR\n  2024), Okayama, Japan, 2024"},{"id":"http://arxiv.org/abs/2410.11327v1","updated":"2024-10-15T06:54:27Z","published":"2024-10-15T06:54:27Z","title":"Sequential LLM Framework for Fashion Recommendation","summary":"  The fashion industry is one of the leading domains in the global e-commerce\nsector, prompting major online retailers to employ recommendation systems for\nproduct suggestions and customer convenience. While recommendation systems have\nbeen widely studied, most are designed for general e-commerce problems and\nstruggle with the unique challenges of the fashion domain. To address these\nissues, we propose a sequential fashion recommendation framework that leverages\na pre-trained large language model (LLM) enhanced with recommendation-specific\nprompts. Our framework employs parameter-efficient fine-tuning with extensive\nfashion data and introduces a novel mix-up-based retrieval technique for\ntranslating text into relevant product suggestions. Extensive experiments show\nour proposed framework significantly enhances fashion recommendation\nperformance.\n","authors":["Han Liu","Xianfeng Tang","Tianlang Chen","Jiapeng Liu","Indu Indu","Henry Peng Zou","Peng Dai","Roberto Fernandez Galan","Michael D Porter","Dongmei Jia","Ning Zhang","Lian Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.11327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00080v3","updated":"2024-10-15T05:34:07Z","published":"2024-04-30T16:35:08Z","title":"Recommenadation aided Caching using Combinatorial Multi-armed Bandits","summary":"  We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.\n","authors":["Pavamana K J","Chandramani Kishore Singh"],"pdf_url":"https://arxiv.org/pdf/2405.00080v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02130v4","updated":"2024-10-15T04:06:44Z","published":"2024-01-04T08:31:47Z","title":"Spectral-Based Graph Neural Networks for Complementary Item\n  Recommendation","summary":"  Modeling complementary relationships greatly helps recommender systems to\naccurately and promptly recommend the subsequent items when one item is\npurchased. Unlike traditional similar relationships, items with complementary\nrelationships may be purchased successively (such as iPhone and Airpods Pro),\nand they not only share relevance but also exhibit dissimilarity. Since the two\nattributes are opposites, modeling complementary relationships is challenging.\nPrevious attempts to exploit these relationships have either ignored or\noversimplified the dissimilarity attribute, resulting in ineffective modeling\nand an inability to balance the two attributes. Since Graph Neural Networks\n(GNNs) can capture the relevance and dissimilarity between nodes in the\nspectral domain, we can leverage spectral-based GNNs to effectively understand\nand model complementary relationships. In this study, we present a novel\napproach called Spectral-based Complementary Graph Neural Networks (SComGNN)\nthat utilizes the spectral properties of complementary item graphs. We make the\nfirst observation that complementary relationships consist of low-frequency and\nmid-frequency components, corresponding to the relevance and dissimilarity\nattributes, respectively. Based on this spectral observation, we design\nspectral graph convolutional networks with low-pass and mid-pass filters to\ncapture the low-frequency and mid-frequency components. Additionally, we\npropose a two-stage attention mechanism to adaptively integrate and balance the\ntwo attributes. Experimental results on four e-commerce datasets demonstrate\nthe effectiveness of our model, with SComGNN significantly outperforming\nexisting baseline models.\n","authors":["Haitong Luo","Xuying Meng","Suhang Wang","Hanyun Cao","Weiyao Zhang","Yequan Wang","Yujun Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.02130v4.pdf","comment":"Accepted by AAAI-24"},{"id":"http://arxiv.org/abs/2410.11217v1","updated":"2024-10-15T03:04:26Z","published":"2024-10-15T03:04:26Z","title":"On the Capacity of Citation Generation by Large Language Models","summary":"  Retrieval-augmented generation (RAG) appears as a promising method to\nalleviate the \"hallucination\" problem in large language models (LLMs), since it\ncan incorporate external traceable resources for response generation. The\nessence of RAG in combating the hallucination issue lies in accurately\nattributing claims in responses to the corresponding retrieved documents.\nHowever, most of existing works focus on improving the quality of generated\nresponses from the LLM, while largely overlooked its ability to attribute\nsources accurately. In this study, we conduct a systematic analysis about the\ncapabilities of LLMs in generating citations within response generation, and\nfurther introduce a novel method to enhance their citation generation\nabilities. Specifically, we evaluate both the correctness and citation quality\nfor seven widely-used LLMs on two benchmark datasets. Meanwhile, we introduce\nnew citation evaluation metrics to eliminate the over-penalization of\nunnecessary and excessive citations in existing metrics. Furthermore, we\npropose a Generate-then-Refine method that completes relevant citations and\nremoves irrelevant ones without altering the response text. The results on\nWebGLM-QA, ASQA and ELI5 datasets show that our method substantially improves\nthe quality of citations in responses generated by LLMs.\n","authors":["Haosheng Qian","Yixing Fan","Ruqing Zhang","Jiafeng Guo"],"pdf_url":"https://arxiv.org/pdf/2410.11217v1.pdf","comment":"Accepted by CCIR 2024"},{"id":"http://arxiv.org/abs/2410.11150v1","updated":"2024-10-15T00:23:18Z","published":"2024-10-15T00:23:18Z","title":"Optimizing Encoder-Only Transformers for Session-Based Recommendation\n  Systems","summary":"  Session-based recommendation is the task of predicting the next item a user\nwill interact with, often without access to historical user data. In this work,\nwe introduce Sequential Masked Modeling, a novel approach for encoder-only\ntransformer architectures to tackle the challenges of single-session\nrecommendation. Our method combines data augmentation through window sliding\nwith a unique penultimate token masking strategy to capture sequential\ndependencies more effectively. By enhancing how transformers handle session\ndata, Sequential Masked Modeling significantly improves next-item prediction\nperformance.\n  We evaluate our approach on three widely-used datasets, Yoochoose 1/64,\nDiginetica, and Tmall, comparing it to state-of-the-art single-session,\ncross-session, and multi-relation approaches. The results demonstrate that our\nTransformer-SMM models consistently outperform all models that rely on the same\namount of information, while even rivaling methods that have access to more\nextensive user history. This study highlights the potential of encoder-only\ntransformers in session-based recommendation and opens the door for further\nimprovements.\n","authors":["Anis Redjdal","Luis Pinto","Michel Desmarais"],"pdf_url":"https://arxiv.org/pdf/2410.11150v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.12051v1","updated":"2024-10-15T20:41:10Z","published":"2024-10-15T20:41:10Z","title":"Enabling Data-Driven and Empathetic Interactions: A Context-Aware 3D\n  Virtual Agent in Mixed Reality for Enhanced Financial Customer Experience","summary":"  In this paper, we introduce a novel system designed to enhance customer\nservice in the financial and retail sectors through a context-aware 3D virtual\nagent, utilizing Mixed Reality (MR) and Vision Language Models (VLMs). Our\napproach focuses on enabling data-driven and empathetic interactions that\nensure customer satisfaction by introducing situational awareness of the\nphysical location, personalized interactions based on customer profiles, and\nrigorous privacy and security standards. We discuss our design considerations\ncritical for deployment in real-world customer service environments, addressing\nchallenges in user data management and sensitive information handling. We also\noutline the system architecture and key features unique to banking and retail\nenvironments. Our work demonstrates the potential of integrating MR and VLMs in\nservice industries, offering practical insights in customer service delivery\nwhile maintaining high standards of security and personalization.\n","authors":["Cindy Xu","Mengyu Chen","Pranav Deshpande","Elvir Azanli","Runqing Yang","Joseph Ligman"],"pdf_url":"https://arxiv.org/pdf/2410.12051v1.pdf","comment":"to appear at 1st Workshop on Intelligent XR: Harnessing AI for\n  Next-Generation XR User Experiences at International Symposium on Mixed and\n  Augmented Reality (ISMAR) 2024"},{"id":"http://arxiv.org/abs/2410.12018v1","updated":"2024-10-15T19:33:57Z","published":"2024-10-15T19:33:57Z","title":"LocoMotion: Learning Motion-Focused Video-Language Representations","summary":"  This paper strives for motion-focused video-language representations.\nExisting methods to learn video-language representations use spatial-focused\ndata, where identifying the objects and scene is often enough to distinguish\nthe relevant caption. We instead propose LocoMotion to learn from\nmotion-focused captions that describe the movement and temporal progression of\nlocal object motions. We achieve this by adding synthetic motions to videos and\nusing the parameters of these motions to generate corresponding captions.\nFurthermore, we propose verb-variation paraphrasing to increase the caption\nvariety and learn the link between primitive motions and high-level verbs. With\nthis, we are able to learn a motion-focused video-language representation.\nExperiments demonstrate our approach is effective for a variety of downstream\ntasks, particularly when limited data is available for fine-tuning. Code is\navailable: https://hazeldoughty.github.io/Papers/LocoMotion/\n","authors":["Hazel Doughty","Fida Mohammad Thoker","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12018v1.pdf","comment":"ACCV 2024"},{"id":"http://arxiv.org/abs/2410.11817v1","updated":"2024-10-15T17:46:31Z","published":"2024-10-15T17:46:31Z","title":"Improving Long-Text Alignment for Text-to-Image Diffusion Models","summary":"  The rapid advancement of text-to-image (T2I) diffusion models has enabled\nthem to generate unprecedented results from given texts. However, as text\ninputs become longer, existing encoding methods like CLIP face limitations, and\naligning the generated images with long texts becomes challenging. To tackle\nthese issues, we propose LongAlign, which includes a segment-level encoding\nmethod for processing long texts and a decomposed preference optimization\nmethod for effective alignment training. For segment-level encoding, long texts\nare divided into multiple segments and processed separately. This method\novercomes the maximum input length limits of pretrained encoding models. For\npreference optimization, we provide decomposed CLIP-based preference models to\nfine-tune diffusion models. Specifically, to utilize CLIP-based preference\nmodels for T2I alignment, we delve into their scoring mechanisms and find that\nthe preference scores can be decomposed into two components: a text-relevant\npart that measures T2I alignment and a text-irrelevant part that assesses other\nvisual aspects of human preference. Additionally, we find that the\ntext-irrelevant part contributes to a common overfitting problem during\nfine-tuning. To address this, we propose a reweighting strategy that assigns\ndifferent weights to these two components, thereby reducing overfitting and\nenhancing alignment. After fine-tuning $512 \\times 512$ Stable Diffusion (SD)\nv1.5 for about 20 hours using our method, the fine-tuned SD outperforms\nstronger foundation models in T2I alignment, such as PixArt-$\\alpha$ and\nKandinsky v2.2. The code is available at\nhttps://github.com/luping-liu/LongAlign.\n","authors":["Luping Liu","Chao Du","Tianyu Pang","Zehan Wang","Chongxuan Li","Dong Xu"],"pdf_url":"https://arxiv.org/pdf/2410.11817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12831v2","updated":"2024-10-15T17:31:56Z","published":"2024-06-18T17:51:37Z","title":"VIA: Unified Spatiotemporal Video Adaptation Framework for Global and\n  Local Video Editing","summary":"  Video editing is a cornerstone of digital media, from entertainment and\neducation to professional communication. However, previous methods often\noverlook the necessity of comprehensively understanding both global and local\ncontexts, leading to inaccurate and inconsistent edits in the spatiotemporal\ndimension, especially for long videos. In this paper, we introduce VIA, a\nunified spatiotemporal Video Adaptation framework for global and local video\nediting, pushing the limits of consistently editing minute-long videos. First,\nto ensure local consistency within individual frames, we designed test-time\nediting adaptation to adapt a pre-trained image editing model for improving\nconsistency between potential editing directions and the text instruction, and\nadapt masked latent variables for precise local control. Furthermore, to\nmaintain global consistency over the video sequence, we introduce\nspatiotemporal adaptation that recursively gather consistent attention\nvariables in key frames and strategically applies them across the whole\nsequence to realize the editing effects. Extensive experiments demonstrate\nthat, compared to baseline methods, our VIA approach produces edits that are\nmore faithful to the source videos, more coherent in the spatiotemporal\ncontext, and more precise in local control. More importantly, we show that VIA\ncan achieve consistent long video editing in minutes, unlocking the potential\nfor advanced video editing tasks over long video sequences.\n","authors":["Jing Gu","Yuwei Fang","Ivan Skorokhodov","Peter Wonka","Xinya Du","Sergey Tulyakov","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2406.12831v2.pdf","comment":"19 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.11779v1","updated":"2024-10-15T16:57:44Z","published":"2024-10-15T16:57:44Z","title":"MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation","summary":"  Multimodal Large Language Models (MLLMs) frequently exhibit hallucination\nphenomena, but the underlying reasons remain poorly understood. In this paper,\nwe present an empirical analysis and find that, although MLLMs incorrectly\ngenerate the objects in the final output, they are actually able to recognize\nvisual objects in the preceding layers. We speculate that this may be due to\nthe strong knowledge priors of the language model suppressing the visual\ninformation, leading to hallucinations. Motivated by this, we propose a novel\ndynamic correction decoding method for MLLMs (DeCo), which adaptively selects\nthe appropriate preceding layers and proportionally integrates knowledge into\nthe final layer to adjust the output logits. Note that DeCo is model agnostic\nand can be seamlessly incorporated with various classic decoding strategies and\napplied to different MLLMs. We evaluate DeCo on widely-used benchmarks,\ndemonstrating that it can reduce hallucination rates by a large margin compared\nto baselines, highlighting its potential to mitigate hallucinations. Code is\navailable at https://github.com/zjunlp/DeCo.\n","authors":["Chenxi Wang","Xiang Chen","Ningyu Zhang","Bozhong Tian","Haoming Xu","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2410.11779v1.pdf","comment":"Ongoing work"},{"id":"http://arxiv.org/abs/2410.11701v1","updated":"2024-10-15T15:39:37Z","published":"2024-10-15T15:39:37Z","title":"Magnifier Prompt: Tackling Multimodal Hallucination via Extremely Simple\n  Instructions","summary":"  Hallucinations in multimodal large language models (MLLMs) hinder their\npractical applications. To address this, we propose a Magnifier Prompt\n(MagPrompt), a simple yet effective method to tackle hallucinations in MLLMs\nvia extremely simple instructions. MagPrompt is based on the following two key\nprinciples, which guide the design of various effective prompts, demonstrating\nrobustness: (1) MLLMs should focus more on the image. (2) When there are\nconflicts between the image and the model's inner knowledge, MLLMs should\nprioritize the image. MagPrompt is training-free and can be applied to\nopen-source and closed-source models, such as GPT-4o and Gemini-pro. It\nperforms well across many datasets and its effectiveness is comparable or even\nbetter than more complex methods like VCD. Furthermore, our prompt design\nprinciples and experimental analyses provide valuable insights into multimodal\nhallucination.\n","authors":["Yuhan Fu","Ruobing Xie","Jiazhen Liu","Bangxiang Lan","Xingwu Sun","Zhanhui Kang","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2410.11701v1.pdf","comment":"9 pages, 13 tables, 4 figures"},{"id":"http://arxiv.org/abs/2410.11582v1","updated":"2024-10-15T13:15:50Z","published":"2024-10-15T13:15:50Z","title":"On-the-fly Modulation for Balanced Multimodal Learning","summary":"  Multimodal learning is expected to boost model performance by integrating\ninformation from different modalities. However, its potential is not fully\nexploited because the widely-used joint training strategy, which has a uniform\nobjective for all modalities, leads to imbalanced and under-optimized uni-modal\nrepresentations. Specifically, we point out that there often exists modality\nwith more discriminative information, e.g., vision of playing football and\nsound of blowing wind. They could dominate the joint training process,\nresulting in other modalities being significantly under-optimized. To alleviate\nthis problem, we first analyze the under-optimized phenomenon from both the\nfeed-forward and the back-propagation stages during optimization. Then,\nOn-the-fly Prediction Modulation (OPM) and On-the-fly Gradient Modulation (OGM)\nstrategies are proposed to modulate the optimization of each modality, by\nmonitoring the discriminative discrepancy between modalities during training.\nConcretely, OPM weakens the influence of the dominant modality by dropping its\nfeature with dynamical probability in the feed-forward stage, while OGM\nmitigates its gradient in the back-propagation stage. In experiments, our\nmethods demonstrate considerable improvement across a variety of multimodal\ntasks. These simple yet effective strategies not only enhance performance in\nvanilla and task-oriented multimodal models, but also in more complex\nmultimodal tasks, showcasing their effectiveness and flexibility. The source\ncode is available at \\url{https://github.com/GeWu-Lab/BML_TPAMI2024}.\n","authors":["Yake Wei","Di Hu","Henghui Du","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2410.11582v1.pdf","comment":"Accepted by T-PAMI 2024"},{"id":"http://arxiv.org/abs/2404.01336v3","updated":"2024-10-15T12:40:39Z","published":"2024-03-30T14:39:09Z","title":"FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain\n  Fake News Detection","summary":"  Existing benchmarks for fake news detection have significantly contributed to\nthe advancement of models in assessing the authenticity of news content.\nHowever, these benchmarks typically focus solely on news pertaining to a single\nsemantic topic or originating from a single platform, thereby failing to\ncapture the diversity of multi-domain news in real scenarios. In order to\nunderstand fake news across various domains, the external knowledge and\nfine-grained annotations are indispensable to provide precise evidence and\nuncover the diverse underlying strategies for fabrication, which are also\nignored by existing benchmarks. To address this gap, we introduce a novel\nmulti-domain knowledge-enhanced benchmark with fine-grained annotations, named\n\\textbf{FineFake}. FineFake encompasses 16,909 data samples spanning six\nsemantic topics and eight platforms. Each news item is enriched with\nmulti-modal content, potential social context, semi-manually verified common\nknowledge, and fine-grained annotations that surpass conventional binary\nlabels. Furthermore, we formulate three challenging tasks based on FineFake and\npropose a knowledge-enhanced domain adaptation network. Extensive experiments\nare conducted on FineFake under various scenarios, providing accurate and\nreliable benchmarks for future endeavors. The entire FineFake project is\npublicly accessible as an open-source repository at\n\\url{https://github.com/Accuser907/FineFake}.\n","authors":["Ziyi Zhou","Xiaoming Zhang","Litian Zhang","Jiacheng Liu","Senzhang Wang","Zheng Liu","Xi Zhang","Chaozhuo Li","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2404.01336v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11522v1","updated":"2024-10-15T11:48:31Z","published":"2024-10-15T11:48:31Z","title":"Leveraging LLM Embeddings for Cross Dataset Label Alignment and Zero\n  Shot Music Emotion Prediction","summary":"  In this work, we present a novel method for music emotion recognition that\nleverages Large Language Model (LLM) embeddings for label alignment across\nmultiple datasets and zero-shot prediction on novel categories. First, we\ncompute LLM embeddings for emotion labels and apply non-parametric clustering\nto group similar labels, across multiple datasets containing disjoint labels.\nWe use these cluster centers to map music features (MERT) to the LLM embedding\nspace. To further enhance the model, we introduce an alignment regularization\nthat enables dissociation of MERT embeddings from different clusters. This\nfurther enhances the model's ability to better adaptation to unseen datasets.\nWe demonstrate the effectiveness of our approach by performing zero-shot\ninference on a new dataset, showcasing its ability to generalize to unseen\nlabels without additional training.\n","authors":["Renhang Liu","Abhinaba Roy","Dorien Herremans"],"pdf_url":"https://arxiv.org/pdf/2410.11522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11417v1","updated":"2024-10-15T09:07:25Z","published":"2024-10-15T09:07:25Z","title":"VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models","summary":"  Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs.\n","authors":["Xiaohan Lan","Yitian Yuan","Zequn Jie","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2410.11417v1.pdf","comment":"9 pages, 4 figures"}]},"2024-10-14T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.05536v2","updated":"2024-10-14T23:20:26Z","published":"2024-10-07T22:25:37Z","title":"On Feature Decorrelation in Cloth-Changing Person Re-identification","summary":"  Cloth-changing person re-identification (CC-ReID) poses a significant\nchallenge in computer vision. A prevailing approach is to prompt models to\nconcentrate on causal attributes, like facial features and hairstyles, rather\nthan confounding elements such as clothing appearance. Traditional methods to\nachieve this involve integrating multi-modality data or employing manually\nannotated clothing labels, which tend to complicate the model and require\nextensive human effort. In our study, we demonstrate that simply reducing\nfeature correlations during training can significantly enhance the baseline\nmodel's performance. We theoretically elucidate this effect and introduce a\nnovel regularization technique based on density ratio estimation. This\ntechnique aims to minimize feature correlation in the training process of\ncloth-changing ReID baselines. Our approach is model-independent, offering\nbroad enhancements without needing additional data or labels. We validate our\nmethod through comprehensive experiments on prevalent CC-ReID datasets, showing\nits effectiveness in improving baseline models' generalization capabilities.\n","authors":["Hongjun Wang","Jiyuan Chen","Renhe Jiang","Xuan Song","Yinqiang Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.05536v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11046v1","updated":"2024-10-14T19:51:32Z","published":"2024-10-14T19:51:32Z","title":"SGUQ: Staged Graph Convolution Neural Network for Alzheimer's Disease\n  Diagnosis using Multi-Omics Data","summary":"  Alzheimer's disease (AD) is a chronic neurodegenerative disorder and the\nleading cause of dementia, significantly impacting cost, mortality, and burden\nworldwide. The advent of high-throughput omics technologies, such as genomics,\ntranscriptomics, proteomics, and epigenomics, has revolutionized the molecular\nunderstanding of AD. Conventional AI approaches typically require the\ncompletion of all omics data at the outset to achieve optimal AD diagnosis,\nwhich are inefficient and may be unnecessary. To reduce the clinical cost and\nimprove the accuracy of AD diagnosis using multi-omics data, we propose a novel\nstaged graph convolutional network with uncertainty quantification (SGUQ). SGUQ\nbegins with mRNA and progressively incorporates DNA methylation and miRNA data\nonly when necessary, reducing overall costs and exposure to harmful tests.\nExperimental results indicate that 46.23% of the samples can be reliably\npredicted using only single-modal omics data (mRNA), while an additional 16.04%\nof the samples can achieve reliable predictions when combining two omics data\ntypes (mRNA + DNA methylation). In addition, the proposed staged SGUQ achieved\nan accuracy of 0.858 on ROSMAP dataset, which outperformed existing methods\nsignificantly. The proposed SGUQ can not only be applied to AD diagnosis using\nmulti-omics data but also has the potential for clinical decision-making using\nmulti-viewed data. Our implementation is publicly available at\nhttps://github.com/chenzhao2023/multiomicsuncertainty.\n","authors":["Liang Tao","Yixin Xie","Jeffrey D Deng","Hui Shen","Hong-Wen Deng","Weihua Zhou","Chen Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.11046v1.pdf","comment":"20 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.10994v1","updated":"2024-10-14T18:20:09Z","published":"2024-10-14T18:20:09Z","title":"GraFPrint: A GNN-Based Approach for Audio Identification","summary":"  This paper introduces GraFPrint, an audio identification framework that\nleverages the structural learning capabilities of Graph Neural Networks (GNNs)\nto create robust audio fingerprints. Our method constructs a k-nearest neighbor\n(k-NN) graph from time-frequency representations and applies max-relative graph\nconvolutions to encode local and global information. The network is trained\nusing a self-supervised contrastive approach, which enhances resilience to\nambient distortions by optimizing feature representation. GraFPrint\ndemonstrates superior performance on large-scale datasets at various levels of\ngranularity, proving to be both lightweight and scalable, making it suitable\nfor real-world applications with extensive reference databases.\n","authors":["Aditya Bhattacharjee","Shubhr Singh","Emmanouil Benetos"],"pdf_url":"https://arxiv.org/pdf/2410.10994v1.pdf","comment":"Submitted to IEEE International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP 2025)"},{"id":"http://arxiv.org/abs/2410.10639v1","updated":"2024-10-14T15:50:35Z","published":"2024-10-14T15:50:35Z","title":"Generating Model Parameters for Controlling: Parameter Diffusion for\n  Controllable Multi-Task Recommendation","summary":"  Commercial recommender systems face the challenge that task requirements from\nplatforms or users often change dynamically (e.g., varying preferences for\naccuracy or diversity). Ideally, the model should be re-trained after resetting\na new objective function, adapting to these changes in task requirements.\nHowever, in practice, the high computational costs associated with retraining\nmake this process impractical for models already deployed to online\nenvironments. This raises a new challenging problem: how to efficiently adapt\nthe learning model to different task requirements by controlling model\nparameters after deployment, without the need for retraining. To address this\nissue, we propose a novel controllable learning approach via Parameter\nDiffusion for controllable multi-task Recommendation (PaDiRec), which allows\nthe customization and adaptation of recommendation model parameters to new task\nrequirements without retraining. Specifically, we first obtain the optimized\nmodel parameters through adapter tunning based on the feasible task\nrequirements. Then, we utilize the diffusion model as a parameter generator,\nemploying classifier-free guidance in conditional training to learn the\ndistribution of optimized model parameters under various task requirements.\nFinally, the diffusion model is applied to effectively generate model\nparameters in a test-time adaptation manner given task requirements. As a\nmodel-agnostic approach, PaDiRec can leverage existing recommendation models as\nbackbones to enhance their controllability. Extensive experiments on public\ndatasets and a dataset from a commercial app, indicate that PaDiRec can\neffectively enhance controllability through efficient model parameter\ngeneration. The code is released at\nhttps://anonymous.4open.science/r/PaDiRec-DD13.\n","authors":["Chenglei Shen","Jiahao Zhao","Xiao Zhang","Weijie Yu","Ming He","Jianping Fan"],"pdf_url":"https://arxiv.org/pdf/2410.10639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10594v1","updated":"2024-10-14T15:04:18Z","published":"2024-10-14T15:04:18Z","title":"VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality\n  Documents","summary":"  Retrieval-augmented generation (RAG) is an effective technique that enables\nlarge language models (LLMs) to utilize external knowledge sources for\ngeneration. However, current RAG systems are solely based on text, rendering it\nimpossible to utilize vision information like layout and images that play\ncrucial roles in real-world multi-modality documents. In this paper, we\nintroduce VisRAG, which tackles this issue by establishing a vision-language\nmodel (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the\ndocument to obtain text, the document is directly embedded using a VLM as an\nimage and then retrieved to enhance the generation of a VLM. Compared to\ntraditional text-based RAG, VisRAG maximizes the retention and utilization of\nthe data information in the original documents, eliminating the information\nloss introduced during the parsing process. We collect both open-source and\nsynthetic data to train the retriever in VisRAG and explore a variety of\ngeneration methods. Experiments demonstrate that VisRAG outperforms traditional\nRAG in both the retrieval and generation stages, achieving a 25--39\\%\nend-to-end performance gain over traditional text-based RAG pipeline. Further\nanalysis reveals that VisRAG is effective in utilizing training data and\ndemonstrates strong generalization capability, positioning it as a promising\nsolution for RAG on multi-modality documents. Our code and data are available\nat https://github.com/openbmb/visrag .\n","authors":["Shi Yu","Chaoyue Tang","Bokai Xu","Junbo Cui","Junhao Ran","Yukun Yan","Zhenghao Liu","Shuo Wang","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.10594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10542v1","updated":"2024-10-14T14:22:12Z","published":"2024-10-14T14:22:12Z","title":"Rethinking Legal Judgement Prediction in a Realistic Scenario in the Era\n  of Large Language Models","summary":"  This study investigates judgment prediction in a realistic scenario within\nthe context of Indian judgments, utilizing a range of transformer-based models,\nincluding InLegalBERT, BERT, and XLNet, alongside LLMs such as Llama-2 and\nGPT-3.5 Turbo. In this realistic scenario, we simulate how judgments are\npredicted at the point when a case is presented for a decision in court, using\nonly the information available at that time, such as the facts of the case,\nstatutes, precedents, and arguments. This approach mimics real-world\nconditions, where decisions must be made without the benefit of hindsight,\nunlike retrospective analyses often found in previous studies. For transformer\nmodels, we experiment with hierarchical transformers and the summarization of\njudgment facts to optimize input for these models. Our experiments with LLMs\nreveal that GPT-3.5 Turbo excels in realistic scenarios, demonstrating robust\nperformance in judgment prediction. Furthermore, incorporating additional legal\ninformation, such as statutes and precedents, significantly improves the\noutcome of the prediction task. The LLMs also provide explanations for their\npredictions. To evaluate the quality of these predictions and explanations, we\nintroduce two human evaluation metrics: Clarity and Linking. Our findings from\nboth automatic and human evaluations indicate that, despite advancements in\nLLMs, they are yet to achieve expert-level performance in judgment prediction\nand explanation tasks.\n","authors":["Shubham Kumar Nigam","Aniket Deroy","Subhankar Maity","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2410.10542v1.pdf","comment":"Accepted on NLLP at EMNLP 2024"},{"id":"http://arxiv.org/abs/2309.00976v4","updated":"2024-10-14T14:11:22Z","published":"2023-09-02T16:20:41Z","title":"Pure Message Passing Can Estimate Common Neighbor for Link Prediction","summary":"  Message Passing Neural Networks (MPNNs) have emerged as the {\\em de facto}\nstandard in graph representation learning. However, when it comes to link\nprediction, they often struggle, surpassed by simple heuristics such as Common\nNeighbor (CN). This discrepancy stems from a fundamental limitation: while\nMPNNs excel in node-level representation, they stumble with encoding the joint\nstructural features essential to link prediction, like CN. To bridge this gap,\nwe posit that, by harnessing the orthogonality of input vectors, pure\nmessage-passing can indeed capture joint structural features. Specifically, we\nstudy the proficiency of MPNNs in approximating CN heuristics. Based on our\nfindings, we introduce the Message Passing Link Predictor (MPLP), a novel link\nprediction model. MPLP taps into quasi-orthogonal vectors to estimate\nlink-level structural features, all while preserving the node-level\ncomplexities. Moreover, our approach demonstrates that leveraging\nmessage-passing to capture structural features could offset MPNNs'\nexpressiveness limitations at the expense of estimation variance. We conduct\nexperiments on benchmark datasets from various domains, where our method\nconsistently outperforms the baseline methods.\n","authors":["Kaiwen Dong","Zhichun Guo","Nitesh V. Chawla"],"pdf_url":"https://arxiv.org/pdf/2309.00976v4.pdf","comment":"Accepted to Neurips'24"},{"id":"http://arxiv.org/abs/2410.10455v1","updated":"2024-10-14T12:49:13Z","published":"2024-10-14T12:49:13Z","title":"Advancing Academic Knowledge Retrieval via LLM-enhanced Representation\n  Similarity Fusion","summary":"  In an era marked by robust technological growth and swift information\nrenewal, furnishing researchers and the populace with top-tier, avant-garde\nacademic insights spanning various domains has become an urgent necessity. The\nKDD Cup 2024 AQA Challenge is geared towards advancing retrieval models to\nidentify pertinent academic terminologies from suitable papers for scientific\ninquiries. This paper introduces the LLM-KnowSimFuser proposed by Robo Space,\nwhich wins the 2nd place in the competition. With inspirations drawed from the\nsuperior performance of LLMs on multiple tasks, after careful analysis of the\nprovided datasets, we firstly perform fine-tuning and inference using\nLLM-enhanced pre-trained retrieval models to introduce the tremendous language\nunderstanding and open-domain knowledge of LLMs into this task, followed by a\nweighted fusion based on the similarity matrix derived from the inference\nresults. Finally, experiments conducted on the competition datasets show the\nsuperiority of our proposal, which achieved a score of 0.20726 on the final\nleaderboard.\n","authors":["Wei Dai","Peng Fu","Chunjing Gan"],"pdf_url":"https://arxiv.org/pdf/2410.10455v1.pdf","comment":"The 2nd Place of KDD Cup 2024 OAG-Challenge AQA"},{"id":"http://arxiv.org/abs/2407.19669v2","updated":"2024-10-14T12:19:44Z","published":"2024-07-29T03:12:28Z","title":"mGTE: Generalized Long-Context Text Representation and Reranking Models\n  for Multilingual Text Retrieval","summary":"  We present systematic efforts in building long-context multilingual text\nrepresentation model (TRM) and reranker from scratch for text retrieval. We\nfirst introduce a text encoder (base size) enhanced with RoPE and unpadding,\npre-trained in a native 8192-token context (longer than 512 of previous\nmultilingual encoders). Then we construct a hybrid TRM and a cross-encoder\nreranker by contrastive learning. Evaluations show that our text encoder\noutperforms the same-sized previous state-of-the-art XLM-R. Meanwhile, our TRM\nand reranker match the performance of large-sized state-of-the-art BGE-M3\nmodels and achieve better results on long-context retrieval benchmarks. Further\nanalysis demonstrate that our proposed models exhibit higher efficiency during\nboth training and inference. We believe their efficiency and effectiveness\ncould benefit various researches and industrial applications.\n","authors":["Xin Zhang","Yanzhao Zhang","Dingkun Long","Wen Xie","Ziqi Dai","Jialong Tang","Huan Lin","Baosong Yang","Pengjun Xie","Fei Huang","Meishan Zhang","Wenjie Li","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.19669v2.pdf","comment":"Camera-ready version of EMNLP 2024: Industry Track"},{"id":"http://arxiv.org/abs/2410.10408v1","updated":"2024-10-14T12:00:58Z","published":"2024-10-14T12:00:58Z","title":"Medico: Towards Hallucination Detection and Correction with Multi-source\n  Evidence Fusion","summary":"  As we all know, hallucinations prevail in Large Language Models (LLMs), where\nthe generated content is coherent but factually incorrect, which inflicts a\nheavy blow on the widespread application of LLMs. Previous studies have shown\nthat LLMs could confidently state non-existent facts rather than answering ``I\ndon't know''. Therefore, it is necessary to resort to external knowledge to\ndetect and correct the hallucinated content. Since manual detection and\ncorrection of factual errors is labor-intensive, developing an automatic\nend-to-end hallucination-checking approach is indeed a needful thing. To this\nend, we present Medico, a Multi-source evidence fusion enhanced hallucination\ndetection and correction framework. It fuses diverse evidence from multiple\nsources, detects whether the generated content contains factual errors,\nprovides the rationale behind the judgment, and iteratively revises the\nhallucinated content. Experimental results on evidence retrieval (0.964 HR@5,\n0.908 MRR@5), hallucination detection (0.927-0.951 F1), and hallucination\ncorrection (0.973-0.979 approval rate) manifest the great potential of Medico.\nA video demo of Medico can be found at https://youtu.be/RtsO6CSesBI.\n","authors":["Xinping Zhao","Jindi Yu","Zhenyu Liu","Jifang Wang","Dongfang Li","Yibin Chen","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.10408v1.pdf","comment":"12 pages, 3 figures, 6 tables. Accepted by EMNLP 2024's demo track"},{"id":"http://arxiv.org/abs/2410.10381v1","updated":"2024-10-14T11:10:15Z","published":"2024-10-14T11:10:15Z","title":"Collaborative filtering based on nonnegative/binary matrix factorization","summary":"  Collaborative filtering generates recommendations based on user-item\nsimilarities through rating data, which may involve numerous unrated items. To\npredict scores for unrated items, matrix factorization techniques, such as\nnonnegative matrix factorization (NMF), are often employed to predict scores\nfor unrated items. Nonnegative/binary matrix factorization (NBMF), which is an\nextension of NMF, approximates a nonnegative matrix as the product of\nnonnegative and binary matrices. Previous studies have employed NBMF for image\nanalysis where the data were dense. In this paper, we propose a modified NBMF\nalgorithm that can be applied to collaborative filtering where data are sparse.\nIn the modified method, unrated elements in a rating matrix are masked, which\nimproves the collaborative filtering performance. Utilizing a low-latency Ising\nmachine in NBMF is advantageous in terms of the computation time, making the\nproposed method beneficial.\n","authors":["Yukino Terui","Yuka Inoue","Yohei Hamakawa","Kosuke Tatsumura","Kazue Kudo"],"pdf_url":"https://arxiv.org/pdf/2410.10381v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.10372v1","updated":"2024-10-14T10:55:58Z","published":"2024-10-14T10:55:58Z","title":"BookWorm: A Dataset for Character Description and Analysis","summary":"  Characters are at the heart of every story, driving the plot and engaging\nreaders. In this study, we explore the understanding of characters in\nfull-length books, which contain complex narratives and numerous interacting\ncharacters. We define two tasks: character description, which generates a brief\nfactual profile, and character analysis, which offers an in-depth\ninterpretation, including character development, personality, and social\ncontext. We introduce the BookWorm dataset, pairing books from the Gutenberg\nProject with human-written descriptions and analyses. Using this dataset, we\nevaluate state-of-the-art long-context models in zero-shot and fine-tuning\nsettings, utilizing both retrieval-based and hierarchical processing for\nbook-length inputs. Our findings show that retrieval-based approaches\noutperform hierarchical ones in both tasks. Additionally, fine-tuned models\nusing coreference-based retrieval produce the most factual descriptions, as\nmeasured by fact- and entailment-based metrics. We hope our dataset,\nexperiments, and analysis will inspire further research in character-based\nnarrative understanding.\n","authors":["Argyrios Papoudakis","Mirella Lapata","Frank Keller"],"pdf_url":"https://arxiv.org/pdf/2410.10372v1.pdf","comment":"30 pages, 2 figures, EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.10367v1","updated":"2024-10-14T10:46:32Z","published":"2024-10-14T10:46:32Z","title":"A Hybrid Filtering for Micro-video Hashtag Recommendation using\n  Graph-based Deep Neural Network","summary":"  Due to the growing volume of user generated content, hashtags are employed as\ntopic indicators to manage content efficiently on social media platforms.\nHowever, finding these vital topics is challenging in microvideos since they\ncontain substantial information in a short duration. Existing methods that\nrecommend hashtags for microvideos primarily focus on content and\npersonalization while disregarding relatedness among users. Moreover, the cold\nstart user issue prevails in hashtag recommendation systems. Considering the\nabove, we propose a hybrid filtering based MIcro-video haSHtag recommendatiON\nMISHON technique to recommend hashtags for micro-videos. Besides content based\nfiltering, we employ user-based collaborative filtering to enhance\nrecommendations. Since hashtags reflect users topical interests, we find\nsimilar users based on historical tagging behavior to model user relatedness.\nWe employ a graph-based deep neural network to model user to user, modality to\nmodality, and user to modality interactions. We then use refined modality\nspecific and user representations to recommend pertinent hashtags for\nmicrovideos. The empirical results on three real world datasets demonstrate\nthat MISHON attains a comparative enhancement of 3.6, 2.8, and 6.5 reported in\npercentage concerning the F1 score, respectively. Since cold start users exist\nwhose historical tagging information is unavailable, we also propose a content\nand social influence based technique to model the relatedness of cold start\nusers with influential users. The proposed solution shows a relative\nimprovement of 15.8 percent in the F1 score over its content only counterpart.\nThese results show that the proposed framework mitigates the cold start user\nproblem.\n","authors":["Shubhi Bansal","Kushaan Gowda","Mohammad Zia Ur Rehman","Chandravardhan Singh Raghaw","Nagendra Kumar"],"pdf_url":"https://arxiv.org/pdf/2410.10367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10360v1","updated":"2024-10-14T10:26:57Z","published":"2024-10-14T10:26:57Z","title":"Parenting: Optimizing Knowledge Selection of Retrieval-Augmented\n  Language Models with Parameter Decoupling and Tailored Tuning","summary":"  Retrieval-Augmented Generation (RAG) offers an effective solution to the\nissues faced by Large Language Models (LLMs) in hallucination generation and\nknowledge obsolescence by incorporating externally retrieved knowledge.\nHowever, due to potential conflicts between internal and external knowledge, as\nwell as retrieval noise, LLMs often struggle to effectively integrate external\nevidence, leading to a decline in performance. Although existing methods\nattempt to tackle these challenges, they often struggle to strike a balance\nbetween model adherence and robustness, resulting in significant learning\nvariance. Inspired by human cognitive processes, we propose Parenting, a novel\nframework that decouples adherence and robustness within the parameter space of\nLLMs. Specifically, Parenting utilizes a key parameter mining method based on\nforward activation gain to identify and isolate the crucial parameter units\nthat are strongly linked to adherence and robustness. Then, Parenting employs a\ntype-guided tailored tuning strategy, applying specific and appropriate\nfine-tuning methods to parameter units representing different capabilities,\naiming to achieve a balanced enhancement of adherence and robustness. Extensive\nexperiments on various datasets and models validate the effectiveness and\ngeneralizability of our methods.\n","authors":["Yongxin Xu","Ruizhe Zhang","Xinke Jiang","Yujie Feng","Yuzhen Xiao","Xinyu Ma","Runchuan Zhu","Xu Chu","Junfeng Zhao","Yasha Wang"],"pdf_url":"https://arxiv.org/pdf/2410.10360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10296v1","updated":"2024-10-14T08:49:11Z","published":"2024-10-14T08:49:11Z","title":"Enhancing Attributed Graph Networks with Alignment and Uniformity\n  Constraints for Session-based Recommendation","summary":"  Session-based Recommendation (SBR), seeking to predict a user's next action\nbased on an anonymous session, has drawn increasing attention for its\npracticability. Most SBR models only rely on the contextual transitions within\na short session to learn item representations while neglecting additional\nvaluable knowledge. As such, their model capacity is largely limited by the\ndata sparsity issue caused by short sessions. A few studies have exploited the\nModeling of Item Attributes (MIA) to enrich item representations. However, they\nusually involve specific model designs that can hardly transfer to existing\nattribute-agnostic SBR models and thus lack universality. In this paper, we\npropose a model-agnostic framework, named AttrGAU (Attributed Graph Networks\nwith Alignment and Uniformity Constraints), to bring the MIA's superiority into\nexisting attribute-agnostic models, to improve their accuracy and robustness\nfor recommendation. Specifically, we first build a bipartite attributed graph\nand design an attribute-aware graph convolution to exploit the rich attribute\nsemantics hidden in the heterogeneous item-attribute relationship. We then\ndecouple existing attribute-agnostic SBR models into the graph neural network\nand attention readout sub-modules to satisfy the non-intrusive requirement.\nLastly, we design two representation constraints, i.e., alignment and\nuniformity, to optimize distribution discrepancy in representation between the\nattribute semantics and collaborative semantics. Extensive experiments on three\npublic benchmark datasets demonstrate that the proposed AttrGAU framework can\nsignificantly enhance backbone models' recommendation performance and\nrobustness against data sparsity and data noise issues. Our implementation\ncodes will be available at https://github.com/ItsukiFujii/AttrGAU.\n","authors":["Xinping Zhao","Chaochao Chen","Jiajie Su","Yizhao Zhang","Baotian Hu"],"pdf_url":"https://arxiv.org/pdf/2410.10296v1.pdf","comment":"11 pages, 4 figures, 5 tables. Accepted by ICWS 2024"},{"id":"http://arxiv.org/abs/2410.10293v1","updated":"2024-10-14T08:47:21Z","published":"2024-10-14T08:47:21Z","title":"FunnelRAG: A Coarse-to-Fine Progressive Retrieval Paradigm for RAG","summary":"  Retrieval-Augmented Generation (RAG) prevails in Large Language Models. It\nmainly consists of retrieval and generation. The retrieval modules (a.k.a.\nretrievers) aim to find useful information used to facilitate generation\nmodules (a.k.a. generators). As such, generators' performance largely depends\non the effectiveness and efficiency of retrievers. However, the retrieval\nparadigm that we design and use remains flat, which treats the retrieval\nprocedures as a one-off deal with constant granularity. Despite effectiveness,\nwe argue that they suffer from two limitations: (1) flat retrieval exerts a\nsignificant burden on one retriever; (2) constant granularity limits the\nceiling of retrieval performance. In this work, we propose a progressive\nretrieval paradigm with coarse-to-fine granularity for RAG, termed FunnelRAG,\nso as to balance effectiveness and efficiency. Specifically, FunnelRAG\nestablishes a progressive retrieval pipeline by collaborating coarse-to-fine\ngranularity, large-to-small quantity, and low-to-high capacity, which can\nrelieve the burden on one retriever and also promote the ceiling of retrieval\nperformance. Extensive experiments manifest that FunnelRAG achieves comparable\nretrieval performance while the time overhead is reduced by nearly 40 percent.\n","authors":["Xinping Zhao","Yan Zhong","Zetian Sun","Xinshuo Hu","Zhenyu Liu","Dongfang Li","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.10293v1.pdf","comment":"18 pages, 6 figures, 13 tables"},{"id":"http://arxiv.org/abs/2410.10286v1","updated":"2024-10-14T08:38:29Z","published":"2024-10-14T08:38:29Z","title":"Back-of-the-Book Index Automation for Arabic Documents","summary":"  Back-of-the-book indexes are crucial for book readability. Their manual\ncreation is laborious and error prone. In this paper, we consider automating\nback-of-the-book index extraction for Arabic books to help simplify both the\ncreation and review tasks. Given a back-of-the-book index, we aim to check and\nidentify the accurate occurrences of index terms relative to the associated\npages. To achieve this, we first define a pool of candidates for each term by\nextracting all possible noun phrases from paragraphs appearing on the relevant\nindex pages. These noun phrases, identified through part-of-speech analysis,\nare stored in a vector database for efficient retrieval. We use several\nmetrics, including exact matches, lexical similarity, and semantic similarity,\nto determine the most appropriate occurrence. The candidate with the highest\nscore based on these metrics is chosen as the occurrence of the term. We\nfine-tuned a heuristic method, that considers the above metrics and that\nachieves an F1-score of .966 (precision=.966, recall=.966). These excellent\nresults open the door for future work related to automation of back-of-the-book\nindex generation and checking.\n","authors":["Nawal Haidar","Fadi A. Zaraket"],"pdf_url":"https://arxiv.org/pdf/2410.10286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08478v2","updated":"2024-10-14T07:55:16Z","published":"2024-10-11T03:10:09Z","title":"Personalized Item Representations in Federated Multimodal Recommendation","summary":"  Federated recommendation systems are essential for providing personalized\nrecommendations while protecting user privacy. However, current methods mainly\nrely on ID-based item embeddings, neglecting the rich multimodal information of\nitems. To address this, we propose a Federated Multimodal Recommendation\nSystem, called FedMR. FedMR uses a foundation model on the server to encode\nmultimodal item data, such as images and text. To handle data heterogeneity\ncaused by user preference differences, FedMR introduces a Mixing Feature Fusion\nModule on each client, which adjusts fusion strategy weights based on user\ninteraction history to generate personalized item representations that capture\nusers' fine-grained preferences. FedMR is compatible with existing ID-based\nfederated recommendation systems, improving performance without modifying the\noriginal framework. Experiments on four real-world multimodal datasets\ndemonstrate FedMR's effectiveness. The code is available at\nhttps://anonymous.4open.science/r/FedMR.\n","authors":["Zhiwei Li","Guodong Long","Jing Jiang","Chengqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.08478v2.pdf","comment":"12 pages, 4 figures, 5 tables, conference"},{"id":"http://arxiv.org/abs/2410.10130v1","updated":"2024-10-14T03:37:47Z","published":"2024-10-14T03:37:47Z","title":"DecKG: Decentralized Collaborative Learning with Knowledge Graph\n  Enhancement for POI Recommendation","summary":"  Decentralized collaborative learning for Point-of-Interest (POI)\nrecommendation has gained research interest due to its advantages in privacy\npreservation and efficiency, as it keeps data locally and leverages\ncollaborative learning among clients to train models in a decentralized manner.\nHowever, since local data is often limited and insufficient for training\naccurate models, a common solution is integrating external knowledge as\nauxiliary information to enhance model performance. Nevertheless, this solution\nposes challenges for decentralized collaborative learning. Due to private\nnature of local data, identifying relevant auxiliary information specific to\neach user is non-trivial. Furthermore, resource-constrained local devices\nstruggle to accommodate all auxiliary information, which places heavy burden on\nlocal storage. To fill the gap, we propose a novel decentralized collaborative\nlearning with knowledge graph enhancement framework for POI recommendation\n(DecKG). Instead of directly uploading interacted items, users generate\ndesensitized check-in data by uploading general categories of interacted items\nand sampling similar items from same category. The server then pretrains KG\nwithout sensitive user-item interactions and deploys relevant partitioned\nsub-KGs to individual users. Entities are further refined on the device,\nallowing client to client communication to exchange knowledge learned from\nlocal data and sub-KGs. Evaluations across two real-world datasets demonstrate\nDecKG's effectiveness recommendation performance.\n","authors":["Ruiqi Zheng","Liang Qu","Guanhua Ye","Tong Chen","Yuhui Shi","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2410.10130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10127v1","updated":"2024-10-14T03:26:51Z","published":"2024-10-14T03:26:51Z","title":"MAIR: A Massive Benchmark for Evaluating Instructed Retrieval","summary":"  Recent information retrieval (IR) models are pre-trained and\ninstruction-tuned on massive datasets and tasks, enabling them to perform well\non a wide range of tasks and potentially generalize to unseen tasks with\ninstructions. However, existing IR benchmarks focus on a limited scope of\ntasks, making them insufficient for evaluating the latest IR models. In this\npaper, we propose MAIR (Massive Instructed Retrieval Benchmark), a\nheterogeneous IR benchmark that includes 126 distinct IR tasks across 6\ndomains, collected from existing datasets. We benchmark state-of-the-art\ninstruction-tuned text embedding models and re-ranking models. Our experiments\nreveal that instruction-tuned models generally achieve superior performance\ncompared to non-instruction-tuned models on MAIR. Additionally, our results\nsuggest that current instruction-tuned text embedding models and re-ranking\nmodels still lack effectiveness in specific long-tail tasks. MAIR is publicly\navailable at https://github.com/sunnweiwei/Mair.\n","authors":["Weiwei Sun","Zhengliang Shi","Jiulong Wu","Lingyong Yan","Xinyu Ma","Yiding Liu","Min Cao","Dawei Yin","Zhaochun Ren"],"pdf_url":"https://arxiv.org/pdf/2410.10127v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2403.16504v3","updated":"2024-10-14T03:26:10Z","published":"2024-03-25T07:38:40Z","title":"LARA: Linguistic-Adaptive Retrieval-Augmentation for Multi-Turn Intent\n  Classification","summary":"  Multi-turn intent classification is notably challenging due to the complexity\nand evolving nature of conversational contexts. This paper introduces LARA, a\nLinguistic-Adaptive Retrieval-Augmentation framework to enhance accuracy in\nmulti-turn classification tasks across six languages, accommodating a large\nnumber of intents in chatbot interactions. LARA combines a fine-tuned smaller\nmodel with a retrieval-augmented mechanism, integrated within the architecture\nof LLMs. The integration allows LARA to dynamically utilize past dialogues and\nrelevant intents, thereby improving the understanding of the context.\nFurthermore, our adaptive retrieval techniques bolster the cross-lingual\ncapabilities of LLMs without extensive retraining and fine-tuning.\nComprehensive experiments demonstrate that LARA achieves state-of-the-art\nperformance on multi-turn intent classification tasks, enhancing the average\naccuracy by 3.67\\% from state-of-the-art single-turn intent classifiers.\n","authors":["Junhua Liu","Yong Keat Tan","Bin Fu","Kwan Hui Lim"],"pdf_url":"https://arxiv.org/pdf/2403.16504v3.pdf","comment":"Accepted to EMNLP'24"},{"id":"http://arxiv.org/abs/2409.02864v2","updated":"2024-10-14T01:07:54Z","published":"2024-09-04T16:43:14Z","title":"Language Model Powered Digital Biology","summary":"  Recent advancements in Large Language Models (LLMs) are transforming biology,\ncomputer science, and many other research fields, as well as impacting everyday\nlife. While transformer-based technologies are currently being deployed in\nbiology, no available agentic system has been developed to tackle\nbioinformatics workflows. We present a prototype Bioinformatics Retrieval\nAugmented Data (BRAD) digital assistant. BRAD is a chatbot and agentic system\nthat integrates a suite of tools to handle bioinformatics tasks, from code\nexecution to online search. We demonstrate its capabilities through (1)\nimproved question-and-answering with retrieval augmented generation (RAG), (2)\nthe ability to run complex software pipelines, and (3) the ability to organize\nand distribute tasks in agentic workflows. We use BRAD for automation,\nperforming tasks ranging from gene enrichment and searching the archive to\nautomatic code generation for running biomarker identification pipelines. BRAD\nis a step toward autonomous, self-driving labs for digital biology.\n","authors":["Joshua Pickard","Marc Andrew Choi","Natalie Oliven","Cooper Stansbury","Jillian Cwycyshyn","Nicholas Galioto","Alex Gorodetsky","Alvaro Velasquez","Indika Rajapakse"],"pdf_url":"https://arxiv.org/pdf/2409.02864v2.pdf","comment":"49 pages, 3 tables, 12 figures"},{"id":"http://arxiv.org/abs/2310.16605v4","updated":"2024-10-14T00:45:35Z","published":"2023-10-25T12:50:34Z","title":"Enhancing Dense Retrievers' Robustness with Group-level Reweighting","summary":"  The anchor-document data derived from web graphs offers a wealth of paired\ninformation for training dense retrieval models in an unsupervised manner.\nHowever, unsupervised data contains diverse patterns across the web graph and\noften exhibits significant imbalance, leading to suboptimal performance in\nunderrepresented or difficult groups. In this paper, we introduce WebDRO, an\nefficient approach for clustering the web graph data and optimizing group\nweights to enhance the robustness of dense retrieval models. Initially, we\nbuild an embedding model for clustering anchor-document pairs. Specifically, we\ncontrastively train the embedding model for link prediction, which guides the\nembedding model in capturing the document features behind the web graph links.\nSubsequently, we employ the group distributional robust optimization to\nrecalibrate the weights across different clusters of anchor-document pairs\nduring training retrieval models. During training, we direct the model to\nassign higher weights to clusters with higher loss and focus more on worst-case\nscenarios. This approach ensures that the model has strong generalization\nability on all data patterns. Our experiments on MS MARCO and BEIR demonstrate\nthat our method can effectively improve retrieval performance in unsupervised\ntraining and finetuning settings. Further analysis confirms the stability and\nvalidity of group weights learned by WebDRO. The code of this paper can be\nobtained from https://github.com/Hanpx20/GroupDRO_Dense_Retrieval.\n","authors":["Peixuan Han","Zhenghao Liu","Zhiyuan Liu","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2310.16605v4.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2408.06753v3","updated":"2024-10-14T16:06:54Z","published":"2024-08-13T09:19:59Z","title":"Detecting Audio-Visual Deepfakes with Fine-Grained Inconsistencies","summary":"  Existing methods on audio-visual deepfake detection mainly focus on\nhigh-level features for modeling inconsistencies between audio and visual data.\nAs a result, these approaches usually overlook finer audio-visual artifacts,\nwhich are inherent to deepfakes. Herein, we propose the introduction of\nfine-grained mechanisms for detecting subtle artifacts in both spatial and\ntemporal domains. First, we introduce a local audio-visual model capable of\ncapturing small spatial regions that are prone to inconsistencies with audio.\nFor that purpose, a fine-grained mechanism based on a spatially-local distance\ncoupled with an attention module is adopted. Second, we introduce a\ntemporally-local pseudo-fake augmentation to include samples incorporating\nsubtle temporal inconsistencies in our training set. Experiments on the DFDC\nand the FakeAVCeleb datasets demonstrate the superiority of the proposed method\nin terms of generalization as compared to the state-of-the-art under both\nin-dataset and cross-dataset settings.\n","authors":["Marcella Astrid","Enjie Ghorbel","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2408.06753v3.pdf","comment":"Accepted in BMVC 2024"},{"id":"http://arxiv.org/abs/2409.02845v2","updated":"2024-10-14T14:55:13Z","published":"2024-09-04T16:17:41Z","title":"Multi-Track MusicLDM: Towards Versatile Music Generation with Latent\n  Diffusion Model","summary":"  Diffusion models have shown promising results in cross-modal generation tasks\ninvolving audio and music, such as text-to-sound and text-to-music generation.\nThese text-controlled music generation models typically focus on generating\nmusic by capturing global musical attributes like genre and mood. However,\nmusic composition is a complex, multilayered task that often involves musical\narrangement as an integral part of the process. This process involves composing\neach instrument to align with existing ones in terms of beat, dynamics,\nharmony, and melody, requiring greater precision and control over tracks than\ntext prompts usually provide. In this work, we address these challenges by\nextending the MusicLDM, a latent diffusion model for music, into a multi-track\ngenerative model. By learning the joint probability of tracks sharing a\ncontext, our model is capable of generating music across several tracks that\ncorrespond well to each other, either conditionally or unconditionally.\nAdditionally, our model is capable of arrangement generation, where the model\ncan generate any subset of tracks given the others (e.g., generating a piano\ntrack complementing given bass and drum tracks). We compared our model with an\nexisting multi-track generative model and demonstrated that our model achieves\nconsiderable improvements across objective metrics for both total and\narrangement generation tasks.\n","authors":["Tornike Karchkhadze","Mohammad Rasool Izadi","Ke Chen","Gerard Assayag","Shlomo Dubnov"],"pdf_url":"https://arxiv.org/pdf/2409.02845v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14485v7","updated":"2024-10-14T14:26:04Z","published":"2024-06-20T16:48:14Z","title":"Proceedings of The second international workshop on eXplainable AI for\n  the Arts (XAIxArts)","summary":"  This second international workshop on explainable AI for the Arts (XAIxArts)\nbrought together a community of researchers in HCI, Interaction Design, AI,\nexplainable AI (XAI), and digital arts to explore the role of XAI for the Arts.\nWorkshop held at the 16th ACM Conference on Creativity and Cognition (C&C\n2024), Chicago, USA.\n","authors":["Nick Bryan-Kinns","Corey Ford","Shuoyang Zheng","Helen Kennedy","Alan Chamberlain","Makayla Lewis","Drew Hemment","Zijin Li","Qiong Wu","Lanxi Xiao","Gus Xia","Jeba Rezwana","Michael Clemens","Gabriel Vigliensoni"],"pdf_url":"https://arxiv.org/pdf/2406.14485v7.pdf","comment":"Proceedings of The second international workshop on eXplainable AI\n  for the Arts (XAIxArts)"},{"id":"http://arxiv.org/abs/2410.10319v1","updated":"2024-10-14T09:25:09Z","published":"2024-10-14T09:25:09Z","title":"Spatial-Aware Efficient Projector for MLLMs via Multi-Layer Feature\n  Aggregation","summary":"  The projector plays a crucial role in multi-modal language models (MLLMs).\nThe number of visual tokens it outputs affects the efficiency of the MLLM,\nwhile the quality of the visual tokens influences the visual understanding\ncapabilities of the MLLM. Current explorations on the projector focus on\nreducing the number of visual tokens to improve efficiency, often overlooking\nthe inherent spatial discrepancy between the serialized 2-dimensional visual\ntoken sequences and natural language token sequences. A Spatial-Aware Efficient\nProjector (SAEP) is proposed to address this issue. In detail, our SAEP method\nemploys an modified separable depthwise convolution module on multi-layer\nvisual features to enhance the spatial information of visual tokens. As a\nresult, our SAEP method can not only largely reduce the number of visual tokens\nby 75\\%, but also significantly improve the multimodal spatial understanding\ncapability of MLLMs. Moreover, compared to existing projectors, our SAEP gets\nbest performances on massive multimodal evaluation benchmarks, which denotes\nits effectiveness on bridging the modality gap.\n","authors":["Shun Qian","Bingquan Liu","Chengjie Sun","Zhen Xu","Baoxun Wang"],"pdf_url":"https://arxiv.org/pdf/2410.10319v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.10291v1","updated":"2024-10-14T08:45:35Z","published":"2024-10-14T08:45:35Z","title":"Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal\n  Perspective","summary":"  Accurate interpretation and visualization of human instructions are crucial\nfor text-to-image (T2I) synthesis. However, current models struggle to capture\nsemantic variations from word order changes, and existing evaluations, relying\non indirect metrics like text-image similarity, fail to reliably assess these\nchallenges. This often obscures poor performance on complex or uncommon\nlinguistic patterns by the focus on frequent word combinations. To address\nthese deficiencies, we propose a novel metric called SemVarEffect and a\nbenchmark named SemVarBench, designed to evaluate the causality between\nsemantic variations in inputs and outputs in T2I synthesis. Semantic variations\nare achieved through two types of linguistic permutations, while avoiding\neasily predictable literal variations. Experiments reveal that the\nCogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.\nSemantic variations in object relations are less understood than attributes,\nscoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in\nUNet or Transformers plays a crucial role in handling semantic variations, a\nfactor previously overlooked by a focus on textual encoders. Our work\nestablishes an effective evaluation framework that advances the T2I synthesis\ncommunity's exploration of human instruction understanding.\n","authors":["Xiangru Zhu","Penglei Sun","Yaoxian Song","Yanghua Xiao","Zhixu Li","Chengyu Wang","Jun Huang","Bei Yang","Xiaoxiao Xu"],"pdf_url":"https://arxiv.org/pdf/2410.10291v1.pdf","comment":"Our benchmark and code are available at\n  https://github.com/zhuxiangru/SemVarBench"},{"id":"http://arxiv.org/abs/2405.07930v2","updated":"2024-10-14T08:19:13Z","published":"2024-05-13T17:01:28Z","title":"Improving Multimodal Learning with Multi-Loss Gradient Modulation","summary":"  Learning from multiple modalities, such as audio and video, offers\nopportunities for leveraging complementary information, enhancing robustness,\nand improving contextual understanding and performance. However, combining such\nmodalities presents challenges, especially when modalities differ in data\nstructure, predictive contribution, and the complexity of their learning\nprocesses. It has been observed that one modality can potentially dominate the\nlearning process, hindering the effective utilization of information from other\nmodalities and leading to sub-optimal model performance. To address this issue\nthe vast majority of previous works suggest to assess the unimodal\ncontributions and dynamically adjust the training to equalize them. We improve\nupon previous work by introducing a multi-loss objective and further refining\nthe balancing process, allowing it to dynamically adjust the learning pace of\neach modality in both directions, acceleration and deceleration, with the\nability to phase out balancing effects upon convergence. We achieve superior\nresults across three audio-video datasets: on CREMA-D, models with ResNet\nbackbone encoders surpass the previous best by 1.9% to 12.4%, and Conformer\nbackbone models deliver improvements ranging from 2.8% to 14.1% across\ndifferent fusion methods. On AVE, improvements range from 2.7% to 7.7%, while\non UCF101, gains reach up to 6.1%.\n","authors":["Konstantinos Kontras","Christos Chatzichristos","Matthew Blaschko","Maarten De Vos"],"pdf_url":"https://arxiv.org/pdf/2405.07930v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10178v1","updated":"2024-10-14T05:51:53Z","published":"2024-10-14T05:51:53Z","title":"GUISE: Graph GaUssIan Shading watErmark","summary":"  In the expanding field of generative artificial intelligence, integrating\nrobust watermarking technologies is essential to protect intellectual property\nand maintain content authenticity. Traditionally, watermarking techniques have\nbeen developed primarily for rich information media such as images and audio.\nHowever, these methods have not been adequately adapted for graph-based data,\nparticularly molecular graphs. Latent 3D graph diffusion(LDM-3DG) is an\nascendant approach in the molecular graph generation field. This model\neffectively manages the complexities of molecular structures, preserving\nessential symmetries and topological features. We adapt the Gaussian Shading, a\nproven performance lossless watermarking technique, to the latent graph\ndiffusion domain to protect this sophisticated new technology. Our adaptation\nsimplifies the watermark diffusion process through duplication and padding,\nmaking it adaptable and suitable for various message types. We conduct several\nexperiments using the LDM-3DG model on publicly available datasets QM9 and\nDrugs, to assess the robustness and effectiveness of our technique. Our results\ndemonstrate that the watermarked molecules maintain statistical parity in 9 out\nof 10 performance metrics compared to the original. Moreover, they exhibit a\n100% detection rate and a 99% extraction rate in a 2D decoded pipeline, while\nalso showing robustness against post-editing attacks.\n","authors":["Renyi Yang"],"pdf_url":"https://arxiv.org/pdf/2410.10178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13711v2","updated":"2024-10-14T01:43:38Z","published":"2024-08-25T02:56:26Z","title":"SceneDreamer360: Text-Driven 3D-Consistent Scene Generation with\n  Panoramic Gaussian Splatting","summary":"  Text-driven 3D scene generation has seen significant advancements recently.\nHowever, most existing methods generate single-view images using generative\nmodels and then stitch them together in 3D space. This independent generation\nfor each view often results in spatial inconsistency and implausibility in the\n3D scenes. To address this challenge, we proposed a novel text-driven\n3D-consistent scene generation model: SceneDreamer360. Our proposed method\nleverages a text-driven panoramic image generation model as a prior for 3D\nscene generation and employs 3D Gaussian Splatting (3DGS) to ensure consistency\nacross multi-view panoramic images. Specifically, SceneDreamer360 enhances the\nfine-tuned Panfusion generator with a three-stage panoramic enhancement,\nenabling the generation of high-resolution, detail-rich panoramic images.\nDuring the 3D scene construction, a novel point cloud fusion initialization\nmethod is used, producing higher quality and spatially consistent point clouds.\nOur extensive experiments demonstrate that compared to other methods,\nSceneDreamer360 with its panoramic image generation and 3DGS can produce higher\nquality, spatially consistent, and visually appealing 3D scenes from any text\nprompt. Our codes are available at\n\\url{https://github.com/liwrui/SceneDreamer360}.\n","authors":["Wenrui Li","Fucheng Cai","Yapeng Mi","Zhe Yang","Wangmeng Zuo","Xingtao Wang","Xiaopeng Fan"],"pdf_url":"https://arxiv.org/pdf/2408.13711v2.pdf","comment":null}]},"2024-10-13T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.09999v1","updated":"2024-10-13T20:45:00Z","published":"2024-10-13T20:45:00Z","title":"Leveraging Customer Feedback for Multi-modal Insight Extraction","summary":"  Businesses can benefit from customer feedback in different modalities, such\nas text and images, to enhance their products and services. However, it is\ndifficult to extract actionable and relevant pairs of text segments and images\nfrom customer feedback in a single pass. In this paper, we propose a novel\nmulti-modal method that fuses image and text information in a latent space and\ndecodes it to extract the relevant feedback segments using an image-text\ngrounded text decoder. We also introduce a weakly-supervised data generation\ntechnique that produces training data for this task. We evaluate our model on\nunseen data and demonstrate that it can effectively mine actionable insights\nfrom multi-modal customer feedback, outperforming the existing baselines by\n$14$ points in F1 score.\n","authors":["Sandeep Sricharan Mukku","Abinesh Kanagarajan","Pushpendu Ghosh","Chetan Aggarwal"],"pdf_url":"https://arxiv.org/pdf/2410.09999v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2410.09942v1","updated":"2024-10-13T17:53:50Z","published":"2024-10-13T17:53:50Z","title":"Learning to Rank for Multiple Retrieval-Augmented Models through\n  Iterative Utility Maximization","summary":"  This paper investigates the design of a unified search engine to serve\nmultiple retrieval-augmented generation (RAG) agents, each with a distinct\ntask, backbone large language model (LLM), and retrieval-augmentation strategy.\nWe introduce an iterative approach where the search engine generates retrieval\nresults for these RAG agents and gathers feedback on the quality of the\nretrieved documents during an offline phase. This feedback is then used to\niteratively optimize the search engine using a novel expectation-maximization\nalgorithm, with the goal of maximizing each agent's utility function.\nAdditionally, we adapt this approach to an online setting, allowing the search\nengine to refine its behavior based on real-time individual agents feedback to\nbetter serve the results for each of them. Experiments on diverse datasets from\nthe Knowledge-Intensive Language Tasks (KILT) benchmark demonstrates that our\napproach significantly on average outperforms competitive baselines across 18\nRAG models. We also demonstrate that our method effectively ``personalizes''\nthe retrieval process for each RAG agent based on the collected feedback.\nFinally, we provide a comprehensive ablation study to explore various aspects\nof our method.\n","authors":["Alireza Salemi","Hamed Zamani"],"pdf_url":"https://arxiv.org/pdf/2410.09942v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09936v1","updated":"2024-10-13T17:44:04Z","published":"2024-10-13T17:44:04Z","title":"The Role of Fake Users in Sequential Recommender Systems","summary":"  Sequential Recommender Systems (SRSs) are widely used to model user behavior\nover time, yet their robustness remains an under-explored area of research. In\nthis paper, we conduct an empirical study to assess how the presence of fake\nusers, who engage in random interactions, follow popular or unpopular items, or\nfocus on a single genre, impacts the performance of SRSs in real-world\nscenarios. We evaluate two SRS models across multiple datasets, using\nestablished metrics such as Normalized Discounted Cumulative Gain (NDCG) and\nRank Sensitivity List (RLS) to measure performance. While traditional metrics\nlike NDCG remain relatively stable, our findings reveal that the presence of\nfake users severely degrades RLS metrics, often reducing them to near-zero\nvalues. These results highlight the need for further investigation into the\neffects of fake users on training data and emphasize the importance of\ndeveloping more resilient SRSs that can withstand different types of\nadversarial attacks.\n","authors":["Filippo Betello"],"pdf_url":"https://arxiv.org/pdf/2410.09936v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.09923v1","updated":"2024-10-13T17:08:16Z","published":"2024-10-13T17:08:16Z","title":"Analysis and Design of a Personalized Recommendation System Based on a\n  Dynamic User Interest Model","summary":"  With the rapid development of the internet and the explosion of information,\nproviding users with accurate personalized recommendations has become an\nimportant research topic. This paper designs and analyzes a personalized\nrecommendation system based on a dynamic user interest model. The system\ncaptures user behavior data, constructs a dynamic user interest model, and\ncombines multiple recommendation algorithms to provide personalized content to\nusers. The research results show that this system significantly improves\nrecommendation accuracy and user satisfaction. This paper discusses the\nsystem's architecture design, algorithm implementation, and experimental\nresults in detail and explores future research directions.\n","authors":["Chunyan Mao","Shuaishuai Huang","Mingxiu Sui","Haowei Yang","Xueshe Wang"],"pdf_url":"https://arxiv.org/pdf/2410.09923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09875v1","updated":"2024-10-13T15:34:11Z","published":"2024-10-13T15:34:11Z","title":"ViFi-ReID: A Two-Stream Vision-WiFi Multimodal Approach for Person\n  Re-identification","summary":"  Person re-identification(ReID), as a crucial technology in the field of\nsecurity, plays a vital role in safety inspections, personnel counting, and\nmore. Most current ReID approaches primarily extract features from images,\nwhich are easily affected by objective conditions such as clothing changes and\nocclusions. In addition to cameras, we leverage widely available routers as\nsensing devices by capturing gait information from pedestrians through the\nChannel State Information (CSI) in WiFi signals and contribute a multimodal\ndataset. We employ a two-stream network to separately process video\nunderstanding and signal analysis tasks, and conduct multi-modal fusion and\ncontrastive learning on pedestrian video and WiFi data. Extensive experiments\nin real-world scenarios demonstrate that our method effectively uncovers the\ncorrelations between heterogeneous data, bridges the gap between visual and\nsignal modalities, significantly expands the sensing range, and improves ReID\naccuracy across multiple sensors.\n","authors":["Chen Mao","Chong Tan","Jingqi Hu","Min Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.09875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10808v2","updated":"2024-10-13T15:33:52Z","published":"2024-08-20T12:58:16Z","title":"ColBERT Retrieval and Ensemble Response Scoring for Language Model\n  Question Answering","summary":"  Domain-specific question answering remains challenging for language models,\ngiven the deep technical knowledge required to answer questions correctly. This\ndifficulty is amplified for smaller language models that cannot encode as much\ninformation in their parameters as larger models. The \"Specializing Large\nLanguage Models for Telecom Networks\" challenge aimed to enhance the\nperformance of two small language models, Phi-2 and Falcon-7B in\ntelecommunication question answering. In this paper, we present our question\nanswering systems for this challenge. Our solutions achieved leading marks of\n81.9% accuracy for Phi-2 and 57.3% for Falcon-7B. We have publicly released our\ncode and fine-tuned models.\n","authors":["Alex Gichamba","Tewodros Kederalah Idris","Brian Ebiyau","Eric Nyberg","Teruko Mitamura"],"pdf_url":"https://arxiv.org/pdf/2408.10808v2.pdf","comment":"7 pages, 2 figures, and 8 tables. This paper has been accepted at the\n  2024 IEEE Global Communications (GLOBECOM) Workshops"},{"id":"http://arxiv.org/abs/2406.16048v2","updated":"2024-10-13T15:30:32Z","published":"2024-06-23T08:24:08Z","title":"Evaluating D-MERIT of Partial-annotation on Information Retrieval","summary":"  Retrieval models are often evaluated on partially-annotated datasets. Each\nquery is mapped to a few relevant texts and the remaining corpus is assumed to\nbe irrelevant. As a result, models that successfully retrieve false negatives\nare punished in evaluation. Unfortunately, completely annotating all texts for\nevery query is not resource efficient. In this work, we show that using\npartially-annotated datasets in evaluation can paint a distorted picture. We\ncurate D-MERIT, a passage retrieval evaluation set from Wikipedia, aspiring to\ncontain all relevant passages for each query. Queries describe a group (e.g.,\n\"journals about linguistics\") and relevant passages are evidence that entities\nbelong to the group (e.g., a passage indicating that \"Language\" is a journal\nabout linguistics). We show that evaluating on a dataset containing annotations\nfor only a subset of the relevant passages might result in misleading ranking\nof the retrieval systems and that as more relevant texts are included in the\nevaluation set, the rankings converge. We propose our dataset as a resource for\nevaluation and our study as a recommendation for balance between\nresource-efficiency and reliable evaluation when annotating evaluation sets for\ntext retrieval.\n","authors":["Royi Rassin","Yaron Fairstein","Oren Kalinsky","Guy Kushilevitz","Nachshon Cohen","Alexander Libov","Yoav Goldberg"],"pdf_url":"https://arxiv.org/pdf/2406.16048v2.pdf","comment":"Accepted to EMNLP 2024 main track. Our dataset can be downloaded from\n  https://D-MERIT.github.io"},{"id":"http://arxiv.org/abs/2410.09871v1","updated":"2024-10-13T15:11:31Z","published":"2024-10-13T15:11:31Z","title":"A Comparative Study of PDF Parsing Tools Across Diverse Document\n  Categories","summary":"  PDF is one of the most prominent data formats, making PDF parsing crucial for\ninformation extraction and retrieval, particularly with the rise of RAG\nsystems. While various PDF parsing tools exist, their effectiveness across\ndifferent document types remains understudied, especially beyond academic\npapers. Our research aims to address this gap by comparing 10 popular PDF\nparsing tools across 6 document categories using the DocLayNet dataset. These\ntools include PyPDF, pdfminer.six, PyMuPDF, pdfplumber, pypdfium2,\nUnstructured, Tabula, Camelot, as well as the deep learning-based tools Nougat\nand Table Transformer(TATR). We evaluated both text extraction and table\ndetection capabilities. For text extraction, PyMuPDF and pypdfium generally\noutperformed others, but all parsers struggled with Scientific and Patent\ndocuments. For these challenging categories, learning-based tools like Nougat\ndemonstrated superior performance. In table detection, TATR excelled in the\nFinancial, Patent, Law & Regulations, and Scientific categories. Table\ndetection tool Camelot performed best for tender documents, while PyMuPDF\nperformed superior in the Manual category. Our findings highlight the\nimportance of selecting appropriate parsing tools based on document type and\nspecific tasks, providing valuable insights for researchers and practitioners\nworking with diverse document sources.\n","authors":["Narayan S. Adhikari","Shradha Agarwal"],"pdf_url":"https://arxiv.org/pdf/2410.09871v1.pdf","comment":"17 pages,11 figures, 5 tables"},{"id":"http://arxiv.org/abs/2410.09829v1","updated":"2024-10-13T13:07:31Z","published":"2024-10-13T13:07:31Z","title":"Generating Driving Simulations via Conversation","summary":"  Cyber-physical systems like autonomous vehicles are tested in simulation\nbefore deployment, using domain-specific programs for scenario specification.\nTo aid the testing of autonomous vehicles in simulation, we design a natural\nlanguage interface, using an instruction-following large language model, to\nassist a non-coding domain expert in synthesising the desired scenarios and\nvehicle behaviours. We show that using it to convert utterances to the symbolic\nprogram is feasible, despite the very small training dataset. Human experiments\nshow that dialogue is critical to successful simulation generation, leading to\na 4.5 times higher success rate than a generation without engaging in extended\nconversation.\n","authors":["Rimvydas Rubavicius","Antonio Valerio Miceli-Barone","Alex Lascarides","Subramanian Ramamoorthy"],"pdf_url":"https://arxiv.org/pdf/2410.09829v1.pdf","comment":"6 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.09781v1","updated":"2024-10-13T08:58:54Z","published":"2024-10-13T08:58:54Z","title":"ContextWIN: Whittle Index Based Mixture-of-Experts Neural Model For\n  Restless Bandits Via Deep RL","summary":"  This study introduces ContextWIN, a novel architecture that extends the\nNeural Whittle Index Network (NeurWIN) model to address Restless Multi-Armed\nBandit (RMAB) problems with a context-aware approach. By integrating a mixture\nof experts within a reinforcement learning framework, ContextWIN adeptly\nutilizes contextual information to inform decision-making in dynamic\nenvironments, particularly in recommendation systems. A key innovation is the\nmodel's ability to assign context-specific weights to a subset of NeurWIN\nnetworks, thus enhancing the efficiency and accuracy of the Whittle index\ncomputation for each arm. The paper presents a thorough exploration of\nContextWIN, from its conceptual foundation to its implementation and potential\napplications. We delve into the complexities of RMABs and the significance of\nincorporating context, highlighting how ContextWIN effectively harnesses these\nelements. The convergence of both the NeurWIN and ContextWIN models is\nrigorously proven, ensuring theoretical robustness. This work lays the\ngroundwork for future advancements in applying contextual information to\ncomplex decision-making scenarios, recognizing the need for comprehensive\ndataset exploration and environment development for full potential realization.\n","authors":["Zhanqiu Guo","Wayne Wang"],"pdf_url":"https://arxiv.org/pdf/2410.09781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09761v1","updated":"2024-10-13T07:38:44Z","published":"2024-10-13T07:38:44Z","title":"ChartKG: A Knowledge-Graph-Based Representation for Chart Images","summary":"  Chart images, such as bar charts, pie charts, and line charts, are\nexplosively produced due to the wide usage of data visualizations. Accordingly,\nknowledge mining from chart images is becoming increasingly important, which\ncan benefit downstream tasks like chart retrieval and knowledge graph\ncompletion. However, existing methods for chart knowledge mining mainly focus\non converting chart images into raw data and often ignore their visual\nencodings and semantic meanings, which can result in information loss for many\ndownstream tasks. In this paper, we propose ChartKG, a novel knowledge graph\n(KG) based representation for chart images, which can model the visual elements\nin a chart image and semantic relations among them including visual encodings\nand visual insights in a unified manner. Further, we develop a general\nframework to convert chart images to the proposed KG-based representation. It\nintegrates a series of image processing techniques to identify visual elements\nand relations, e.g., CNNs to classify charts, yolov5 and optical character\nrecognition to parse charts, and rule-based methods to construct graphs. We\npresent four cases to illustrate how our knowledge-graph-based representation\ncan model the detailed visual elements and semantic relations in charts, and\nfurther demonstrate how our approach can benefit downstream applications such\nas semantic-aware chart retrieval and chart question answering. We also conduct\nquantitative evaluations to assess the two fundamental building blocks of our\nchart-to-KG framework, i.e., object recognition and optical character\nrecognition. The results provide support for the usefulness and effectiveness\nof ChartKG.\n","authors":["Zhiguang Zhou","Haoxuan Wang","Zhengqing Zhao","Fengling Zheng","Yongheng Wang","Wei Chen","Yong Wang"],"pdf_url":"https://arxiv.org/pdf/2410.09761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11890v1","updated":"2024-10-13T07:20:47Z","published":"2024-10-13T07:20:47Z","title":"Online Digital Investigative Journalism using SociaLens","summary":"  Media companies witnessed a significant transformation with the rise of the\ninternet, bigdata, machine learning (ML) and AI. Recent emergence of large\nlanguage models (LLM) have added another aspect to this transformation.\nResearchers believe that with the help of these technologies, investigative\ndigital journalism will enter a new era. Using a smart set of data gathering\nand analysis tools, journalists will be able to create data driven contents and\ninsights in unprecedented ways. In this paper, we introduce a versatile and\nautonomous investigative journalism tool, called {\\em SociaLens}, for\nidentifying and extracting query specific data from online sources, responding\nto probing queries and drawing conclusions entailed by large volumes of data\nusing ML analytics fully autonomously. We envision its use in investigative\njournalism, law enforcement and social policy planning. The proposed system\ncapitalizes on the integration of ML technology with LLMs and advanced bigdata\nsearch techniques. We illustrate the functionality of SociaLens using a focused\ncase study on rape incidents in a developing country and demonstrate that\njournalists can gain nuanced insights without requiring coding expertise they\nmight lack. SociaLens is designed as a ChatBot that is capable of contextual\nconversation, find and collect data relevant to queries, initiate ML tasks to\nrespond to queries, generate textual and visual reports, all fully autonomously\nwithin the ChatBot environment.\n","authors":["Hasan M. Jamil","Sajratul Y. Rubaiat"],"pdf_url":"https://arxiv.org/pdf/2410.11890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04408v2","updated":"2024-10-13T07:05:51Z","published":"2024-01-09T08:04:11Z","title":"Fine-Grained Embedding Dimension Optimization During Training for\n  Recommender Systems","summary":"  Huge embedding tables in modern deep learning recommender models (DLRM)\nrequire prohibitively large memory during training and inference. This paper\nproposes FIITED, a system to automatically reduce the memory footprint via\nFIne-grained In-Training Embedding Dimension pruning. By leveraging the key\ninsight that embedding vectors are not equally important, FIITED adaptively\nadjusts the dimension of each individual embedding vector during model\ntraining, assigning larger dimensions to more important embeddings while\nadapting to dynamic changes in data. We prioritize embedding dimensions with\nhigher frequencies and gradients as more important. To enable efficient pruning\nof embeddings and their dimensions during model training, we propose an\nembedding storage system based on virtually-hashed physically-indexed hash\ntables. Experiments on two industry models and months of realistic datasets\nshow that FIITED can reduce DLRM embedding size by more than 65% while\npreserving model quality, outperforming state-of-the-art in-training embedding\npruning methods. On public datasets, FIITED can reduce the size of embedding\ntables by 2.1x to 800x with negligible accuracy drop, while improving model\nthroughput.\n","authors":["Qinyi Luo","Penghan Wang","Wei Zhang","Fan Lai","Jiachen Mao","Xiaohan Wei","Jun Song","Wei-Yu Tsai","Shuai Yang","Yuxi Hu","Xuehai Qian"],"pdf_url":"https://arxiv.org/pdf/2401.04408v2.pdf","comment":"12 pages, 15 figures"},{"id":"http://arxiv.org/abs/2310.08891v2","updated":"2024-10-13T04:49:45Z","published":"2023-10-13T06:53:02Z","title":"EHI: End-to-end Learning of Hierarchical Index for Efficient Dense\n  Retrieval","summary":"  Dense embedding-based retrieval is widely used for semantic search and\nranking. However, conventional two-stage approaches, involving contrastive\nembedding learning followed by approximate nearest neighbor search (ANNS), can\nsuffer from misalignment between these stages. This mismatch degrades retrieval\nperformance. We propose End-to-end Hierarchical Indexing (EHI), a novel method\nthat directly addresses this issue by jointly optimizing embedding generation\nand ANNS structure. EHI leverages a dual encoder for embedding queries and\ndocuments while simultaneously learning an inverted file index (IVF)-style tree\nstructure. To facilitate the effective learning of this discrete structure, EHI\nintroduces dense path embeddings that encodes the path traversed by queries and\ndocuments within the tree. Extensive evaluations on standard benchmarks,\nincluding MS MARCO (Dev set) and TREC DL19, demonstrate EHI's superiority over\ntraditional ANNS index. Under the same computational constraints, EHI\noutperforms existing state-of-the-art methods by +1.45% in MRR@10 on MS MARCO\n(Dev) and +8.2% in nDCG@10 on TREC DL19, highlighting the benefits of our\nend-to-end approach.\n","authors":["Ramnath Kumar","Anshul Mittal","Nilesh Gupta","Aditya Kusupati","Inderjit Dhillon","Prateek Jain"],"pdf_url":"https://arxiv.org/pdf/2310.08891v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09713v1","updated":"2024-10-13T03:45:24Z","published":"2024-10-13T03:45:24Z","title":"Agentic Information Retrieval","summary":"  What will information entry look like in the next generation of digital\nproducts? Since the 1970s, user access to relevant information has relied on\ndomain-specific architectures of information retrieval (IR). Over the past two\ndecades, the advent of modern IR systems, including web search engines and\npersonalized recommender systems, has greatly improved the efficiency of\nretrieving relevant information from vast data corpora. However, the core\nparadigm of these IR systems remains largely unchanged, relying on filtering a\npredefined set of candidate items. Since 2022, breakthroughs in large language\nmodels (LLMs) have begun transforming how information is accessed, establishing\na new technical paradigm. In this position paper, we introduce Agentic\nInformation Retrieval (Agentic IR), a novel IR paradigm shaped by the\ncapabilities of LLM agents. Agentic IR expands the scope of accessible tasks\nand leverages a suite of new techniques to redefine information retrieval. We\ndiscuss three types of cutting-edge applications of agentic IR and the\nchallenges faced. We propose that agentic IR holds promise for generating\ninnovative applications, potentially becoming a central information entry point\nin future digital ecosystems.\n","authors":["Weinan Zhang","Junwei Liao","Ning Li","Kounianhua Du"],"pdf_url":"https://arxiv.org/pdf/2410.09713v1.pdf","comment":"11 pages, position paper"}],"Multimedia":[{"id":"http://arxiv.org/abs/2406.04321v2","updated":"2024-10-13T17:59:22Z","published":"2024-06-06T17:58:11Z","title":"VidMuse: A Simple Video-to-Music Generation Framework with\n  Long-Short-Term Modeling","summary":"  In this work, we systematically study music generation conditioned solely on\nthe video. First, we present a large-scale dataset comprising 360K video-music\npairs, including various genres such as movie trailers, advertisements, and\ndocumentaries. Furthermore, we propose VidMuse, a simple framework for\ngenerating music aligned with video inputs. VidMuse stands out by producing\nhigh-fidelity music that is both acoustically and semantically aligned with the\nvideo. By incorporating local and global visual cues, VidMuse enables the\ncreation of musically coherent audio tracks that consistently match the video\ncontent through Long-Short-Term modeling. Through extensive experiments,\nVidMuse outperforms existing models in terms of audio quality, diversity, and\naudio-visual alignment. The code and datasets will be available at\nhttps://github.com/ZeyueT/VidMuse/.\n","authors":["Zeyue Tian","Zhaoyang Liu","Ruibin Yuan","Jiahao Pan","Qifeng Liu","Xu Tan","Qifeng Chen","Wei Xue","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2406.04321v2.pdf","comment":"The code and datasets will be available at\n  https://github.com/ZeyueT/VidMuse/"},{"id":"http://arxiv.org/abs/2410.09872v1","updated":"2024-10-13T15:13:00Z","published":"2024-10-13T15:13:00Z","title":"Towards Reproducible Learning-based Compression","summary":"  A deep learning system typically suffers from a lack of reproducibility that\nis partially rooted in hardware or software implementation details. The\nirreproducibility leads to skepticism in deep learning technologies and it can\nhinder them from being deployed in many applications. In this work, the\nirreproducibility issue is analyzed where deep learning is employed in\ncompression systems while the encoding and decoding may be run on devices from\ndifferent manufacturers. The decoding process can even crash due to a single\nbit difference, e.g., in a learning-based entropy coder. For a given deep\nlearning-based module with limited resources for protection, we first suggest\nthat reproducibility can only be assured when the mismatches are bounded. Then\na safeguarding mechanism is proposed to tackle the challenges. The proposed\nmethod may be applied for different levels of protection either at the\nreconstruction level or at a selected decoding level. Furthermore, the overhead\nintroduced for the protection can be scaled down accordingly when the error\nbound is being suppressed. Experiments demonstrate the effectiveness of the\nproposed approach for learning-based compression systems, e.g., in image\ncompression and point cloud compression.\n","authors":["Jiahao Pang","Muhammad Asad Lodhi","Junghyun Ahn","Yuning Huang","Dong Tian"],"pdf_url":"https://arxiv.org/pdf/2410.09872v1.pdf","comment":"Accepted at MMSP 2024"},{"id":"http://arxiv.org/abs/2405.13984v2","updated":"2024-10-13T14:24:49Z","published":"2024-05-22T20:40:53Z","title":"Less for More: Enhanced Feedback-aligned Mixed LLMs for Molecule Caption\n  Generation and Fine-Grained NLI Evaluation","summary":"  Scientific language models drive research innovation but require extensive\nfine-tuning on large datasets. This work enhances such models by improving\ntheir inference and evaluation capabilities with minimal or no additional\ntraining. Focusing on molecule caption generation, we explore synergies between\nalignment fine-tuning and model merging in a cross-modal setup. We reveal\nintriguing insights into the behaviour and suitability of such methods while\nsignificantly surpassing state-of-the-art models. Moreover, we propose a novel\natomic-level evaluation method leveraging off-the-shelf Natural Language\nInference (NLI) models for use in the unseen chemical domain. Our experiments\ndemonstrate that our evaluation operates at the right level of granularity,\neffectively handling multiple content units and subsentence reasoning, while\nwidely adopted NLI methods consistently misalign with assessment criteria.\n","authors":["Dimitris Gkoumas","Maria Liakata"],"pdf_url":"https://arxiv.org/pdf/2405.13984v2.pdf","comment":null}]},"2024-10-17T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.13857v1","updated":"2024-10-17T17:59:35Z","published":"2024-10-17T17:59:35Z","title":"How Numerical Precision Affects Mathematical Reasoning Capabilities of\n  LLMs","summary":"  Despite the remarkable success of Transformer-based Large Language Models\n(LLMs) across various domains, understanding and enhancing their mathematical\ncapabilities remains a significant challenge. In this paper, we conduct a\nrigorous theoretical analysis of LLMs' mathematical abilities, with a specific\nfocus on their arithmetic performances. We identify numerical precision as a\nkey factor that influences their effectiveness in mathematical tasks. Our\nresults show that Transformers operating with low numerical precision fail to\naddress arithmetic tasks, such as iterated addition and integer multiplication,\nunless the model size grows super-polynomially with respect to the input\nlength. In contrast, Transformers with standard numerical precision can\nefficiently handle these tasks with significantly smaller model sizes. We\nfurther support our theoretical findings through empirical experiments that\nexplore the impact of varying numerical precision on arithmetic tasks,\nproviding valuable insights for improving the mathematical reasoning\ncapabilities of LLMs.\n","authors":["Guhao Feng","Kai Yang","Yuntian Gu","Xinyue Ai","Shengjie Luo","Jiacheng Sun","Di He","Zhenguo Li","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13854v1","updated":"2024-10-17T17:59:24Z","published":"2024-10-17T17:59:24Z","title":"Can MLLMs Understand the Deep Implication Behind Chinese Images?","summary":"  As the capabilities of Multimodal Large Language Models (MLLMs) continue to\nimprove, the need for higher-order capability evaluation of MLLMs is\nincreasing. However, there is a lack of work evaluating MLLM for higher-order\nperception and understanding of Chinese visual content. To fill the gap, we\nintroduce the **C**hinese **I**mage **I**mplication understanding\n**Bench**mark, **CII-Bench**, which aims to assess the higher-order perception\nand understanding capabilities of MLLMs for Chinese images. CII-Bench stands\nout in several ways compared to existing benchmarks. Firstly, to ensure the\nauthenticity of the Chinese context, images in CII-Bench are sourced from the\nChinese Internet and manually reviewed, with corresponding answers also\nmanually crafted. Additionally, CII-Bench incorporates images that represent\nChinese traditional culture, such as famous Chinese traditional paintings,\nwhich can deeply reflect the model's understanding of Chinese traditional\nculture. Through extensive experiments on CII-Bench across multiple MLLMs, we\nhave made significant findings. Initially, a substantial gap is observed\nbetween the performance of MLLMs and humans on CII-Bench. The highest accuracy\nof MLLMs attains 64.4%, where as human accuracy averages 78.2%, peaking at an\nimpressive 81.0%. Subsequently, MLLMs perform worse on Chinese traditional\nculture images, suggesting limitations in their ability to understand\nhigh-level semantics and lack a deep knowledge base of Chinese traditional\nculture. Finally, it is observed that most models exhibit enhanced accuracy\nwhen image emotion hints are incorporated into the prompts. We believe that\nCII-Bench will enable MLLMs to gain a better understanding of Chinese semantics\nand Chinese-specific images, advancing the journey towards expert artificial\ngeneral intelligence (AGI). Our project is publicly available at\nhttps://cii-bench.github.io/.\n","authors":["Chenhao Zhang","Xi Feng","Yuelin Bai","Xinrun Du","Jinchang Hou","Kaixin Deng","Guangzeng Han","Qinrui Li","Bingli Wang","Jiaheng Liu","Xingwei Qu","Yifei Zhang","Qixuan Zhao","Yiming Liang","Ziqiang Liu","Feiteng Fang","Min Yang","Wenhao Huang","Chenghua Lin","Ge Zhang","Shiwen Ni"],"pdf_url":"https://arxiv.org/pdf/2410.13854v1.pdf","comment":"32 pages,18 figures. Project Page: https://cii-bench.github.io/ Code:\n  https://github.com/MING_X/CII-Bench Dataset:\n  https://huggingface.co/datasets/m-a-p/CII-Bench"},{"id":"http://arxiv.org/abs/2410.13852v1","updated":"2024-10-17T17:59:03Z","published":"2024-10-17T17:59:03Z","title":"Retrospective Learning from Interactions","summary":"  Multi-turn interactions between large language models (LLMs) and users\nnaturally include implicit feedback signals. If an LLM responds in an\nunexpected way to an instruction, the user is likely to signal it by rephrasing\nthe request, expressing frustration, or pivoting to an alternative task. Such\nsignals are task-independent and occupy a relatively constrained subspace of\nlanguage, allowing the LLM to identify them even if it fails on the actual\ntask. This creates an avenue for continually learning from interactions without\nadditional annotations. We introduce ReSpect, a method to learn from such\nsignals in past interactions via retrospection. We deploy ReSpect in a new\nmultimodal interaction scenario, where humans instruct an LLM to solve an\nabstract reasoning task with a combinatorial solution space. Through thousands\nof interactions with humans, we show how ReSpect gradually improves task\ncompletion rate from 31% to 82%, all without any external annotation.\n","authors":["Zizhao Chen","Mustafa Omer Gul","Yiwei Chen","Gloria Geng","Anne Wu","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2410.13852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08928v2","updated":"2024-10-17T17:58:53Z","published":"2024-10-11T15:53:24Z","title":"Towards Multilingual LLM Evaluation for European Languages","summary":"  The rise of Large Language Models (LLMs) has revolutionized natural language\nprocessing across numerous languages and tasks. However, evaluating LLM\nperformance in a consistent and meaningful way across multiple European\nlanguages remains challenging, especially due to the scarcity of\nlanguage-parallel multilingual benchmarks. We introduce a multilingual\nevaluation approach tailored for European languages. We employ translated\nversions of five widely-used benchmarks to assess the capabilities of 40 LLMs\nacross 21 European languages. Our contributions include examining the\neffectiveness of translated benchmarks, assessing the impact of different\ntranslation services, and offering a multilingual evaluation framework for LLMs\nthat includes newly created datasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC,\nEU20-TruthfulQA, and EU20-GSM8K. The benchmarks and results are made publicly\navailable to encourage further research in multilingual LLM evaluation.\n","authors":["Klaudia Thellmann","Bernhard Stadler","Michael Fromm","Jasper Schulze Buschhoff","Alex Jude","Fabio Barth","Johannes Leveling","Nicolas Flores-Herr","Joachim Köhler","René Jäkel","Mehdi Ali"],"pdf_url":"https://arxiv.org/pdf/2410.08928v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13848v1","updated":"2024-10-17T17:58:37Z","published":"2024-10-17T17:58:37Z","title":"Janus: Decoupling Visual Encoding for Unified Multimodal Understanding\n  and Generation","summary":"  In this paper, we introduce Janus, an autoregressive framework that unifies\nmultimodal understanding and generation. Prior research often relies on a\nsingle visual encoder for both tasks, such as Chameleon. However, due to the\ndiffering levels of information granularity required by multimodal\nunderstanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple\nvisual encoding into separate pathways, while still leveraging a single,\nunified transformer architecture for processing. The decoupling not only\nalleviates the conflict between the visual encoder's roles in understanding and\ngeneration, but also enhances the framework's flexibility. For instance, both\nthe multimodal understanding and generation components can independently select\ntheir most suitable encoding methods. Experiments show that Janus surpasses\nprevious unified model and matches or exceeds the performance of task-specific\nmodels. The simplicity, high flexibility, and effectiveness of Janus make it a\nstrong candidate for next-generation unified multimodal models.\n","authors":["Chengyue Wu","Xiaokang Chen","Zhiyu Wu","Yiyang Ma","Xingchao Liu","Zizheng Pan","Wen Liu","Zhenda Xie","Xingkai Yu","Chong Ruan","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2410.13848v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2410.13846v1","updated":"2024-10-17T17:58:14Z","published":"2024-10-17T17:58:14Z","title":"SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction","summary":"  Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV.\n","authors":["Xuan Zhang","Cunxiao Du","Chao Du","Tianyu Pang","Wei Gao","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.13846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13841v1","updated":"2024-10-17T17:56:53Z","published":"2024-10-17T17:56:53Z","title":"A Unified View of Delta Parameter Editing in Post-Trained Large-Scale\n  Models","summary":"  Post-training has emerged as a crucial paradigm for adapting large-scale\npre-trained models to various tasks, whose effects are fully reflected by delta\nparameters (i.e., the disparity between post-trained and pre-trained\nparameters). While numerous studies have explored delta parameter properties\nvia operations like pruning, quantization, low-rank approximation, and\nextrapolation, a unified framework for systematically examining these\ncharacteristics has been lacking. In this paper, we propose a novel perspective\nbased on Riemann sum approximation of the loss function to elucidate delta\nparameter editing operations. Our analysis categorizes existing methods into\nthree classes based on their post-editing performance: competitive, decreased,\nand improved, explaining how they are expressed by the Riemann sum\napproximation term and how they alter the model performance. Extensive\nexperiments on both visual and language models, including ViT, LLaMA 3, Qwen 2,\nand Mistral, corroborate our theoretical findings. Furthermore, we introduce\nextensions to existing techniques like DARE and BitDelta, highlighting their\nlimitations in leveraging the properties of delta parameters and reorganizing\nthem into general expressions to enhance the applicability and effectiveness of\ndelta parameter editing in post-trained models.\n","authors":["Qiaoyu Tang","Le Yu","Bowen Yu","Hongyu Lin","Keming Lu","Yaojie Lu","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2410.13841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13828v1","updated":"2024-10-17T17:52:01Z","published":"2024-10-17T17:52:01Z","title":"A Common Pitfall of Margin-based Language Model Alignment: Gradient\n  Entanglement","summary":"  Reinforcement Learning from Human Feedback (RLHF) has become the predominant\napproach for language model (LM) alignment. At its core, RLHF uses a\nmargin-based loss for preference optimization, specifying ideal LM behavior\nonly by the difference between preferred and dispreferred responses. In this\npaper, we identify a common pitfall of margin-based methods -- the\nunder-specification of ideal LM behavior on preferred and dispreferred\nresponses individually, which leads to two unintended consequences as the\nmargin increases: (1) The probability of dispreferred (e.g., unsafe) responses\nmay increase, resulting in potential safety alignment failures. (2) The\nprobability of preferred responses may decrease, even when those responses are\nideal. We demystify the reasons behind these problematic behaviors:\nmargin-based losses couple the change in the preferred probability to the\ngradient of the dispreferred one, and vice versa, often preventing the\npreferred probability from increasing while the dispreferred one decreases, and\nthus causing a synchronized increase or decrease in both probabilities. We term\nthis effect, inherent in margin-based objectives, gradient entanglement.\nFormally, we derive conditions for general margin-based alignment objectives\nunder which gradient entanglement becomes concerning: the inner product of the\ngradients of preferred and dispreferred log-probabilities is large relative to\nthe individual gradient norms. We theoretically investigate why such inner\nproducts can be large when aligning language models and empirically validate\nour findings. Empirical implications of our framework extend to explaining\nimportant differences in the training dynamics of various preference\noptimization algorithms, and suggesting potential algorithm designs to mitigate\nthe under-specification issue of margin-based methods and thereby improving\nlanguage model alignment.\n","authors":["Hui Yuan","Yifan Zeng","Yue Wu","Huazheng Wang","Mengdi Wang","Liu Leqi"],"pdf_url":"https://arxiv.org/pdf/2410.13828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16833v2","updated":"2024-10-17T17:51:19Z","published":"2024-07-23T20:51:52Z","title":"Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive\n  Study and Hybrid Approach","summary":"  Retrieval Augmented Generation (RAG) has been a powerful tool for Large\nLanguage Models (LLMs) to efficiently process overly lengthy contexts. However,\nrecent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to\nunderstand long contexts directly. We conduct a comprehensive comparison\nbetween RAG and long-context (LC) LLMs, aiming to leverage the strengths of\nboth. We benchmark RAG and LC across various public datasets using three latest\nLLMs. Results reveal that when resourced sufficiently, LC consistently\noutperforms RAG in terms of average performance. However, RAG's significantly\nlower cost remains a distinct advantage. Based on this observation, we propose\nSelf-Route, a simple yet effective method that routes queries to RAG or LC\nbased on model self-reflection. Self-Route significantly reduces the\ncomputation cost while maintaining a comparable performance to LC. Our findings\nprovide a guideline for long-context applications of LLMs using RAG and LC.\n","authors":["Zhuowan Li","Cheng Li","Mingyang Zhang","Qiaozhu Mei","Michael Bendersky"],"pdf_url":"https://arxiv.org/pdf/2407.16833v2.pdf","comment":"Accepted to EMNLP 2024 industry track"},{"id":"http://arxiv.org/abs/2410.13825v1","updated":"2024-10-17T17:50:38Z","published":"2024-10-17T17:50:38Z","title":"AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents","summary":"  Autonomy via agents using large language models (LLMs) for personalized,\nstandardized tasks boosts human efficiency. Automating web tasks (like booking\nhotels within a budget) is increasingly sought after. Fulfilling practical\nneeds, the web agent also serves as an important proof-of-concept example for\nvarious agent grounding scenarios, with its success promising advancements in\nmany future applications. Prior research often handcrafts web agent strategies\n(e.g., prompting templates, multi-agent systems, search methods, etc.) and the\ncorresponding in-context examples, which may not generalize well across all\nreal-world scenarios. On the other hand, there has been limited study on the\nmisalignment between a web agent's observation/action representation and the\npre-training data of the LLM it's based on. This discrepancy is especially\nnotable when LLMs are primarily trained for language completion rather than\ntasks involving embodied navigation actions and symbolic web elements. Our\nstudy enhances an LLM-based web agent by simply refining its observation and\naction space to better align with the LLM's capabilities. This approach enables\nour base agent to significantly outperform previous methods on a wide variety\nof web tasks. Specifically, on WebArena, a benchmark featuring general-purpose\nweb interaction tasks, our agent AgentOccam surpasses the previous\nstate-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute\npoints respectively, and boosts the success rate by 26.6 points (+161%) over\nsimilar plain web agents with its observation and action space alignment. We\nachieve this without using in-context examples, new agent roles, online\nfeedback or search strategies. AgentOccam's simple design highlights LLMs'\nimpressive zero-shot performance on web tasks, and underlines the critical role\nof carefully tuning observation and action spaces for LLM-based agents.\n","authors":["Ke Yang","Yao Liu","Sapana Chaudhary","Rasool Fakoor","Pratik Chaudhari","George Karypis","Huzefa Rangwala"],"pdf_url":"https://arxiv.org/pdf/2410.13825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13824v1","updated":"2024-10-17T17:48:54Z","published":"2024-10-17T17:48:54Z","title":"Harnessing Webpage UIs for Text-Rich Visual Understanding","summary":"  Text-rich visual understanding-the ability to process environments where\ndense textual content is integrated with visuals-is crucial for multimodal\nlarge language models (MLLMs) to interact effectively with structured\nenvironments. To enhance this capability, we propose synthesizing general\nmultimodal instructions from webpage UIs using text-based large language models\n(LLMs). Despite lacking direct visual input, text-based LLMs are able to\nprocess structured text representations from webpage accessibility trees. These\ninstructions are then paired with UI screenshots to train multimodal models. We\nintroduce MultiUI, a dataset containing 7.3 million samples from 1 million\nwebsites, covering diverse multimodal tasks and UI layouts. Models trained on\nMultiUI not only excel in web UI tasks-achieving up to a 48\\% improvement on\nVisualWebBench and a 19.1\\% boost in action accuracy on a web agent dataset\nMind2Web-but also generalize surprisingly well to non-web UI tasks and even to\nnon-UI domains, such as document understanding, OCR, and chart interpretation.\nThese results highlight the broad applicability of web UI data for advancing\ntext-rich visual understanding across various scenarios.\n","authors":["Junpeng Liu","Tianyue Ou","Yifan Song","Yuxiao Qu","Wai Lam","Chenyan Xiong","Wenhu Chen","Graham Neubig","Xiang Yue"],"pdf_url":"https://arxiv.org/pdf/2410.13824v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11018v3","updated":"2024-10-17T17:45:09Z","published":"2024-04-17T02:49:26Z","title":"Many-Shot In-Context Learning","summary":"  Large language models (LLMs) excel at few-shot in-context learning (ICL) --\nlearning from a few examples provided in context at inference, without any\nweight updates. Newly expanded context windows allow us to investigate ICL with\nhundreds or thousands of examples -- the many-shot regime. Going from few-shot\nto many-shot, we observe significant performance gains across a wide variety of\ngenerative and discriminative tasks. While promising, many-shot ICL can be\nbottlenecked by the available amount of human-generated examples. To mitigate\nthis limitation, we explore two new settings: Reinforced and Unsupervised ICL.\nReinforced ICL uses model-generated chain-of-thought rationales in place of\nhuman examples. Unsupervised ICL removes rationales from the prompt altogether,\nand prompts the model only with domain-specific questions. We find that both\nReinforced and Unsupervised ICL can be quite effective in the many-shot regime,\nparticularly on complex reasoning tasks. Finally, we demonstrate that, unlike\nfew-shot learning, many-shot learning is effective at overriding pretraining\nbiases, can learn high-dimensional functions with numerical inputs, and\nperforms comparably to fine-tuning. We also find that inference cost increases\nlinearly in the many-shot regime, and frontier LLMs benefit from many-shot ICL\nto varying degrees. Our analysis also reveals the limitations of next-token\nprediction loss as an indicator of downstream ICL performance.\n","authors":["Rishabh Agarwal","Avi Singh","Lei M. Zhang","Bernd Bohnet","Luis Rosias","Stephanie Chan","Biao Zhang","Ankesh Anand","Zaheer Abbas","Azade Nova","John D. Co-Reyes","Eric Chu","Feryal Behbahani","Aleksandra Faust","Hugo Larochelle"],"pdf_url":"https://arxiv.org/pdf/2404.11018v3.pdf","comment":"NeurIPS (Spotlight)"},{"id":"http://arxiv.org/abs/2410.13808v1","updated":"2024-10-17T17:42:10Z","published":"2024-10-17T17:42:10Z","title":"De-mark: Watermark Removal in Large Language Models","summary":"  Watermarking techniques offer a promising way to identify machine-generated\ncontent via embedding covert information into the contents generated from\nlanguage models (LMs). However, the robustness of the watermarking schemes has\nnot been well explored. In this paper, we present De-mark, an advanced\nframework designed to remove n-gram-based watermarks effectively. Our method\nutilizes a novel querying strategy, termed random selection probing, which aids\nin assessing the strength of the watermark and identifying the red-green list\nwithin the n-gram watermark. Experiments on popular LMs, such as Llama3 and\nChatGPT, demonstrate the efficiency and effectiveness of De-mark in watermark\nremoval and exploitation tasks.\n","authors":["Ruibo Chen","Yihan Wu","Junfeng Guo","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2410.13808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13805v1","updated":"2024-10-17T17:41:28Z","published":"2024-10-17T17:41:28Z","title":"A Watermark for Order-Agnostic Language Models","summary":"  Statistical watermarking techniques are well-established for sequentially\ndecoded language models (LMs). However, these techniques cannot be directly\napplied to order-agnostic LMs, as the tokens in order-agnostic LMs are not\ngenerated sequentially. In this work, we introduce Pattern-mark, a\npattern-based watermarking framework specifically designed for order-agnostic\nLMs. We develop a Markov-chain-based watermark generator that produces\nwatermark key sequences with high-frequency key patterns. Correspondingly, we\npropose a statistical pattern-based detection algorithm that recovers the key\nsequence during detection and conducts statistical tests based on the count of\nhigh-frequency patterns. Our extensive evaluations on order-agnostic LMs, such\nas ProteinMPNN and CMLM, demonstrate Pattern-mark's enhanced detection\nefficiency, generation quality, and robustness, positioning it as a superior\nwatermarking technique for order-agnostic LMs.\n","authors":["Ruibo Chen","Yihan Wu","Yanshuo Chen","Chenxi Liu","Junfeng Guo","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2410.13805v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13804v1","updated":"2024-10-17T17:41:15Z","published":"2024-10-17T17:41:15Z","title":"BenTo: Benchmark Task Reduction with In-Context Transferability","summary":"  Evaluating large language models (LLMs) is costly: it requires the generation\nand examination of LLM outputs on a large-scale benchmark of various tasks.\nThis paper investigates how to efficiently reduce the tasks used to benchmark\nLLMs without affecting the evaluation quality. Our study reveals that task\ntransferability and relevance provide critical information to identify the most\nrepresentative subset of tasks via optimizing a facility location function. We\npropose a practically efficient metric for estimating the transferability\nbetween two tasks via in-context learning (ICL). By analyzing the pairwise\ntransferability, we can reduce tasks in a modern LLM benchmark (e.g., MMLU or\nFLAN) to 5% while inducing only a <4% difference to the evaluation on the\noriginal benchmark. Compared to prior works, our method is training-free,\ngradient-free, and highly efficient requiring ICL only.\n","authors":["Hongyu Zhao","Ming Li","Lichao Sun","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.13804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14180v2","updated":"2024-10-17T17:38:00Z","published":"2023-12-19T00:36:53Z","title":"Dynamic Topic Language Model on Heterogeneous Children's Mental Health\n  Clinical Notes","summary":"  Mental health diseases affect children's lives and well-beings which have\nreceived increased attention since the COVID-19 pandemic. Analyzing psychiatric\nclinical notes with topic models is critical to evaluating children's mental\nstatus over time. However, few topic models are built for longitudinal\nsettings, and most existing approaches fail to capture temporal trajectories\nfor each document. To address these challenges, we develop a dynamic topic\nmodel with consistent topics and individualized temporal dependencies on the\nevolving document metadata. Our model preserves the semantic meaning of\ndiscovered topics over time and incorporates heterogeneity among documents. In\nparticular, when documents can be categorized, we propose a classifier-free\napproach to maximize topic heterogeneity across different document groups. We\nalso present an efficient variational optimization procedure adapted for the\nmultistage longitudinal setting. In this case study, we apply our method to the\npsychiatric clinical notes from a large tertiary pediatric hospital in Southern\nCalifornia and achieve a 38% increase in the overall coherence of extracted\ntopics. Our real data analysis reveals that children tend to express more\nnegative emotions during state shutdowns and more positive when schools reopen.\nFurthermore, it suggests that sexual and gender minority (SGM) children display\nmore pronounced reactions to major COVID-19 events and a greater sensitivity to\nvaccine-related news than non-SGM children. This study examines children's\nmental health progression during the pandemic and offers clinicians valuable\ninsights to recognize disparities in children's mental health related to their\nsexual and gender identities.\n","authors":["Hanwen Ye","Tatiana Moreno","Adrianne Alpern","Louis Ehwerhemuepha","Annie Qu"],"pdf_url":"https://arxiv.org/pdf/2312.14180v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09013v2","updated":"2024-10-17T17:30:52Z","published":"2024-10-11T17:30:02Z","title":"The Impact of Visual Information in Chinese Characters: Evaluating Large\n  Models' Ability to Recognize and Utilize Radicals","summary":"  The glyphic writing system of Chinese incorporates information-rich visual\nfeatures in each character, such as radicals that provide hints about meaning\nor pronunciation. However, there has been no investigation into whether\ncontemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) can\nharness these sub-character features in Chinese through prompting. In this\nstudy, we establish a benchmark to evaluate LLMs' and VLMs' understanding of\nvisual elements in Chinese characters, including radicals, composition\nstructures, strokes, and stroke counts. Our results reveal that models\nsurprisingly exhibit some, but still limited, knowledge of the visual\ninformation, regardless of whether images of characters are provided. To incite\nmodels' ability to use radicals, we further experiment with incorporating\nradicals into the prompts for Chinese language processing (CLP) tasks. We\nobserve consistent improvement in Part-Of-Speech tagging when providing\nadditional information about radicals, suggesting the potential to enhance CLP\nby integrating sub-character information.\n","authors":["Xiaofeng Wu","Karl Stratos","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2410.09013v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13788v1","updated":"2024-10-17T17:29:04Z","published":"2024-10-17T17:29:04Z","title":"Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying\n  Questions","summary":"  Large language models (LLMs) must often respond to highly ambiguous user\nrequests. In such cases, the LLM's best response may be to ask a clarifying\nquestion to elicit more information. We observe existing LLMs often respond by\npresupposing a single interpretation of such ambiguous requests, frustrating\nusers who intended a different interpretation. We speculate this is caused by\ncurrent preference data labeling practice, where LLM responses are evaluated\nonly on their prior contexts. To address this, we propose to assign preference\nlabels by simulating their expected outcomes in the future turns. This allows\nLLMs to learn to ask clarifying questions when it can generate responses that\nare tailored to each user interpretation in future turns. In experiments on\nopen-domain QA, we compare systems that trained using our proposed preference\nlabeling methods against standard methods, which assign preferences based on\nonly prior context. We evaluate systems based on their ability to ask\nclarifying questions that can recover each user's interpretation and expected\nanswer, and find that our training with our proposed method trains LLMs to ask\nclarifying questions with a 5% improvement in F1 measured against the answer\nset from different interpretations of each query\n","authors":["Michael J. Q. Zhang","W. Bradley Knox","Eunsol Choi"],"pdf_url":"https://arxiv.org/pdf/2410.13788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13787v1","updated":"2024-10-17T17:24:10Z","published":"2024-10-17T17:24:10Z","title":"Looking Inward: Language Models Can Learn About Themselves by\n  Introspection","summary":"  Humans acquire knowledge by observing the external world, but also by\nintrospection. Introspection gives a person privileged access to their current\nstate of mind (e.g., thoughts and feelings) that is not accessible to external\nobservers. Can LLMs introspect? We define introspection as acquiring knowledge\nthat is not contained in or derived from training data but instead originates\nfrom internal states. Such a capability could enhance model interpretability.\nInstead of painstakingly analyzing a model's internal workings, we could simply\nask the model about its beliefs, world models, and goals. More speculatively,\nan introspective model might self-report on whether it possesses certain\ninternal states such as subjective feelings or desires and this could inform us\nabout the moral status of these states. Such self-reports would not be entirely\ndictated by the model's training data.\n  We study introspection by finetuning LLMs to predict properties of their own\nbehavior in hypothetical scenarios. For example, \"Given the input P, would your\noutput favor the short- or long-term option?\" If a model M1 can introspect, it\nshould outperform a different model M2 in predicting M1's behavior even if M2\nis trained on M1's ground-truth behavior. The idea is that M1 has privileged\naccess to its own behavioral tendencies, and this enables it to predict itself\nbetter than M2 (even if M2 is generally stronger).\n  In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to\npredict itself), we find that the model M1 outperforms M2 in predicting itself,\nproviding evidence for introspection. Notably, M1 continues to predict its\nbehavior accurately even after we intentionally modify its ground-truth\nbehavior. However, while we successfully elicit introspection on simple tasks,\nwe are unsuccessful on more complex tasks or those requiring\nout-of-distribution generalization.\n","authors":["Felix J Binder","James Chua","Tomek Korbak","Henry Sleight","John Hughes","Robert Long","Ethan Perez","Miles Turpin","Owain Evans"],"pdf_url":"https://arxiv.org/pdf/2410.13787v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.13785v1","updated":"2024-10-17T17:22:05Z","published":"2024-10-17T17:22:05Z","title":"PopAlign: Diversifying Contrasting Patterns for a More Comprehensive\n  Alignment","summary":"  Alignment of large language models (LLMs) involves training models on\npreference-contrastive output pairs to adjust their responses according to\nhuman preferences. To obtain such contrastive pairs, traditional methods like\nRLHF and RLAIF rely on limited contrasting patterns, such as varying model\nvariants or decoding temperatures. This singularity leads to two issues: (1)\nalignment is not comprehensive; and thereby (2) models are susceptible to\njailbreaking attacks. To address these issues, we investigate how to construct\nmore comprehensive and diversified contrasting patterns to enhance preference\ndata (RQ1) and verify the impact of the diversification of contrasting patterns\non model alignment (RQ2). For RQ1, we propose PopAlign, a framework that\nintegrates diversified contrasting patterns across the prompt, model, and\npipeline levels, introducing six contrasting strategies that do not require\nadditional feedback labeling procedures. Regarding RQ2, we conduct thorough\nexperiments demonstrating that PopAlign significantly outperforms existing\nmethods, leading to more comprehensive alignment.\n","authors":["Zekun Moore Wang","Shawn Wang","Kang Zhu","Jiaheng Liu","Ke Xu","Jie Fu","Wangchunshu Zhou","Wenhao Huang"],"pdf_url":"https://arxiv.org/pdf/2410.13785v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2410.13783v1","updated":"2024-10-17T17:20:40Z","published":"2024-10-17T17:20:40Z","title":"Quantity vs. Quality of Monolingual Source Data in Automatic Text\n  Translation: Can It Be Too Little If It Is Too Good?","summary":"  Monolingual data, being readily available in large quantities, has been used\nto upscale the scarcely available parallel data to train better models for\nautomatic translation. Self-learning, where a model is made to learn from its\noutput, is one approach to exploit such data. However, it has been shown that\ntoo much of this data can be detrimental to the performance of the model if the\navailable parallel data is comparatively extremely low. In this study, we\ninvestigate whether the monolingual data can also be too little and if this\nreduction, based on quality, has any effect on the performance of the\ntranslation model. Experiments have shown that on English-German low-resource\nNMT, it is often better to select only the most useful additional data, based\non quality or closeness to the domain of the test data, than utilizing all of\nthe available data.\n","authors":["Idris Abdulmumin","Bashir Shehu Galadanci","Garba Aliyu","Shamsuddeen Hassan Muhammad"],"pdf_url":"https://arxiv.org/pdf/2410.13783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13780v1","updated":"2024-10-17T17:19:48Z","published":"2024-10-17T17:19:48Z","title":"Optimal Quantization for Matrix Multiplication","summary":"  Recent work in machine learning community proposed multiple methods for\nperforming lossy compression (quantization) of large matrices. This\nquantization is important for accelerating matrix multiplication (main\ncomponent of large language models), which is often bottlenecked by the speed\nof loading these matrices from memory. Unlike classical vector quantization and\nrate-distortion theory, the goal of these new compression algorithms is to be\nable to approximate not the matrices themselves, but their matrix product.\nSpecifically, given a pair of real matrices $A,B$ an encoder (compressor) is\napplied to each of them independently producing descriptions with $R$ bits per\nentry. These representations subsequently are used by the decoder to estimate\nmatrix product $A^\\top B$. In this work, we provide a non-asymptotic lower\nbound on the mean squared error of this approximation (as a function of rate\n$R$) for the case of matrices $A,B$ with iid Gaussian entries. Algorithmically,\nwe construct a universal quantizer based on nested lattices with an explicit\nguarantee of approximation error for any (non-random) pair of matrices $A$, $B$\nin terms of only Frobenius norms $\\|A\\|_F, \\|B\\|_F$ and $\\|A^\\top B\\|_F$. For\niid Gaussian matrices our quantizer achieves the lower bound and is, thus,\nasymptotically optimal. A practical low-complexity version of our quantizer\nachieves performance quite close to optimal. In information-theoretic terms we\nderive rate-distortion function for matrix multiplication of iid Gaussian\nmatrices.\n","authors":["Or Ordentlich","Yury Polyanskiy"],"pdf_url":"https://arxiv.org/pdf/2410.13780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20967v2","updated":"2024-10-17T17:19:32Z","published":"2024-05-31T16:14:06Z","title":"Superlatives in Context: Modeling the Implicit Semantics of Superlatives","summary":"  Superlatives are used to single out elements with a maximal/minimal property.\nSemantically, superlatives perform a set comparison: something (or some things)\nhas the min/max property out of a set. As such, superlatives provide an ideal\nphenomenon for studying implicit phenomena and discourse restrictions. While\nthis comparison set is often not explicitly defined, its (implicit)\nrestrictions can be inferred from the discourse context the expression appears\nin. In this work we provide an extensive computational study on the semantics\nof superlatives. We propose a unified account of superlative semantics which\nallows us to derive a broad-coverage annotation schema. Using this unified\nschema we annotated a multi-domain dataset of superlatives and their semantic\ninterpretations. We specifically focus on interpreting implicit or ambiguous\nsuperlative expressions, by analyzing how the discourse context restricts the\nset of interpretations. In a set of experiments we then analyze how well models\nperform at variations of predicting superlative semantics, with and without\ncontext. We show that the fine-grained semantics of superlatives in context can\nbe challenging for contemporary models, including GPT-4.\n","authors":["Valentina Pyatkin","Bonnie Webber","Ido Dagan","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2405.20967v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2410.13779v1","updated":"2024-10-17T17:18:30Z","published":"2024-10-17T17:18:30Z","title":"The Mystery of the Pathological Path-star Task for Language Models","summary":"  The recently introduced path-star task is a minimal task designed to\nexemplify limitations to the abilities of language models (Bachmann and\nNagarajan, 2024). It involves a path-star graph where multiple arms radiate\nfrom a single starting node and each node is unique. Given the start node and a\nspecified target node that ends an arm, the task is to generate the arm\ncontaining that target node. This is straightforward for a human but\nsurprisingly difficult for language models, which did not outperform the random\nbaseline. The authors hypothesized this is due to a deficiency in\nteacher-forcing and the next-token prediction paradigm.\n  We demonstrate the task is learnable using teacher-forcing in alternative\nsettings and that the issue is partially due to representation. We introduce a\nregularization method using structured samples of the same graph but with\ndiffering target nodes, improving results across a variety of model types. We\nprovide RASP proofs showing the task is theoretically solvable. Finally, we\nfind settings where an encoder-only model can consistently solve the task.\n","authors":["Arvid Frydenlund"],"pdf_url":"https://arxiv.org/pdf/2410.13779v1.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.13776v1","updated":"2024-10-17T17:16:00Z","published":"2024-10-17T17:16:00Z","title":"Aggregation Artifacts in Subjective Tasks Collapse Large Language\n  Models' Posteriors","summary":"  In-context Learning (ICL) has become the primary method for performing\nnatural language tasks with Large Language Models (LLMs). The knowledge\nacquired during pre-training is crucial for this few-shot capability, providing\nthe model with task priors. However, recent studies have shown that ICL\npredominantly relies on retrieving task priors rather than \"learning\" to\nperform tasks. This limitation is particularly evident in complex subjective\ndomains such as emotion and morality, where priors significantly influence\nposterior predictions. In this work, we examine whether this is the result of\nthe aggregation used in corresponding datasets, where trying to combine\nlow-agreement, disparate annotations might lead to annotation artifacts that\ncreate detrimental noise in the prompt. Moreover, we evaluate the posterior\nbias towards certain annotators by grounding our study in appropriate,\nquantitative measures of LLM priors. Our results indicate that aggregation is a\nconfounding factor in the modeling of subjective tasks, and advocate focusing\non modeling individuals instead. However, aggregation does not explain the\nentire gap between ICL and the state of the art, meaning other factors in such\ntasks also account for the observed phenomena. Finally, by rigorously studying\nannotator-level labels, we find that it is possible for minority annotators to\nboth better align with LLMs and have their perspectives further amplified.\n","authors":["Georgios Chochlakis","Alexandros Potamianos","Kristina Lerman","Shrikanth Narayanan"],"pdf_url":"https://arxiv.org/pdf/2410.13776v1.pdf","comment":"12 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.13765v1","updated":"2024-10-17T17:03:23Z","published":"2024-10-17T17:03:23Z","title":"Knowledge-Aware Query Expansion with Large Language Models for Textual\n  and Relational Retrieval","summary":"  Large language models (LLMs) have been used to generate query expansions\naugmenting original queries for improving information search. Recent studies\nalso explore providing LLMs with initial retrieval results to generate query\nexpansions more grounded to document corpus. However, these methods mostly\nfocus on enhancing textual similarities between search queries and target\ndocuments, overlooking document relations. For queries like \"Find me a highly\nrated camera for wildlife photography compatible with my Nikon F-Mount lenses\",\nexisting methods may generate expansions that are semantically similar but\nstructurally unrelated to user intents. To handle such semi-structured queries\nwith both textual and relational requirements, in this paper we propose a\nknowledge-aware query expansion framework, augmenting LLMs with structured\ndocument relations from knowledge graph (KG). To further address the limitation\nof entity-based scoring in existing KG-based methods, we leverage document\ntexts as rich KG node representations and use document-based relation filtering\nfor our Knowledge-Aware Retrieval (KAR). Extensive experiments on three\ndatasets of diverse domains show the advantages of our method compared against\nstate-of-the-art baselines on textual and relational semi-structured retrieval.\n","authors":["Yu Xia","Junda Wu","Sungchul Kim","Tong Yu","Ryan A. Rossi","Haoliang Wang","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2410.13765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06173v3","updated":"2024-10-17T17:02:02Z","published":"2024-09-10T03:06:17Z","title":"Larger Language Models Don't Care How You Think: Why Chain-of-Thought\n  Prompting Fails in Subjective Tasks","summary":"  In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the\ndominant technique for performing natural language tasks, as it does not\nrequire updating the model parameters with gradient-based methods. ICL promises\nto \"adapt\" the LLM to perform the present task at a competitive or\nstate-of-the-art level at a fraction of the computational cost. ICL can be\naugmented by incorporating the reasoning process to arrive at the final label\nexplicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.\nHowever, recent work has found that ICL relies mostly on the retrieval of task\npriors and less so on \"learning\" to perform tasks, especially for complex\nsubjective domains like emotion and morality, where priors ossify posterior\npredictions. In this work, we examine whether \"enabling\" reasoning also creates\nthe same behavior in LLMs, wherein the format of CoT retrieves reasoning priors\nthat remain relatively unchanged despite the evidence in the prompt. We find\nthat, surprisingly, CoT indeed suffers from the same posterior collapse as ICL\nfor larger language models. Code is avalaible at\nhttps://github.com/gchochla/cot-priors.\n","authors":["Georgios Chochlakis","Niyantha Maruthu Pandiyan","Kristina Lerman","Shrikanth Narayanan"],"pdf_url":"https://arxiv.org/pdf/2409.06173v3.pdf","comment":"5 pages, 2 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:2403.17125"},{"id":"http://arxiv.org/abs/2409.13057v2","updated":"2024-10-17T16:56:34Z","published":"2024-09-19T19:14:50Z","title":"Natural Language Processing Methods for the Study of Protein-Ligand\n  Interactions","summary":"  Recent advances in Natural Language Processing (NLP) have ignited interest in\ndeveloping effective methods for predicting protein-ligand interactions (PLIs)\ngiven their relevance to drug discovery and protein engineering efforts and the\never-growing volume of biochemical sequence and structural data available. The\nparallels between human languages and the \"languages\" used to represent\nproteins and ligands have enabled the use of NLP machine learning approaches to\nadvance PLI studies. In this review, we explain where and how such approaches\nhave been applied in the recent literature and discuss useful mechanisms such\nas long short-term memory, transformers, and attention. We conclude with a\ndiscussion of the current limitations of NLP methods for the study of PLIs as\nwell as key challenges that need to be addressed in future work.\n","authors":["James Michels","Ramya Bandarupalli","Amin Ahangar Akbari","Thai Le","Hong Xiao","Jing Li","Erik F. Y. Hom"],"pdf_url":"https://arxiv.org/pdf/2409.13057v2.pdf","comment":"52 Pages and 3 Figures"},{"id":"http://arxiv.org/abs/2410.13757v1","updated":"2024-10-17T16:53:50Z","published":"2024-10-17T16:53:50Z","title":"MobA: A Two-Level Agent System for Efficient Mobile Task Automation","summary":"  Current mobile assistants are limited by dependence on system APIs or\nstruggle with complex user instructions and diverse interfaces due to\nrestricted comprehension and decision-making abilities. To address these\nchallenges, we propose MobA, a novel Mobile phone Agent powered by multimodal\nlarge language models that enhances comprehension and planning capabilities\nthrough a sophisticated two-level agent architecture. The high-level Global\nAgent (GA) is responsible for understanding user commands, tracking history\nmemories, and planning tasks. The low-level Local Agent (LA) predicts detailed\nactions in the form of function calls, guided by sub-tasks and memory from the\nGA. Integrating a Reflection Module allows for efficient task completion and\nenables the system to handle previously unseen complex tasks. MobA demonstrates\nsignificant improvements in task execution efficiency and completion rate in\nreal-life evaluations, underscoring the potential of MLLM-empowered mobile\nassistants.\n","authors":["Zichen Zhu","Hao Tang","Yansi Li","Kunyao Lan","Yixuan Jiang","Hao Zhou","Yixiao Wang","Situo Zhang","Liangtai Sun","Lu Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2410.13757v1.pdf","comment":"27 pages, 6 figures, and 5 tables. We will release our source code in\n  a few days"},{"id":"http://arxiv.org/abs/2406.19465v2","updated":"2024-10-17T16:53:12Z","published":"2024-06-27T18:07:40Z","title":"Can Large Language Models Generate High-quality Patent Claims?","summary":"  Large language models (LLMs) have shown exceptional performance across\nvarious text generation tasks but remain under-explored in the patent domain,\nwhich offers highly structured and precise language. This paper constructs a\ndataset to investigate the performance of current LLMs in patent claim\ngeneration. Our results demonstrate that generating claims based on patent\ndescriptions outperforms previous research relying on abstracts. Interestingly,\ncurrent patent-specific LLMs perform much worse than state-of-the-art general\nLLMs, highlighting the necessity for future research on in-domain LLMs. We also\nfind that LLMs can produce high-quality first independent claims, but their\nperformances markedly decrease for subsequent dependent claims. Moreover,\nfine-tuning can enhance the completeness of inventions' features, conceptual\nclarity, and feature linkage. Among the tested LLMs, GPT-4 demonstrates the\nbest performance in comprehensive human evaluations by patent experts, with\nbetter feature coverage, conceptual clarity, and technical coherence. Despite\nthese capabilities, comprehensive revision and modification are still necessary\nto pass rigorous patent scrutiny and ensure legal robustness.\n","authors":["Lekang Jiang","Caiqi Zhang","Pascal A Scherz","Stephan Goetz"],"pdf_url":"https://arxiv.org/pdf/2406.19465v2.pdf","comment":"16 pages, 2 figures, 12 tables"},{"id":"http://arxiv.org/abs/2410.13727v1","updated":"2024-10-17T16:33:01Z","published":"2024-10-17T16:33:01Z","title":"LLM-Human Pipeline for Cultural Context Grounding of Conversations","summary":"  Conversations often adhere to well-understood social norms that vary across\ncultures. For example, while \"addressing parents by name\" is commonplace in the\nWest, it is rare in most Asian cultures. Adherence or violation of such norms\noften dictates the tenor of conversations. Humans are able to navigate social\nsituations requiring cultural awareness quite adeptly. However, it is a hard\ntask for NLP models.\n  In this paper, we tackle this problem by introducing a \"Cultural Context\nSchema\" for conversations. It comprises (1) conversational information such as\nemotions, dialogue acts, etc., and (2) cultural information such as social\nnorms, violations, etc. We generate ~110k social norm and violation\ndescriptions for ~23k conversations from Chinese culture using LLMs. We refine\nthem using automated verification strategies which are evaluated against\nculturally aware human judgements. We organize these descriptions into\nmeaningful structures we call \"Norm Concepts\", using an interactive\nhuman-in-loop framework. We ground the norm concepts and the descriptions in\nconversations using symbolic annotation. Finally, we use the obtained dataset\nfor downstream tasks such as emotion, sentiment, and dialogue act detection. We\nshow that it significantly improves the empirical performance.\n","authors":["Rajkumar Pujari","Dan Goldwasser"],"pdf_url":"https://arxiv.org/pdf/2410.13727v1.pdf","comment":"19 pages, 9 figures, 7 tables"},{"id":"http://arxiv.org/abs/2406.14462v2","updated":"2024-10-17T16:32:46Z","published":"2024-06-20T16:24:07Z","title":"Modeling Human Subjectivity in LLMs Using Explicit and Implicit Human\n  Factors in Personas","summary":"  Large language models (LLMs) are increasingly being used in human-centered\nsocial scientific tasks, such as data annotation, synthetic data creation, and\nengaging in dialog. However, these tasks are highly subjective and dependent on\nhuman factors, such as one's environment, attitudes, beliefs, and lived\nexperiences. Thus, it may be the case that employing LLMs (which do not have\nsuch human factors) in these tasks results in a lack of variation in data,\nfailing to reflect the diversity of human experiences. In this paper, we\nexamine the role of prompting LLMs with human-like personas and asking the\nmodels to answer as if they were a specific human. This is done explicitly,\nwith exact demographics, political beliefs, and lived experiences, or\nimplicitly via names prevalent in specific populations. The LLM personas are\nthen evaluated via (1) subjective annotation task (e.g., detecting toxicity)\nand (2) a belief generation task, where both tasks are known to vary across\nhuman factors. We examine the impact of explicit vs. implicit personas and\ninvestigate which human factors LLMs recognize and respond to. Results show\nthat explicit LLM personas show mixed results when reproducing known human\nbiases, but generally fail to demonstrate implicit biases. We conclude that\nLLMs may capture the statistical patterns of how people speak, but are\ngenerally unable to model the complex interactions and subtleties of human\nperceptions, potentially limiting their effectiveness in social science\napplications.\n","authors":["Salvatore Giorgi","Tingting Liu","Ankit Aich","Kelsey Isman","Garrick Sherman","Zachary Fried","João Sedoc","Lyle H. Ungar","Brenda Curtis"],"pdf_url":"https://arxiv.org/pdf/2406.14462v2.pdf","comment":"Accepted at Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.13716v1","updated":"2024-10-17T16:18:49Z","published":"2024-10-17T16:18:49Z","title":"MIRAGE-Bench: Automatic Multilingual Benchmark Arena for\n  Retrieval-Augmented Generation Systems","summary":"  Traditional Retrieval-Augmented Generation (RAG) benchmarks rely on different\nheuristic-based metrics for evaluation, but these require human preferences as\nground truth for reference. In contrast, arena-based benchmarks, where two\nmodels compete each other, require an expensive Large Language Model (LLM) as a\njudge for a reliable evaluation. We present an easy and efficient technique to\nget the best of both worlds. The idea is to train a learning to rank model as a\n\"surrogate\" judge using RAG-based evaluation heuristics as input, to produce a\nsynthetic arena-based leaderboard. Using this idea, We develop MIRAGE-Bench, a\nstandardized arena-based multilingual RAG benchmark for 18 diverse languages on\nWikipedia. The benchmark is constructed using MIRACL, a retrieval dataset, and\nextended for multilingual generation evaluation. MIRAGE-Bench evaluates RAG\nextensively coupling both heuristic features and LLM as a judge evaluator. In\nour work, we benchmark 19 diverse multilingual-focused LLMs, and achieve a high\ncorrelation (Kendall Tau ($\\tau$) = 0.909) using our surrogate judge learned\nusing heuristic features with pairwise evaluations and between GPT-4o as a\nteacher on the MIRAGE-Bench leaderboard using the Bradley-Terry framework. We\nobserve proprietary and large open-source LLMs currently dominate in\nmultilingual RAG. MIRAGE-Bench is available at:\nhttps://github.com/vectara/mirage-bench.\n","authors":["Nandan Thakur","Suleman Kazi","Ge Luo","Jimmy Lin","Amin Ahmad"],"pdf_url":"https://arxiv.org/pdf/2410.13716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01257v3","updated":"2024-10-17T16:15:16Z","published":"2024-07-01T13:07:01Z","title":"uDistil-Whisper: Label-Free Data Filtering for Knowledge Distillation in\n  Low-Data Regimes","summary":"  Recent work on distilling Whisper's knowledge into small models using\npseudo-labels shows promising performance while reducing the size by up to\n50\\%. This results in small, efficient, and dedicated models. However, a\ncritical step of distillation from pseudo-labels involves filtering\nhigh-quality predictions and using only those during training. This step\nrequires ground truth labels to compare and filter low-quality examples making\nthe whole process supervised. In addition to that, the distillation process\nrequires a large amount of data thereby limiting the ability to distill models\nin low-resource settings. To address this challenge, we propose a distillation\nframework that does not require any labeled data. Through experimentation, we\nshow that our best distilled models outperform the teacher model by 5-7 points\nin terms of WER compared to those without filtering and are on par with or\nperform better than similar supervised data filtering setups. When we scale the\ndata, our models significantly outperform all zero-shot and supervised models.\nWe demonstrate that it is possible to distill large Whisper models into\nrelatively small ones without using any labeled data. Our distilled models are\nalso 25-50\\% more compute- and memory-efficient while maintaining performance\nequal to or better than that of the teacher model.\n","authors":["Abdul Waheed","Karima Kadaoui","Bhiksha Raj","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2407.01257v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2402.01521v2","updated":"2024-10-17T16:08:15Z","published":"2024-02-02T16:07:05Z","title":"K-Level Reasoning: Establishing Higher Order Beliefs in Large Language\n  Models for Strategic Reasoning","summary":"  Strategic reasoning is a complex yet essential capability for intelligent\nagents. It requires Large Language Model (LLM) agents to adapt their strategies\ndynamically in multi-agent environments. Unlike static reasoning tasks, success\nin these contexts depends on anticipating other agents' beliefs and actions\nwhile continuously adjusting strategies to achieve individual goals. LLMs and\nLLM agents often struggle with strategic reasoning due to the absence of a\nreasoning framework that enables them to dynamically infer others' perspectives\nand adapt to changing environments. Inspired by the Level-K framework from game\ntheory and behavioral economics, which extends reasoning from simple reactions\nto structured strategic depth, we propose a novel framework: \"K-Level Reasoning\nwith Large Language Models (K-R).\" This framework employs recursive mechanisms\nto enable LLMs to achieve varying levels of strategic depth, allowing agents to\nform higher order beliefs - beliefs about others' beliefs. We validate this\nframework through rigorous testing on four testbeds: two classical game theory\nproblems and two social intelligence tasks. The results demonstrate the\nadvantages of K-R in strategic reasoning. Our work presents the first recursive\nimplementation of strategic depth in large language models (LLMs). It\nestablishes a foundation for future research into theory of mind and strategic\nreasoning in LLMs.\n","authors":["Yadong Zhang","Shaoguang Mao","Tao Ge","Xun Wang","Yan Xia","Man Lan","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2402.01521v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13708v1","updated":"2024-10-17T16:08:06Z","published":"2024-10-17T16:08:06Z","title":"On the Role of Attention Heads in Large Language Model Safety","summary":"  Large language models (LLMs) achieve state-of-the-art performance on multiple\nlanguage tasks, yet their safety guardrails can be circumvented, leading to\nharmful generations. In light of this, recent research on safety mechanisms has\nemerged, revealing that when safety representations or component are\nsuppressed, the safety capability of LLMs are compromised. However, existing\nresearch tends to overlook the safety impact of multi-head attention\nmechanisms, despite their crucial role in various model functionalities. Hence,\nin this paper, we aim to explore the connection between standard attention\nmechanisms and safety capability to fill this gap in the safety-related\nmechanistic interpretability. We propose a novel metric which tailored for\nmulti-head attention, the Safety Head ImPortant Score (Ships), to assess the\nindividual heads' contributions to model safety. Based on this, we generalize\nShips to the dataset level and further introduce the Safety Attention Head\nAttRibution Algorithm (Sahara) to attribute the critical safety attention heads\ninside the model. Our findings show that the special attention head has a\nsignificant impact on safety. Ablating a single safety head allows aligned\nmodel (e.g., Llama-2-7b-chat) to respond to 16 times more harmful queries,\nwhile only modifying 0.006% of the parameters, in contrast to the ~ 5%\nmodification required in previous studies. More importantly, we demonstrate\nthat attention heads primarily function as feature extractors for safety and\nmodels fine-tuned from the same base model exhibit overlapping safety heads\nthrough comprehensive experiments. Together, our attribution approach and\nfindings provide a novel perspective for unpacking the black box of safety\nmechanisms within large models.\n","authors":["Zhenhong Zhou","Haiyang Yu","Xinghua Zhang","Rongwu Xu","Fei Huang","Kun Wang","Yang Liu","Junfeng Fang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2410.13708v1.pdf","comment":"28 pages, 18 figures, 7 tables"},{"id":"http://arxiv.org/abs/2410.13699v1","updated":"2024-10-17T16:04:07Z","published":"2024-10-17T16:04:07Z","title":"Unconstrained Model Merging for Enhanced LLM Reasoning","summary":"  Recent advancements in building domain-specific large language models (LLMs)\nhave shown remarkable success, especially in tasks requiring reasoning\nabilities like logical inference over complex relationships and multi-step\nproblem solving. However, creating a powerful all-in-one LLM remains\nchallenging due to the need for proprietary data and vast computational\nresources. As a resource-friendly alternative, we explore the potential of\nmerging multiple expert models into a single LLM. Existing studies on model\nmerging mainly focus on generalist LLMs instead of domain experts, or the LLMs\nunder the same architecture and size. In this work, we propose an unconstrained\nmodel merging framework that accommodates both homogeneous and heterogeneous\nmodel architectures with a focus on reasoning tasks. A fine-grained layer-wise\nweight merging strategy is designed for homogeneous models merging, while\nheterogeneous model merging is built upon the probabilistic distribution\nknowledge derived from instruction-response fine-tuning data. Across 7\nbenchmarks and 9 reasoning-optimized LLMs, we reveal key findings that\ncombinatorial reasoning emerges from merging which surpasses simple additive\neffects. We propose that unconstrained model merging could serve as a\nfoundation for decentralized LLMs, marking a notable progression from the\nexisting centralized LLM framework. This evolution could enhance wider\nparticipation and stimulate additional advancement in the field of artificial\nintelligence, effectively addressing the constraints posed by centralized\nmodels.\n","authors":["Yiming Zhang","Baoyi He","Shengyu Zhang","Yuhao Fu","Qi Zhou","Zhijie Sang","Zijin Hong","Kejing Yang","Wenjun Wang","Jianbo Yuan","Guangning Han","Linyi Li","Chunlin Ji","Fei Wu","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2410.13699v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.13694v1","updated":"2024-10-17T15:59:52Z","published":"2024-10-17T15:59:52Z","title":"Exploring the Design Space of Visual Context Representation in Video\n  MLLMs","summary":"  Video Multimodal Large Language Models (MLLMs) have shown remarkable\ncapability of understanding the video semantics on various downstream tasks.\nDespite the advancements, there is still a lack of systematic research on\nvisual context representation, which refers to the scheme to select frames from\na video and further select the tokens from a frame. In this paper, we explore\nthe design space for visual context representation, and aim to improve the\nperformance of video MLLMs by finding more effective representation schemes.\nFirstly, we formulate the task of visual context representation as a\nconstrained optimization problem, and model the language modeling loss as a\nfunction of the number of frames and the number of embeddings (or tokens) per\nframe, given the maximum visual context window size. Then, we explore the\nscaling effects in frame selection and token selection respectively, and fit\nthe corresponding function curve by conducting extensive empirical experiments.\nWe examine the effectiveness of typical selection strategies and present\nempirical findings to determine the two factors. Furthermore, we study the\njoint effect of frame selection and token selection, and derive the optimal\nformula for determining the two factors. We demonstrate that the derived\noptimal settings show alignment with the best-performed results of empirical\nexperiments. Our code and model are available at:\nhttps://github.com/RUCAIBox/Opt-Visor.\n","authors":["Yifan Du","Yuqi Huo","Kun Zhou","Zijia Zhao","Haoyu Lu","Han Huang","Wayne Xin Zhao","Bingning Wang","Weipeng Chen","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2410.13694v1.pdf","comment":"Long Video MLLM; work in progress"},{"id":"http://arxiv.org/abs/2410.12407v2","updated":"2024-10-17T15:59:34Z","published":"2024-10-16T09:42:29Z","title":"Beyond Coarse-Grained Matching in Video-Text Retrieval","summary":"  Video-text retrieval has seen significant advancements, yet the ability of\nmodels to discern subtle differences in captions still requires verification.\nIn this paper, we introduce a new approach for fine-grained evaluation. Our\napproach can be applied to existing datasets by automatically generating hard\nnegative test captions with subtle single-word variations across nouns, verbs,\nadjectives, adverbs, and prepositions. We perform comprehensive experiments\nusing four state-of-the-art models across two standard benchmarks (MSR-VTT and\nVATEX) and two specially curated datasets enriched with detailed descriptions\n(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our\nanalyses show that the current evaluation benchmarks fall short in detecting a\nmodel's ability to perceive subtle single-word differences, 2) our fine-grained\nevaluation highlights the difficulty models face in distinguishing such subtle\nvariations. To enhance fine-grained understanding, we propose a new baseline\nthat can be easily combined with current methods. Experiments on our\nfine-grained evaluations demonstrate that this approach enhances a model's\nability to understand fine-grained differences.\n","authors":["Aozhu Chen","Hazel Doughty","Xirong Li","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12407v2.pdf","comment":"Accepted to ACCV 2024"},{"id":"http://arxiv.org/abs/2406.20052v2","updated":"2024-10-17T15:57:10Z","published":"2024-06-28T17:03:51Z","title":"Understanding and Mitigating Language Confusion in LLMs","summary":"  We investigate a surprising limitation of LLMs: their inability to\nconsistently generate text in a user's desired language. We create the Language\nConfusion Benchmark (LCB) to evaluate such failures, covering 15 typologically\ndiverse languages with existing and newly-created English and multilingual\nprompts. We evaluate a range of LLMs on monolingual and cross-lingual\ngeneration reflecting practical use cases, finding that Llama Instruct and\nMistral models exhibit high degrees of language confusion and even the\nstrongest models fail to consistently respond in the correct language. We\nobserve that base and English-centric instruct models are more prone to\nlanguage confusion, which is aggravated by complex prompts and high sampling\ntemperatures. We find that language confusion can be partially mitigated via\nfew-shot prompting, multilingual SFT and preference tuning. We release our\nlanguage confusion benchmark, which serves as a first layer of efficient,\nscalable multilingual evaluation at\nhttps://github.com/for-ai/language-confusion.\n","authors":["Kelly Marchisio","Wei-Yin Ko","Alexandre Bérard","Théo Dehaze","Sebastian Ruder"],"pdf_url":"https://arxiv.org/pdf/2406.20052v2.pdf","comment":"EMNLP 2024 Main Conference Camera-ready"},{"id":"http://arxiv.org/abs/2406.16635v2","updated":"2024-10-17T15:45:10Z","published":"2024-06-24T13:41:08Z","title":"ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models","summary":"  The high power consumption and latency-sensitive deployments of large\nlanguage models (LLMs) have motivated efficiency techniques like quantization\nand sparsity. Contextual sparsity, where the sparsity pattern is\ninput-dependent, is crucial in LLMs because the permanent removal of attention\nheads or neurons from LLMs can significantly degrade accuracy. Prior work has\nattempted to model contextual sparsity using neural networks trained to predict\nactivation magnitudes, which can be used to dynamically prune structures with\nlow predicted activation magnitude. In this paper, we look beyond\nmagnitude-based pruning criteria to assess attention head and neuron importance\nin LLMs. We develop a novel predictor called ShadowLLM, which can shadow the\nLLM behavior and enforce better sparsity patterns, resulting in over 15%\nimprovement in end-to-end accuracy compared to prior methods. In addition,\nShadowLLM achieves up to a 20% speed-up over the state-of-the-art DejaVu\nframework. These enhancements are validated on Llama-2 and OPT models with up\nto 30 billion parameters. Our code is available at\n\\href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM}.\n","authors":["Yash Akhauri","Ahmed F AbouElhamayed","Jordan Dotzel","Zhiru Zhang","Alexander M Rush","Safeen Huda","Mohamed S Abdelfattah"],"pdf_url":"https://arxiv.org/pdf/2406.16635v2.pdf","comment":"Accepted to EMNLP 2024 (Main, Long Paper)"},{"id":"http://arxiv.org/abs/2410.13675v1","updated":"2024-10-17T15:33:54Z","published":"2024-10-17T15:33:54Z","title":"Pose-Based Sign Language Appearance Transfer","summary":"  We introduce a method for transferring the signer's appearance in sign\nlanguage skeletal poses while preserving the sign content. Using estimated\nposes, we transfer the appearance of one signer to another, maintaining natural\nmovements and transitions. This approach improves pose-based rendering and sign\nstitching while obfuscating identity. Our experiments show that while the\nmethod reduces signer identification accuracy, it slightly harms sign\nrecognition performance, highlighting a tradeoff between privacy and utility.\nOur code is available at\n\\url{https://github.com/sign-language-processing/pose-anonymization}.\n","authors":["Amit Moryossef","Gerard Sant","Zifan Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.13675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13671v1","updated":"2024-10-17T15:29:57Z","published":"2024-10-17T15:29:57Z","title":"HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World\n  Multilingual Settings","summary":"  Assessing the capabilities and limitations of large language models (LLMs)\nhas garnered significant interest, yet the evaluation of multiple models in\nreal-world scenarios remains rare. Multilingual evaluation often relies on\ntranslated benchmarks, which typically do not capture linguistic and cultural\nnuances present in the source language. This study provides an extensive\nassessment of 24 LLMs on real world data collected from Indian patients\ninteracting with a medical chatbot in Indian English and 4 other Indic\nlanguages. We employ a uniform Retrieval Augmented Generation framework to\ngenerate responses, which are evaluated using both automated techniques and\nhuman evaluators on four specific metrics relevant to our application. We find\nthat models vary significantly in their performance and that instruction tuned\nIndic models do not always perform well on Indic language queries. Further, we\nempirically show that factual correctness is generally lower for responses to\nIndic queries compared to English queries. Finally, our qualitative work shows\nthat code-mixed and culturally relevant queries in our dataset pose challenges\nto evaluated models.\n","authors":["Varun Gumma","Anandhita Raghunath","Mohit Jain","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2410.13671v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2410.13668v1","updated":"2024-10-17T15:28:45Z","published":"2024-10-17T15:28:45Z","title":"signwriting-evaluation: Effective Sign Language Evaluation via\n  SignWriting","summary":"  The lack of automatic evaluation metrics tailored for SignWriting presents a\nsignificant obstacle in developing effective transcription and translation\nmodels for signed languages. This paper introduces a comprehensive suite of\nevaluation metrics specifically designed for SignWriting, including adaptations\nof standard metrics such as \\texttt{BLEU} and \\texttt{chrF}, the application of\n\\texttt{CLIPScore} to SignWriting images, and a novel symbol distance metric\nunique to our approach. We address the distinct challenges of evaluating single\nsigns versus continuous signing and provide qualitative demonstrations of\nmetric efficacy through score distribution analyses and nearest-neighbor\nsearches within the SignBank corpus. Our findings reveal the strengths and\nlimitations of each metric, offering valuable insights for future advancements\nusing SignWriting. This work contributes essential tools for evaluating\nSignWriting models, facilitating progress in the field of sign language\nprocessing. Our code is available at\n\\url{https://github.com/sign-language-processing/signwriting-evaluation}.\n","authors":["Amit Moryossef","Rotem Zilberman","Ohad Langer"],"pdf_url":"https://arxiv.org/pdf/2410.13668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13667v1","updated":"2024-10-17T15:28:27Z","published":"2024-10-17T15:28:27Z","title":"ORCHID: A Chinese Debate Corpus for Target-Independent Stance Detection\n  and Argumentative Dialogue Summarization","summary":"  Dialogue agents have been receiving increasing attention for years, and this\ntrend has been further boosted by the recent progress of large language models\n(LLMs). Stance detection and dialogue summarization are two core tasks of\ndialogue agents in application scenarios that involve argumentative dialogues.\nHowever, research on these tasks is limited by the insufficiency of public\ndatasets, especially for non-English languages. To address this language\nresource gap in Chinese, we present ORCHID (Oral Chinese Debate), the first\nChinese dataset for benchmarking target-independent stance detection and debate\nsummarization. Our dataset consists of 1,218 real-world debates that were\nconducted in Chinese on 476 unique topics, containing 2,436 stance-specific\nsummaries and 14,133 fully annotated utterances. Besides providing a versatile\ntestbed for future research, we also conduct an empirical study on the dataset\nand propose an integrated task. The results show the challenging nature of the\ndataset and suggest a potential of incorporating stance detection in\nsummarization for argumentative dialogue.\n","authors":["Xiutian Zhao","Ke Wang","Wei Peng"],"pdf_url":"https://arxiv.org/pdf/2410.13667v1.pdf","comment":"In EMNLP 2023"},{"id":"http://arxiv.org/abs/2409.15355v4","updated":"2024-10-17T15:27:30Z","published":"2024-09-14T02:34:26Z","title":"Block-Attention for Efficient RAG","summary":"  We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.\n","authors":["East Sun","Yan Wang","Lan Tian"],"pdf_url":"https://arxiv.org/pdf/2409.15355v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13666v1","updated":"2024-10-17T15:27:17Z","published":"2024-10-17T15:27:17Z","title":"VL-GLUE: A Suite of Fundamental yet Challenging Visuo-Linguistic\n  Reasoning Tasks","summary":"  Deriving inference from heterogeneous inputs (such as images, text, and\naudio) is an important skill for humans to perform day-to-day tasks. A similar\nability is desirable for the development of advanced Artificial Intelligence\n(AI) systems. While state-of-the-art models are rapidly closing the gap with\nhuman-level performance on diverse computer vision and NLP tasks separately,\nthey struggle to solve tasks that require joint reasoning over visual and\ntextual modalities. Inspired by GLUE (Wang et. al., 2018)- a multitask\nbenchmark for natural language understanding, we propose VL-GLUE in this paper.\nVL-GLUE consists of over 100k samples spanned across seven different tasks,\nwhich at their core require visuo-linguistic reasoning. Moreover, our benchmark\ncomprises of diverse image types (from synthetically rendered figures, and\nday-to-day scenes to charts and complex diagrams) and includes a broad variety\nof domain-specific text (from cooking, politics, and sports to high-school\ncurricula), demonstrating the need for multi-modal understanding in the\nreal-world. We show that this benchmark is quite challenging for existing\nlarge-scale vision-language models and encourage development of systems that\npossess robust visuo-linguistic reasoning capabilities.\n","authors":["Shailaja Keyur Sampat","Mutsumi Nakamura","Shankar Kailas","Kartik Aggarwal","Mandy Zhou","Yezhou Yang","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2410.13666v1.pdf","comment":"18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2404.00489v2","updated":"2024-10-17T15:20:23Z","published":"2024-03-30T23:07:58Z","title":"Prompt-SAW: Leveraging Relation-Aware Graphs for Textual Prompt\n  Compression","summary":"  Large Language Models (LLMs) have shown exceptional abilities for multiple\ndifferent natural language processing tasks. While prompting is a crucial tool\nfor LLM inference, we observe that there is a significant cost associated with\nexceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead\nto substandard results in terms of readability/interpretability of the\ncompressed prompt, with a detrimental impact on prompt utility. To address\nthis, we propose PromptSAW: Prompt compresSion via Relation AWare graphs, an\neffective strategy for prompt compression over task-agnostic and task-aware\nprompts. Prompt-SAW uses the prompt's textual information to build a graph and\nlater extracts key information elements in the graph to come up with the\ncompressed prompt. We also propose GSM8K-aug, i.e., an extended version of the\nexisting GSM8K benchmark for task-agnostic prompts in order to provide a\ncomprehensive evaluation platform. Experimental evaluation using benchmark\ndatasets shows that prompts compressed by Prompt-SAW are not only better in\nterms of readability, but they also outperform the best-performing baseline\nmodels by up to 10.1 and 77.1, respectively, for task-agnostic and task-aware\nsettings while compressing the original prompt text by 34.9 and 56.7.\n","authors":["Muhammad Asif Ali","Zhengping Li","Shu Yang","Keyuan Cheng","Yang Cao","Tianhao Huang","Guimin Hu","Weimin Lyu","Lijie Hu","Lu Yu","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2404.00489v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2410.13654v1","updated":"2024-10-17T15:19:03Z","published":"2024-10-17T15:19:03Z","title":"Red and blue language: Word choices in the Trump & Harris 2024\n  presidential debate","summary":"  Political debates are a peculiar type of political discourse, in which\ncandidates directly confront one another, addressing not only the the\nmoderator's questions, but also their opponent's statements, as well as the\nconcerns of voters from both parties and undecided voters. Therefore, language\nis adjusted to meet specific expectations and achieve persuasion. We analyse\nhow the language of Trump and Harris during the debate (September 10th 2024)\ndiffers in relation to the following semantic and pragmatic features, for which\nwe formulated targeted hypotheses: framing values and ideology, appealing to\nemotion, using words with different degrees of concreteness and specificity,\naddressing others through singular or plural pronouns. Our findings include:\ndifferences in the use of figurative frames (Harris often framing issues around\nrecovery and empowerment, Trump often focused on crisis and decline); similar\nuse of emotional language, with Trump showing a slight higher tendency toward\nnegativity and toward less subjective language compared to Harris; no\nsignificant difference in the specificity of candidates' responses; similar use\nof abstract language, with Trump showing more variability than Harris,\ndepending on the subject discussed; differences in addressing the opponent,\nwith Trump not mentioning Harris by name, while Harris referring to Trump\nfrequently; different uses of pronouns, with Harris using both singular and\nplural pronouns equally, while Trump using more singular pronouns. The results\nare discussed in relation to previous literature on Red and Blue language,\nwhich refers to distinct linguistic patterns associated with conservative (Red)\nand liberal (Blue) political ideologies.\n","authors":["Philipp Wicke","Marianna M. Bolognesi"],"pdf_url":"https://arxiv.org/pdf/2410.13654v1.pdf","comment":"Submitted to PLOS ONE, under review"},{"id":"http://arxiv.org/abs/2410.13649v1","updated":"2024-10-17T15:15:12Z","published":"2024-10-17T15:15:12Z","title":"A new approach for fine-tuning sentence transformers for intent\n  classification and out-of-scope detection tasks","summary":"  In virtual assistant (VA) systems it is important to reject or redirect user\nqueries that fall outside the scope of the system. One of the most accurate\napproaches for out-of-scope (OOS) rejection is to combine it with the task of\nintent classification on in-scope queries, and to use methods based on the\nsimilarity of embeddings produced by transformer-based sentence encoders.\nTypically, such encoders are fine-tuned for the intent-classification task,\nusing cross-entropy loss. Recent work has shown that while this produces\nsuitable embeddings for the intent-classification task, it also tends to\ndisperse in-scope embeddings over the full sentence embedding space. This\ncauses the in-scope embeddings to potentially overlap with OOS embeddings,\nthereby making OOS rejection difficult. This is compounded when OOS data is\nunknown. To mitigate this issue our work proposes to regularize the\ncross-entropy loss with an in-scope embedding reconstruction loss learned using\nan auto-encoder. Our method achieves a 1-4% improvement in the area under the\nprecision-recall curve for rejecting out-of-sample (OOS) instances, without\ncompromising intent classification performance.\n","authors":["Tianyi Zhang","Atta Norouzian","Aanchan Mohan","Frederick Ducatelle"],"pdf_url":"https://arxiv.org/pdf/2410.13649v1.pdf","comment":"Appearing at Empirical Methods in Natural Language Processing 2025 -\n  Industry Track"},{"id":"http://arxiv.org/abs/2410.13648v1","updated":"2024-10-17T15:15:00Z","published":"2024-10-17T15:15:00Z","title":"SimpleToM: Exposing the Gap between Explicit ToM Inference and Implicit\n  ToM Application in LLMs","summary":"  While prior work has explored whether large language models (LLMs) possess a\n\"theory of mind\" (ToM) - the ability to attribute mental states to oneself and\nothers - there has been little work testing whether LLMs can implicitly apply\nsuch knowledge to predict behavior, or to judge whether an observed behavior is\nrational. Such skills are critical for appropriate interaction in social\nenvironments. We create a new dataset, SimpleTom, containing concise, diverse\nstories (e.g., \"The can of Pringles has moldy chips in it. Mary picks up the\ncan in the supermarket and walks to the cashier.\"), each with three questions\nthat test different degrees of ToM reasoning, asking models to predict (a)\nmental state (\"Is Mary aware of the mold?\"), (b) behavior (\"Will Mary pay for\nthe chips or report the mold?\"), and (c) judgment (\"Mary paid for the chips.\nWas that reasonable?\"). To our knowledge, SimpleToM is the first dataset to\nsystematically explore downstream reasoning requiring knowledge of mental\nstates in realistic scenarios. Our experimental results are intriguing: While\nmost models can reliably predict mental state on our dataset (a), they often\nfail to correctly predict the behavior (b), and fare even worse at judging\nwhether given behaviors are reasonable (c), despite being correctly aware of\nthe protagonist's mental state should make such secondary predictions obvious.\nWe further show that we can help models do better at (b) and (c) via\ninterventions such as reminding the model of its earlier mental state answer\nand mental-state-specific chain-of-thought prompting, raising the action\nprediction accuracies (e.g., from 49.5% to 93.5% for GPT-4o) and judgment\naccuracies (e.g., from 15.3% to 94.7% in GPT-4o). While this shows that models\ncan be coaxed to perform well, it requires task-specific interventions, and the\nnatural model performances remain low, a cautionary tale for LLM deployment.\n","authors":["Yuling Gu","Oyvind Tafjord","Hyunwoo Kim","Jared Moore","Ronan Le Bras","Peter Clark","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2410.13648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11341v3","updated":"2024-10-17T15:12:21Z","published":"2024-06-17T08:59:04Z","title":"A Systematic Analysis of Large Language Models as Soft Reasoners: The\n  Case of Syllogistic Inferences","summary":"  The reasoning abilities of Large Language Models (LLMs) are becoming a\ncentral focus of study in NLP. In this paper, we consider the case of\nsyllogistic reasoning, an area of deductive reasoning studied extensively in\nlogic and cognitive psychology. Previous research has shown that pre-trained\nLLMs exhibit reasoning biases, such as $\\textit{content effects}$, avoid\nanswering that $\\textit{no conclusion follows}$, display human-like\ndifficulties, and struggle with multi-step reasoning. We contribute to this\nresearch line by systematically investigating the effects of chain-of-thought\nreasoning, in-context learning (ICL), and supervised fine-tuning (SFT) on\nsyllogistic reasoning, considering syllogisms with conclusions that support or\nviolate world knowledge, as well as ones with multiple premises. Crucially, we\ngo beyond the standard focus on accuracy, with an in-depth analysis of the\nconclusions generated by the models. Our results suggest that the behavior of\npre-trained LLMs can be explained by heuristics studied in cognitive science\nand that both ICL and SFT improve model performance on valid inferences,\nalthough only the latter mitigates most reasoning biases without harming model\nconsistency.\n","authors":["Leonardo Bertolazzi","Albert Gatt","Raffaella Bernardi"],"pdf_url":"https://arxiv.org/pdf/2406.11341v3.pdf","comment":"Accepted to EMNLP 2024 (main conference)"},{"id":"http://arxiv.org/abs/2410.13641v1","updated":"2024-10-17T15:09:35Z","published":"2024-10-17T15:09:35Z","title":"An Active Learning Framework for Inclusive Generation by Large Language\n  Models","summary":"  Ensuring that Large Language Models (LLMs) generate text representative of\ndiverse sub-populations is essential, particularly when key concepts related to\nunder-represented groups are scarce in the training data. We address this\nchallenge with a novel clustering-based active learning framework, enhanced\nwith knowledge distillation. The proposed framework transforms the intermediate\noutputs of the learner model, enabling effective active learning for generative\ntasks for the first time. Integration of clustering and knowledge distillation\nyields more representative models without prior knowledge of underlying data\ndistribution and overbearing human efforts. We validate our approach in\npractice through case studies in counter-narration and style transfer. We\nconstruct two new datasets in tandem with model training, showing a performance\nimprovement of 2%-10% over baseline models. Our results also show more\nconsistent performance across various data subgroups and increased lexical\ndiversity, underscoring our model's resilience to skewness in available data.\nFurther, our results show that the data acquired via our approach improves the\nperformance of secondary models not involved in the learning loop, showcasing\npractical utility of the framework.\n","authors":["Sabit Hassan","Anthony Sicilia","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2410.13641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13640v1","updated":"2024-10-17T15:09:24Z","published":"2024-10-17T15:09:24Z","title":"Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation","summary":"  LLM self-evaluation relies on the LLM's own ability to estimate response\ncorrectness, which can greatly improve its deployment reliability. In this\nresearch track, we propose the Chain-of-Embedding (CoE) in the latent space to\nenable LLMs to perform output-free self-evaluation. CoE consists of all\nprogressive hidden states produced during the inference time, which can be\ntreated as the latent thinking path of LLMs. We find that when LLMs respond\ncorrectly and incorrectly, their CoE features differ, these discrepancies\nassist us in estimating LLM response correctness. Experiments in four diverse\ndomains and seven LLMs fully demonstrate the effectiveness of our method.\nMeanwhile, its label-free design intent without any training and\nmillisecond-level computational cost ensure real-time feedback in large-scale\nscenarios. More importantly, we provide interesting insights into LLM response\ncorrectness from the perspective of hidden state changes inside LLMs.\n","authors":["Yiming Wang","Pei Zhang","Baosong Yang","Derek F. Wong","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13640v1.pdf","comment":"33 pages, 18 figures, 12 tables"},{"id":"http://arxiv.org/abs/2410.13639v1","updated":"2024-10-17T15:09:03Z","published":"2024-10-17T15:09:03Z","title":"A Comparative Study on Reasoning Patterns of OpenAI's o1 Model","summary":"  Enabling Large Language Models (LLMs) to handle a wider range of complex\ntasks (e.g., coding, math) has drawn great attention from many researchers. As\nLLMs continue to evolve, merely increasing the number of model parameters\nyields diminishing performance improvements and heavy computational costs.\nRecently, OpenAI's o1 model has shown that inference strategies (i.e.,\nTest-time Compute methods) can also significantly enhance the reasoning\ncapabilities of LLMs. However, the mechanisms behind these methods are still\nunexplored. In our work, to investigate the reasoning patterns of o1, we\ncompare o1 with existing Test-time Compute methods (BoN, Step-wise BoN, Agent\nWorkflow, and Self-Refine) by using OpenAI's GPT-4o as a backbone on general\nreasoning benchmarks in three domains (i.e., math, coding, commonsense\nreasoning). Specifically, first, our experiments show that the o1 model has\nachieved the best performance on most datasets. Second, as for the methods of\nsearching diverse responses (e.g., BoN), we find the reward models' capability\nand the search space both limit the upper boundary of these methods. Third, as\nfor the methods that break the problem into many sub-problems, the Agent\nWorkflow has achieved better performance than Step-wise BoN due to the\ndomain-specific system prompt for planning better reasoning processes. Fourth,\nit is worth mentioning that we have summarized six reasoning patterns of o1,\nand provided a detailed analysis on several reasoning benchmarks.\n","authors":["Siwei Wu","Zhongyuan Peng","Xinrun Du","Tuney Zheng","Minghao Liu","Jialong Wu","Jiachen Ma","Yizhi Li","Jian Yang","Wangchunshu Zhou","Qunshu Lin","Junbo Zhao","Zhaoxiang Zhang","Wenhao Huang","Ge Zhang","Chenghua Lin","J. H. Liu"],"pdf_url":"https://arxiv.org/pdf/2410.13639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14545v2","updated":"2024-10-17T15:06:23Z","published":"2024-06-20T17:54:33Z","title":"Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference\n  Attacks in Text-to-SQL Systems","summary":"  Text-to-SQL systems empower users to interact with databases using natural\nlanguage, automatically translating queries into executable SQL code. However,\ntheir reliance on database schema information for SQL generation exposes them\nto significant security vulnerabilities, particularly schema inference attacks\nthat can lead to unauthorized data access or manipulation. In this paper, we\nintroduce a novel zero-knowledge framework for reconstructing the underlying\ndatabase schema of text-to-SQL models without any prior knowledge of the\ndatabase. Our approach systematically probes text-to-SQL models with specially\ncrafted questions and leverages a surrogate GPT-4 model to interpret the\noutputs, effectively uncovering hidden schema elements -- including tables,\ncolumns, and data types. We demonstrate that our method achieves high accuracy\nin reconstructing table names, with F1 scores of up to .99 for generative\nmodels and .78 for fine-tuned models, underscoring the severity of schema\nleakage risks. Furthermore, we propose a simple protection mechanism for\ngenerative models and empirically show its limitations in mitigating these\nattacks.\n","authors":["Đorđe Klisura","Anthony Rios"],"pdf_url":"https://arxiv.org/pdf/2406.14545v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09693v3","updated":"2024-10-17T15:03:11Z","published":"2023-11-16T09:09:22Z","title":"BLT: Can Large Language Models Handle Basic Legal Text?","summary":"  We find that the best publicly available LLMs like GPT-4 and Claude currently\nperform poorly on basic legal text handling. This motivates the creation of a\nbenchmark consisting of examples that lawyers and paralegals would expect LLMs\nto handle zero-shot, such as looking up the text at a line of a witness\ndeposition or at a subsection of a contract. LLMs' poor performance on this\nbenchmark casts into doubt their reliability as-is for legal practice. However,\nfine-tuning on our training set brings even a small model to near-perfect\nperformance. This benchmark will be useful for fine-tuning LLMs for downstream\nlegal tasks, as well as for tracking LLMs' reliability as-is for basic legal\ntasks.\n","authors":["Andrew Blair-Stanek","Nils Holzenberger","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2311.09693v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11382v2","updated":"2024-10-17T14:59:37Z","published":"2024-08-21T07:23:34Z","title":"Towards Inducing Document-Level Abilities in Standard Multilingual\n  Neural Machine Translation Models","summary":"  Neural Machine Translation (NMT) models have traditionally used Sinusoidal\nPositional Embeddings (PEs), which often struggle to capture long-range\ndependencies and are less efficient for handling extended context or\ndocument-level translation tasks. This work addresses the challenge of\ntransitioning pre-trained NMT models from absolute sinusoidal PEs to relative\nPEs, such as Rotary Positional Embeddings (ROPE) and Attention with Linear\nBiases (ALIBI), without compromising performance. We demonstrate that\nparameter-efficient fine-tuning, using only a small amount of high-quality\ndata, can successfully facilitate this transition. Experimental results\nindicate that switching from sinusoidal to relative PEs results in competitive\ntranslation quality on sentence-level evaluation benchmarks. Additionally,\nmodels trained with ROPE consistently outperform those using ALIBI and\nSinusoidal PEs on document-level benchmarks across both string-based metrics\nand qualitative evaluations. Moreover, we find that a small amount of\nlong-context data in a few languages is sufficient for cross-lingual length\ngeneralization, thereby inducing long-context capabilities.\n","authors":["Varun Gumma","Pranjal A. Chitale","Kalika Bali"],"pdf_url":"https://arxiv.org/pdf/2408.11382v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2407.04952v2","updated":"2024-10-17T14:58:53Z","published":"2024-07-06T04:06:55Z","title":"Granular Privacy Control for Geolocation with Vision Language Models","summary":"  Vision Language Models (VLMs) are rapidly advancing in their capability to\nanswer information-seeking questions. As these models are widely deployed in\nconsumer applications, they could lead to new privacy risks due to emergent\nabilities to identify people in photos, geolocate images, etc. As we\ndemonstrate, somewhat surprisingly, current open-source and proprietary VLMs\nare very capable image geolocators, making widespread geolocation with VLMs an\nimmediate privacy risk, rather than merely a theoretical future concern. As a\nfirst step to address this challenge, we develop a new benchmark, GPTGeoChat,\nto test the ability of VLMs to moderate geolocation dialogues with users. We\ncollect a set of 1,000 image geolocation conversations between in-house\nannotators and GPT-4v, which are annotated with the granularity of location\ninformation revealed at each turn. Using this new dataset, we evaluate the\nability of various VLMs to moderate GPT-4v geolocation conversations by\ndetermining when too much location information has been revealed. We find that\ncustom fine-tuned models perform on par with prompted API-based models when\nidentifying leaked location information at the country or city level; however,\nfine-tuning on supervised data appears to be needed to accurately moderate\nfiner granularities, such as the name of a restaurant or building.\n","authors":["Ethan Mendes","Yang Chen","James Hays","Sauvik Das","Wei Xu","Alan Ritter"],"pdf_url":"https://arxiv.org/pdf/2407.04952v2.pdf","comment":"Accepted to EMNLP 2024 main conference"},{"id":"http://arxiv.org/abs/2410.13611v1","updated":"2024-10-17T14:46:34Z","published":"2024-10-17T14:46:34Z","title":"H2OVL-Mississippi Vision Language Models Technical Report","summary":"  Smaller vision-language models (VLMs) are becoming increasingly important for\nprivacy-focused, on-device applications due to their ability to run efficiently\non consumer hardware for processing enterprise commercial documents and images.\nThese models require strong language understanding and visual capabilities to\nenhance human-machine interaction. To address this need, we present\nH2OVL-Mississippi, a pair of small VLMs trained on 37 million image-text pairs\nusing 240 hours of compute on 8 x H100 GPUs. H2OVL-Mississippi-0.8B is a tiny\nmodel with 0.8 billion parameters that specializes in text recognition,\nachieving state of the art performance on the Text Recognition portion of\nOCRBench and surpassing much larger models in this area. Additionally, we are\nreleasing H2OVL-Mississippi-2B, a 2 billion parameter model for general use\ncases, exhibiting highly competitive metrics across various academic\nbenchmarks. Both models build upon our prior work with H2O-Danube language\nmodels, extending their capabilities into the visual domain. We release them\nunder the Apache 2.0 license, making VLMs accessible to everyone, democratizing\ndocument AI and visual LLMs.\n","authors":["Shaikat Galib","Shanshan Wang","Guanshuo Xu","Pascal Pfeiffer","Ryan Chesler","Mark Landry","Sri Satish Ambati"],"pdf_url":"https://arxiv.org/pdf/2410.13611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13610v1","updated":"2024-10-17T14:46:22Z","published":"2024-10-17T14:46:22Z","title":"MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool\n  Calling","summary":"  Integrating tools into Large Language Models (LLMs) has facilitated the\nwidespread application. Despite this, in specialized downstream task contexts,\nreliance solely on tools is insufficient to fully address the complexities of\nthe real world. This particularly restricts the effective deployment of LLMs in\nfields such as medicine. In this paper, we focus on the downstream tasks of\nmedical calculators, which use standardized tests to assess an individual's\nhealth status. We introduce MeNTi, a universal agent architecture for LLMs.\nMeNTi integrates a specialized medical toolkit and employs meta-tool and nested\ncalling mechanisms to enhance LLM tool utilization. Specifically, it achieves\nflexible tool selection and nested tool calling to address practical issues\nfaced in intricate medical scenarios, including calculator selection, slot\nfilling, and unit conversion. To assess the capabilities of LLMs for\nquantitative assessment throughout the clinical process of calculator\nscenarios, we introduce CalcQA. This benchmark requires LLMs to use medical\ncalculators to perform calculations and assess patient health status. CalcQA is\nconstructed by professional physicians and includes 100 case-calculator pairs,\ncomplemented by a toolkit of 281 medical tools. The experimental results\ndemonstrate significant performance improvements with our framework. This\nresearch paves new directions for applying LLMs in demanding scenarios of\nmedicine.\n","authors":["Yakun Zhu","Shaohang Wei","Xu Wang","Kui Xue","Xiaofan Zhang","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.13610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07991v2","updated":"2024-10-17T14:44:45Z","published":"2024-10-10T14:48:57Z","title":"Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic\n  Analysis of Annotators and Targets","summary":"  The rise of online platforms exacerbated the spread of hate speech, demanding\nscalable and effective detection. However, the accuracy of hate speech\ndetection systems heavily relies on human-labeled data, which is inherently\nsusceptible to biases. While previous work has examined the issue, the\ninterplay between the characteristics of the annotator and those of the target\nof the hate are still unexplored. We fill this gap by leveraging an extensive\ndataset with rich socio-demographic information of both annotators and targets,\nuncovering how human biases manifest in relation to the target's attributes.\nOur analysis surfaces the presence of widespread biases, which we\nquantitatively describe and characterize based on their intensity and\nprevalence, revealing marked differences. Furthermore, we compare human biases\nwith those exhibited by persona-based LLMs. Our findings indicate that while\npersona-based LLMs do exhibit biases, these differ significantly from those of\nhuman annotators. Overall, our work offers new and nuanced results on human\nbiases in hate speech annotations, as well as fresh insights into the design of\nAI-driven hate speech detection systems.\n","authors":["Tommaso Giorgi","Lorenzo Cima","Tiziano Fagni","Marco Avvenuti","Stefano Cresci"],"pdf_url":"https://arxiv.org/pdf/2410.07991v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17648v3","updated":"2024-10-17T14:41:39Z","published":"2024-09-26T08:55:21Z","title":"Efficient In-Domain Question Answering for Resource-Constrained\n  Environments","summary":"  Retrieval Augmented Generation (RAG) is a common method for integrating\nexternal knowledge into pretrained Large Language Models (LLMs) to enhance\naccuracy and relevancy in question answering (QA) tasks. However, prompt\nengineering and resource efficiency remain significant bottlenecks in\ndeveloping optimal and robust RAG solutions for real-world QA applications.\nRecent studies have shown success in using fine tuning to address these\nproblems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to\nsmaller 7B models has demonstrated superior performance compared to RAG setups\nwith much larger models such as GPT-3.5. The combination of RAFT with\nparameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation\n(LoRA), promises an even more efficient solution, yet remains an unexplored\narea. In this work, we combine RAFT with LoRA to reduce fine tuning and storage\nrequirements and gain faster inference times while maintaining comparable RAG\nperformance. This results in a more compute-efficient RAFT, or CRAFT, which is\nparticularly useful for knowledge-intensive QA tasks in resource-constrained\nenvironments where internet access may be restricted and hardware resources\nlimited.\n","authors":["Isaac Chung","Phat Vo","Arman C. Kizilkale","Aaron Reite"],"pdf_url":"https://arxiv.org/pdf/2409.17648v3.pdf","comment":"6 pages, 2 tables"},{"id":"http://arxiv.org/abs/2410.13604v1","updated":"2024-10-17T14:39:24Z","published":"2024-10-17T14:39:24Z","title":"Large Language Models as Narrative-Driven Recommenders","summary":"  Narrative-driven recommenders aim to provide personalized suggestions for\nuser requests expressed in free-form text such as \"I want to watch a thriller\nwith a mind-bending story, like Shutter Island.\" Although large language models\n(LLMs) have been shown to excel in processing general natural language queries,\ntheir effectiveness for handling such recommendation requests remains\nrelatively unexplored. To close this gap, we compare the performance of 38\nopen- and closed-source LLMs of various sizes, such as LLama 3.2 and GPT-4o, in\na movie recommendation setting. For this, we utilize a gold-standard,\ncrowdworker-annotated dataset of posts from reddit's movie suggestion community\nand employ various prompting strategies, including zero-shot, identity, and\nfew-shot prompting. Our findings demonstrate the ability of LLMs to generate\ncontextually relevant movie recommendations, significantly outperforming other\nstate-of-the-art approaches, such as doc2vec. While we find that closed-source\nand large-parameterized models generally perform best, medium-sized open-source\nmodels remain competitive, being only slightly outperformed by their more\ncomputationally expensive counterparts. Furthermore, we observe no significant\ndifferences across prompting strategies for most models, underscoring the\neffectiveness of simple approaches such as zero-shot prompting for\nnarrative-driven recommendations. Overall, this work offers valuable insights\nfor recommender system researchers as well as practitioners aiming to integrate\nLLMs into real-world recommendation tools.\n","authors":["Lukas Eberhard","Thorsten Ruprechter","Denis Helic"],"pdf_url":"https://arxiv.org/pdf/2410.13604v1.pdf","comment":"Under review; 19 pages"},{"id":"http://arxiv.org/abs/2410.13562v1","updated":"2024-10-17T14:00:13Z","published":"2024-10-17T14:00:13Z","title":"Enhancing Fact Retrieval in PLMs through Truthfulness","summary":"  Pre-trained Language Models (PLMs) encode various facts about the world at\ntheir pre-training phase as they are trained to predict the next or missing\nword in a sentence. There has a been an interest in quantifying and improving\nthe amount of facts that can be extracted from PLMs, as they have been\nenvisioned to act as soft knowledge bases, which can be queried in natural\nlanguage. Different approaches exist to enhance fact retrieval from PLM. Recent\nwork shows that the hidden states of PLMs can be leveraged to determine the\ntruthfulness of the PLMs' inputs. Leveraging this finding to improve factual\nknowledge retrieval remains unexplored. In this work, we investigate the use of\na helper model to improve fact retrieval. The helper model assesses the\ntruthfulness of an input based on the corresponding hidden states\nrepresentations from the PLMs. We evaluate this approach on several masked PLMs\nand show that it enhances fact retrieval by up to 33\\%. Our findings highlight\nthe potential of hidden states representations from PLMs in improving their\nfactual knowledge retrieval.\n","authors":["Paul Youssef","Jörg Schlötterer","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2410.13562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13553v1","updated":"2024-10-17T13:51:03Z","published":"2024-10-17T13:51:03Z","title":"Integrating Temporal Representations for Dynamic Memory Retrieval and\n  Management in Large Language Models","summary":"  Conventional dialogue agents often struggle with effective memory recall,\nleading to redundant retrieval and inadequate management of unique user\nassociations. To address this, we propose SynapticRAG, a novel approach\nintegrating synaptic dynamics into Retrieval-Augmented Generation (RAG).\nSynapticRAG integrates temporal representations into memory vectors, mimicking\nbiological synapses by differentiating events based on occurrence times and\ndynamically updating memory significance. This model employs temporal scoring\nfor memory connections and a synaptic-inspired propagation control mechanism.\nExperiments across English, Japanese, and Chinese datasets demonstrate\nSynapticRAG's superiority over existing methods, including traditional RAG,\nwith up to 14.66\\% improvement in memory retrieval accuracy. Our approach\nadvances context-aware dialogue AI systems by enhancing long-term context\nmaintenance and specific information extraction from conversations.\n","authors":["Yuki Hou","Haruki Tamoto","Homei Miyashita"],"pdf_url":"https://arxiv.org/pdf/2410.13553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16710v3","updated":"2024-10-17T13:50:46Z","published":"2024-04-25T16:20:23Z","title":"LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding","summary":"  We present LayerSkip, an end-to-end solution to speed-up inference of large\nlanguage models (LLMs). First, during training we apply layer dropout, with low\ndropout rates for earlier layers and higher dropout rates for later layers, and\nan early exit loss where all transformer layers share the same exit. Second,\nduring inference, we show that this training recipe increases the accuracy of\nearly exit at earlier layers, without adding any auxiliary layers or modules to\nthe model. Third, we present a novel self-speculative decoding solution where\nwe exit at early layers and verify and correct with remaining layers of the\nmodel. Our proposed self-speculative decoding approach has less memory\nfootprint than other speculative decoding approaches and benefits from shared\ncompute and activations of the draft and verification stages. We run\nexperiments on different Llama model sizes on different types of training:\npretraining from scratch, continual pretraining, finetuning on specific data\ndomain, and finetuning on specific task. We implement our inference solution\nand show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x\non coding, and 2.0x on TOPv2 semantic parsing task. We open source our code and\ncheckpoints at https://github.com/facebookresearch/LayerSkip.\n","authors":["Mostafa Elhoushi","Akshat Shrivastava","Diana Liskovich","Basil Hosmer","Bram Wasti","Liangzhen Lai","Anas Mahmoud","Bilge Acun","Saurabh Agarwal","Ahmed Roman","Ahmed A Aly","Beidi Chen","Carole-Jean Wu"],"pdf_url":"https://arxiv.org/pdf/2404.16710v3.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2410.12691v2","updated":"2024-10-17T13:32:23Z","published":"2024-10-16T15:51:18Z","title":"Building Better: Avoiding Pitfalls in Developing Language Resources when\n  Data is Scarce","summary":"  Language is a symbolic capital that affects people's lives in many ways\n(Bourdieu, 1977, 1991). It is a powerful tool that accounts for identities,\ncultures, traditions, and societies in general. Hence, data in a given language\nshould be viewed as more than a collection of tokens. Good data collection and\nlabeling practices are key to building more human-centered and socially aware\ntechnologies. While there has been a rising interest in mid- to low-resource\nlanguages within the NLP community, work in this space has to overcome unique\nchallenges such as data scarcity and access to suitable annotators. In this\npaper, we collect feedback from those directly involved in and impacted by NLP\nartefacts for mid- to low-resource languages. We conduct a quantitative and\nqualitative analysis of the responses and highlight the main issues related to\n(1) data quality such as linguistic and cultural data suitability; and (2) the\nethics of common annotation practices such as the misuse of online community\nservices. Based on these findings, we make several recommendations for the\ncreation of high-quality language artefacts that reflect the cultural milieu of\nits speakers, while simultaneously respecting the dignity and labor of data\nworkers.\n","authors":["Nedjma Ousidhoum","Meriem Beloucif","Saif M. Mohammad"],"pdf_url":"https://arxiv.org/pdf/2410.12691v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12294v2","updated":"2024-10-17T13:27:43Z","published":"2024-10-16T06:51:09Z","title":"LLM-based Cognitive Models of Students with Misconceptions","summary":"  Accurately modeling student cognition is crucial for developing effective\nAI-driven educational technologies. A key challenge is creating realistic\nstudent models that satisfy two essential properties: (1) accurately\nreplicating specific misconceptions, and (2) correctly solving problems where\nthese misconceptions are not applicable. This dual requirement reflects the\ncomplex nature of student understanding, where misconceptions coexist with\ncorrect knowledge. This paper investigates whether Large Language Models (LLMs)\ncan be instruction-tuned to meet this dual requirement and effectively simulate\nstudent thinking in algebra. We introduce MalAlgoPy, a novel Python library\nthat generates datasets reflecting authentic student solution patterns through\na graph-based representation of algebraic problem-solving. Utilizing MalAlgoPy,\nwe define and examine Cognitive Student Models (CSMs) - LLMs instruction tuned\nto faithfully emulate realistic student behavior. Our findings reveal that LLMs\ntrained on misconception examples can efficiently learn to replicate errors.\nHowever, the training diminishes the model's ability to solve problems\ncorrectly, particularly for problem types where the misconceptions are not\napplicable, thus failing to satisfy second property of CSMs. We demonstrate\nthat by carefully calibrating the ratio of correct to misconception examples in\nthe training data - sometimes as low as 0.25 - it is possible to develop CSMs\nthat satisfy both properties. Our insights enhance our understanding of\nAI-based student models and pave the way for effective adaptive learning\nsystems.\n","authors":["Shashank Sonkar","Xinghe Chen","Naiming Liu","Richard G. Baraniuk","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.12294v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03857v2","updated":"2024-10-17T13:08:13Z","published":"2024-06-06T08:42:36Z","title":"MuJo: Multimodal Joint Feature Space Learning for Human Activity\n  Recognition","summary":"  Human Activity Recognition (HAR) is a longstanding problem in AI with\napplications in a broad range of areas, including healthcare, sports and\nfitness, security, and more. The performance of HAR in real-world settings is\nstrongly dependent on the type and quality of the input signal that can be\nacquired. Given an unobstructed, high-quality camera view of a scene, computer\nvision systems, in particular in conjunction with foundation models, can today\nfairly reliably distinguish complex activities. On the other hand, recognition\nusing modalities such as wearable sensors (which are often more broadly\navailable, e.g., in mobile phones and smartwatches) is a more difficult\nproblem, as the signals often contain less information and labeled training\ndata is more difficult to acquire. To alleviate the need for labeled data, we\nintroduce our comprehensive Fitness Multimodal Activity Dataset (FiMAD) in this\nwork, which can be used with the proposed pre-training method MuJo (Multimodal\nJoint Feature Space Learning) to enhance HAR performance across various\nmodalities. FiMAD was created using YouTube fitness videos and contains\nparallel video, language, pose, and simulated IMU sensor data. MuJo utilizes\nthis dataset to learn a joint feature space for these modalities. We show that\nclassifiers pre-trained on FiMAD can increase the performance on real HAR\ndatasets such as MM-Fit, MyoGym, MotionSense, and MHEALTH. For instance, on\nMM-Fit, we achieve an Macro F1-Score of up to 0.855 when fine-tuning on only 2%\nof the training data and 0.942 when utilizing the full training set for\nclassification tasks. We have compared our approach to other self-supervised\nones and showed that, unlike them, ours can consistently improve on the\nbaseline network performance as well as provide a better data-efficiency.\n","authors":["Stefan Gerd Fritsch","Cennet Oguz","Vitor Fortes Rey","Lala Ray","Maximilian Kiefer-Emmanouilidis","Paul Lukowicz"],"pdf_url":"https://arxiv.org/pdf/2406.03857v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13517v1","updated":"2024-10-17T13:06:02Z","published":"2024-10-17T13:06:02Z","title":"Bias in the Mirror : Are LLMs opinions robust to their own adversarial\n  attacks ?","summary":"  Large language models (LLMs) inherit biases from their training data and\nalignment processes, influencing their responses in subtle ways. While many\nstudies have examined these biases, little work has explored their robustness\nduring interactions. In this paper, we introduce a novel approach where two\ninstances of an LLM engage in self-debate, arguing opposing viewpoints to\npersuade a neutral version of the model. Through this, we evaluate how firmly\nbiases hold and whether models are susceptible to reinforcing misinformation or\nshifting to harmful viewpoints. Our experiments span multiple LLMs of varying\nsizes, origins, and languages, providing deeper insights into bias persistence\nand flexibility across linguistic and cultural contexts.\n","authors":["Virgile Rennard","Christos Xypolopoulos","Michalis Vazirgiannis"],"pdf_url":"https://arxiv.org/pdf/2410.13517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13510v1","updated":"2024-10-17T12:56:52Z","published":"2024-10-17T12:56:52Z","title":"GeoCoder: Solving Geometry Problems by Generating Modular Code through\n  Vision-Language Models","summary":"  Geometry problem-solving demands advanced reasoning abilities to process\nmultimodal inputs and employ mathematical knowledge effectively.\nVision-language models (VLMs) have made significant progress in various\nmultimodal tasks. Yet, they still struggle with geometry problems and are\nsignificantly limited by their inability to perform mathematical operations not\nseen during pre-training, such as calculating the cosine of an arbitrary angle,\nand by difficulties in correctly applying relevant geometry formulas. To\novercome these challenges, we present GeoCoder, which leverages modular\ncode-finetuning to generate and execute code using a predefined geometry\nfunction library. By executing the code, we achieve accurate and deterministic\ncalculations, contrasting the stochastic nature of autoregressive token\nprediction, while the function library minimizes errors in formula usage. We\nalso propose a multimodal retrieval-augmented variant of GeoCoder, named\nRAG-GeoCoder, which incorporates a non-parametric memory module for retrieving\nfunctions from the geometry library, thereby reducing reliance on parametric\nmemory. Our modular code-finetuning approach enhances the geometric reasoning\ncapabilities of VLMs, yielding an average improvement of over 16% across\nvarious question complexities on the GeomVerse dataset compared to other\nfinetuning methods.\n","authors":["Aditya Sharma","Aman Dalmia","Mehran Kazemi","Amal Zouaq","Christopher J. Pal"],"pdf_url":"https://arxiv.org/pdf/2410.13510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13509v1","updated":"2024-10-17T12:53:29Z","published":"2024-10-17T12:53:29Z","title":"RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable\n  Data Rewards","summary":"  Retrieval-Augmented Generation (RAG) has proven its effectiveness in\nmitigating hallucinations in Large Language Models (LLMs) by retrieving\nknowledge from external resources. To adapt LLMs for RAG pipelines, current\napproaches use instruction tuning to optimize LLMs, improving their ability to\nutilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses\non equipping LLMs to handle diverse RAG tasks using different instructions.\nHowever, it trains RAG modules to overfit training signals and overlooks the\nvarying data preferences among agents within the RAG system. In this paper, we\npropose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG\nsystems by aligning data preferences between different RAG modules. DDR works\nby collecting the rewards to optimize each agent with a rollout method. This\nmethod prompts agents to sample some potential responses as perturbations,\nevaluates the impact of these perturbations on the whole RAG system, and\nsubsequently optimizes the agent to produce outputs that improve the\nperformance of the RAG system. Our experiments on various knowledge-intensive\ntasks demonstrate that DDR significantly outperforms the SFT method,\nparticularly for LLMs with smaller-scale parameters that depend more on the\nretrieved knowledge. Additionally, DDR exhibits a stronger capability to align\nthe data preference between RAG modules. The DDR method makes generation module\nmore effective in extracting key information from documents and mitigating\nconflicts between parametric memory and external knowledge. All codes are\navailable at https://github.com/OpenMatch/RAG-DDR.\n","authors":["Xinze Li","Sen Mei","Zhenghao Liu","Yukun Yan","Shuo Wang","Shi Yu","Zheni Zeng","Hao Chen","Ge Yu","Zhiyuan Liu","Maosong Sun","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.13509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13502v1","updated":"2024-10-17T12:48:14Z","published":"2024-10-17T12:48:14Z","title":"MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily\n  Complex Proofs","summary":"  Large language models (LLMs) can solve arithmetic word problems with high\naccuracy, but little is known about how well they generalize to problems that\nare more complex than the ones on which they have been trained. Empirical\ninvestigations of such questions are impeded by two major flaws of current\nevaluations: (i) much of the evaluation data is contaminated, in the sense that\nit has already been seen during training, and (ii) benchmark datasets do not\ncapture how problem proofs may be arbitrarily complex in various ways. As a\nstep towards addressing these issues, we present a framework for evaluating\nLLMs on problems that have arbitrarily complex arithmetic proofs, called\nMathGAP. MathGAP generates problems that follow fixed proof specifications --\nalong with chain-of-thought reasoning annotations -- enabling systematic\nstudies on generalization with respect to arithmetic proof complexity. We apply\nMathGAP to analyze how in-context learning interacts with generalization to\nproblems that have more complex proofs. We find that among the models tested,\nmost show a significant decrease in performance as proofs get deeper and wider.\nThis effect is more pronounced in complex, nonlinear proof structures, which\nare challenging even for GPT-4o. Surprisingly, providing in-context examples\nfrom the same distribution as the test set is not always beneficial for\nperformance. In particular, zero-shot prompting as well as demonstrating a\ndiverse range of examples that are less complex than the test data sometimes\nyield similar or higher accuracies.\n","authors":["Andreas Opedal","Haruki Shirakami","Bernhard Schölkopf","Abulhair Saparov","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.13502v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.13498v1","updated":"2024-10-17T12:43:49Z","published":"2024-10-17T12:43:49Z","title":"Enhancing Text Generation in Joint NLG/NLU Learning Through Curriculum\n  Learning, Semi-Supervised Training, and Advanced Optimization Techniques","summary":"  Text generation is the automated process of producing written or spoken\nlanguage using computational methods. It involves generating coherent and\ncontextually relevant text based on predefined rules or learned patterns.\nHowever, challenges in text generation arise from maintaining coherence,\nensuring diversity and creativity, and avoiding biases or inappropriate\ncontent. This research paper developed a novel approach to improve text\ngeneration in the context of joint Natural Language Generation (NLG) and\nNatural Language Understanding (NLU) learning. The data is prepared by\ngathering and preprocessing annotated datasets, including cleaning,\ntokenization, stemming, and stop-word removal. Feature extraction techniques\nsuch as POS tagging, Bag of words, and Term Frequency-Inverse Document\nFrequency (TF-IDF) are applied. Transformer-based encoders and decoders,\ncapturing long range dependencies and improving source-target sequence\nmodelling. Pre-trained language models like Optimized BERT are incorporated,\nalong with a Hybrid Redfox Artificial Hummingbird Algorithm (HRAHA).\nReinforcement learning with policy gradient techniques, semi-supervised\ntraining, improved attention mechanisms, and differentiable approximations like\nstraight-through Gumbel SoftMax estimator are employed to fine-tune the models\nand handle complex linguistic tasks effectively. The proposed model is\nimplemented using Python.\n","authors":["Rahimanuddin Shaik","Katikela Sreeharsha Kishore"],"pdf_url":"https://arxiv.org/pdf/2410.13498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13497v1","updated":"2024-10-17T12:43:47Z","published":"2024-10-17T12:43:47Z","title":"Repetition Neurons: How Do Language Models Produce Repetitions?","summary":"  This paper introduces repetition neurons, regarded as skill neurons\nresponsible for the repetition problem in text generation tasks. These neurons\nare progressively activated more strongly as repetition continues, indicating\nthat they perceive repetition as a task to copy the previous context\nrepeatedly, similar to in-context learning. We identify these repetition\nneurons by comparing activation values before and after the onset of repetition\nin texts generated by recent pre-trained language models. We analyze the\nrepetition neurons in three English and one Japanese pre-trained language\nmodels and observe similar patterns across them.\n","authors":["Tatsuya Hiraoka","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2410.13497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17453v3","updated":"2024-10-17T12:43:21Z","published":"2024-06-25T10:44:01Z","title":"Learning to Ask Informative Questions: Enhancing LLMs with Preference\n  Optimization and Expected Information Gain","summary":"  Questions are essential tools for acquiring the necessary information to\ncomplete information-seeking tasks. However, large language models (LLMs),\nespecially open-source models, often perform poorly in generating informative\nquestions, as measured by expected information gain (EIG). In this paper, we\npropose a method to enhance the informativeness of LLM-generated questions in\n20-question game dialogues. We sample multiple questions from the same model\n(LLAMA 2-CHAT 7B) for each game and create pairs of low-EIG and high-EIG\nquestions to apply a Direct Preference Optimization (DPO) algorithm. Our\nresults show that this method produces more effective questions (in terms of\nEIG), even in domains different from those used to train the DPO model.\n","authors":["Davide Mazzaccara","Alberto Testoni","Raffaella Bernardi"],"pdf_url":"https://arxiv.org/pdf/2406.17453v3.pdf","comment":"Accepted to EMNLP 2024 (Findings)"},{"id":"http://arxiv.org/abs/2405.02933v2","updated":"2024-10-17T12:39:05Z","published":"2024-05-05T13:42:25Z","title":"Relay Decoding: Concatenating Large Language Models for Machine\n  Translation","summary":"  Leveraging large language models for machine translation has demonstrated\npromising results. However, it does require the large language models to\npossess the capability of handling both the source and target languages in\nmachine translation. When it is challenging to find large models that support\nthe desired languages, resorting to continuous learning methods becomes a\ncostly endeavor. To mitigate these expenses, we propose an innovative approach\ncalled RD (Relay Decoding), which entails concatenating two distinct large\nmodels that individually support the source and target languages. By\nincorporating a simple mapping layer to facilitate the connection between these\ntwo models and utilizing a limited amount of parallel data for training, we\nsuccessfully achieve superior results in the machine translation task.\nExperimental results conducted on the Multi30k and WikiMatrix datasets validate\nthe effectiveness of our proposed method.\n","authors":["Chengpeng Fu","Xiaocheng Feng","Yichong Huang","Wenshuai Huo","Baohang Li","Hui Wang","Bin Qin","Ting Liu"],"pdf_url":"https://arxiv.org/pdf/2405.02933v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.13488v1","updated":"2024-10-17T12:32:00Z","published":"2024-10-17T12:32:00Z","title":"Seeing Through VisualBERT: A Causal Adventure on Memetic Landscapes","summary":"  Detecting offensive memes is crucial, yet standard deep neural network\nsystems often remain opaque. Various input attribution-based methods attempt to\ninterpret their behavior, but they face challenges with implicitly offensive\nmemes and non-causal attributions. To address these issues, we propose a\nframework based on a Structural Causal Model (SCM). In this framework,\nVisualBERT is trained to predict the class of an input meme based on both meme\ninput and causal concepts, allowing for transparent interpretation. Our\nqualitative evaluation demonstrates the framework's effectiveness in\nunderstanding model behavior, particularly in determining whether the model was\nright due to the right reason, and in identifying reasons behind\nmisclassification. Additionally, quantitative analysis assesses the\nsignificance of proposed modelling choices, such as de-confounding, adversarial\nlearning, and dynamic routing, and compares them with input attribution\nmethods. Surprisingly, we find that input attribution methods do not guarantee\ncausality within our framework, raising questions about their reliability in\nsafety-critical applications. The project page is at:\nhttps://newcodevelop.github.io/causality_adventure/\n","authors":["Dibyanayan Bandyopadhyay","Mohammed Hasanuzzaman","Asif Ekbal"],"pdf_url":"https://arxiv.org/pdf/2410.13488v1.pdf","comment":"Accepted at EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2410.10850v2","updated":"2024-10-17T11:52:38Z","published":"2024-10-06T07:40:11Z","title":"On the Reliability of Large Language Models to Misinformed and\n  Demographically-Informed Prompts","summary":"  We investigate and observe the behaviour and performance of Large Language\nModel (LLM)-backed chatbots in addressing misinformed prompts and questions\nwith demographic information within the domains of Climate Change and Mental\nHealth. Through a combination of quantitative and qualitative methods, we\nassess the chatbots' ability to discern the veracity of statements, their\nadherence to facts, and the presence of bias or misinformation in their\nresponses. Our quantitative analysis using True/False questions reveals that\nthese chatbots can be relied on to give the right answers to these close-ended\nquestions. However, the qualitative insights, gathered from domain experts,\nshows that there are still concerns regarding privacy, ethical implications,\nand the necessity for chatbots to direct users to professional services. We\nconclude that while these chatbots hold significant promise, their deployment\nin sensitive areas necessitates careful consideration, ethical oversight, and\nrigorous refinement to ensure they serve as a beneficial augmentation to human\nexpertise rather than an autonomous solution.\n","authors":["Toluwani Aremu","Oluwakemi Akinwehinmi","Chukwuemeka Nwagu","Syed Ishtiaque Ahmed","Rita Orji","Pedro Arnau Del Amo","Abdulmotaleb El Saddik"],"pdf_url":"https://arxiv.org/pdf/2410.10850v2.pdf","comment":"Study conducted between August and December 2023. Under review at\n  AAAI-AI Magazine. Submitted for archival purposes only"},{"id":"http://arxiv.org/abs/2410.13464v1","updated":"2024-10-17T11:48:57Z","published":"2024-10-17T11:48:57Z","title":"IterSelectTune: An Iterative Training Framework for Efficient\n  Instruction-Tuning Data Selection","summary":"  As large language models (LLMs) continue to advance, instruction tuning has\nbecome critical for improving their ability to generate accurate and\ncontextually appropriate responses. Although numerous instruction-tuning\ndatasets have been developed to enhance LLM performance, selecting high-quality\ninstruction data from large source datasets typically demands significant human\neffort. In this work, we introduce $\\textbf{IterSelectTune}$, an efficient,\ncost-effective iterative training policy for selecting high-quality instruction\ndata with no human involvement and limited reliance on GPT-4. By fine-tuning on\napproximately 20\\% of the source data, our method consistently outperforms\nmodels fine-tuned on the full dataset across multiple benchmarks and public\ntest datasets. These results highlight the effectiveness of our approach in\nenhancing LLM performance while reducing the computational resources required\nfor instruction tuning.\n","authors":["Jielin Song","Siyu Liu","Bin Zhu","Yanghui Rao"],"pdf_url":"https://arxiv.org/pdf/2410.13464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13461v1","updated":"2024-10-17T11:46:33Z","published":"2024-10-17T11:46:33Z","title":"Progressive Mixed-Precision Decoding for Efficient LLM Inference","summary":"  In spite of the great potential of large language models (LLMs) across\nvarious tasks, their deployment on resource-constrained devices remains\nchallenging due to their excessive computational and memory demands.\nQuantization has emerged as an effective solution by storing weights in reduced\nprecision. However, utilizing low precisions (i.e.~2/3-bit) to substantially\nalleviate the memory-boundedness of LLM decoding, still suffers from\nprohibitive performance drop. In this work, we argue that existing approaches\nfail to explore the diversity in computational patterns, redundancy, and\nsensitivity to approximations of the different phases of LLM inference,\nresorting to a uniform quantization policy throughout. Instead, we propose a\nnovel phase-aware method that selectively allocates precision during different\nphases of LLM inference, achieving both strong context extraction during\nprefill and efficient memory bandwidth utilization during decoding. To further\naddress the memory-boundedness of the decoding phase, we introduce Progressive\nMixed-Precision Decoding (PMPD), a technique that enables the gradual lowering\nof precision deeper in the generated sequence, together with a spectrum of\nprecision-switching schedulers that dynamically drive the precision-lowering\ndecisions in either task-adaptive or prompt-adaptive manner. Extensive\nevaluation across diverse language tasks shows that when targeting Nvidia GPUs,\nPMPD achieves 1.4$-$12.2$\\times$ speedup in matrix-vector multiplications over\nfp16 models, while when targeting an LLM-optimized NPU, our approach delivers a\nthroughput gain of 3.8$-$8.0$\\times$ over fp16 models and up to 1.54$\\times$\nover uniform quantization approaches while preserving the output quality.\n","authors":["Hao Mark Chen","Fuwen Tan","Alexandros Kouris","Royson Lee","Hongxiang Fan","Stylianos I. Venieris"],"pdf_url":"https://arxiv.org/pdf/2410.13461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16807v2","updated":"2024-10-17T11:46:18Z","published":"2024-06-24T17:19:34Z","title":"Beyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback\n  for Text-to-Image Generation","summary":"  Human feedback plays a critical role in learning and refining reward models\nfor text-to-image generation, but the optimal form the feedback should take for\nlearning an accurate reward function has not been conclusively established.\nThis paper investigates the effectiveness of fine-grained feedback which\ncaptures nuanced distinctions in image quality and prompt-alignment, compared\nto traditional coarse-grained feedback (for example, thumbs up/down or ranking\nbetween a set of options). While fine-grained feedback holds promise,\nparticularly for systems catering to diverse societal preferences, we show that\ndemonstrating its superiority to coarse-grained feedback is not automatic.\nThrough experiments on real and synthetic preference data, we surface the\ncomplexities of building effective models due to the interplay of model choice,\nfeedback type, and the alignment between human judgment and computational\ninterpretation. We identify key challenges in eliciting and utilizing\nfine-grained feedback, prompting a reassessment of its assumed benefits and\npracticality. Our findings -- e.g., that fine-grained feedback can lead to\nworse models for a fixed budget, in some settings; however, in controlled\nsettings with known attributes, fine grained rewards can indeed be more helpful\n-- call for careful consideration of feedback attributes and potentially beckon\nnovel modeling approaches to appropriately unlock the potential value of\nfine-grained feedback in-the-wild.\n","authors":["Katherine M. Collins","Najoung Kim","Yonatan Bitton","Verena Rieser","Shayegan Omidshafiei","Yushi Hu","Sherol Chen","Senjuti Dutta","Minsuk Chang","Kimin Lee","Youwei Liang","Georgina Evans","Sahil Singla","Gang Li","Adrian Weller","Junfeng He","Deepak Ramachandran","Krishnamurthy Dj Dvijotham"],"pdf_url":"https://arxiv.org/pdf/2406.16807v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13460v1","updated":"2024-10-17T11:43:16Z","published":"2024-10-17T11:43:16Z","title":"Breaking the Manual Annotation Bottleneck: Creating a Comprehensive\n  Legal Case Criticality Dataset through Semi-Automated Labeling","summary":"  Predicting case criticality helps legal professionals in the court system\nmanage large volumes of case law. This paper introduces the Criticality\nPrediction dataset, a new resource for evaluating the potential influence of\nSwiss Federal Supreme Court decisions on future jurisprudence. Unlike existing\napproaches that rely on resource-intensive manual annotations, we\nsemi-automatically derive labels leading to a much larger dataset than\notherwise possible. Our dataset features a two-tier labeling system: (1) the\nLD-Label, which identifies cases published as Leading Decisions (LD), and (2)\nthe Citation-Label, which ranks cases by their citation frequency and recency.\nThis allows for a more nuanced evaluation of case importance. We evaluate\nseveral multilingual models, including fine-tuned variants and large language\nmodels, and find that fine-tuned models consistently outperform zero-shot\nbaselines, demonstrating the need for task-specific adaptation. Our\ncontributions include the introduction of this task and the release of a\nmultilingual dataset to the research community.\n","authors":["Ronja Stern","Ken Kawamura","Matthias Stürmer","Ilias Chalkidis","Joel Niklaus"],"pdf_url":"https://arxiv.org/pdf/2410.13460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13458v1","updated":"2024-10-17T11:38:54Z","published":"2024-10-17T11:38:54Z","title":"MedINST: Meta Dataset of Biomedical Instructions","summary":"  The integration of large language model (LLM) techniques in the field of\nmedical analysis has brought about significant advancements, yet the scarcity\nof large, diverse, and well-annotated datasets remains a major challenge.\nMedical data and tasks, which vary in format, size, and other parameters,\nrequire extensive preprocessing and standardization for effective use in\ntraining LLMs. To address these challenges, we introduce MedINST, the Meta\nDataset of Biomedical Instructions, a novel multi-domain, multi-task\ninstructional meta-dataset. MedINST comprises 133 biomedical NLP tasks and over\n7 million training samples, making it the most comprehensive biomedical\ninstruction dataset to date. Using MedINST as the meta dataset, we curate\nMedINST32, a challenging benchmark with different task difficulties aiming to\nevaluate LLMs' generalization ability. We fine-tune several LLMs on MedINST and\nevaluate on MedINST32, showcasing enhanced cross-task generalization.\n","authors":["Wenhan Han","Meng Fang","Zihan Zhang","Yu Yin","Zirui Song","Ling Chen","Mykola Pechenizkiy","Qingyu Chen"],"pdf_url":"https://arxiv.org/pdf/2410.13458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13456v1","updated":"2024-10-17T11:34:07Z","published":"2024-10-17T11:34:07Z","title":"Unlocking Legal Knowledge: A Multilingual Dataset for Judicial\n  Summarization in Switzerland","summary":"  Legal research is a time-consuming task that most lawyers face on a daily\nbasis. A large part of legal research entails looking up relevant caselaw and\nbringing it in relation to the case at hand. Lawyers heavily rely on summaries\n(also called headnotes) to find the right cases quickly. However, not all\ndecisions are annotated with headnotes and writing them is time-consuming.\nAutomated headnote creation has the potential to make hundreds of thousands of\ndecisions more accessible for legal research in Switzerland alone. To kickstart\nthis, we introduce the Swiss Leading Decision Summarization ( SLDS) dataset, a\nnovel cross-lingual resource featuring 18K court rulings from the Swiss Federal\nSupreme Court (SFSC), in German, French, and Italian, along with German\nheadnotes. We fine-tune and evaluate three mT5 variants, along with proprietary\nmodels. Our analysis highlights that while proprietary models perform well in\nzero-shot and one-shot settings, fine-tuned smaller models still provide a\nstrong competitive edge. We publicly release the dataset to facilitate further\nresearch in multilingual legal summarization and the development of assistive\ntechnologies for legal professionals\n","authors":["Luca Rolshoven","Vishvaksenan Rasiah","Srinanda Brügger Bose","Matthias Stürmer","Joel Niklaus"],"pdf_url":"https://arxiv.org/pdf/2410.13456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11843v2","updated":"2024-10-17T11:26:10Z","published":"2024-07-16T15:24:44Z","title":"InferAct: Inferring Safe Actions for LLM-Based Agents Through Preemptive\n  Evaluation and Human Feedback","summary":"  A crucial requirement for deploying LLM-based agents in real-life\napplications is the robustness against risky or even irreversible mistakes.\nHowever, the existing research lacks a focus on preemptive evaluation of\nreasoning trajectories performed by LLM agents, leading to a gap in ensuring\nsafe and reliable operations. To explore better solutions, this paper\nintroduces InferAct, a novel approach that leverages the belief reasoning\nability of LLMs, grounded in Theory-of-Mind, to proactively detect potential\nerrors before risky actions are executed (e.g., `buy-now' in automatic online\ntrading or web shopping). InferAct acts as a human proxy, detecting unsafe\nactions and alerting users for intervention, which helps prevent irreversible\nrisks in time and enhances the actor agent's decision-making process.\nExperiments on three widely-used tasks demonstrate the effectiveness of\nInferAct, presenting a novel solution for safely developing LLM agents in\nenvironments involving critical decision-making.\n","authors":["Haishuo Fang","Xiaodan Zhu","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2407.11843v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13445v1","updated":"2024-10-17T11:19:44Z","published":"2024-10-17T11:19:44Z","title":"Parameter-efficient Adaptation of Multilingual Multimodal Models for\n  Low-resource ASR","summary":"  Automatic speech recognition (ASR) for low-resource languages remains a\nchallenge due to the scarcity of labeled training data. Parameter-efficient\nfine-tuning and text-only adaptation are two popular methods that have been\nused to address such low-resource settings. In this work, we investigate how\nthese techniques can be effectively combined using a multilingual multimodal\nmodel like SeamlessM4T. Multimodal models are able to leverage unlabeled text\nvia text-only adaptation with further parameter-efficient ASR fine-tuning, thus\nboosting ASR performance. We also show cross-lingual transfer from a\nhigh-resource language, achieving up to a relative 17% WER reduction over a\nbaseline in a zero-shot setting without any labeled speech.\n","authors":["Abhishek Gupta","Amruta Parulekar","Sameep Chattopadhyay","Preethi Jyothi"],"pdf_url":"https://arxiv.org/pdf/2410.13445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13443v1","updated":"2024-10-17T11:18:23Z","published":"2024-10-17T11:18:23Z","title":"NLIP_Lab-IITH Multilingual MT System for WAT24 MT Shared Task","summary":"  This paper describes NLIP Lab's multilingual machine translation system for\nthe WAT24 shared task on multilingual Indic MT task for 22 scheduled languages\nbelonging to 4 language families. We explore pre-training for Indic languages\nusing alignment agreement objectives. We utilize bi-lingual dictionaries to\nsubstitute words from source sentences. Furthermore, we fine-tuned language\ndirection-specific multilingual translation models using small and high-quality\nseed data. Our primary submission is a 243M parameters multilingual translation\nmodel covering 22 Indic languages. In the IN22-Gen benchmark, we achieved an\naverage chrF++ score of 46.80 and 18.19 BLEU score for the En-Indic direction.\nIn the Indic-En direction, we achieved an average chrF++ score of 56.34 and\n30.82 BLEU score. In the In22-Conv benchmark, we achieved an average chrF++\nscore of 43.43 and BLEU score of 16.58 in the En-Indic direction, and in the\nIndic-En direction, we achieved an average of 52.44 and 29.77 for chrF++ and\nBLEU respectively. Our model\\footnote{Our code and models are available at\n\\url{https://github.com/maharajbrahma/WAT2024-MultiIndicMT}} is competitive\nwith IndicTransv1 (474M parameter model).\n","authors":["Maharaj Brahma","Pramit Sahoo","Maunendra Sankar Desarkar"],"pdf_url":"https://arxiv.org/pdf/2410.13443v1.pdf","comment":"WMT 24 WAT Shared Task IndicMultiMT (Best System)"},{"id":"http://arxiv.org/abs/2410.13439v1","updated":"2024-10-17T11:12:55Z","published":"2024-10-17T11:12:55Z","title":"Similarity-Dissimilarity Loss with Supervised Contrastive Learning for\n  Multi-label Classification","summary":"  Supervised contrastive learning has been explored in making use of label\ninformation for multi-label classification, but determining positive samples in\nmulti-label scenario remains challenging. Previous studies have examined\nstrategies for identifying positive samples, considering label overlap\nproportion between anchors and samples. However, they ignore various relations\nbetween given anchors and samples, as well as how to dynamically adjust the\nweights in contrastive loss functions based on different relations, leading to\ngreat ambiguity. In this paper, we introduce five distinct relations between\nmulti-label samples and propose a Similarity-Dissimilarity Loss with\ncontrastive learning for multi-label classification. Our loss function\nre-weights the loss by computing the similarity and dissimilarity between\npositive samples and a given anchor based on the introduced relations. We\nmainly conduct experiments for multi-label text classification on MIMIC\ndatasets, then further extend the evaluation on MS-COCO. The Experimental\nresults show that our proposed loss effectively improves the performance on all\nencoders under supervised contrastive learning paradigm, demonstrating its\neffectiveness and robustness.\n","authors":["Guangming Huang","Yunfei Long","Cunjin Luo","Sheng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.13439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12298v2","updated":"2024-10-17T11:00:37Z","published":"2024-10-16T06:57:18Z","title":"Pyramid-Driven Alignment: Pyramid Principle Guided Integration of Large\n  Language Models and Knowledge Graphs","summary":"  Large Language Models (LLMs) possess impressive reasoning abilities but are\nprone to generating incorrect information, often referred to as hallucinations.\nWhile incorporating external Knowledge Graphs (KGs) can partially mitigate this\nissue, existing methods primarily treat KGs as static knowledge repositories,\noverlooking the critical disparity between KG and LLM knowledge, and failing to\nfully exploit the reasoning capabilities inherent in KGs. To address these\nlimitations, we propose Pyramid-Driven Alignment (PDA), a novel framework for\nseamlessly integrating LLMs with KGs. PDA utilizes Pyramid Principle analysis\nto construct a hierarchical pyramid structure. This structure is designed to\nreflect the input question and generate more validated deductive knowledge,\nthereby enhancing the alignment of LLMs and KGs and ensuring more cohesive\nintegration. Furthermore, PDA employs a recursive mechanism to harness the\nunderlying reasoning abilities of KGs, resulting in more accurate knowledge\nretrieval for question-answering tasks. Our experimental results reveal a\nsubstantial performance advantage of PDA over state-of-the-art baselines, with\nimprovements reaching 26.70% and 26.78%.\n","authors":["Lei Sun","Xinchen Wang","Youdi Li"],"pdf_url":"https://arxiv.org/pdf/2410.12298v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14928v2","updated":"2024-10-17T10:30:41Z","published":"2024-06-21T07:37:19Z","title":"Autonomous Agents for Collaborative Task under Information Asymmetry","summary":"  Large Language Model Multi-Agent Systems (LLM-MAS) have achieved great\nprogress in solving complex tasks. It performs communication among agents\nwithin the system to collaboratively solve tasks, under the premise of shared\ninformation. However, when agents' collaborations are leveraged to perform\nmulti-person tasks, a new challenge arises due to information asymmetry, since\neach agent can only access the information of its human user. Previous MAS\nstruggle to complete tasks under this condition. To address this, we propose a\nnew MAS paradigm termed iAgents, which denotes Informative Multi-Agent Systems.\nIn iAgents, the human social network is mirrored in the agent network, where\nagents proactively exchange human information necessary for task resolution,\nthereby overcoming information asymmetry. iAgents employs a novel agent\nreasoning mechanism, InfoNav, to navigate agents' communication toward\neffective information exchange. Together with InfoNav, iAgents organizes human\ninformation in a mixed memory to provide agents with accurate and comprehensive\ninformation for exchange. Additionally, we introduce InformativeBench, the\nfirst benchmark tailored for evaluating LLM agents' task-solving ability under\ninformation asymmetry. Experimental results show that iAgents can collaborate\nwithin a social network of 140 individuals and 588 relationships, autonomously\ncommunicate over 30 turns, and retrieve information from nearly 70,000 messages\nto complete tasks within 3 minutes.\n","authors":["Wei Liu","Chenxi Wang","Yifei Wang","Zihao Xie","Rennai Qiu","Yufan Dang","Zhuoyun Du","Weize Chen","Cheng Yang","Chen Qian"],"pdf_url":"https://arxiv.org/pdf/2406.14928v2.pdf","comment":"32 pages, 12 figures, 6 tables, accepted by NeurIPS 2024, see detail\n  at https://thinkwee.top/iagents"},{"id":"http://arxiv.org/abs/2410.13413v1","updated":"2024-10-17T10:23:24Z","published":"2024-10-17T10:23:24Z","title":"Think Thrice Before You Act: Progressive Thought Refinement in Large\n  Language Models","summary":"  Recent advancements in large language models (LLMs) have demonstrated that\nprogressive refinement, rather than providing a single answer, results in more\naccurate and thoughtful outputs. However, existing methods often rely heavily\non supervision signals to evaluate previous responses, making it difficult to\nassess output quality in more open-ended scenarios effectively. Additionally,\nthese methods are typically designed for specific tasks, which limits their\ngeneralization to new domains. To address these limitations, we propose\nProgressive Thought Refinement (PTR), a framework that enables LLMs to refine\ntheir responses progressively. PTR operates in two phases: (1) Thought data\nconstruction stage: We propose a weak and strong model collaborative selection\nstrategy to build a high-quality progressive refinement dataset to ensure\nlogical consistency from thought to answers, and the answers are gradually\nrefined in each round. (2) Thought-Mask Fine-Tuning Phase: We design a training\nstructure to mask the \"thought\" and adjust loss weights to encourage LLMs to\nrefine prior thought, teaching them to implicitly understand \"how to improve\"\nrather than \"what is correct.\" Experimental results show that PTR significantly\nenhances LLM performance across ten diverse tasks (avg. from 49.6% to 53.5%)\nwithout task-specific fine-tuning. Notably, in more open-ended tasks, LLMs also\ndemonstrate substantial improvements in the quality of responses beyond mere\naccuracy, suggesting that PTR truly teaches LLMs to self-improve over time.\n","authors":["Chengyu Du","Jinyi Han","Yizhou Ying","Aili Chen","Qianyu He","Haokun Zhao","Sirui Xia","Haoran Guo","Jiaqing Liang","Zulong Chen","Liangyue Li","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2410.13413v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.13409v1","updated":"2024-10-17T10:16:56Z","published":"2024-10-17T10:16:56Z","title":"Attr-Int: A Simple and Effective Entity Alignment Framework for\n  Heterogeneous Knowledge Graphs","summary":"  Entity alignment (EA) refers to the task of linking entities in different\nknowledge graphs (KGs). Existing EA methods rely heavily on structural\nisomorphism. However, in real-world KGs, aligned entities usually have\nnon-isomorphic neighborhood structures, which paralyses the application of\nthese structure-dependent methods. In this paper, we investigate and tackle the\nproblem of entity alignment between heterogeneous KGs. First, we propose two\nnew benchmarks to closely simulate real-world EA scenarios of heterogeneity.\nThen we conduct extensive experiments to evaluate the performance of\nrepresentative EA methods on the new benchmarks. Finally, we propose a simple\nand effective entity alignment framework called Attr-Int, in which innovative\nattribute information interaction methods can be seamlessly integrated with any\nembedding encoder for entity alignment, improving the performance of existing\nentity alignment techniques. Experiments demonstrate that our framework\noutperforms the state-of-the-art approaches on two new benchmarks.\n","authors":["Linyan Yang","Jingwei Cheng","Chuanhao Xu","Xihao Wang","Jiayi Li","Fu Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.13409v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13408v1","updated":"2024-10-17T10:14:52Z","published":"2024-10-17T10:14:52Z","title":"MoR: Mixture of Ranks for Low-Rank Adaptation Tuning","summary":"  Low-Rank Adaptation (LoRA) drives research to align its performance with full\nfine-tuning. However, significant challenges remain: (1) Simply increasing the\nrank size of LoRA does not effectively capture high-rank information, which\nleads to a performance bottleneck.(2) MoE-style LoRA methods substantially\nincrease parameters and inference latency, contradicting the goals of efficient\nfine-tuning and ease of application. To address these challenges, we introduce\nMixture of Ranks (MoR), which learns rank-specific information for different\ntasks based on input and efficiently integrates multi-rank information. We\nfirstly propose a new framework that equates the integration of multiple LoRAs\nto expanding the rank of LoRA. Moreover, we hypothesize that low-rank LoRA\nalready captures sufficient intrinsic information, and MoR can derive high-rank\ninformation through mathematical transformations of the low-rank components.\nThus, MoR can reduces the learning difficulty of LoRA and enhances its\nmulti-task capabilities. MoR achieves impressive results, with MoR delivering a\n1.31\\% performance improvement while using only 93.93\\% of the parameters\ncompared to baseline methods.\n","authors":["Chuanyu Tang","Yilong Chen","Zhenyu Zhang","Junyuan Shang","Wenyuan Zhang","Yong Huang","Tingwen Liu"],"pdf_url":"https://arxiv.org/pdf/2410.13408v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.13400v1","updated":"2024-10-17T09:54:54Z","published":"2024-10-17T09:54:54Z","title":"Towards Hybrid Intelligence in Journalism: Findings and Lessons Learnt\n  from a Collaborative Analysis of Greek Political Rhetoric by ChatGPT and\n  Humans","summary":"  This chapter introduces a research project titled \"Analyzing the Political\nDiscourse: A Collaboration Between Humans and Artificial Intelligence\", which\nwas initiated in preparation for Greece's 2023 general elections. The project\nfocused on the analysis of political leaders' campaign speeches, employing\nArtificial Intelligence (AI), in conjunction with an interdisciplinary team\ncomprising journalists, a political scientist, and data scientists. The chapter\ndelves into various aspects of political discourse analysis, including\nsentiment analysis, polarization, populism, topic detection, and Named Entities\nRecognition (NER). This experimental study investigates the capabilities of\nlarge language model (LLMs), and in particular OpenAI's ChatGPT, for analyzing\npolitical speech, evaluates its strengths and weaknesses, and highlights the\nessential role of human oversight in using AI in journalism projects and\npotentially other societal sectors. The project stands as an innovative example\nof human-AI collaboration (known also as \"hybrid intelligence\") within the\nrealm of digital humanities, offering valuable insights for future initiatives.\n","authors":["Thanasis Troboukis","Kelly Kiki","Antonis Galanopoulos","Pavlos Sermpezis","Stelios Karamanidis","Ilias Dimitriadis","Athena Vakali"],"pdf_url":"https://arxiv.org/pdf/2410.13400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13396v1","updated":"2024-10-17T09:48:08Z","published":"2024-10-17T09:48:08Z","title":"Linguistically Grounded Analysis of Language Models using Shapley Head\n  Values","summary":"  Understanding how linguistic knowledge is encoded in language models is\ncrucial for improving their generalisation capabilities. In this paper, we\ninvestigate the processing of morphosyntactic phenomena, by leveraging a\nrecently proposed method for probing language models via Shapley Head Values\n(SHVs). Using the English language BLiMP dataset, we test our approach on two\nwidely used models, BERT and RoBERTa, and compare how linguistic constructions\nsuch as anaphor agreement and filler-gap dependencies are handled. Through\nquantitative pruning and qualitative clustering analysis, we demonstrate that\nattention heads responsible for processing related linguistic phenomena cluster\ntogether. Our results show that SHV-based attributions reveal distinct patterns\nacross both models, providing insights into how language models organize and\nprocess linguistic information. These findings support the hypothesis that\nlanguage models learn subnetworks corresponding to linguistic theory, with\npotential implications for cross-linguistic model analysis and interpretability\nin Natural Language Processing (NLP).\n","authors":["Marcell Fekete","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2410.13396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13394v1","updated":"2024-10-17T09:45:32Z","published":"2024-10-17T09:45:32Z","title":"Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs","summary":"  Evaluating machine-generated text remains a significant challenge in NLP,\nespecially for non-English languages. Current methodologies, including\nautomated metrics, human assessments, and LLM-based evaluations, predominantly\nfocus on English, revealing a significant gap in multilingual evaluation\nframeworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an\nextensible framework that includes evaluator LLMs (Hercule) and a novel test\nset (Recon) specifically designed for multilingual evaluation. Our test set\nfeatures 500 human-annotated instructions spanning various task capabilities\nalong with human judgment scores across six languages. This would enable\nbenchmarking of general-purpose multilingual LLMs and facilitate\nmeta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a\ncross-lingual evaluation model that addresses the scarcity of reference answers\nin the target language by learning to assign scores to responses based on\neasily available reference answers in English. Our experiments demonstrate that\nHercule aligns more closely with human judgments compared to proprietary\nmodels, demonstrating the effectiveness of such cross-lingual evaluation in low\nresource scenarios. Further, it is also effective in zero-shot evaluation on\nunseen languages. This study is the first comprehensive examination of\ncross-lingual evaluation using LLMs, presenting a scalable and effective\napproach for multilingual assessment. All code, datasets, and models will be\npublicly available to enable further research in this important area.\n","authors":["Sumanth Doddapaneni","Mohammed Safi Ur Rahman Khan","Dilip Venkatesh","Raj Dabre","Anoop Kunchukuttan","Mitesh M. Khapra"],"pdf_url":"https://arxiv.org/pdf/2410.13394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13392v1","updated":"2024-10-17T09:42:30Z","published":"2024-10-17T09:42:30Z","title":"Metacognitive Monitoring: A Human Ability Beyond Generative Artificial\n  Intelligence","summary":"  Large language models (LLMs) have shown impressive alignment with human\ncognitive processes, raising questions about the extent of their similarity to\nhuman cognition. This study investigates whether LLMs, specifically ChatGPT,\npossess metacognitive monitoring abilities akin to humans-particularly in\npredicting memory performance on an item-by-item basis. We employed a\ncross-agent prediction model to compare the metacognitive performance of humans\nand ChatGPT in a language-based memory task involving garden-path sentences\npreceded by either fitting or unfitting context sentences. Both humans and\nChatGPT rated the memorability of these sentences; humans then completed a\nsurprise recognition memory test. Our findings reveal a significant positive\nrelationship between humans' memorability ratings and their actual recognition\nperformance, indicating reliable metacognitive monitoring. In contrast, ChatGPT\ndid not exhibit a similar predictive capability. Bootstrapping analyses\ndemonstrated that none of the GPT models tested (GPT-3.5-turbo, GPT-4-turbo,\nGPT-4o) could accurately predict human memory performance on a per-item basis.\nThis suggests that, despite their advanced language processing abilities and\nalignment with human cognition at the object level, current LLMs lack the\nmetacognitive mechanisms that enable humans to anticipate their memory\nperformance. These results highlight a fundamental difference between human and\nAI cognition at the metacognitive level. Addressing this gap is crucial for\ndeveloping AI systems capable of effective self-monitoring and adaptation to\nhuman needs, thereby enhancing human-AI interactions across domains such as\neducation and personalized learning.\n","authors":["Markus Huff","Elanur Ulakçı"],"pdf_url":"https://arxiv.org/pdf/2410.13392v1.pdf","comment":"28 pages, 2 figures. arXiv admin note: substantial text overlap with\n  arXiv:2403.05152"},{"id":"http://arxiv.org/abs/2410.13385v1","updated":"2024-10-17T09:37:20Z","published":"2024-10-17T09:37:20Z","title":"On the Use of Audio to Improve Dialogue Policies","summary":"  With the significant progress of speech technologies, spoken goal-oriented\ndialogue systems are becoming increasingly popular. One of the main modules of\na dialogue system is typically the dialogue policy, which is responsible for\ndetermining system actions. This component usually relies only on audio\ntranscriptions, being strongly dependent on their quality and ignoring very\nimportant extralinguistic information embedded in the user's speech. In this\npaper, we propose new architectures to add audio information by combining\nspeech and text embeddings using a Double Multi-Head Attention component. Our\nexperiments show that audio embedding-aware dialogue policies outperform\ntext-based ones, particularly in noisy transcription scenarios, and that how\ntext and audio embeddings are combined is crucial to improve performance. We\nobtained a 9.8% relative improvement in the User Request Score compared to an\nonly-text-based dialogue system on the DSTC2 dataset.\n","authors":["Daniel Roncel","Federico Costa","Javier Hernando"],"pdf_url":"https://arxiv.org/pdf/2410.13385v1.pdf","comment":"IberSpeech 2024"},{"id":"http://arxiv.org/abs/2410.12532v2","updated":"2024-10-17T09:22:41Z","published":"2024-10-16T13:10:27Z","title":"MedAide: Towards an Omni Medical Aide via Specialized LLM-based\n  Multi-Agent Collaboration","summary":"  Large Language Model (LLM)-driven interactive systems currently show\npotential promise in healthcare domains. Despite their remarkable capabilities,\nLLMs typically lack personalized recommendations and diagnosis analysis in\nsophisticated medical applications, causing hallucinations and performance\nbottlenecks. To address these challenges, this paper proposes MedAide, an\nLLM-based omni medical multi-agent collaboration framework for specialized\nhealthcare services. Specifically, MedAide first performs query rewriting\nthrough retrieval-augmented generation to accomplish accurate medical intent\nunderstanding. Immediately, we devise a contextual encoder to obtain intent\nprototype embeddings, which are used to recognize fine-grained intents by\nsimilarity matching. According to the intent relevance, the activated agents\ncollaborate effectively to provide integrated decision analysis. Extensive\nexperiments are conducted on four medical benchmarks with composite intents.\nExperimental results from automated metrics and expert doctor evaluations show\nthat MedAide outperforms current LLMs and improves their medical proficiency\nand strategic reasoning.\n","authors":["Jinjie Wei","Dingkang Yang","Yanshu Li","Qingyao Xu","Zhaoyu Chen","Mingcheng Li","Yue Jiang","Xiaolu Hou","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12532v2.pdf","comment":"LLM-based Multi-Agent Collaboration for Medical Applications"},{"id":"http://arxiv.org/abs/2410.13360v1","updated":"2024-10-17T09:10:26Z","published":"2024-10-17T09:10:26Z","title":"Remember, Retrieve and Generate: Understanding Infinite Visual Concepts\n  as Your Personalized Assistant","summary":"  The development of large language models (LLMs) has significantly enhanced\nthe capabilities of multimodal LLMs (MLLMs) as general assistants. However,\nlack of user-specific knowledge still restricts their application in human's\ndaily life. In this paper, we introduce the Retrieval Augmented Personalization\n(RAP) framework for MLLMs' personalization. Starting from a general MLLM, we\nturn it into a personalized assistant in three steps. (a) Remember: We design a\nkey-value database to store user-related information, e.g., user's name, avatar\nand other attributes. (b) Retrieve: When the user initiates a conversation, RAP\nwill retrieve relevant information from the database using a multimodal\nretriever. (c) Generate: The input query and retrieved concepts' information\nare fed into MLLMs to generate personalized, knowledge-augmented responses.\nUnlike previous methods, RAP allows real-time concept editing via updating the\nexternal database. To further improve generation quality and alignment with\nuser-specific information, we design a pipeline for data collection and create\na specialized dataset for personalized training of MLLMs. Based on the dataset,\nwe train a series of MLLMs as personalized multimodal assistants. By\npretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual\nconcepts without additional finetuning. Our models demonstrate outstanding\nflexibility and generation quality across a variety of tasks, such as\npersonalized image captioning, question answering and visual recognition. The\ncode, data and models are available at https://github.com/Hoar012/RAP-MLLM.\n","authors":["Haoran Hao","Jiaming Han","Changsheng Li","Yu-Feng Li","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2410.13360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13352v1","updated":"2024-10-17T09:03:38Z","published":"2024-10-17T09:03:38Z","title":"LAR-ECHR: A New Legal Argument Reasoning Task and Dataset for Cases of\n  the European Court of Human Rights","summary":"  We present Legal Argument Reasoning (LAR), a novel task designed to evaluate\nthe legal reasoning capabilities of Large Language Models (LLMs). The task\nrequires selecting the correct next statement (from multiple choice options) in\na chain of legal arguments from court proceedings, given the facts of the case.\nWe constructed a dataset (LAR-ECHR) for this task using cases from the European\nCourt of Human Rights (ECHR). We evaluated seven general-purpose LLMs on\nLAR-ECHR and found that (a) the ranking of the models is aligned with that of\nLegalBench, an established US-based legal reasoning benchmark, even though\nLAR-ECHR is based on EU law, (b) LAR-ECHR distinguishes top models more\nclearly, compared to LegalBench, (c) even the best model (GPT-4o) obtains 75.8%\naccuracy on LAR-ECHR, indicating significant potential for further model\nimprovement. The process followed to construct LAR-ECHR can be replicated with\ncases from other legal systems.\n","authors":["Odysseas S. Chlapanis","Dimitrios Galanis","Ion Androutsopoulos"],"pdf_url":"https://arxiv.org/pdf/2410.13352v1.pdf","comment":"Published in Natural Legal Language Processing (NLLP) 2024 workshop"},{"id":"http://arxiv.org/abs/2410.13351v1","updated":"2024-10-17T09:02:28Z","published":"2024-10-17T09:02:28Z","title":"Representation Learning of Structured Data for Medical Foundation Models","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious domains, including healthcare. However, their ability to effectively\nrepresent structured non-textual data, such as the alphanumeric medical codes\nused in records like ICD-10 or SNOMED-CT, is limited and has been particularly\nexposed in recent research. This paper examines the challenges LLMs face in\nprocessing medical codes due to the shortcomings of current tokenization\nmethods. As a result, we introduce the UniStruct architecture to design a\nmultimodal medical foundation model of unstructured text and structured data,\nwhich addresses these challenges by adapting subword tokenization techniques\nspecifically for the structured medical codes. Our approach is validated\nthrough model pre-training on both an extensive internal medical database and a\npublic repository of structured medical records. Trained on over 1 billion\ntokens on the internal medical database, the proposed model achieves up to a\n23% improvement in evaluation metrics, with around 2% gain attributed to our\nproposed tokenization. Additionally, when evaluated on the EHRSHOT public\nbenchmark with a 1/1000 fraction of the pre-training data, the UniStruct model\nimproves performance on over 42% of the downstream tasks. Our approach not only\nenhances the representation and generalization capabilities of patient-centric\nmodels but also bridges a critical gap in representation learning models'\nability to handle complex structured medical data, alongside unstructured text.\n","authors":["Vijay Prakash Dwivedi","Viktor Schlegel","Andy T. Liu","Thanh-Tung Nguyen","Abhinav Ramesh Kashyap","Jeng Wei","Wei-Hsian Yin","Stefan Winkler","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2410.13351v1.pdf","comment":"NeurIPS 2024 Workshop on Unifying Representations in Neural Models\n  (UniReps 2024)"},{"id":"http://arxiv.org/abs/2404.11916v2","updated":"2024-10-17T09:01:27Z","published":"2024-04-18T05:43:50Z","title":"Skeleton: A New Framework for Accelerating Language Models via Task\n  Neuron Localized Prompt Tuning","summary":"  Prompt tuning methods have shown comparable performance to general training\nmethods as parameter-efficient fine-tuning (PEFT) methods in various natural\nlanguage understanding tasks. However, existing prompt tuning methods still\nutilize the entire model architecture even when solving a specific task, which\nprevents them from accelerating inference speed during the application\nprocedure. In this paper, we propose a novel prompt tuning framework called\nSkeleton to efficiently utilize a language model in terms of memory and time\ncomplexity for solving various tasks, retaining only task-relevant neurons by\nusing an explainability method. From our framework, we can efficiently solve\nvarious tasks by using only task-relevant neurons and prepending adequate\ntask-specific prompt tokens with only a single language model. Experiments\nreveal that our method significantly enhances inference efficiency (at most x\n1.73 speed up) for various widely used benchmarks, showing comparable\nperformance to the prompt tuning method. Moreover, our method is applicable\nacross various transformer-based architectures, confirming its practicality and\nscalability.\n","authors":["Nakyeong Yang","Jiwon Moon","Junseok Kim","Yunah Jang","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2404.11916v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2410.13344v1","updated":"2024-10-17T08:55:18Z","published":"2024-10-17T08:55:18Z","title":"Cerberus: Efficient Inference with Adaptive Parallel Decoding and\n  Sequential Knowledge Enhancement","summary":"  Large language models (LLMs) often face a bottleneck in inference speed due\nto their reliance on auto-regressive decoding. Recently, parallel decoding has\nshown significant promise in enhancing inference efficiency. However, we have\nidentified two key issues with existing parallel decoding frameworks: (1)\ndecoding heads fail to balance prediction accuracy and the parallelism of\nexecution, and (2) parallel decoding is not a universal solution, as it can\nbring unnecessary overheads at some challenging decoding steps. To address\nthese issues, we propose Cerberus, an adaptive parallel decoding framework\nintroduces the gating mechanism to enable the LLMs to adaptively choose\nappropriate decoding approaches at each decoding step, along with introducing a\nnew paradigm of decoding heads that introduce the sequential knowledge while\nmaintaining execution parallelism. The experiment results demonstrate that the\nCerberus can achieve up to 2.12x speed up compared to auto-regressive decoding,\nand outperforms one of the leading parallel decoding frameworks, Medusa, with a\n10% - 30% increase in acceleration and superior generation quality.\n","authors":["Yuxuan Liu","Wenyuan Li","Laizhong Cui","Hailiang Yang"],"pdf_url":"https://arxiv.org/pdf/2410.13344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07979v2","updated":"2024-10-17T08:54:37Z","published":"2024-04-11T17:57:22Z","title":"LLoCO: Learning Long Contexts Offline","summary":"  Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco.\n","authors":["Sijun Tan","Xiuyu Li","Shishir Patil","Ziyang Wu","Tianjun Zhang","Kurt Keutzer","Joseph E. Gonzalez","Raluca Ada Popa"],"pdf_url":"https://arxiv.org/pdf/2404.07979v2.pdf","comment":"EMNLP 2024. The first two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2410.13343v1","updated":"2024-10-17T08:52:52Z","published":"2024-10-17T08:52:52Z","title":"Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges\n  in Large Language Models","summary":"  Large Language Models (LLMs) have shown remarkable capabilities in various\nnatural language processing tasks. However, LLMs may rely on dataset biases as\nshortcuts for prediction, which can significantly impair their robustness and\ngeneralization capabilities. This paper presents Shortcut Suite, a\ncomprehensive test suite designed to evaluate the impact of shortcuts on LLMs'\nperformance, incorporating six shortcut types, five evaluation metrics, and\nfour prompting strategies. Our extensive experiments yield several key\nfindings: 1) LLMs demonstrate varying reliance on shortcuts for downstream\ntasks, significantly impairing their performance. 2) Larger LLMs are more\nlikely to utilize shortcuts under zero-shot and few-shot in-context learning\nprompts. 3) Chain-of-thought prompting notably reduces shortcut reliance and\noutperforms other prompting strategies, while few-shot prompts generally\nunderperform compared to zero-shot prompts. 4) LLMs often exhibit\noverconfidence in their predictions, especially when dealing with datasets that\ncontain shortcuts. 5) LLMs generally have a lower explanation quality in\nshortcut-laden datasets, with errors falling into three types: distraction,\ndisguised comprehension, and logical fallacy. Our findings offer new insights\nfor evaluating robustness and generalization in LLMs and suggest potential\ndirections for mitigating the reliance on shortcuts. The code is available at\n\\url {https://github.com/yyhappier/ShortcutSuite.git}.\n","authors":["Yu Yuan","Lili Zhao","Kai Zhang","Guangting Zheng","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2410.13343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13339v1","updated":"2024-10-17T08:48:54Z","published":"2024-10-17T08:48:54Z","title":"Probing-RAG: Self-Probing to Guide Language Models in Selective Document\n  Retrieval","summary":"  Retrieval-Augmented Generation (RAG) enhances language models by retrieving\nand incorporating relevant external knowledge. However, traditional\nretrieve-and-generate processes may not be optimized for real-world scenarios,\nwhere queries might require multiple retrieval steps or none at all. In this\npaper, we propose a Probing-RAG, which utilizes the hidden state\nrepresentations from the intermediate layers of language models to adaptively\ndetermine the necessity of additional retrievals for a given query. By\nemploying a pre-trained prober, Probing-RAG effectively captures the model's\ninternal cognition, enabling reliable decision-making about retrieving external\ndocuments. Experimental results across five open-domain QA datasets demonstrate\nthat Probing-RAG outperforms previous methods while reducing the number of\nredundant retrieval steps.\n","authors":["Ingeol Baek","Hwan Chang","Byeongjeong Kim","Jimin Lee","Hwanhee Lee"],"pdf_url":"https://arxiv.org/pdf/2410.13339v1.pdf","comment":"6 figures, 13 tables"},{"id":"http://arxiv.org/abs/2410.13334v1","updated":"2024-10-17T08:46:09Z","published":"2024-10-17T08:46:09Z","title":"Do LLMs Have Political Correctness? Analyzing Ethical Biases and\n  Jailbreak Vulnerabilities in AI Systems","summary":"  Although large language models (LLMs) demonstrate impressive proficiency in\nvarious tasks, they present potential safety risks, such as `jailbreaks', where\nmalicious inputs can coerce LLMs into generating harmful content. To address\nthese issues, many LLM developers have implemented various safety measures to\nalign these models. This alignment involves several techniques, including data\nfiltering during pre-training, supervised fine-tuning, reinforcement learning\nfrom human feedback, and red-teaming exercises. These methods often introduce\ndeliberate and intentional biases similar to Political Correctness (PC) to\nensure the ethical behavior of LLMs. In this paper, we delve into the\nintentional biases injected into LLMs for safety purposes and examine methods\nto circumvent these safety alignment techniques. Notably, these intentional\nbiases result in a jailbreaking success rate in GPT-4o models that differs by\n20% between non-binary and cisgender keywords and by 16% between white and\nblack keywords, even when the other parts of the prompts are identical. We\nintroduce the concept of PCJailbreak, highlighting the inherent risks posed by\nthese safety-induced biases. Additionally, we propose an efficient defense\nmethod PCDefense, which prevents jailbreak attempts by injecting defense\nprompts prior to generation. PCDefense stands as an appealing alternative to\nGuard Models, such as Llama-Guard, that require additional inference cost after\ntext generation. Our findings emphasize the urgent need for LLM developers to\nadopt a more responsible approach when designing and implementing safety\nmeasures.\n","authors":["Isack Lee","Haebin Seong"],"pdf_url":"https://arxiv.org/pdf/2410.13334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13332v1","updated":"2024-10-17T08:45:02Z","published":"2024-10-17T08:45:02Z","title":"Fine-Tuning Language Models on Multiple Datasets for Citation Intention\n  Classification","summary":"  Citation intention Classification (CIC) tools classify citations by their\nintention (e.g., background, motivation) and assist readers in evaluating the\ncontribution of scientific literature. Prior research has shown that pretrained\nlanguage models (PLMs) such as SciBERT can achieve state-of-the-art performance\non CIC benchmarks. PLMs are trained via self-supervision tasks on a large\ncorpus of general text and can quickly adapt to CIC tasks via moderate\nfine-tuning on the corresponding dataset. Despite their advantages, PLMs can\neasily overfit small datasets during fine-tuning. In this paper, we propose a\nmulti-task learning (MTL) framework that jointly fine-tunes PLMs on a dataset\nof primary interest together with multiple auxiliary CIC datasets to take\nadvantage of additional supervision signals. We develop a data-driven task\nrelation learning (TRL) method that controls the contribution of auxiliary\ndatasets to avoid negative transfer and expensive hyper-parameter tuning. We\nconduct experiments on three CIC datasets and show that fine-tuning with\nadditional datasets can improve the PLMs' generalization performance on the\nprimary dataset. PLMs fine-tuned with our proposed framework outperform the\ncurrent state-of-the-art models by 7% to 11% on small datasets while aligning\nwith the best-performing model on a large dataset.\n","authors":["Zeren Shui","Petros Karypis","Daniel S. Karls","Mingjian Wen","Saurav Manchanda","Ellad B. Tadmor","George Karypis"],"pdf_url":"https://arxiv.org/pdf/2410.13332v1.pdf","comment":"To be appear as a Findings paper at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02028v2","updated":"2024-10-17T08:31:32Z","published":"2024-10-02T20:48:28Z","title":"Are Large Language Models Good Classifiers? A Study on Edit Intent\n  Classification in Scientific Document Revisions","summary":"  Classification is a core NLP task architecture with many potential\napplications. While large language models (LLMs) have brought substantial\nadvancements in text generation, their potential for enhancing classification\ntasks remains underexplored. To address this gap, we propose a framework for\nthoroughly investigating fine-tuning LLMs for classification, including both\ngeneration- and encoding-based approaches. We instantiate this framework in\nedit intent classification (EIC), a challenging and underexplored\nclassification task. Our extensive experiments and systematic comparisons with\nvarious training approaches and a representative selection of LLMs yield new\ninsights into their application for EIC. We investigate the generalizability of\nthese findings on five further classification tasks. To demonstrate the\nproposed methods and address the data shortage for empirical edit analysis, we\nuse our best-performing EIC model to create Re3-Sci2.0, a new large-scale\ndataset of 1,780 scientific document revisions with over 94k labeled edits. The\nquality of the dataset is assessed through human evaluation. The new dataset\nenables an in-depth empirical study of human editing behavior in academic\nwriting. We make our experimental framework, models and data publicly\navailable.\n","authors":["Qian Ruan","Ilia Kuznetsov","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2410.02028v2.pdf","comment":"EMNLP2024 Main"},{"id":"http://arxiv.org/abs/2410.12622v2","updated":"2024-10-17T08:28:45Z","published":"2024-10-16T14:42:23Z","title":"From Measurement Instruments to Data: Leveraging Theory-Driven Synthetic\n  Training Data for Classifying Social Constructs","summary":"  Computational text classification is a challenging task, especially for\nmulti-dimensional social constructs. Recently, there has been increasing\ndiscussion that synthetic training data could enhance classification by\noffering examples of how these constructs are represented in texts. In this\npaper, we systematically examine the potential of theory-driven synthetic\ntraining data for improving the measurement of social constructs. In\nparticular, we explore how researchers can transfer established knowledge from\nmeasurement instruments in the social sciences, such as survey scales or\nannotation codebooks, into theory-driven generation of synthetic data. Using\ntwo studies on measuring sexism and political topics, we assess the added value\nof synthetic training data for fine-tuning text classification models. Although\nthe results of the sexism study were less promising, our findings demonstrate\nthat synthetic data can be highly effective in reducing the need for labeled\ndata in political topic classification. With only a minimal drop in\nperformance, synthetic data allows for substituting large amounts of labeled\ndata. Furthermore, theory-driven synthetic data performed markedly better than\ndata generated without conceptual information in mind.\n","authors":["Lukas Birkenmaier","Matthias Roth","Indira Sen"],"pdf_url":"https://arxiv.org/pdf/2410.12622v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13321v1","updated":"2024-10-17T08:24:27Z","published":"2024-10-17T08:24:27Z","title":"Mitigating Hallucinations in Large Vision-Language Models via\n  Summary-Guided Decoding","summary":"  Large Vision-Language Models (LVLMs) demonstrate impressive capabilities in\ngenerating detailed and coherent responses from visual inputs. However, they\nare prone to generate hallucinations due to an over-reliance on language\npriors. To address this issue, we investigate the language priors in LVLMs and\nmake two key observations: (1) Even when predicting the tokens associated with\nimage-related part-of-speech (POS), models increasingly rely on linguistic\npriors as the token sequences grow, thereby amplifying hallucinations. (2)\nMethods that directly calibrate LVLM's output distribution to mitigate language\npriors can lead to a degradation in text quality or even exacerbate\nhallucinations. Based on these findings, we propose a novel method,\nSummary-Guided Decoding (SGD). This method naturally encourages the model to\nfocus more on image information by reducing the text context through summaries,\nwhile controlling only the image-related POS tokens to maintain text quality.\nThrough experiments, we demonstrate that SGD achieves state-of-the-art\nperformance on object hallucination benchmarks. Furthermore, in terms of the\ntrade-off between precision and recall, SGD achieves Pareto optimality among\nthe existing methods. Lastly, we observe that although existing methods\nstruggle to balance the reduction of object hallucinations with maintaining\ntext quality, SGD demonstrates robustness in handling this challenge.\n","authors":["Kyungmin Min","Minbeom Kim","Kang-il Lee","Dongryeol Lee","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2410.13321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13318v1","updated":"2024-10-17T08:20:29Z","published":"2024-10-17T08:20:29Z","title":"Computational Approaches to Arabic-English Code-Switching","summary":"  Natural Language Processing (NLP) is a vital computational method for\naddressing language processing, analysis, and generation. NLP tasks form the\ncore of many daily applications, from automatic text correction to speech\nrecognition. While significant research has focused on NLP tasks for the\nEnglish language, less attention has been given to Modern Standard Arabic and\nDialectal Arabic. Globalization has also contributed to the rise of\nCode-Switching (CS), where speakers mix languages within conversations and even\nwithin individual words (intra-word CS). This is especially common in Arab\ncountries, where people often switch between dialects or between dialects and a\nforeign language they master. CS between Arabic and English is frequent in\nEgypt, especially on social media. Consequently, a significant amount of\ncode-switched content can be found online. Such code-switched data needs to be\ninvestigated and analyzed for several NLP tasks to tackle the challenges of\nthis multilingual phenomenon and Arabic language challenges. No work has been\ndone before for several integral NLP tasks on Arabic-English CS data. In this\nwork, we focus on the Named Entity Recognition (NER) task and other tasks that\nhelp propose a solution for the NER task on CS data, e.g., Language\nIdentification. This work addresses this gap by proposing and applying\nstate-of-the-art techniques for Modern Standard Arabic and Arabic-English NER.\nWe have created the first annotated CS Arabic-English corpus for the NER task.\nAlso, we apply two enhancement techniques to improve the NER tagger on CS data\nusing CS contextual embeddings and data augmentation techniques. All methods\nshowed improvements in the performance of the NER taggers on CS data. Finally,\nwe propose several intra-word language identification approaches to determine\nthe language type of a mixed text and identify whether it is a named entity or\nnot.\n","authors":["Caroline Sabty"],"pdf_url":"https://arxiv.org/pdf/2410.13318v1.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2403.12675v2","updated":"2024-10-17T08:14:21Z","published":"2024-03-19T12:21:20Z","title":"Pragmatic Competence Evaluation of Large Language Models for the Korean\n  Language","summary":"  Benchmarks play a significant role in the current evaluation of Large\nLanguage Models (LLMs), yet they often overlook the models' abilities to\ncapture the nuances of human language, primarily focusing on evaluating\nembedded knowledge and technical skills. To address this gap, our study\nevaluates how well LLMs understand context-dependent expressions from a\npragmatic standpoint, specifically in Korean. We use both Multiple-Choice\nQuestions (MCQs) for automatic evaluation and Open-Ended Questions (OEQs)\nassessed by human experts. Our results show that GPT-4 leads with scores of\n81.11 in MCQs and 85.69 in OEQs, closely followed by HyperCLOVA X.\nAdditionally, while few-shot learning generally improves performance,\nChain-of-Thought (CoT) prompting tends to encourage literal interpretations,\nwhich may limit effective pragmatic inference. Our findings highlight the need\nfor LLMs to better understand and generate language that reflects human\ncommunicative norms.\n","authors":["Dojun Park","Jiwoo Lee","Hyeyun Jeong","Seohyun Park","Sungeun Lee"],"pdf_url":"https://arxiv.org/pdf/2403.12675v2.pdf","comment":"38th Pacific Asia Conference on Language, Information and Computation"},{"id":"http://arxiv.org/abs/2410.13313v1","updated":"2024-10-17T08:10:24Z","published":"2024-10-17T08:10:24Z","title":"Mitigating Biases to Embrace Diversity: A Comprehensive Annotation\n  Benchmark for Toxic Language","summary":"  This study introduces a prescriptive annotation benchmark grounded in\nhumanities research to ensure consistent, unbiased labeling of offensive\nlanguage, particularly for casual and non-mainstream language uses. We\ncontribute two newly annotated datasets that achieve higher inter-annotator\nagreement between human and language model (LLM) annotations compared to\noriginal datasets based on descriptive instructions. Our experiments show that\nLLMs can serve as effective alternatives when professional annotators are\nunavailable. Moreover, smaller models fine-tuned on multi-source LLM-annotated\ndata outperform models trained on larger, single-source human-annotated\ndatasets. These findings highlight the value of structured guidelines in\nreducing subjective variability, maintaining performance with limited data, and\nembracing language diversity.\n  Content Warning: This article only analyzes offensive language for academic\npurposes. Discretion is advised.\n","authors":["Xinmeng Hou"],"pdf_url":"https://arxiv.org/pdf/2410.13313v1.pdf","comment":"12 pages, 9 figures, EMNLP-NLP4DH 2024"},{"id":"http://arxiv.org/abs/2406.12494v2","updated":"2024-10-17T08:09:37Z","published":"2024-06-18T10:57:27Z","title":"LightPAL: Lightweight Passage Retrieval for Open Domain Multi-Document\n  Summarization","summary":"  Open-Domain Multi-Document Summarization (ODMDS) is the task of generating\nsummaries from large document collections in response to user queries. This\ntask is crucial for efficiently addressing diverse information needs from\nusers. Traditional retrieve-then-summarize approaches fall short for open-ended\nqueries in ODMDS tasks. These queries often require broader context than\ninitially retrieved passages provide, making it challenging to retrieve all\nrelevant information in a single search. While iterative retrieval methods has\nbeen explored for multi-hop question answering (MQA), it's impractical for\nODMDS due to high latency from repeated LLM inference. Accordingly, we propose\nLightPAL, a lightweight passage retrieval method for ODMDS. LightPAL leverages\nan LLM to pre-construct a graph representing passage relationships, then\nemploys random walk during retrieval, avoiding iterative LLM inference.\nExperiments demonstrate that LightPAL outperforms naive sparse and pre-trained\ndense retrievers in both retrieval and summarization metrics, while achieving\nhigher efficiency compared to iterative MQA approaches.\n","authors":["Masafumi Enomoto","Kunihiro Takeoka","Kosuke Akimoto","Kiril Gashteovski","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2406.12494v2.pdf","comment":"15 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2410.13305v1","updated":"2024-10-17T08:05:02Z","published":"2024-10-17T08:05:02Z","title":"Reference-Based Post-OCR Processing with LLM for Diacritic Languages","summary":"  Extracting fine-grained OCR text from aged documents in diacritic languages\nremains challenging due to unexpected artifacts, time-induced degradation, and\nlack of datasets. While standalone spell correction approaches have been\nproposed, they show limited performance for historical documents due to\nnumerous possible OCR error combinations and differences between modern and\nclassical corpus distributions. We propose a method utilizing available\ncontent-focused ebooks as a reference base to correct imperfect OCR-generated\ntext, supported by large language models. This technique generates\nhigh-precision pseudo-page-to-page labels for diacritic languages, where small\nstrokes pose significant challenges in historical conditions. The pipeline\neliminates various types of noise from aged documents and addresses issues such\nas missing characters, words, and disordered sequences. Our post-processing\nmethod, which generated a large OCR dataset of classical Vietnamese books,\nachieved a mean grading score of 8.72 on a 10-point scale. This outperformed\nthe state-of-the-art transformer-based Vietnamese spell correction model, which\nscored 7.03 when evaluated on a sampled subset of the dataset. We also trained\na baseline OCR model to assess and compare it with well-known engines.\nExperimental results demonstrate the strength of our baseline model compared to\nwidely used open-source solutions. The resulting dataset will be released\npublicly to support future studies.\n","authors":["Thao Do"],"pdf_url":"https://arxiv.org/pdf/2410.13305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13298v1","updated":"2024-10-17T07:55:33Z","published":"2024-10-17T07:55:33Z","title":"Advancing Large Language Model Attribution through Self-Improving","summary":"  Teaching large language models (LLMs) to generate text with citations to\nevidence sources can mitigate hallucinations and enhance verifiability in\ninformation-seeking systems. However, improving this capability requires\nhigh-quality attribution data, which is costly and labor-intensive. Inspired by\nrecent advances in self-improvement that enhance LLMs without manual\nannotation, we present START, a Self-Taught AttRibuTion framework for\niteratively improving the attribution capability of LLMs. First, to prevent\nmodels from stagnating due to initially insufficient supervision signals, START\nleverages the model to self-construct synthetic training data for warming up.\nTo further self-improve the model's attribution ability, START iteratively\nutilizes fine-grained preference supervision signals constructed from its\nsampled responses to encourage robust, comprehensive, and attributable\ngeneration. Experiments on three open-domain question-answering datasets,\ncovering long-form QA and multi-step reasoning, demonstrate significant\nperformance gains of 25.13% on average without relying on human annotations and\nmore advanced models. Further analysis reveals that START excels in aggregating\ninformation across multiple sources.\n","authors":["Lei Huang","Xiaocheng Feng","Weitao Ma","Liang Zhao","Yuchun Fan","Weihong Zhong","Dongliang Xu","Qing Yang","Hongtao Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2410.13298v1.pdf","comment":"Accepted by EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2404.06666v3","updated":"2024-10-17T07:28:23Z","published":"2024-04-10T00:26:08Z","title":"SafeGen: Mitigating Sexually Explicit Content Generation in\n  Text-to-Image Models","summary":"  Text-to-image (T2I) models, such as Stable Diffusion, have exhibited\nremarkable performance in generating high-quality images from text descriptions\nin recent years. However, text-to-image models may be tricked into generating\nnot-safe-for-work (NSFW) content, particularly in sexually explicit scenarios.\nExisting countermeasures mostly focus on filtering inappropriate inputs and\noutputs, or suppressing improper text embeddings, which can block sexually\nexplicit content (e.g., naked) but may still be vulnerable to adversarial\nprompts -- inputs that appear innocent but are ill-intended. In this paper, we\npresent SafeGen, a framework to mitigate sexual content generation by\ntext-to-image models in a text-agnostic manner. The key idea is to eliminate\nexplicit visual representations from the model regardless of the text input. In\nthis way, the text-to-image model is resistant to adversarial prompts since\nsuch unsafe visual representations are obstructed from within. Extensive\nexperiments conducted on four datasets and large-scale user studies demonstrate\nSafeGen's effectiveness in mitigating sexually explicit content generation\nwhile preserving the high-fidelity of benign images. SafeGen outperforms eight\nstate-of-the-art baseline methods and achieves 99.4% sexual content removal\nperformance. Furthermore, our constructed benchmark of adversarial prompts\nprovides a basis for future development and evaluation of anti-NSFW-generation\nmethods.\n","authors":["Xinfeng Li","Yuchen Yang","Jiangyi Deng","Chen Yan","Yanjiao Chen","Xiaoyu Ji","Wenyuan Xu"],"pdf_url":"https://arxiv.org/pdf/2404.06666v3.pdf","comment":"Accepted by ACM CCS 2024. Please cite this paper as \"Xinfeng Li,\n  Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu Ji, Wenyuan Xu.\n  SafeGen: Mitigating Sexually Explicit Content Generation in Text-to-Image\n  Models. In Proceedings of ACM Conference on Computer and Communications\n  Security (CCS), 2024.\""},{"id":"http://arxiv.org/abs/2410.13284v1","updated":"2024-10-17T07:28:18Z","published":"2024-10-17T07:28:18Z","title":"Learning to Route with Confidence Tokens","summary":"  Large language models (LLMs) have demonstrated impressive performance on\nseveral tasks and are increasingly deployed in real-world applications.\nHowever, especially in high-stakes settings, it becomes vital to know when the\noutput of an LLM may be unreliable. Depending on whether an answer is\ntrustworthy, a system can then choose to route the question to another expert,\nor otherwise fall back on a safe default behavior. In this work, we study the\nextent to which LLMs can reliably indicate confidence in their answers, and how\nthis notion of confidence can translate into downstream accuracy gains. We\npropose Self-REF, a lightweight training strategy to teach LLMs to express\nconfidence in whether their answers are correct in a reliable manner. Self-REF\nintroduces confidence tokens into the LLM, from which a confidence score can be\nextracted. Compared to conventional approaches such as verbalizing confidence\nand examining token probabilities, we demonstrate empirically that confidence\ntokens show significant improvements in downstream routing and rejection\nlearning tasks.\n","authors":["Yu-Neng Chuang","Helen Zhou","Prathusha Kameswara Sarma","Parikshit Gopalan","John Boccio","Sara Bolouki","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2410.13284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14991v2","updated":"2024-10-17T07:23:23Z","published":"2024-06-21T09:06:45Z","title":"SpreadsheetBench: Towards Challenging Real World Spreadsheet\n  Manipulation","summary":"  We introduce SpreadsheetBench, a challenging spreadsheet manipulation\nbenchmark exclusively derived from real-world scenarios, designed to immerse\ncurrent large language models (LLMs) in the actual workflow of spreadsheet\nusers. Unlike existing benchmarks that rely on synthesized queries and\nsimplified spreadsheet files, SpreadsheetBench is built from 912 real questions\ngathered from online Excel forums, which reflect the intricate needs of users.\nThe associated spreadsheets from the forums contain a variety of tabular data\nsuch as multiple tables, non-standard relational tables, and abundant\nnon-textual elements. Furthermore, we propose a more reliable evaluation metric\nakin to online judge platforms, where multiple spreadsheet files are created as\ntest cases for each instruction, ensuring the evaluation of robust solutions\ncapable of handling spreadsheets with varying values. Our comprehensive\nevaluation of various LLMs under both single-round and multi-round inference\nsettings reveals a substantial gap between the state-of-the-art (SOTA) models\nand human performance, highlighting the benchmark's difficulty.\n","authors":["Zeyao Ma","Bohan Zhang","Jing Zhang","Jifan Yu","Xiaokang Zhang","Xiaohan Zhang","Sijia Luo","Xi Wang","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2406.14991v2.pdf","comment":"Neurips 2024 (Spotlight); Homepage:\n  https://spreadsheetbench.github.io/"},{"id":"http://arxiv.org/abs/2410.13281v1","updated":"2024-10-17T07:15:15Z","published":"2024-10-17T07:15:15Z","title":"BANTH: A Multi-label Hate Speech Detection Dataset for Transliterated\n  Bangla","summary":"  The proliferation of transliterated texts in digital spaces has emphasized\nthe need for detecting and classifying hate speech in languages beyond English,\nparticularly in low-resource languages. As online discourse can perpetuate\ndiscrimination based on target groups, e.g. gender, religion, and origin,\nmulti-label classification of hateful content can help in comprehending hate\nmotivation and enhance content moderation. While previous efforts have focused\non monolingual or binary hate classification tasks, no work has yet addressed\nthe challenge of multi-label hate speech classification in transliterated\nBangla. We introduce BanTH, the first multi-label transliterated Bangla hate\nspeech dataset comprising 37.3k samples. The samples are sourced from YouTube\ncomments, where each instance is labeled with one or more target groups,\nreflecting the regional demographic. We establish novel transformer\nencoder-based baselines by further pre-training on transliterated Bangla\ncorpus. We also propose a novel translation-based LLM prompting strategy for\ntransliterated text. Experiments reveal that our further pre-trained encoders\nare achieving state-of-the-art performance on the BanTH dataset, while our\ntranslation-based prompting outperforms other strategies in the zero-shot\nsetting. The introduction of BanTH not only fills a critical gap in hate speech\nresearch for Bangla but also sets the stage for future exploration into\ncode-mixed and multi-label classification challenges in underrepresented\nlanguages.\n","authors":["Fabiha Haider","Fariha Tanjim Shifat","Md Farhan Ishmam","Deeparghya Dutta Barua","Md Sakib Ul Rahman Sourove","Md Fahim","Md Farhad Alam"],"pdf_url":"https://arxiv.org/pdf/2410.13281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13276v1","updated":"2024-10-17T07:07:09Z","published":"2024-10-17T07:07:09Z","title":"SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs","summary":"  Attention is the cornerstone of modern Large Language Models (LLMs). Yet its\nquadratic complexity limits the efficiency and scalability of LLMs, especially\nfor those with a long-context window. A promising approach addressing this\nlimitation is to leverage the sparsity in attention. However, existing\nsparsity-based solutions predominantly rely on predefined patterns or\nheuristics to approximate sparsity. This practice falls short to fully capture\nthe dynamic nature of attention sparsity in language-based tasks. This paper\nargues that attention sparsity should be learned rather than predefined. To\nthis end, we design SeerAttention, a new Attention mechanism that augments the\nconventional attention with a learnable gate that adaptively selects\nsignificant blocks in an attention map and deems the rest blocks sparse. Such\nblock-level sparsity effectively balances accuracy and speedup. To enable\nefficient learning of the gating network, we develop a customized\nFlashAttention implementation that extracts the block-level ground truth of\nattention map with minimum overhead. SeerAttention not only applies to\npost-training, but also excels in long-context fine-tuning. Our results show\nthat at post-training stages, SeerAttention significantly outperforms\nstate-of-the-art static or heuristic-based sparse attention methods, while also\nbeing more versatile and flexible to adapt to varying context lengths and\nsparsity ratios. When applied to long-context fine-tuning with YaRN,\nSeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context\nlength with minimal perplexity loss, offering a 5.67x speedup over\nFlashAttention-2.\n","authors":["Yizhao Gao","Zhichen Zeng","Dayou Du","Shijie Cao","Hayden Kwok-Hay So","Ting Cao","Fan Yang","Mao Yang"],"pdf_url":"https://arxiv.org/pdf/2410.13276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08440v4","updated":"2024-10-17T07:00:19Z","published":"2024-07-11T12:26:55Z","title":"Beyond Instruction Following: Evaluating Inferential Rule Following of\n  Large Language Models","summary":"  Although Large Language Models (LLMs) have demonstrated strong ability, they\nare further supposed to be controlled and guided by in real-world scenarios to\nbe safe, accurate, and intelligent. This demands the possession of capability\nof LLMs. However, no prior work has made a clear evaluation of the inferential\nrule-following capability of LLMs. Previous studies that try to evaluate the\ninferential rule-following capability of LLMs fail to distinguish the\ninferential rule-following scenarios from the instruction-following scenarios.\nTherefore, this paper first clarifies the concept of inferential rule-following\nand proposes a comprehensive benchmark, RuleBench, to evaluate a diversified\nrange of inferential rule-following abilities. Our experimental results on a\nvariety of LLMs show that they are still limited in following rules. Our\nanalysis based on the evaluation results provides insights into the\nimprovements for LLMs toward a better inferential rule-following intelligent\nagent. We further propose Inferential Rule-Following Tuning (IRFT). The\nexperimental results show that through IRFT, LLMs can learn abstract\nrule-following abilities from purely synthetic data and then generalize to\nRuleBench. The data and code can be found at:\nhttps://anonymous.4open.science/r/llm-rule-following-B3E3/\n","authors":["Wangtao Sun","Chenxiang Zhang","XueYou Zhang","Xuanqing Yu","Ziyang Huang","Pei Chen","Haotian Xu","Shizhu He","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.08440v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13274v1","updated":"2024-10-17T07:00:15Z","published":"2024-10-17T07:00:15Z","title":"Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning","summary":"  Large language models (LLMs) serve as giant information stores, often\nincluding personal or copyrighted data, and retraining them from scratch is not\na viable option. This has led to the development of various fast, approximate\nunlearning techniques to selectively remove knowledge from LLMs. Prior research\nhas largely focused on minimizing the probabilities of specific token sequences\nby reversing the language modeling objective. However, these methods still\nleave LLMs vulnerable to adversarial attacks that exploit indirect references.\nIn this work, we examine the limitations of current unlearning techniques in\neffectively erasing a particular type of indirect prompt: multi-hop queries.\nOur findings reveal that existing methods fail to completely remove multi-hop\nknowledge when one of the intermediate hops is unlearned. To address this\nissue, we propose MUNCH, a simple uncertainty-based approach that breaks down\nmulti-hop queries into subquestions and leverages the uncertainty of the\nunlearned model in final decision-making. Empirical results demonstrate the\neffectiveness of our framework, and MUNCH can be easily integrated with\nexisting unlearning techniques, making it a flexible and useful solution for\nenhancing unlearning processes.\n","authors":["Minseok Choi","ChaeHun Park","Dohyun Lee","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2410.13274v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.13268v1","updated":"2024-10-17T06:44:06Z","published":"2024-10-17T06:44:06Z","title":"Roadmap towards Superhuman Speech Understanding using Large Language\n  Models","summary":"  The success of large language models (LLMs) has prompted efforts to integrate\nspeech and audio data, aiming to create general foundation models capable of\nprocessing both textual and non-textual inputs. Recent advances, such as\nGPT-4o, highlight the potential for end-to-end speech LLMs, which preserves\nnon-semantic information and world knowledge for deeper speech understanding.\nTo guide the development of speech LLMs, we propose a five-level roadmap,\nranging from basic automatic speech recognition (ASR) to advanced superhuman\nmodels capable of integrating non-semantic information with abstract acoustic\nknowledge for complex tasks. Moreover, we design a benchmark, SAGI Bechmark,\nthat standardizes critical aspects across various tasks in these five levels,\nuncovering challenges in using abstract acoustic knowledge and completeness of\ncapability. Our findings reveal gaps in handling paralinguistic cues and\nabstract acoustic knowledge, and we offer future directions. This paper\noutlines a roadmap for advancing speech LLMs, introduces a benchmark for\nevaluation, and provides key insights into their current limitations and\npotential.\n","authors":["Fan Bu","Yuhao Zhang","Xidong Wang","Benyou Wang","Qun Liu","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2410.13268v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13267v1","updated":"2024-10-17T06:43:54Z","published":"2024-10-17T06:43:54Z","title":"CLaMP 2: Multimodal Music Information Retrieval Across 101 Languages\n  Using Large Language Models","summary":"  Challenges in managing linguistic diversity and integrating various musical\nmodalities are faced by current music information retrieval systems. These\nlimitations reduce their effectiveness in a global, multimodal music\nenvironment. To address these issues, we introduce CLaMP 2, a system compatible\nwith 101 languages that supports both ABC notation (a text-based musical\nnotation format) and MIDI (Musical Instrument Digital Interface) for music\ninformation retrieval. CLaMP 2, pre-trained on 1.5 million ABC-MIDI-text\ntriplets, includes a multilingual text encoder and a multimodal music encoder\naligned via contrastive learning. By leveraging large language models, we\nobtain refined and consistent multilingual descriptions at scale, significantly\nreducing textual noise and balancing language distribution. Our experiments\nshow that CLaMP 2 achieves state-of-the-art results in both multilingual\nsemantic search and music classification across modalities, thus establishing a\nnew standard for inclusive and global music information retrieval.\n","authors":["Shangda Wu","Yashan Wang","Ruibin Yuan","Zhancheng Guo","Xu Tan","Ge Zhang","Monan Zhou","Jing Chen","Xuefeng Mu","Yuejie Gao","Yuanliang Dong","Jiafeng Liu","Xiaobing Li","Feng Yu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.13267v1.pdf","comment":"17 pages, 10 figures, 4 tables"},{"id":"http://arxiv.org/abs/2409.14065v2","updated":"2024-10-17T06:37:44Z","published":"2024-09-21T08:41:08Z","title":"Temporally Consistent Factuality Probing for Large Language Models","summary":"  The prolific use of Large Language Models (LLMs) as an alternate knowledge\nbase requires them to be factually consistent, necessitating both correctness\nand consistency traits for paraphrased queries. Recently, significant attempts\nhave been made to benchmark datasets and metrics to evaluate LLMs for these\ntraits. However, structural simplicity (subject-relation-object) and\ncontemporary association in their query formulation limit the broader\ndefinition of factuality and consistency. In this study, we introduce TeCFaP, a\nnovel Temporally Consistent Factuality Probe task to expand the consistent\nfactuality probe in the temporal dimension. To this end, we propose TEMP-COFAC,\na high-quality dataset of prefix-style English query paraphrases. Subsequently,\nwe extend the definitions of existing metrics to represent consistent\nfactuality across temporal dimension. We experiment with a diverse set of LLMs\nand find most of them performing poorly on TeCFaP. Next, we propose a novel\nsolution CoTSeLF (Consistent-Time-Sensitive Learning Framework) combining\nmulti-task instruction tuning (MT-IT) with consistent-time-sensitive\nreinforcement learning (CTSRL) to improve temporally consistent factuality in\nLLMs. Our experiments demonstrate the efficacy of CoTSeLF over several\nbaselines.\n","authors":["Ashutosh Bajpai","Aaryan Goyal","Atif Anwer","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2409.14065v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.03087v2","updated":"2024-10-17T06:35:00Z","published":"2023-04-06T14:12:02Z","title":"Investigating Chain-of-thought with ChatGPT for Stance Detection on\n  Social Media","summary":"  Stance detection predicts attitudes towards targets in texts and has gained\nattention with the rise of social media. Traditional approaches include\nconventional machine learning, early deep neural networks, and pre-trained\nfine-tuning models. However, with the evolution of very large pre-trained\nlanguage models (VLPLMs) like ChatGPT (GPT-3.5), traditional methods face\ndeployment challenges. The parameter-free Chain-of-Thought (CoT) approach, not\nrequiring backpropagation training, has emerged as a promising alternative.\nThis paper examines CoT's effectiveness in stance detection tasks,\ndemonstrating its superior accuracy and discussing associated challenges.\n","authors":["Bowen Zhang","Xianghua Fu","Daijun Ding","Hu Huang","Genan Dai","Nan Yin","Yangyang Li","Liwen Jing"],"pdf_url":"https://arxiv.org/pdf/2304.03087v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2212.14548"},{"id":"http://arxiv.org/abs/2410.13259v1","updated":"2024-10-17T06:31:49Z","published":"2024-10-17T06:31:49Z","title":"From Babbling to Fluency: Evaluating the Evolution of Language Models in\n  Terms of Human Language Acquisition","summary":"  We examine the language capabilities of language models (LMs) from the\ncritical perspective of human language acquisition. Building on classical\nlanguage development theories, we propose a three-stage framework to assess the\nabilities of LMs, ranging from preliminary word understanding to complex\ngrammar and complex logical reasoning. Using this framework, we evaluate the\ngenerative capacities of LMs using methods from linguistic research. Results\nindicate that although recent LMs outperform earlier models in overall\nperformance, their developmental trajectory does not strictly follow the path\nof human language acquisition. Notably, in generation tasks, LMs are more\nsimilar to human performance in areas where information is easier to extract\nfrom the corpus, such as average word length, clauses, and auxiliary verbs.\nNewer LMs did not exhibit significant progress in terms of specific dimensions,\nsuch as clauses and auxiliary verbs, where the variation across corpora is\nrelatively limited. Register theory offers a plausible explanation for these\nobservations, suggesting that the linguistic features of the training data have\na substantial impact on the models' abilities.\n","authors":["Qiyuan Yang","Pengda Wang","Luke D. Plonsky","Frederick L. Oswald","Hanjie Chen"],"pdf_url":"https://arxiv.org/pdf/2410.13259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13258v1","updated":"2024-10-17T06:30:55Z","published":"2024-10-17T06:30:55Z","title":"A Systematic Investigation of Knowledge Retrieval and Selection for\n  Retrieval Augmented Generation","summary":"  Retrieval-augmented generation (RAG) has emerged as a powerful method for\nenhancing natural language generation by integrating external knowledge into a\nmodel's output. While prior work has demonstrated the importance of improving\nknowledge retrieval for boosting generation quality, the role of knowledge\nselection remains less clear. In this paper, we perform a comprehensive\nanalysis of how knowledge retrieval and selection influence downstream\ngeneration performance in RAG systems. By simulating different retrieval and\nselection conditions through a controlled mixture of gold and distractor\nknowledge, we assess the impact of these factors on generation outcomes. Our\nfindings indicate that the downstream generator model's capability, as well as\nthe complexity of the task and dataset, significantly influence the impact of\nknowledge retrieval and selection on the overall RAG system performance. In\ntypical scenarios, improving the knowledge recall score is key to enhancing\ngeneration outcomes, with the knowledge selector providing a limited additional\nbenefit when a strong generator model is used on clear, well-defined tasks. For\nweaker generator models or more ambiguous tasks and datasets, the knowledge F1\nscore becomes a critical factor, and the knowledge selector plays a more\nprominent role in improving overall performance.\n","authors":["Xiangci Li","Jessica Ouyang"],"pdf_url":"https://arxiv.org/pdf/2410.13258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13255v1","updated":"2024-10-17T06:21:38Z","published":"2024-10-17T06:21:38Z","title":"Automatic Translation Alignment Pipeline for Multilingual Digital\n  Editions of Literary Works","summary":"  This paper investigates the application of translation alignment algorithms\nin the creation of a Multilingual Digital Edition (MDE) of Alessandro Manzoni's\nItalian novel \"I promessi sposi\" (\"The Betrothed\"), with translations in eight\nlanguages (English, Spanish, French, German, Dutch, Polish, Russian and\nChinese) from the 19th and 20th centuries. We identify key requirements for the\nMDE to improve both the reader experience and support for translation studies.\nOur research highlights the limitations of current state-of-the-art algorithms\nwhen applied to the translation of literary texts and outlines an automated\npipeline for MDE creation. This pipeline transforms raw texts into web-based,\nside-by-side representations of original and translated texts with different\nrendering options. In addition, we propose new metrics for evaluating the\nalignment of literary translations and suggest visualization techniques for\nfuture analysis.\n","authors":["Maria Levchenko"],"pdf_url":"https://arxiv.org/pdf/2410.13255v1.pdf","comment":"18 pages, Computational Humanities Research Conference, December 4-6,\n  2024, Aarhus, Denmark"},{"id":"http://arxiv.org/abs/2410.13248v1","updated":"2024-10-17T06:15:00Z","published":"2024-10-17T06:15:00Z","title":"Disentangling Likes and Dislikes in Personalized Generative Explainable\n  Recommendation","summary":"  Recent research on explainable recommendation generally frames the task as a\nstandard text generation problem, and evaluates models simply based on the\ntextual similarity between the predicted and ground-truth explanations.\nHowever, this approach fails to consider one crucial aspect of the systems:\nwhether their outputs accurately reflect the users' (post-purchase) sentiments,\ni.e., whether and why they would like and/or dislike the recommended items. To\nshed light on this issue, we introduce new datasets and evaluation methods that\nfocus on the users' sentiments. Specifically, we construct the datasets by\nexplicitly extracting users' positive and negative opinions from their\npost-purchase reviews using an LLM, and propose to evaluate systems based on\nwhether the generated explanations 1) align well with the users' sentiments,\nand 2) accurately identify both positive and negative opinions of users on the\ntarget items. We benchmark several recent models on our datasets and\ndemonstrate that achieving strong performance on existing metrics does not\nensure that the generated explanations align well with the users' sentiments.\nLastly, we find that existing models can provide more sentiment-aware\nexplanations when the users' (predicted) ratings for the target items are\ndirectly fed into the models as input. We will release our code and datasets\nupon acceptance.\n","authors":["Ryotaro Shimizu","Takashi Wada","Yu Wang","Johannes Kruse","Sean O'Brien","Sai HtaungKham","Linxin Song","Yuya Yoshikawa","Yuki Saito","Fugee Tsung","Masayuki Goto","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2410.13248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13246v1","updated":"2024-10-17T06:09:26Z","published":"2024-10-17T06:09:26Z","title":"Atomic Calibration of LLMs in Long-Form Generations","summary":"  Large language models (LLMs) often suffer from hallucinations, posing\nsignificant challenges for real-world applications. Confidence calibration,\nwhich estimates the underlying uncertainty of model predictions, is essential\nto enhance the LLMs' trustworthiness. Existing research on LLM calibration has\nprimarily focused on short-form tasks, providing a single confidence score at\nthe response level (macro calibration). However, this approach is insufficient\nfor long-form generations, where responses often contain more complex\nstatements and may include both accurate and inaccurate information. Therefore,\nwe introduce atomic calibration, a novel approach that evaluates factuality\ncalibration at a fine-grained level by breaking down long responses into atomic\nclaims. We classify confidence elicitation methods into discriminative and\ngenerative types and demonstrate that their combination can enhance\ncalibration. Our extensive experiments on various LLMs and datasets show that\natomic calibration is well-suited for long-form generation and can also improve\nmacro calibration results. Additionally, atomic calibration reveals insightful\npatterns in LLM confidence throughout the generation process.\n","authors":["Caiqi Zhang","Ruihan Yang","Zhisong Zhang","Xinting Huang","Sen Yang","Dong Yu","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2410.13246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17484v3","updated":"2024-10-17T06:04:54Z","published":"2024-06-25T12:05:56Z","title":"MedCare: Advancing Medical LLMs through Decoupling Clinical Alignment\n  and Knowledge Aggregation","summary":"  Large language models (LLMs) have shown substantial progress in natural\nlanguage understanding and generation, proving valuable especially in the\nmedical field. Despite advancements, challenges persist due to the complexity\nand diversity inherent in medical tasks, which can be categorized as\nknowledge-intensive tasks and alignment-required tasks. Previous approaches\neither ignore the latter task or focus on a minority of tasks and hence lose\ngeneralization. To address these drawbacks, we propose a progressive\nfine-tuning pipeline. This pipeline employs a Knowledge Aggregator and a Noise\naggregator to encode diverse knowledge in the first stage and filter out\ndetrimental information. In the second stage, we drop the Noise Aggregator to\navoid the interference of suboptimal representation and leverage an additional\nalignment module optimized towards an orthogonal direction to the knowledge\nspace to mitigate knowledge forgetting. Based on this two-stage paradigm, we\nproposed a Medical LLM through decoupling Clinical Alignment and Knowledge\nAggregation (MedCare), which is designed to achieve state-of-the-art (SOTA)\nperformance on over 20 medical tasks, as well as SOTA results on specific\nmedical alignment tasks. Various model sizes of MedCare (1.8B, 7B, 14B) all\ndemonstrate significant improvements over existing models with similar model\nsizes.\n","authors":["Yusheng Liao","Shuyang Jiang","Zhe Chen","Yanfeng Wang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2406.17484v3.pdf","comment":"EMNLP2024 Findings"},{"id":"http://arxiv.org/abs/2410.13237v1","updated":"2024-10-17T05:43:30Z","published":"2024-10-17T05:43:30Z","title":"Large Language Models are Easily Confused: A Quantitative Metric,\n  Security Implications and Typological Analysis","summary":"  Language Confusion is a phenomenon where Large Language Models (LLMs)\ngenerate text that is neither in the desired language, nor in a contextually\nappropriate language. This phenomenon presents a critical challenge in text\ngeneration by LLMs, often appearing as erratic and unpredictable behavior. We\nhypothesize that there are linguistic regularities to this inherent\nvulnerability in LLMs and shed light on patterns of language confusion across\nLLMs. We introduce a novel metric, Language Confusion Entropy, designed to\ndirectly measure and quantify this confusion, based on language distributions\ninformed by linguistic typology and lexical variation. Comprehensive\ncomparisons with the Language Confusion Benchmark (Marchisio et al., 2024)\nconfirm the effectiveness of our metric, revealing patterns of language\nconfusion across LLMs. We further link language confusion to LLM security, and\nfind patterns in the case of multilingual embedding inversion attacks. Our\nanalysis demonstrates that linguistic typology offers theoretically grounded\ninterpretation, and valuable insights into leveraging language similarities as\na prior for LLM alignment and security.\n","authors":["Yiyi Chen","Qiongxiu Li","Russa Biswas","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2410.13237v1.pdf","comment":"17 pages, 6 figures, 14 tables"},{"id":"http://arxiv.org/abs/2410.13236v1","updated":"2024-10-17T05:40:54Z","published":"2024-10-17T05:40:54Z","title":"SPIN: Self-Supervised Prompt INjection","summary":"  Large Language Models (LLMs) are increasingly used in a variety of important\napplications, yet their safety and reliability remain as major concerns.\nVarious adversarial and jailbreak attacks have been proposed to bypass the\nsafety alignment and cause the model to produce harmful responses. We introduce\nSelf-supervised Prompt INjection (SPIN) which can detect and reverse these\nvarious attacks on LLMs. As our self-supervised prompt defense is done at\ninference-time, it is also compatible with existing alignment and adds an\nadditional layer of safety for defense. Our benchmarks demonstrate that our\nsystem can reduce the attack success rate by up to 87.9%, while maintaining the\nperformance on benign user requests. In addition, we discuss the situation of\nan adaptive attacker and show that our method is still resilient against\nattackers who are aware of our defense.\n","authors":["Leon Zhou","Junfeng Yang","Chengzhi Mao"],"pdf_url":"https://arxiv.org/pdf/2410.13236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13232v1","updated":"2024-10-17T05:37:00Z","published":"2024-10-17T05:37:00Z","title":"Web Agents with World Models: Learning and Leveraging Environment\n  Dynamics in Web Navigation","summary":"  Large language models (LLMs) have recently gained much attention in building\nautonomous agents. However, the performance of current LLM-based web agents in\nlong-horizon tasks is far from optimal, often yielding errors such as\nrepeatedly buying a non-refundable flight ticket. By contrast, humans can avoid\nsuch an irreversible mistake, as we have an awareness of the potential outcomes\n(e.g., losing money) of our actions, also known as the \"world model\". Motivated\nby this, our study first starts with preliminary analyses, confirming the\nabsence of world models in current LLMs (e.g., GPT-4o, Claude-3.5-Sonnet,\netc.). Then, we present a World-model-augmented (WMA) web agent, which\nsimulates the outcomes of its actions for better decision-making. To overcome\nthe challenges in training LLMs as world models predicting next observations,\nsuch as repeated elements across observations and long HTML inputs, we propose\na transition-focused observation abstraction, where the prediction objectives\nare free-form natural language descriptions exclusively highlighting important\nstate differences between time steps. Experiments on WebArena and Mind2Web show\nthat our world models improve agents' policy selection without training and\ndemonstrate our agents' cost- and time-efficiency compared to recent\ntree-search-based agents.\n","authors":["Hyungjoo Chae","Namyoung Kim","Kai Tzu-iunn Ong","Minju Gwak","Gwanwoo Song","Jihoon Kim","Sunghwan Kim","Dongha Lee","Jinyoung Yeo"],"pdf_url":"https://arxiv.org/pdf/2410.13232v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2406.19764v2","updated":"2024-10-17T05:28:24Z","published":"2024-06-28T09:09:36Z","title":"Belief Revision: The Adaptability of Large Language Models Reasoning","summary":"  The capability to reason from text is crucial for real-world NLP\napplications. Real-world scenarios often involve incomplete or evolving data.\nIn response, individuals update their beliefs and understandings accordingly.\nHowever, most existing evaluations assume that language models (LMs) operate\nwith consistent information. We introduce Belief-R, a new dataset designed to\ntest LMs' belief revision ability when presented with new evidence. Inspired by\nhow humans suppress prior inferences, this task assesses LMs within the newly\nproposed delta reasoning ($\\Delta R$) framework. Belief-R features sequences of\npremises designed to simulate scenarios where additional information could\nnecessitate prior conclusions drawn by LMs. We evaluate $\\sim$30 LMs across\ndiverse prompting strategies and found that LMs generally struggle to\nappropriately revise their beliefs in response to new information. Further,\nmodels adept at updating often underperformed in scenarios without necessary\nupdates, highlighting a critical trade-off. These insights underscore the\nimportance of improving LMs' adaptiveness to changing information, a step\ntoward more reliable AI systems.\n","authors":["Bryan Wilie","Samuel Cahyawijaya","Etsuko Ishii","Junxian He","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2406.19764v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12897v3","updated":"2024-10-17T05:21:24Z","published":"2024-04-19T14:05:03Z","title":"Enabling Natural Zero-Shot Prompting on Encoder Models via\n  Statement-Tuning","summary":"  While Large Language Models (LLMs) exhibit remarkable capabilities in\nzero-shot and few-shot scenarios, they often require computationally\nprohibitive sizes. Conversely, smaller Masked Language Models (MLMs) like BERT\nand RoBERTa achieve state-of-the-art results through fine-tuning but struggle\nwith extending to few-shot and zero-shot settings due to their architectural\nconstraints. Hence, we propose Statement-Tuning, a technique that models\ndiscriminative tasks as a set of finite statements and trains an encoder model\nto discriminate between the potential statements to determine the label. We do\nStatement-Tuning on multiple tasks to enable cross-task generalization.\nExperimental results demonstrate that Statement-Tuning achieves competitive\nperformance compared to state-of-the-art LLMs with significantly fewer\nparameters. Moreover, the study investigates the impact of several design\nchoices on few-shot and zero-shot generalization, revealing that\nStatement-Tuning can achieve strong performance with modest training data and\nbenefits from task and statement diversity for unseen task generalizability.\n","authors":["Ahmed Elshabrawy","Yongxin Huang","Iryna Gurevych","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2404.12897v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13224v1","updated":"2024-10-17T05:10:12Z","published":"2024-10-17T05:10:12Z","title":"Proof Flow: Preliminary Study on Generative Flow Network Language Model\n  Tuning for Formal Reasoning","summary":"  Reasoning is a fundamental substrate for solving novel and complex problems.\nDeliberate efforts in learning and developing frameworks around System 2\nreasoning have made great strides, yet problems of sufficient complexity remain\nlargely out of reach for open models. To address this gap, we examine the\npotential of Generative Flow Networks as a fine-tuning method for LLMs to\nunlock advanced reasoning capabilities. In this paper, we present a proof of\nconcept in the domain of formal reasoning, specifically in the Neural Theorem\nProving (NTP) setting, where proofs specified in a formal language such as Lean\ncan be deterministically and objectively verified. Unlike classical\nreward-maximization reinforcement learning, which frequently over-exploits\nhigh-reward actions and fails to effectively explore the state space, GFlowNets\nhave emerged as a promising approach for sampling compositional objects,\nimproving generalization, and enabling models to maintain diverse hypotheses.\nOur early results demonstrate GFlowNet fine-tuning's potential for enhancing\nmodel performance in a search setting, which is especially relevant given the\nparadigm shift towards inference time compute scaling and \"thinking slowly.\"\n","authors":["Matthew Ho","Vincent Zhu","Xiaoyin Chen","Moksh Jain","Nikolay Malkin","Edwin Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.13224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13218v1","updated":"2024-10-17T04:52:57Z","published":"2024-10-17T04:52:57Z","title":"CBT-Bench: Evaluating Large Language Models on Assisting Cognitive\n  Behavior Therapy","summary":"  There is a significant gap between patient needs and available mental health\nsupport today. In this paper, we aim to thoroughly examine the potential of\nusing Large Language Models (LLMs) to assist professional psychotherapy. To\nthis end, we propose a new benchmark, CBT-BENCH, for the systematic evaluation\nof cognitive behavioral therapy (CBT) assistance. We include three levels of\ntasks in CBT-BENCH: I: Basic CBT knowledge acquisition, with the task of\nmultiple-choice questions; II: Cognitive model understanding, with the tasks of\ncognitive distortion classification, primary core belief classification, and\nfine-grained core belief classification; III: Therapeutic response generation,\nwith the task of generating responses to patient speech in CBT therapy\nsessions. These tasks encompass key aspects of CBT that could potentially be\nenhanced through AI assistance, while also outlining a hierarchy of capability\nrequirements, ranging from basic knowledge recitation to engaging in real\ntherapeutic conversations. We evaluated representative LLMs on our benchmark.\nExperimental results indicate that while LLMs perform well in reciting CBT\nknowledge, they fall short in complex real-world scenarios requiring deep\nanalysis of patients' cognitive structures and generating effective responses,\nsuggesting potential future work.\n","authors":["Mian Zhang","Xianjun Yang","Xinlu Zhang","Travis Labrum","Jamie C. Chiu","Shaun M. Eack","Fei Fang","William Yang Wang","Zhiyu Zoey Chen"],"pdf_url":"https://arxiv.org/pdf/2410.13218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00138v2","updated":"2024-10-17T04:43:40Z","published":"2024-08-29T17:58:38Z","title":"PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in\n  Action","summary":"  As language models (LMs) are widely utilized in personalized communication\nscenarios (e.g., sending emails, writing social media posts) and endowed with a\ncertain level of agency, ensuring they act in accordance with the contextual\nprivacy norms becomes increasingly critical. However, quantifying the privacy\nnorm awareness of LMs and the emerging privacy risk in LM-mediated\ncommunication is challenging due to (1) the contextual and long-tailed nature\nof privacy-sensitive cases, and (2) the lack of evaluation approaches that\ncapture realistic application scenarios. To address these challenges, we\npropose PrivacyLens, a novel framework designed to extend privacy-sensitive\nseeds into expressive vignettes and further into agent trajectories, enabling\nmulti-level evaluation of privacy leakage in LM agents' actions. We instantiate\nPrivacyLens with a collection of privacy norms grounded in privacy literature\nand crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM\nperformance in answering probing questions and their actual behavior when\nexecuting user instructions in an agent setup. State-of-the-art LMs, like GPT-4\nand Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even\nwhen prompted with privacy-enhancing instructions. We also demonstrate the\ndynamic nature of PrivacyLens by extending each seed into multiple trajectories\nto red-team LM privacy leakage risk. Dataset and code are available at\nhttps://github.com/SALT-NLP/PrivacyLens.\n","authors":["Yijia Shao","Tianshi Li","Weiyan Shi","Yanchen Liu","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2409.00138v2.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2410.13216v1","updated":"2024-10-17T04:42:48Z","published":"2024-10-17T04:42:48Z","title":"Anchored Alignment for Self-Explanations Enhancement","summary":"  In this work, we introduce a methodology for alignment designed to enhance\nthe ability of large language models (LLMs) to articulate their reasoning\n(self-explanation) even in the absence of annotated rationale explanations. Our\nalignment methodology comprises three key components: explanation quality\nassessment, self-instruction dataset generation, and model alignment.\nAdditionally, we present a novel technique called Alignment with Anchor\nPreference Pairs, which improves the selection of preference pairs by\ncategorizing model outputs into three groups: consistently correct,\nconsistently incorrect, and variable. By applying tailored strategies to each\ncategory, we enhance the effectiveness of Direct Preference Optimization (DPO).\nOur experimental results demonstrate that this approach significantly improves\nexplanation quality while maintaining accuracy compared to other fine-tuning\nstrategies.\n","authors":["Luis Felipe Villa-Arenas","Ata Nizamoglu","Qianli Wang","Sebastian Möller","Vera Schmitt"],"pdf_url":"https://arxiv.org/pdf/2410.13216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13210v1","updated":"2024-10-17T04:30:46Z","published":"2024-10-17T04:30:46Z","title":"FaithBench: A Diverse Hallucination Benchmark for Summarization by\n  Modern LLMs","summary":"  Summarization is one of the most common tasks performed by large language\nmodels (LLMs), especially in applications like Retrieval-Augmented Generation\n(RAG). However, existing evaluations of hallucinations in LLM-generated\nsummaries, and evaluations of hallucination detection models both suffer from a\nlack of diversity and recency in the LLM and LLM families considered. This\npaper introduces FaithBench, a summarization hallucination benchmark comprising\nchallenging hallucinations made by 10 modern LLMs from 8 different families,\nwith ground truth annotations by human experts. ``Challenging'' here means\nsummaries on which popular, state-of-the-art hallucination detection models,\nincluding GPT-4o-as-a-judge, disagreed on. Our results show GPT-4o and\nGPT-3.5-Turbo produce the least hallucinations. However, even the best\nhallucination detection models have near 50\\% accuracies on FaithBench,\nindicating lots of room for future improvement. The repo is\nhttps://github.com/vectara/FaithBench\n","authors":["Forrest Sheng Bao","Miaoran Li","Renyi Qu","Ge Luo","Erana Wan","Yujia Tang","Weisi Fan","Manveer Singh Tamber","Suleman Kazi","Vivek Sourabh","Mike Qi","Ruixuan Tu","Chenyu Xu","Matthew Gonzales","Ofer Mendelevitch","Amin Ahmad"],"pdf_url":"https://arxiv.org/pdf/2410.13210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13206v1","updated":"2024-10-17T04:19:26Z","published":"2024-10-17T04:19:26Z","title":"BQA: Body Language Question Answering Dataset for Video Large Language\n  Models","summary":"  A large part of human communication relies on nonverbal cues such as facial\nexpressions, eye contact, and body language. Unlike language or sign language,\nsuch nonverbal communication lacks formal rules, requiring complex reasoning\nbased on commonsense understanding. Enabling current Video Large Language\nModels (VideoLLMs) to accurately interpret body language is a crucial\nchallenge, as human unconscious actions can easily cause the model to\nmisinterpret their intent. To address this, we propose a dataset, BQA, a body\nlanguage question answering dataset, to validate whether the model can\ncorrectly interpret emotions from short clips of body language comprising 26\nemotion labels of videos of body language. We evaluated various VideoLLMs on\nBQA and revealed that understanding body language is challenging, and our\nanalyses of the wrong answers by VideoLLMs show that certain VideoLLMs made\nsignificantly biased answers depending on the age group and ethnicity of the\nindividuals in the video. The dataset is available.\n","authors":["Shintaro Ozaki","Kazuki Hayashi","Miyu Oba","Yusuke Sakai","Hidetaka Kamigaito","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2410.13206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13204v1","updated":"2024-10-17T04:12:17Z","published":"2024-10-17T04:12:17Z","title":"Measuring Free-Form Decision-Making Inconsistency of Language Models in\n  Military Crisis Simulations","summary":"  There is an increasing interest in using language models (LMs) for automated\ndecision-making, with multiple countries actively testing LMs to aid in\nmilitary crisis decision-making. To scrutinize relying on LM decision-making in\nhigh-stakes settings, we examine the inconsistency of responses in a crisis\nsimulation (\"wargame\"), similar to reported tests conducted by the US military.\nPrior work illustrated escalatory tendencies and varying levels of aggression\namong LMs but were constrained to simulations with pre-defined actions. This\nwas due to the challenges associated with quantitatively measuring semantic\ndifferences and evaluating natural language decision-making without relying on\npre-defined actions. In this work, we query LMs for free form responses and use\na metric based on BERTScore to measure response inconsistency quantitatively.\nLeveraging the benefits of BERTScore, we show that the inconsistency metric is\nrobust to linguistic variations that preserve semantic meaning in a\nquestion-answering setting across text lengths. We show that all five tested\nLMs exhibit levels of inconsistency that indicate semantic differences, even\nwhen adjusting the wargame setting, anonymizing involved conflict countries, or\nadjusting the sampling temperature parameter $T$. Further qualitative\nevaluation shows that models recommend courses of action that share few to no\nsimilarities. We also study the impact of different prompt sensitivity\nvariations on inconsistency at temperature $T = 0$. We find that inconsistency\ndue to semantically equivalent prompt variations can exceed response\ninconsistency from temperature sampling for most studied models across\ndifferent levels of ablations. Given the high-stakes nature of military\ndeployment, we recommend further consideration be taken before using LMs to\ninform military decisions or other cases of high-stakes decision-making.\n","authors":["Aryan Shrivastava","Jessica Hullman","Max Lamparth"],"pdf_url":"https://arxiv.org/pdf/2410.13204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12388v2","updated":"2024-10-17T04:09:09Z","published":"2024-10-16T09:13:23Z","title":"Prompt Compression for Large Language Models: A Survey","summary":"  Leveraging large language models (LLMs) for complex natural language tasks\ntypically requires long-form prompts to convey detailed requirements and\ninformation, which results in increased memory usage and inference costs. To\nmitigate these challenges, multiple efficient methods have been proposed, with\nprompt compression gaining significant research interest. This survey provides\nan overview of prompt compression techniques, categorized into hard prompt\nmethods and soft prompt methods. First, the technical approaches of these\nmethods are compared, followed by an exploration of various ways to understand\ntheir mechanisms, including the perspectives of attention optimization,\nParameter-Efficient Fine-Tuning (PEFT), modality integration, and new synthetic\nlanguage. We also examine the downstream adaptations of various prompt\ncompression techniques. Finally, the limitations of current prompt compression\nmethods are analyzed, and several future directions are outlined, such as\noptimizing the compression encoder, combining hard and soft prompts methods,\nand leveraging insights from multimodality.\n","authors":["Zongqian Li","Yinhong Liu","Yixuan Su","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2410.12388v2.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2410.13863v1","updated":"2024-10-17T17:59:59Z","published":"2024-10-17T17:59:59Z","title":"Fluid: Scaling Autoregressive Text-to-image Generative Models with\n  Continuous Tokens","summary":"  Scaling up autoregressive models in vision has not proven as beneficial as in\nlarge language models. In this work, we investigate this scaling problem in the\ncontext of text-to-image generation, focusing on two critical factors: whether\nmodels use discrete or continuous tokens, and whether tokens are generated in a\nrandom or fixed raster order using BERT- or GPT-like transformer architectures.\nOur empirical results show that, while all models scale effectively in terms of\nvalidation loss, their evaluation performance -- measured by FID, GenEval\nscore, and visual quality -- follows different trends. Models based on\ncontinuous tokens achieve significantly better visual quality than those using\ndiscrete tokens. Furthermore, the generation order and attention mechanisms\nsignificantly affect the GenEval score: random-order models achieve notably\nbetter GenEval scores compared to raster-order models. Inspired by these\nfindings, we train Fluid, a random-order autoregressive model on continuous\ntokens. Fluid 10.5B model achieves a new state-of-the-art zero-shot FID of 6.16\non MS-COCO 30K, and 0.69 overall score on the GenEval benchmark. We hope our\nfindings and results will encourage future efforts to further bridge the\nscaling gap between vision and language models.\n","authors":["Lijie Fan","Tianhong Li","Siyang Qin","Yuanzhen Li","Chen Sun","Michael Rubinstein","Deqing Sun","Kaiming He","Yonglong Tian"],"pdf_url":"https://arxiv.org/pdf/2410.13863v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2410.13864v1","updated":"2024-10-17T17:59:59Z","published":"2024-10-17T17:59:59Z","title":"UniDrive: Towards Universal Driving Perception Across Camera\n  Configurations","summary":"  Vision-centric autonomous driving has demonstrated excellent performance with\neconomical sensors. As the fundamental step, 3D perception aims to infer 3D\ninformation from 2D images based on 3D-2D projection. This makes driving\nperception models susceptible to sensor configuration (e.g., camera intrinsics\nand extrinsics) variations. However, generalizing across camera configurations\nis important for deploying autonomous driving models on different car models.\nIn this paper, we present UniDrive, a novel framework for vision-centric\nautonomous driving to achieve universal perception across camera\nconfigurations. We deploy a set of unified virtual cameras and propose a\nground-aware projection method to effectively transform the original images\ninto these unified virtual views. We further propose a virtual configuration\noptimization method by minimizing the expected projection error between\noriginal cameras and virtual cameras. The proposed virtual camera projection\ncan be applied to existing 3D perception methods as a plug-and-play module to\nmitigate the challenges posed by camera parameter variability, resulting in\nmore adaptable and reliable driving perception models. To evaluate the\neffectiveness of our framework, we collect a dataset on Carla by driving the\nsame routes while only modifying the camera configurations. Experimental\nresults demonstrate that our method trained on one specific camera\nconfiguration can generalize to varying configurations with minor performance\ndegradation.\n","authors":["Ye Li","Wenzhao Zheng","Xiaonan Huang","Kurt Keutzer"],"pdf_url":"https://arxiv.org/pdf/2410.13864v1.pdf","comment":"Preprint; 14 pages, 5 figures, 2 tables; Code at\n  https://github.com/ywyeli/UniDrive"},{"id":"http://arxiv.org/abs/2410.13862v1","updated":"2024-10-17T17:59:58Z","published":"2024-10-17T17:59:58Z","title":"DepthSplat: Connecting Gaussian Splatting and Depth","summary":"  Gaussian splatting and single/multi-view depth estimation are typically\nstudied in isolation. In this paper, we present DepthSplat to connect Gaussian\nsplatting and depth estimation and study their interactions. More specifically,\nwe first contribute a robust multi-view depth model by leveraging pre-trained\nmonocular depth features, leading to high-quality feed-forward 3D Gaussian\nsplatting reconstructions. We also show that Gaussian splatting can serve as an\nunsupervised pre-training objective for learning powerful depth models from\nlarge-scale unlabelled datasets. We validate the synergy between Gaussian\nsplatting and depth estimation through extensive ablation and cross-task\ntransfer experiments. Our DepthSplat achieves state-of-the-art performance on\nScanNet, RealEstate10K and DL3DV datasets in terms of both depth estimation and\nnovel view synthesis, demonstrating the mutual benefits of connecting both\ntasks. Our code, models, and video results are available at\nhttps://haofeixu.github.io/depthsplat/.\n","authors":["Haofei Xu","Songyou Peng","Fangjinhua Wang","Hermann Blum","Daniel Barath","Andreas Geiger","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2410.13862v1.pdf","comment":"Project page: https://haofeixu.github.io/depthsplat/"},{"id":"http://arxiv.org/abs/2410.13861v1","updated":"2024-10-17T17:59:57Z","published":"2024-10-17T17:59:57Z","title":"PUMA: Empowering Unified MLLM with Multi-granular Visual Generation","summary":"  Recent advancements in multimodal foundation models have yielded significant\nprogress in vision-language understanding. Initial attempts have also explored\nthe potential of multimodal large language models (MLLMs) for visual content\ngeneration. However, existing works have insufficiently addressed the varying\ngranularity demands of different image generation tasks within a unified MLLM\nparadigm - from the diversity required in text-to-image generation to the\nprecise controllability needed in image manipulation. In this work, we propose\nPUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA\nunifies multi-granular visual features as both inputs and outputs of MLLMs,\nelegantly addressing the different granularity requirements of various image\ngeneration tasks within a unified MLLM framework. Following multimodal\npretraining and task-specific instruction tuning, PUMA demonstrates proficiency\nin a wide range of multimodal tasks. This work represents a significant step\ntowards a truly unified MLLM capable of adapting to the granularity demands of\nvarious visual tasks. The code and model will be released in\nhttps://github.com/rongyaofang/PUMA.\n","authors":["Rongyao Fang","Chengqi Duan","Kun Wang","Hao Li","Hao Tian","Xingyu Zeng","Rui Zhao","Jifeng Dai","Hongsheng Li","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2410.13861v1.pdf","comment":"Project page: https://rongyaofang.github.io/puma/"},{"id":"http://arxiv.org/abs/2410.13860v1","updated":"2024-10-17T17:59:55Z","published":"2024-10-17T17:59:55Z","title":"VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding","summary":"  3D visual grounding is crucial for robots, requiring integration of natural\nlanguage and 3D scene understanding. Traditional methods depending on\nsupervised learning with 3D point clouds are limited by scarce datasets.\nRecently zero-shot methods leveraging LLMs have been proposed to address the\ndata issue. While effective, these methods only use object-centric information,\nlimiting their ability to handle complex queries. In this work, we present\nVLM-Grounder, a novel framework using vision-language models (VLMs) for\nzero-shot 3D visual grounding based solely on 2D images. VLM-Grounder\ndynamically stitches image sequences, employs a grounding and feedback scheme\nto find the target object, and uses a multi-view ensemble projection to\naccurately estimate 3D bounding boxes. Experiments on ScanRefer and Nr3D\ndatasets show VLM-Grounder outperforms previous zero-shot methods, achieving\n51.6% Acc@0.25 on ScanRefer and 48.0% Acc on Nr3D, without relying on 3D\ngeometry or object priors. Codes are available at\nhttps://github.com/OpenRobotLab/VLM-Grounder .\n","authors":["Runsen Xu","Zhiwei Huang","Tai Wang","Yilun Chen","Jiangmiao Pang","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2410.13860v1.pdf","comment":"CoRL 2024 Camera Ready. 25 pages. A novel zero-shot 3D visual\n  grounding framework based solely on 2D images"},{"id":"http://arxiv.org/abs/2410.13859v1","updated":"2024-10-17T17:59:53Z","published":"2024-10-17T17:59:53Z","title":"$γ-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large\n  Language Models","summary":"  Despite the significant progress in multimodal large language models (MLLMs),\ntheir high computational cost remains a barrier to real-world deployment.\nInspired by the mixture of depths (MoDs) in natural language processing, we aim\nto address this limitation from the perspective of ``activated tokens''. Our\nkey insight is that if most tokens are redundant for the layer computation,\nthen can be skipped directly via the MoD layer. However, directly converting\nthe dense layers of MLLMs to MoD layers leads to substantial performance\ndegradation. To address this issue, we propose an innovative MoD adaptation\nstrategy for existing MLLMs called $\\gamma$-MoD. In $\\gamma$-MoD, a novel\nmetric is proposed to guide the deployment of MoDs in the MLLM, namely rank of\nattention maps (ARank). Through ARank, we can effectively identify which layer\nis redundant and should be replaced with the MoD layer. Based on ARank, we\nfurther propose two novel designs to maximize the computational sparsity of\nMLLM while maintaining its performance, namely shared vision-language router\nand masked routing learning. With these designs, more than 90% dense layers of\nthe MLLM can be effectively converted to the MoD ones. To validate our method,\nwe apply it to three popular MLLMs, and conduct extensive experiments on 9\nbenchmark datasets. Experimental results not only validate the significant\nefficiency benefit of $\\gamma$-MoD to existing MLLMs but also confirm its\ngeneralization ability on various MLLMs. For example, with a minor performance\ndrop, i.e., -1.5%, $\\gamma$-MoD can reduce the training and inference time of\nLLaVA-HR by 31.0% and 53.2%, respectively.\n","authors":["Yaxin Luo","Gen Luo","Jiayi Ji","Yiyi Zhou","Xiaoshuai Sun","Zhiqiang Shen","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2410.13859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13854v1","updated":"2024-10-17T17:59:24Z","published":"2024-10-17T17:59:24Z","title":"Can MLLMs Understand the Deep Implication Behind Chinese Images?","summary":"  As the capabilities of Multimodal Large Language Models (MLLMs) continue to\nimprove, the need for higher-order capability evaluation of MLLMs is\nincreasing. However, there is a lack of work evaluating MLLM for higher-order\nperception and understanding of Chinese visual content. To fill the gap, we\nintroduce the **C**hinese **I**mage **I**mplication understanding\n**Bench**mark, **CII-Bench**, which aims to assess the higher-order perception\nand understanding capabilities of MLLMs for Chinese images. CII-Bench stands\nout in several ways compared to existing benchmarks. Firstly, to ensure the\nauthenticity of the Chinese context, images in CII-Bench are sourced from the\nChinese Internet and manually reviewed, with corresponding answers also\nmanually crafted. Additionally, CII-Bench incorporates images that represent\nChinese traditional culture, such as famous Chinese traditional paintings,\nwhich can deeply reflect the model's understanding of Chinese traditional\nculture. Through extensive experiments on CII-Bench across multiple MLLMs, we\nhave made significant findings. Initially, a substantial gap is observed\nbetween the performance of MLLMs and humans on CII-Bench. The highest accuracy\nof MLLMs attains 64.4%, where as human accuracy averages 78.2%, peaking at an\nimpressive 81.0%. Subsequently, MLLMs perform worse on Chinese traditional\nculture images, suggesting limitations in their ability to understand\nhigh-level semantics and lack a deep knowledge base of Chinese traditional\nculture. Finally, it is observed that most models exhibit enhanced accuracy\nwhen image emotion hints are incorporated into the prompts. We believe that\nCII-Bench will enable MLLMs to gain a better understanding of Chinese semantics\nand Chinese-specific images, advancing the journey towards expert artificial\ngeneral intelligence (AGI). Our project is publicly available at\nhttps://cii-bench.github.io/.\n","authors":["Chenhao Zhang","Xi Feng","Yuelin Bai","Xinrun Du","Jinchang Hou","Kaixin Deng","Guangzeng Han","Qinrui Li","Bingli Wang","Jiaheng Liu","Xingwei Qu","Yifei Zhang","Qixuan Zhao","Yiming Liang","Ziqiang Liu","Feiteng Fang","Min Yang","Wenhao Huang","Chenghua Lin","Ge Zhang","Shiwen Ni"],"pdf_url":"https://arxiv.org/pdf/2410.13854v1.pdf","comment":"32 pages,18 figures. Project Page: https://cii-bench.github.io/ Code:\n  https://github.com/MING_X/CII-Bench Dataset:\n  https://huggingface.co/datasets/m-a-p/CII-Bench"},{"id":"http://arxiv.org/abs/2410.13852v1","updated":"2024-10-17T17:59:03Z","published":"2024-10-17T17:59:03Z","title":"Retrospective Learning from Interactions","summary":"  Multi-turn interactions between large language models (LLMs) and users\nnaturally include implicit feedback signals. If an LLM responds in an\nunexpected way to an instruction, the user is likely to signal it by rephrasing\nthe request, expressing frustration, or pivoting to an alternative task. Such\nsignals are task-independent and occupy a relatively constrained subspace of\nlanguage, allowing the LLM to identify them even if it fails on the actual\ntask. This creates an avenue for continually learning from interactions without\nadditional annotations. We introduce ReSpect, a method to learn from such\nsignals in past interactions via retrospection. We deploy ReSpect in a new\nmultimodal interaction scenario, where humans instruct an LLM to solve an\nabstract reasoning task with a combinatorial solution space. Through thousands\nof interactions with humans, we show how ReSpect gradually improves task\ncompletion rate from 31% to 82%, all without any external annotation.\n","authors":["Zizhao Chen","Mustafa Omer Gul","Yiwei Chen","Gloria Geng","Anne Wu","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2410.13852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13851v1","updated":"2024-10-17T17:59:02Z","published":"2024-10-17T17:59:02Z","title":"Differentiable Robot Rendering","summary":"  Vision foundation models trained on massive amounts of visual data have shown\nunprecedented reasoning and planning skills in open-world settings. A key\nchallenge in applying them to robotic tasks is the modality gap between visual\ndata and action data. We introduce differentiable robot rendering, a method\nallowing the visual appearance of a robot body to be directly differentiable\nwith respect to its control parameters. Our model integrates a kinematics-aware\ndeformable model and Gaussians Splatting and is compatible with any robot form\nfactors and degrees of freedom. We demonstrate its capability and usage in\napplications including reconstruction of robot poses from images and\ncontrolling robots through vision language models. Quantitative and qualitative\nresults show that our differentiable rendering model provides effective\ngradients for robotic control directly from pixels, setting the foundation for\nthe future applications of vision foundation models in robotics.\n","authors":["Ruoshi Liu","Alper Canberk","Shuran Song","Carl Vondrick"],"pdf_url":"https://arxiv.org/pdf/2410.13851v1.pdf","comment":"Project Page: https://drrobot.cs.columbia.edu/"},{"id":"http://arxiv.org/abs/2410.13848v1","updated":"2024-10-17T17:58:37Z","published":"2024-10-17T17:58:37Z","title":"Janus: Decoupling Visual Encoding for Unified Multimodal Understanding\n  and Generation","summary":"  In this paper, we introduce Janus, an autoregressive framework that unifies\nmultimodal understanding and generation. Prior research often relies on a\nsingle visual encoder for both tasks, such as Chameleon. However, due to the\ndiffering levels of information granularity required by multimodal\nunderstanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple\nvisual encoding into separate pathways, while still leveraging a single,\nunified transformer architecture for processing. The decoupling not only\nalleviates the conflict between the visual encoder's roles in understanding and\ngeneration, but also enhances the framework's flexibility. For instance, both\nthe multimodal understanding and generation components can independently select\ntheir most suitable encoding methods. Experiments show that Janus surpasses\nprevious unified model and matches or exceeds the performance of task-specific\nmodels. The simplicity, high flexibility, and effectiveness of Janus make it a\nstrong candidate for next-generation unified multimodal models.\n","authors":["Chengyue Wu","Xiaokang Chen","Zhiyu Wu","Yiyang Ma","Xingchao Liu","Zizheng Pan","Wen Liu","Zhenda Xie","Xingkai Yu","Chong Ruan","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2410.13848v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2410.13842v1","updated":"2024-10-17T17:57:01Z","published":"2024-10-17T17:57:01Z","title":"D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution\n  Refinement","summary":"  We introduce D-FINE, a powerful real-time object detector that achieves\noutstanding localization precision by redefining the bounding box regression\ntask in DETR models. D-FINE comprises two key components: Fine-grained\nDistribution Refinement (FDR) and Global Optimal Localization Self-Distillation\n(GO-LSD). FDR transforms the regression process from predicting fixed\ncoordinates to iteratively refining probability distributions, providing a\nfine-grained intermediate representation that significantly enhances\nlocalization accuracy. GO-LSD is a bidirectional optimization strategy that\ntransfers localization knowledge from refined distributions to shallower layers\nthrough self-distillation, while also simplifying the residual prediction tasks\nfor deeper layers. Additionally, D-FINE incorporates lightweight optimizations\nin computationally intensive modules and operations, achieving a better balance\nbetween speed and accuracy. Specifically, D-FINE-L / X achieves 54.0% / 55.8%\nAP on the COCO dataset at 124 / 78 FPS on an NVIDIA T4 GPU. When pretrained on\nObjects365, D-FINE-L / X attains 57.1% / 59.3% AP, surpassing all existing\nreal-time detectors. Furthermore, our method significantly enhances the\nperformance of a wide range of DETR models by up to 5.3% AP with negligible\nextra parameters and training costs. Our code and pretrained models:\nhttps://github.com/Peterande/D-FINE.\n","authors":["Yansong Peng","Hebei Li","Peixi Wu","Yueyi Zhang","Xiaoyan Sun","Feng Wu"],"pdf_url":"https://arxiv.org/pdf/2410.13842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13832v1","updated":"2024-10-17T17:53:24Z","published":"2024-10-17T17:53:24Z","title":"VidPanos: Generative Panoramic Videos from Casual Panning Videos","summary":"  Panoramic image stitching provides a unified, wide-angle view of a scene that\nextends beyond the camera's field of view. Stitching frames of a panning video\ninto a panoramic photograph is a well-understood problem for stationary scenes,\nbut when objects are moving, a still panorama cannot capture the scene. We\npresent a method for synthesizing a panoramic video from a casually-captured\npanning video, as if the original video were captured with a wide-angle camera.\nWe pose panorama synthesis as a space-time outpainting problem, where we aim to\ncreate a full panoramic video of the same length as the input video. Consistent\ncompletion of the space-time volume requires a powerful, realistic prior over\nvideo content and motion, for which we adapt generative video models. Existing\ngenerative models do not, however, immediately extend to panorama completion,\nas we show. We instead apply video generation as a component of our panorama\nsynthesis system, and demonstrate how to exploit the strengths of the models\nwhile minimizing their limitations. Our system can create video panoramas for a\nrange of in-the-wild scenes including people, vehicles, and flowing water, as\nwell as stationary background features.\n","authors":["Jingwei Ma","Erika Lu","Roni Paiss","Shiran Zada","Aleksander Holynski","Tali Dekel","Brian Curless","Michael Rubinstein","Forrester Cole"],"pdf_url":"https://arxiv.org/pdf/2410.13832v1.pdf","comment":"Project page at https://vidpanos.github.io/. To appear at SIGGRAPH\n  Asia 2024 (conference track)"},{"id":"http://arxiv.org/abs/2410.13830v1","updated":"2024-10-17T17:52:57Z","published":"2024-10-17T17:52:57Z","title":"DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise\n  Motion Control","summary":"  Recent advances in customized video generation have enabled users to create\nvideos tailored to both specific subjects and motion trajectories. However,\nexisting methods often require complicated test-time fine-tuning and struggle\nwith balancing subject learning and motion control, limiting their real-world\napplications. In this paper, we present DreamVideo-2, a zero-shot video\ncustomization framework capable of generating videos with a specific subject\nand motion trajectory, guided by a single image and a bounding box sequence,\nrespectively, and without the need for test-time fine-tuning. Specifically, we\nintroduce reference attention, which leverages the model's inherent\ncapabilities for subject learning, and devise a mask-guided motion module to\nachieve precise motion control by fully utilizing the robust motion signal of\nbox masks derived from bounding boxes. While these two components achieve their\nintended functions, we empirically observe that motion control tends to\ndominate over subject learning. To address this, we propose two key designs: 1)\nthe masked reference attention, which integrates a blended latent mask modeling\nscheme into reference attention to enhance subject representations at the\ndesired positions, and 2) a reweighted diffusion loss, which differentiates the\ncontributions of regions inside and outside the bounding boxes to ensure a\nbalance between subject and motion control. Extensive experimental results on a\nnewly curated dataset demonstrate that DreamVideo-2 outperforms\nstate-of-the-art methods in both subject customization and motion control. The\ndataset, code, and models will be made publicly available.\n","authors":["Yujie Wei","Shiwei Zhang","Hangjie Yuan","Xiang Wang","Haonan Qiu","Rui Zhao","Yutong Feng","Feng Liu","Zhizhong Huang","Jiaxin Ye","Yingya Zhang","Hongming Shan"],"pdf_url":"https://arxiv.org/pdf/2410.13830v1.pdf","comment":"Project page: https://dreamvideo2.github.io/"},{"id":"http://arxiv.org/abs/2410.13826v1","updated":"2024-10-17T17:51:40Z","published":"2024-10-17T17:51:40Z","title":"Unearthing Skill-Level Insights for Understanding Trade-Offs of\n  Foundation Models","summary":"  With models getting stronger, evaluations have grown more complex, testing\nmultiple skills in one benchmark and even in the same instance at once.\nHowever, skill-wise performance is obscured when inspecting aggregate accuracy,\nunder-utilizing the rich signal modern benchmarks contain. We propose an\nautomatic approach to recover the underlying skills relevant for any evaluation\ninstance, by way of inspecting model-generated rationales. After validating the\nrelevance of rationale-parsed skills and inferring skills for $46$k instances\nover $12$ benchmarks, we observe many skills to be common across benchmarks,\nresulting in the curation of hundreds of skill-slices (i.e. sets of instances\ntesting a common skill). Inspecting accuracy over these slices yields novel\ninsights on model trade-offs: e.g., compared to GPT-4o and Claude 3.5 Sonnet,\non average, Gemini 1.5 Pro is $18\\%$ more accurate in \"computing molar mass\",\nbut $19\\%$ less accurate in \"applying constitutional law\", despite the overall\naccuracies of the three models differing by a mere $0.4\\%$. Furthermore, we\ndemonstrate the practical utility of our approach by showing that insights\nderived from skill slice analysis can generalize to held-out instances: when\nrouting each instance to the model strongest on the relevant skills, we see a\n$3\\%$ accuracy improvement over our $12$ dataset corpus. Our skill-slices and\nframework open a new avenue in model evaluation, leveraging skill-specific\nanalyses to unlock a more granular and actionable understanding of model\ncapabilities.\n","authors":["Mazda Moayeri","Vidhisha Balachandran","Varun Chandrasekaran","Safoora Yousefi","Thomas Fel","Soheil Feizi","Besmira Nushi","Neel Joshi","Vibhav Vineet"],"pdf_url":"https://arxiv.org/pdf/2410.13826v1.pdf","comment":"Code at: github.com/microsoft/skill-slice-insights"},{"id":"http://arxiv.org/abs/2410.13824v1","updated":"2024-10-17T17:48:54Z","published":"2024-10-17T17:48:54Z","title":"Harnessing Webpage UIs for Text-Rich Visual Understanding","summary":"  Text-rich visual understanding-the ability to process environments where\ndense textual content is integrated with visuals-is crucial for multimodal\nlarge language models (MLLMs) to interact effectively with structured\nenvironments. To enhance this capability, we propose synthesizing general\nmultimodal instructions from webpage UIs using text-based large language models\n(LLMs). Despite lacking direct visual input, text-based LLMs are able to\nprocess structured text representations from webpage accessibility trees. These\ninstructions are then paired with UI screenshots to train multimodal models. We\nintroduce MultiUI, a dataset containing 7.3 million samples from 1 million\nwebsites, covering diverse multimodal tasks and UI layouts. Models trained on\nMultiUI not only excel in web UI tasks-achieving up to a 48\\% improvement on\nVisualWebBench and a 19.1\\% boost in action accuracy on a web agent dataset\nMind2Web-but also generalize surprisingly well to non-web UI tasks and even to\nnon-UI domains, such as document understanding, OCR, and chart interpretation.\nThese results highlight the broad applicability of web UI data for advancing\ntext-rich visual understanding across various scenarios.\n","authors":["Junpeng Liu","Tianyue Ou","Yifan Song","Yuxiao Qu","Wai Lam","Chenyan Xiong","Wenhu Chen","Graham Neubig","Xiang Yue"],"pdf_url":"https://arxiv.org/pdf/2410.13824v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13823v1","updated":"2024-10-17T17:48:36Z","published":"2024-10-17T17:48:36Z","title":"Deep Generative Models Unveil Patterns in Medical Images Through\n  Vision-Language Conditioning","summary":"  Deep generative models have significantly advanced medical imaging analysis\nby enhancing dataset size and quality. Beyond mere data augmentation, our\nresearch in this paper highlights an additional, significant capacity of deep\ngenerative models: their ability to reveal and demonstrate patterns in medical\nimages. We employ a generative structure with hybrid conditions, combining\nclinical data and segmentation masks to guide the image synthesis process.\nFurthermore, we innovatively transformed the tabular clinical data into textual\ndescriptions. This approach simplifies the handling of missing values and also\nenables us to leverage large pre-trained vision-language models that\ninvestigate the relations between independent clinical entries and comprehend\ngeneral terms, such as gender and smoking status. Our approach differs from and\npresents a more challenging task than traditional medical report-guided\nsynthesis due to the less visual correlation of our clinical information with\nthe images. To overcome this, we introduce a text-visual embedding mechanism\nthat strengthens the conditions, ensuring the network effectively utilizes the\nprovided information. Our pipeline is generalizable to both GAN-based and\ndiffusion models. Experiments on chest CT, particularly focusing on the smoking\nstatus, demonstrated a consistent intensity shift in the lungs which is in\nagreement with clinical observations, indicating the effectiveness of our\nmethod in capturing and visualizing the impact of specific attributes on\nmedical image patterns. Our methods offer a new avenue for the early detection\nand precise visualization of complex clinical conditions with deep generative\nmodels. All codes are https://github.com/junzhin/DGM-VLC.\n","authors":["Xiaodan Xing","Junzhi Ning","Yang Nan","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2410.13823v1.pdf","comment":"Accepted by AIM-FM Workshop of NeurIPS2024"},{"id":"http://arxiv.org/abs/2410.13822v1","updated":"2024-10-17T17:48:17Z","published":"2024-10-17T17:48:17Z","title":"Multi-style conversion for semantic segmentation of lesions in fundus\n  images by adversarial attacks","summary":"  The diagnosis of diabetic retinopathy, which relies on fundus images, faces\nchallenges in achieving transparency and interpretability when using a global\nclassification approach. However, segmentation-based databases are\nsignificantly more expensive to acquire and combining them is often\nproblematic. This paper introduces a novel method, termed adversarial style\nconversion, to address the lack of standardization in annotation styles across\ndiverse databases. By training a single architecture on combined databases, the\nmodel spontaneously modifies its segmentation style depending on the input,\ndemonstrating the ability to convert among different labeling styles. The\nproposed methodology adds a linear probe to detect dataset origin based on\nencoder features and employs adversarial attacks to condition the model's\nsegmentation style. Results indicate significant qualitative and quantitative\nthrough dataset combination, offering avenues for improved model\ngeneralization, uncertainty estimation and continuous interpolation between\nannotation styles. Our approach enables training a segmentation model with\ndiverse databases while controlling and leveraging annotation styles for\nimproved retinopathy diagnosis.\n","authors":["Clément Playout","Renaud Duval","Marie Carole Boucher","Farida Cheriet"],"pdf_url":"https://arxiv.org/pdf/2410.13822v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.13807v1","updated":"2024-10-17T17:41:52Z","published":"2024-10-17T17:41:52Z","title":"ConsisSR: Delving Deep into Consistency in Diffusion-based Image\n  Super-Resolution","summary":"  Real-world image super-resolution (Real-ISR) aims at restoring high-quality\n(HQ) images from low-quality (LQ) inputs corrupted by unknown and complex\ndegradations. In particular, pretrained text-to-image (T2I) diffusion models\nprovide strong generative priors to reconstruct credible and intricate details.\nHowever, T2I generation focuses on semantic consistency while Real-ISR\nemphasizes pixel-level reconstruction, which hinders existing methods from\nfully exploiting diffusion priors. To address this challenge, we introduce\nConsisSR to handle both semantic and pixel-level consistency. Specifically,\ncompared to coarse-grained text prompts, we exploit the more powerful CLIP\nimage embedding and effectively leverage both modalities through our Hybrid\nPrompt Adapter (HPA) for semantic guidance. Secondly, we introduce Time-aware\nLatent Augmentation (TALA) to mitigate the inherent gap between T2I generation\nand Real-ISR consistency requirements. By randomly mixing LQ and HQ latent\ninputs, our model not only handle timestep-specific diffusion noise but also\nrefine the accumulated latent representations. Last but not least, our\nGAN-Embedding strategy employs the pretrained Real-ESRGAN model to refine the\ndiffusion start point. This accelerates the inference process to 10 steps while\npreserving sampling quality, in a training-free manner.Our method demonstrates\nstate-of-the-art performance among both full-scale and accelerated models. The\ncode will be made publicly available.\n","authors":["Junhao Gu","Peng-Tao Jiang","Hao Zhang","Mi Zhou","Jinwei Chen","Wenming Yang","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2410.13807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13790v1","updated":"2024-10-17T17:31:24Z","published":"2024-10-17T17:31:24Z","title":"MotionBank: A Large-scale Video Motion Benchmark with Disentangled\n  Rule-based Annotations","summary":"  In this paper, we tackle the problem of how to build and benchmark a large\nmotion model (LMM). The ultimate goal of LMM is to serve as a foundation model\nfor versatile motion-related tasks, e.g., human motion generation, with\ninterpretability and generalizability. Though advanced, recent LMM-related\nworks are still limited by small-scale motion data and costly text\ndescriptions. Besides, previous motion benchmarks primarily focus on pure body\nmovements, neglecting the ubiquitous motions in context, i.e., humans\ninteracting with humans, objects, and scenes. To address these limitations, we\nconsolidate large-scale video action datasets as knowledge banks to build\nMotionBank, which comprises 13 video action datasets, 1.24M motion sequences,\nand 132.9M frames of natural and diverse human motions. Different from\nlaboratory-captured motions, in-the-wild human-centric videos contain abundant\nmotions in context. To facilitate better motion text alignment, we also\nmeticulously devise a motion caption generation algorithm to automatically\nproduce rule-based, unbiased, and disentangled text descriptions via the\nkinematic characteristics for each motion. Extensive experiments show that our\nMotionBank is beneficial for general motion-related tasks of human motion\ngeneration, motion in-context generation, and motion understanding. Video\nmotions together with the rule-based text annotations could serve as an\nefficient alternative for larger LMMs. Our dataset, codes, and benchmark will\nbe publicly available at https://github.com/liangxuy/MotionBank.\n","authors":["Liang Xu","Shaoyang Hua","Zili Lin","Yifan Liu","Feipeng Ma","Yichao Yan","Xin Jin","Xiaokang Yang","Wenjun Zeng"],"pdf_url":"https://arxiv.org/pdf/2410.13790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13786v1","updated":"2024-10-17T17:22:59Z","published":"2024-10-17T17:22:59Z","title":"Emphasizing Semantic Consistency of Salient Posture for Speech-Driven\n  Gesture Generation","summary":"  Speech-driven gesture generation aims at synthesizing a gesture sequence\nsynchronized with the input speech signal. Previous methods leverage neural\nnetworks to directly map a compact audio representation to the gesture\nsequence, ignoring the semantic association of different modalities and failing\nto deal with salient gestures. In this paper, we propose a novel speech-driven\ngesture generation method by emphasizing the semantic consistency of salient\nposture. Specifically, we first learn a joint manifold space for the individual\nrepresentation of audio and body pose to exploit the inherent semantic\nassociation between two modalities, and propose to enforce semantic consistency\nvia a consistency loss. Furthermore, we emphasize the semantic consistency of\nsalient postures by introducing a weakly-supervised detector to identify\nsalient postures, and reweighting the consistency loss to focus more on\nlearning the correspondence between salient postures and the high-level\nsemantics of speech content. In addition, we propose to extract audio features\ndedicated to facial expression and body gesture separately, and design separate\nbranches for face and body gesture synthesis. Extensive experimental results\ndemonstrate the superiority of our method over the state-of-the-art approaches.\n","authors":["Fengqi Liu","Hexiang Wang","Jingyu Gong","Ran Yi","Qianyu Zhou","Xuequan Lu","Jiangbo Lu","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2410.13786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13569v1","updated":"2024-10-17T17:17:09Z","published":"2024-10-17T17:17:09Z","title":"Representing Model Weights with Language using Tree Experts","summary":"  The increasing availability of public models begs the question: can we train\nneural networks that use other networks as input? This paper learns to\nrepresent models within a joint space that embeds both model weights and\nlanguage. However, machine learning on model weights is challenging as model\nweights often exhibit significant variation unrelated to the models' semantic\nproperties (nuisance variation). We identify a key property of real-world\nmodels: most public models belong to a small set of Model Trees, where all\nmodels within a tree are fine-tuned from a common ancestor (e.g., a foundation\nmodel). Importantly, we find that within each tree there is less nuisance\nvariation between models. For example, while classifying models according to\ntheir training dataset generally requires complex architectures, in our case,\neven a linear classifier trained on a single layer is often effective. While\neffective, linear layers are computationally expensive as model weights are\nvery high dimensional. To address this, we introduce Probing Experts (ProbeX),\na theoretically motivated, lightweight probing method. Notably, ProbeX is the\nfirst probing method designed to learn from the weights of just a single model\nlayer. We also construct and release a dataset that simulates the structure of\npublic model repositories. Our results show that ProbeX can effectively map the\nweights of large models into a shared weight-language embedding space.\nFurthermore, we demonstrate the impressive generalization of our method,\nachieving zero-shot model classification and retrieval.\n","authors":["Eliahu Horwitz","Bar Cavia","Jonathan Kahana","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2410.13569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13760v1","updated":"2024-10-17T16:55:14Z","published":"2024-10-17T16:55:14Z","title":"Eyelid Fold Consistency in Facial Modeling","summary":"  Eyelid shape is integral to identity and likeness in human facial modeling.\nHuman eyelids are diverse in appearance with varied skin fold and epicanthal\nfold morphology between individuals. Existing parametric face models express\neyelid shape variation to an extent, but do not preserve sufficient likeness\nacross a diverse range of individuals. We propose a new definition of eyelid\nfold consistency and implement geometric processing techniques to model diverse\neyelid shapes in a unified topology. Using this method we reprocess data used\nto train a parametric face model and demonstrate significant improvements in\nface-related machine learning tasks.\n","authors":["Lohit Petikam","Charlie Hewitt","Fatemeh Saleh","Tadas Baltrušaitis"],"pdf_url":"https://arxiv.org/pdf/2410.13760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14015v2","updated":"2024-10-17T16:47:51Z","published":"2024-02-21T18:54:37Z","title":"Corrective Machine Unlearning","summary":"  Machine Learning models increasingly face data integrity challenges due to\nthe use of large-scale training datasets drawn from the Internet. We study what\nmodel developers can do if they detect that some data was manipulated or\nincorrect. Such manipulated data can cause adverse effects including\nvulnerability to backdoored samples, systemic biases, and reduced accuracy on\ncertain input domains. Realistically, all manipulated training samples cannot\nbe identified, and only a small, representative subset of the affected data can\nbe flagged.\n  We formalize Corrective Machine Unlearning as the problem of mitigating the\nimpact of data affected by unknown manipulations on a trained model, only\nhaving identified a subset of the corrupted data. We demonstrate that the\nproblem of corrective unlearning has significantly different requirements from\ntraditional privacy-oriented unlearning. We find most existing unlearning\nmethods, including retraining-from-scratch without the deletion set, require\nmost of the manipulated data to be identified for effective corrective\nunlearning. However, one approach, Selective Synaptic Dampening, achieves\nlimited success, unlearning adverse effects with just a small portion of the\nmanipulated samples in our setting, which shows encouraging signs for future\nprogress. We hope our work spurs research towards developing better methods for\ncorrective unlearning and offers practitioners a new strategy to handle data\nintegrity challenges arising from web-scale training. Code is available at\nhttps://github.com/drimpossible/corrective-unlearning-bench.\n","authors":["Shashwat Goel","Ameya Prabhu","Philip Torr","Ponnurangam Kumaraguru","Amartya Sanyal"],"pdf_url":"https://arxiv.org/pdf/2402.14015v2.pdf","comment":"Published in Transactions of Machine Learning Research (TMLR), 17\n  pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.13733v1","updated":"2024-10-17T16:36:38Z","published":"2024-10-17T16:36:38Z","title":"Improving Multi-modal Large Language Model through Boosting Vision\n  Capabilities","summary":"  We focus on improving the visual understanding capability for boosting the\nvision-language models. We propose \\textbf{Arcana}, a multiModal language\nmodel, which introduces two crucial techniques. First, we present Multimodal\nLoRA (MM-LoRA), a module designed to enhance the decoder. Unlike traditional\nlanguage-driven decoders, MM-LoRA consists of two parallel LoRAs -- one for\nvision and one for language -- each with its own parameters. This disentangled\nparameters design allows for more specialized learning in each modality and\nbetter integration of multimodal information. Second, we introduce the Query\nLadder adapter (QLadder) to improve the visual encoder. QLadder employs a\nlearnable ``\\textit{ladder}'' structure to deeply aggregates the intermediate\nrepresentations from the frozen pretrained visual encoder (e.g., CLIP image\nencoder). This enables the model to learn new and informative visual features,\nas well as remaining the powerful capabilities of the pretrained visual\nencoder. These techniques collectively enhance Arcana's visual perception\npower, enabling it to leverage improved visual information for more accurate\nand contextually relevant outputs across various multimodal scenarios.\nExtensive experiments and ablation studies demonstrate the effectiveness and\ngeneralization capability of our Arcana. The code and re-annotated data are\navailable at \\url{https://arcana-project-page.github.io}.\n","authors":["Yanpeng Sun","Huaxin Zhang","Qiang Chen","Xinyu Zhang","Nong Sang","Gang Zhang","Jingdong Wang","Zechao Li"],"pdf_url":"https://arxiv.org/pdf/2410.13733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13726v1","updated":"2024-10-17T16:32:36Z","published":"2024-10-17T16:32:36Z","title":"DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework\n  for Talking Head Video Generation","summary":"  Talking head generation intends to produce vivid and realistic talking head\nvideos from a single portrait and speech audio clip. Although significant\nprogress has been made in diffusion-based talking head generation, almost all\nmethods rely on autoregressive strategies, which suffer from limited context\nutilization beyond the current generation step, error accumulation, and slower\ngeneration speed. To address these challenges, we present DAWN (Dynamic frame\nAvatar With Non-autoregressive diffusion), a framework that enables all-at-once\ngeneration of dynamic-length video sequences. Specifically, it consists of two\nmain components: (1) audio-driven holistic facial dynamics generation in the\nlatent motion space, and (2) audio-driven head pose and blink generation.\nExtensive experiments demonstrate that our method generates authentic and vivid\nvideos with precise lip motions, and natural pose/blink movements.\nAdditionally, with a high generation speed, DAWN possesses strong extrapolation\ncapabilities, ensuring the stable production of high-quality long videos. These\nresults highlight the considerable promise and potential impact of DAWN in the\nfield of talking head video generation. Furthermore, we hope that DAWN sparks\nfurther exploration of non-autoregressive approaches in diffusion models. Our\ncode will be publicly at https://github.com/Hanbo-Cheng/DAWN-pytorch.\n","authors":["Hanbo Cheng","Limin Lin","Chenyu Liu","Pengcheng Xia","Pengfei Hu","Jiefeng Ma","Jun Du","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2410.13726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13720v1","updated":"2024-10-17T16:22:46Z","published":"2024-10-17T16:22:46Z","title":"Movie Gen: A Cast of Media Foundation Models","summary":"  We present Movie Gen, a cast of foundation models that generates\nhigh-quality, 1080p HD videos with different aspect ratios and synchronized\naudio. We also show additional capabilities such as precise instruction-based\nvideo editing and generation of personalized videos based on a user's image.\nOur models set a new state-of-the-art on multiple tasks: text-to-video\nsynthesis, video personalization, video editing, video-to-audio generation, and\ntext-to-audio generation. Our largest video generation model is a 30B parameter\ntransformer trained with a maximum context length of 73K video tokens,\ncorresponding to a generated video of 16 seconds at 16 frames-per-second. We\nshow multiple technical innovations and simplifications on the architecture,\nlatent spaces, training objectives and recipes, data curation, evaluation\nprotocols, parallelization techniques, and inference optimizations that allow\nus to reap the benefits of scaling pre-training data, model size, and training\ncompute for training large scale media generation models. We hope this paper\nhelps the research community to accelerate progress and innovation in media\ngeneration models. All videos from this paper are available at\nhttps://go.fb.me/MovieGenResearchVideos.\n","authors":["Adam Polyak","Amit Zohar","Andrew Brown","Andros Tjandra","Animesh Sinha","Ann Lee","Apoorv Vyas","Bowen Shi","Chih-Yao Ma","Ching-Yao Chuang","David Yan","Dhruv Choudhary","Dingkang Wang","Geet Sethi","Guan Pang","Haoyu Ma","Ishan Misra","Ji Hou","Jialiang Wang","Kiran Jagadeesh","Kunpeng Li","Luxin Zhang","Mannat Singh","Mary Williamson","Matt Le","Matthew Yu","Mitesh Kumar Singh","Peizhao Zhang","Peter Vajda","Quentin Duval","Rohit Girdhar","Roshan Sumbaly","Sai Saketh Rambhatla","Sam Tsai","Samaneh Azadi","Samyak Datta","Sanyuan Chen","Sean Bell","Sharadh Ramaswamy","Shelly Sheynin","Siddharth Bhattacharya","Simran Motwani","Tao Xu","Tianhe Li","Tingbo Hou","Wei-Ning Hsu","Xi Yin","Xiaoliang Dai","Yaniv Taigman","Yaqiao Luo","Yen-Cheng Liu","Yi-Chiao Wu","Yue Zhao","Yuval Kirstain","Zecheng He","Zijian He","Albert Pumarola","Ali Thabet","Artsiom Sanakoyeu","Arun Mallya","Baishan Guo","Boris Araya","Breena Kerr","Carleigh Wood","Ce Liu","Cen Peng","Dimitry Vengertsev","Edgar Schonfeld","Elliot Blanchard","Felix Juefei-Xu","Fraylie Nord","Jeff Liang","John Hoffman","Jonas Kohler","Kaolin Fire","Karthik Sivakumar","Lawrence Chen","Licheng Yu","Luya Gao","Markos Georgopoulos","Rashel Moritz","Sara K. Sampson","Shikai Li","Simone Parmeggiani","Steve Fine","Tara Fowler","Vladan Petrovic","Yuming Du"],"pdf_url":"https://arxiv.org/pdf/2410.13720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12214v2","updated":"2024-10-17T16:16:33Z","published":"2024-10-16T04:19:28Z","title":"Order-aware Interactive Segmentation","summary":"  Interactive segmentation aims to accurately segment target objects with\nminimal user interactions. However, current methods often fail to accurately\nseparate target objects from the background, due to a limited understanding of\norder, the relative depth between objects in a scene. To address this issue, we\npropose OIS: order-aware interactive segmentation, where we explicitly encode\nthe relative depth between objects into order maps. We introduce a novel\norder-aware attention, where the order maps seamlessly guide the user\ninteractions (in the form of clicks) to attend to the image features. We\nfurther present an object-aware attention module to incorporate a strong\nobject-level understanding to better differentiate objects with similar order.\nOur approach allows both dense and sparse integration of user clicks, enhancing\nboth accuracy and efficiency as compared to prior works. Experimental results\ndemonstrate that OIS achieves state-of-the-art performance, improving mIoU\nafter one click by 7.61 on the HQSeg44K dataset and 1.32 on the DAVIS dataset\nas compared to the previous state-of-the-art SegNext, while also doubling\ninference speed compared to current leading methods. The project page is\nhttps://ukaukaaaa.github.io/projects/OIS/index.html\n","authors":["Bin Wang","Anwesa Choudhuri","Meng Zheng","Zhongpai Gao","Benjamin Planche","Andong Deng","Qin Liu","Terrence Chen","Ulas Bagci","Ziyan Wu"],"pdf_url":"https://arxiv.org/pdf/2410.12214v2.pdf","comment":"Interactive demo can be found in project page:\n  https://ukaukaaaa.github.io/projects/OIS/index.html"},{"id":"http://arxiv.org/abs/2410.11092v2","updated":"2024-10-17T16:13:49Z","published":"2024-10-14T21:10:56Z","title":"EchoApex: A General-Purpose Vision Foundation Model for Echocardiography","summary":"  Quantitative evaluation of echocardiography is essential for precise\nassessment of cardiac condition, monitoring disease progression, and guiding\ntreatment decisions. The diverse nature of echo images, including variations in\nprobe types, manufacturers, and pathologies, poses challenges for developing\nartificial intelligent models that can generalize across different clinical\npractice. We introduce EchoApex, the first general-purpose vision foundation\nmodel echocardiography with applications on a variety of clinical practice.\nLeveraging self-supervised learning, EchoApex is pretrained on over 20 million\necho images from 11 clinical centres. By incorporating task-specific decoders\nand adapter modules, we demonstrate the effectiveness of EchoApex on 4\ndifferent kind of clinical applications with 28 sub-tasks, including view\nclassification, interactive structure segmentation, left ventricle hypertrophy\ndetection and automated ejection fraction estimation from view sequences.\nCompared to state-of-the-art task-specific models, EchoApex attains improved\nperformance with a unified image encoding architecture, demonstrating the\nbenefits of model pretraining at scale with in-domain data. Furthermore,\nEchoApex illustrates the potential for developing a general-purpose vision\nfoundation model tailored specifically for echocardiography, capable of\naddressing a diverse range of clinical applications with high efficiency and\nefficacy.\n","authors":["Abdoul Aziz Amadou","Yue Zhang","Sebastien Piat","Paul Klein","Ingo Schmuecking","Tiziano Passerini","Puneet Sharma"],"pdf_url":"https://arxiv.org/pdf/2410.11092v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12040v5","updated":"2024-10-17T16:11:43Z","published":"2024-07-01T17:59:55Z","title":"Comprehensive Performance Evaluation of YOLO11, YOLOv10, YOLOv9 and\n  YOLOv8 on Detecting and Counting Fruitlet in Complex Orchard Environments","summary":"  This study extensively evaluated You Only Look Once (YOLO) object detection\nalgorithms across all configurations (total 22) of YOLOv8, YOLOv9, YOLOv10, and\nYOLO11 for green fruit detection in commercial orchards. The research also\nvalidated in-field fruitlet counting using an iPhone and machine vision sensors\nacross four apple varieties: Scifresh, Scilate, Honeycrisp and Cosmic Crisp.\nAmong the 22 configurations evaluated, YOLO11s and YOLOv9 gelan-base\noutperformed others with mAP@50 scores of 0.933 and 0.935 respectively. In\nterms of recall, YOLOv9 gelan-base achieved the highest value among YOLOv9\nconfigurations at 0.899, while YOLO11m led YOLO11 variants with 0.897. YOLO11n\nemerged as the fastest model, achieving fastest inference speed of only 2.4 ms,\nsignificantly outpacing the leading configurations of YOLOv10n, YOLOv9 gelan-s,\nand YOLOv8n, with speeds of 5.5, 11.5, and 4.1 ms, respectively. This\ncomparative evaluation highlights the strengths of YOLO11, YOLOv9, and YOLOv10,\noffering researchers essential insights to choose the best-suited model for\nfruitlet detection and possible automation in commercial orchards. For\nreal-time automation related work in relevant datasets, we recommend using\nYOLO11n due to its high detection and image processing speed. Keywords: YOLO11,\nYOLO11 Object Detection, YOLOv10, YOLOv9, YOLOv8, You Only Look Once, Fruitlet\nDetection, Greenfruit Detection, Green Apple Detection, Agricultural\nAutomation, Artificial Intelligence, Deep Learning, Machine Learning, Zero-shot\nDetection\n","authors":["Ranjan Sapkota","Zhichao Meng","Martin Churuvija","Xiaoqiang Du","Zenghong Ma","Manoj Karkee"],"pdf_url":"https://arxiv.org/pdf/2407.12040v5.pdf","comment":"15 figures, 2 tables"},{"id":"http://arxiv.org/abs/2406.10322v2","updated":"2024-10-17T16:06:18Z","published":"2024-06-14T17:41:55Z","title":"LieRE: Generalizing Rotary Position Encodings","summary":"  While Rotary Position Embeddings (RoPE) for large language models have become\nwidely adopted, their application for other modalities has been slower. Here,\nwe introduce Lie group Relative position Encodings (LieRE) that goes beyond\nRoPE in supporting n-dimensional inputs. We evaluate the performance of LieRE\non 2D and 3D image classification tasks and observe that LieRE leads to marked\nrelative improvements in performance (up to 9.7% for 2D and up to 25.5% for\n3D), training efficiency (3.5x reduction), data efficiency (30%) compared to\nthe baselines of DeiT III, RoPE-Mixed and Vision-Llama.\nhttps://github.com/Stanford-AIMI/LieRE\n","authors":["Sophie Ostmeier","Brian Axelrod","Michael E. Moseley","Akshay Chaudhari","Curtis Langlotz"],"pdf_url":"https://arxiv.org/pdf/2406.10322v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13694v1","updated":"2024-10-17T15:59:52Z","published":"2024-10-17T15:59:52Z","title":"Exploring the Design Space of Visual Context Representation in Video\n  MLLMs","summary":"  Video Multimodal Large Language Models (MLLMs) have shown remarkable\ncapability of understanding the video semantics on various downstream tasks.\nDespite the advancements, there is still a lack of systematic research on\nvisual context representation, which refers to the scheme to select frames from\na video and further select the tokens from a frame. In this paper, we explore\nthe design space for visual context representation, and aim to improve the\nperformance of video MLLMs by finding more effective representation schemes.\nFirstly, we formulate the task of visual context representation as a\nconstrained optimization problem, and model the language modeling loss as a\nfunction of the number of frames and the number of embeddings (or tokens) per\nframe, given the maximum visual context window size. Then, we explore the\nscaling effects in frame selection and token selection respectively, and fit\nthe corresponding function curve by conducting extensive empirical experiments.\nWe examine the effectiveness of typical selection strategies and present\nempirical findings to determine the two factors. Furthermore, we study the\njoint effect of frame selection and token selection, and derive the optimal\nformula for determining the two factors. We demonstrate that the derived\noptimal settings show alignment with the best-performed results of empirical\nexperiments. Our code and model are available at:\nhttps://github.com/RUCAIBox/Opt-Visor.\n","authors":["Yifan Du","Yuqi Huo","Kun Zhou","Zijia Zhao","Haoyu Lu","Han Huang","Wayne Xin Zhao","Bingning Wang","Weipeng Chen","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2410.13694v1.pdf","comment":"Long Video MLLM; work in progress"},{"id":"http://arxiv.org/abs/2410.12407v2","updated":"2024-10-17T15:59:34Z","published":"2024-10-16T09:42:29Z","title":"Beyond Coarse-Grained Matching in Video-Text Retrieval","summary":"  Video-text retrieval has seen significant advancements, yet the ability of\nmodels to discern subtle differences in captions still requires verification.\nIn this paper, we introduce a new approach for fine-grained evaluation. Our\napproach can be applied to existing datasets by automatically generating hard\nnegative test captions with subtle single-word variations across nouns, verbs,\nadjectives, adverbs, and prepositions. We perform comprehensive experiments\nusing four state-of-the-art models across two standard benchmarks (MSR-VTT and\nVATEX) and two specially curated datasets enriched with detailed descriptions\n(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our\nanalyses show that the current evaluation benchmarks fall short in detecting a\nmodel's ability to perceive subtle single-word differences, 2) our fine-grained\nevaluation highlights the difficulty models face in distinguishing such subtle\nvariations. To enhance fine-grained understanding, we propose a new baseline\nthat can be easily combined with current methods. Experiments on our\nfine-grained evaluations demonstrate that this approach enhances a model's\nability to understand fine-grained differences.\n","authors":["Aozhu Chen","Hazel Doughty","Xirong Li","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12407v2.pdf","comment":"Accepted to ACCV 2024"},{"id":"http://arxiv.org/abs/2410.13685v1","updated":"2024-10-17T15:47:12Z","published":"2024-10-17T15:47:12Z","title":"Label-free prediction of fluorescence markers in bovine satellite cells\n  using deep learning","summary":"  Assessing the quality of bovine satellite cells (BSCs) is essential for the\ncultivated meat industry, which aims to address global food sustainability\nchallenges. This study aims to develop a label-free method for predicting\nfluorescence markers in isolated BSCs using deep learning. We employed a\nU-Net-based CNN model to predict multiple fluorescence signals from a single\nbright-field microscopy image of cell culture. Two key biomarkers, DAPI and\nPax7, were used to determine the abundance and quality of BSCs. The image\npre-processing pipeline included fluorescence denoising to improve prediction\nperformance and consistency. A total of 48 biological replicates were used,\nwith statistical performance metrics such as Pearson correlation coefficient\nand SSIM employed for model evaluation. The model exhibited better performance\nwith DAPI predictions due to uniform staining. Pax7 predictions were more\nvariable, reflecting biological heterogeneity. Enhanced visualization\ntechniques, including color mapping and image overlay, improved the\ninterpretability of the predictions by providing better contextual and\nperceptual information. The findings highlight the importance of data\npre-processing and demonstrate the potential of deep learning to advance\nnon-invasive, label-free assessment techniques in the cultivated meat industry,\npaving the way for reliable and actionable AI-driven evaluations.\n","authors":["Sania Sinha","Aarham Wasit","Won Seob Kim","Jongkyoo Kim","Jiyoon Yi"],"pdf_url":"https://arxiv.org/pdf/2410.13685v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2402.13251v3","updated":"2024-10-17T15:45:06Z","published":"2024-02-20T18:59:00Z","title":"FlashTex: Fast Relightable Mesh Texturing with LightControlNet","summary":"  Manually creating textures for 3D meshes is time-consuming, even for expert\nvisual content creators. We propose a fast approach for automatically texturing\nan input 3D mesh based on a user-provided text prompt. Importantly, our\napproach disentangles lighting from surface material/reflectance in the\nresulting texture so that the mesh can be properly relit and rendered in any\nlighting environment. We introduce LightControlNet, a new text-to-image model\nbased on the ControlNet architecture, which allows the specification of the\ndesired lighting as a conditioning image to the model. Our text-to-texture\npipeline then constructs the texture in two stages. The first stage produces a\nsparse set of visually consistent reference views of the mesh using\nLightControlNet. The second stage applies a texture optimization based on Score\nDistillation Sampling (SDS) that works with LightControlNet to increase the\ntexture quality while disentangling surface material from lighting. Our\nalgorithm is significantly faster than previous text-to-texture methods, while\nproducing high-quality and relightable textures.\n","authors":["Kangle Deng","Timothy Omernick","Alexander Weiss","Deva Ramanan","Jun-Yan Zhu","Tinghui Zhou","Maneesh Agrawala"],"pdf_url":"https://arxiv.org/pdf/2402.13251v3.pdf","comment":"Project page: https://flashtex.github.io/"},{"id":"http://arxiv.org/abs/2410.13675v1","updated":"2024-10-17T15:33:54Z","published":"2024-10-17T15:33:54Z","title":"Pose-Based Sign Language Appearance Transfer","summary":"  We introduce a method for transferring the signer's appearance in sign\nlanguage skeletal poses while preserving the sign content. Using estimated\nposes, we transfer the appearance of one signer to another, maintaining natural\nmovements and transitions. This approach improves pose-based rendering and sign\nstitching while obfuscating identity. Our experiments show that while the\nmethod reduces signer identification accuracy, it slightly harms sign\nrecognition performance, highlighting a tradeoff between privacy and utility.\nOur code is available at\n\\url{https://github.com/sign-language-processing/pose-anonymization}.\n","authors":["Amit Moryossef","Gerard Sant","Zifan Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.13675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13674v1","updated":"2024-10-17T15:33:35Z","published":"2024-10-17T15:33:35Z","title":"Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning\n  via Image-Guided Diffusion","summary":"  Low-quality or scarce data has posed significant challenges for training deep\nneural networks in practice. While classical data augmentation cannot\ncontribute very different new data, diffusion models opens up a new door to\nbuild self-evolving AI by generating high-quality and diverse synthetic data\nthrough text-guided prompts. However, text-only guidance cannot control\nsynthetic images' proximity to the original images, resulting in\nout-of-distribution data detrimental to the model performance. To overcome the\nlimitation, we study image guidance to achieve a spectrum of interpolations\nbetween synthetic and real images. With stronger image guidance, the generated\nimages are similar to the training data but hard to learn. While with weaker\nimage guidance, the synthetic images will be easier for model but contribute to\na larger distribution gap with the original data. The generated full spectrum\nof data enables us to build a novel \"Diffusion Curriculum (DisCL)\". DisCL\nadjusts the image guidance level of image synthesis for each training stage: It\nidentifies and focuses on hard samples for the model and assesses the most\neffective guidance level of synthetic images to improve hard data learning. We\napply DisCL to two challenging tasks: long-tail (LT) classification and\nlearning from low-quality data. It focuses on lower-guidance images of\nhigh-quality to learn prototypical features as a warm-up of learning\nhigher-guidance images that might be weak on diversity or quality. Extensive\nexperiments showcase a gain of 2.7% and 2.1% in OOD and ID macro-accuracy when\napplying DisCL to iWildCam dataset. On ImageNet-LT, DisCL improves the base\nmodel's tail-class accuracy from 4.4% to 23.64% and leads to a 4.02%\nimprovement in all-class accuracy.\n","authors":["Yijun Liang","Shweta Bhardwaj","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.13674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16005v3","updated":"2024-10-17T15:28:15Z","published":"2024-05-25T02:02:08Z","title":"PTQ4DiT: Post-training Quantization for Diffusion Transformers","summary":"  The recent introduction of Diffusion Transformers (DiTs) has demonstrated\nexceptional capabilities in image generation by using a different backbone\narchitecture, departing from traditional U-Nets and embracing the scalable\nnature of transformers. Despite their advanced capabilities, the wide\ndeployment of DiTs, particularly for real-time applications, is currently\nhampered by considerable computational demands at the inference stage.\nPost-training Quantization (PTQ) has emerged as a fast and data-efficient\nsolution that can significantly reduce computation and memory footprint by\nusing low-bit weights and activations. However, its applicability to DiTs has\nnot yet been explored and faces non-trivial difficulties due to the unique\ndesign of DiTs. In this paper, we propose PTQ4DiT, a specifically designed PTQ\nmethod for DiTs. We discover two primary quantization challenges inherent in\nDiTs, notably the presence of salient channels with extreme magnitudes and the\ntemporal variability in distributions of salient activation over multiple\ntimesteps. To tackle these challenges, we propose Channel-wise Salience\nBalancing (CSB) and Spearmen's $\\rho$-guided Salience Calibration (SSC). CSB\nleverages the complementarity property of channel magnitudes to redistribute\nthe extremes, alleviating quantization errors for both activations and weights.\nSSC extends this approach by dynamically adjusting the balanced salience to\ncapture the temporal variations in activation. Additionally, to eliminate extra\ncomputational costs caused by PTQ4DiT during inference, we design an offline\nre-parameterization strategy for DiTs. Experiments demonstrate that our PTQ4DiT\nsuccessfully quantizes DiTs to 8-bit precision (W8A8) while preserving\ncomparable generation ability and further enables effective quantization to\n4-bit weight precision (W4A8) for the first time.\n","authors":["Junyi Wu","Haoxuan Wang","Yuzhang Shang","Mubarak Shah","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2405.16005v3.pdf","comment":"NeurIPS 2024. Code is available at\n  https://github.com/adreamwu/PTQ4DiT"},{"id":"http://arxiv.org/abs/2410.13666v1","updated":"2024-10-17T15:27:17Z","published":"2024-10-17T15:27:17Z","title":"VL-GLUE: A Suite of Fundamental yet Challenging Visuo-Linguistic\n  Reasoning Tasks","summary":"  Deriving inference from heterogeneous inputs (such as images, text, and\naudio) is an important skill for humans to perform day-to-day tasks. A similar\nability is desirable for the development of advanced Artificial Intelligence\n(AI) systems. While state-of-the-art models are rapidly closing the gap with\nhuman-level performance on diverse computer vision and NLP tasks separately,\nthey struggle to solve tasks that require joint reasoning over visual and\ntextual modalities. Inspired by GLUE (Wang et. al., 2018)- a multitask\nbenchmark for natural language understanding, we propose VL-GLUE in this paper.\nVL-GLUE consists of over 100k samples spanned across seven different tasks,\nwhich at their core require visuo-linguistic reasoning. Moreover, our benchmark\ncomprises of diverse image types (from synthetically rendered figures, and\nday-to-day scenes to charts and complex diagrams) and includes a broad variety\nof domain-specific text (from cooking, politics, and sports to high-school\ncurricula), demonstrating the need for multi-modal understanding in the\nreal-world. We show that this benchmark is quite challenging for existing\nlarge-scale vision-language models and encourage development of systems that\npossess robust visuo-linguistic reasoning capabilities.\n","authors":["Shailaja Keyur Sampat","Mutsumi Nakamura","Shankar Kailas","Kartik Aggarwal","Mandy Zhou","Yezhou Yang","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2410.13666v1.pdf","comment":"18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.13663v1","updated":"2024-10-17T15:25:13Z","published":"2024-10-17T15:25:13Z","title":"DiRecNetV2: A Transformer-Enhanced Network for Aerial Disaster\n  Recognition","summary":"  The integration of Unmanned Aerial Vehicles (UAVs) with artificial\nintelligence (AI) models for aerial imagery processing in disaster assessment,\nnecessitates models that demonstrate exceptional accuracy, computational\nefficiency, and real-time processing capabilities. Traditionally Convolutional\nNeural Networks (CNNs), demonstrate efficiency in local feature extraction but\nare limited by their potential for global context interpretation. On the other\nhand, Vision Transformers (ViTs) show promise for improved global context\ninterpretation through the use of attention mechanisms, although they still\nremain underinvestigated in UAV-based disaster response applications. Bridging\nthis research gap, we introduce DiRecNetV2, an improved hybrid model that\nutilizes convolutional and transformer layers. It merges the inductive biases\nof CNNs for robust feature extraction with the global context understanding of\nTransformers, maintaining a low computational load ideal for UAV applications.\nAdditionally, we introduce a new, compact multi-label dataset of disasters, to\nset an initial benchmark for future research, exploring how models trained on\nsingle-label data perform in a multi-label test set. The study assesses\nlightweight CNNs and ViTs on the AIDERSv2 dataset, based on the frames per\nsecond (FPS) for efficiency and the weighted F1 scores for classification\nperformance. DiRecNetV2 not only achieves a weighted F1 score of 0.964 on a\nsingle-label test set but also demonstrates adaptability, with a score of 0.614\non a complex multi-label test set, while functioning at 176.13 FPS on the\nNvidia Orin Jetson device.\n","authors":["Demetris Shianios","Panayiotis Kolios","Christos Kyrkou"],"pdf_url":"https://arxiv.org/pdf/2410.13663v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2410.13662v1","updated":"2024-10-17T15:22:57Z","published":"2024-10-17T15:22:57Z","title":"ActionCOMET: A Zero-shot Approach to Learn Image-specific Commonsense\n  Concepts about Actions","summary":"  Humans observe various actions being performed by other humans (physically or\nin videos/images) and can draw a wide range of inferences about it beyond what\nthey can visually perceive. Such inferences include determining the aspects of\nthe world that make action execution possible (e.g. liquid objects can undergo\npouring), predicting how the world will change as a result of the action (e.g.\npotatoes being golden and crispy after frying), high-level goals associated\nwith the action (e.g. beat the eggs to make an omelet) and reasoning about\nactions that possibly precede or follow the current action (e.g. crack eggs\nbefore whisking or draining pasta after boiling). Similar reasoning ability is\nhighly desirable in autonomous systems that would assist us in performing\neveryday tasks. To that end, we propose a multi-modal task to learn\naforementioned concepts about actions being performed in images. We develop a\ndataset consisting of 8.5k images and 59.3k inferences about actions grounded\nin those images, collected from an annotated cooking-video dataset. We propose\nActionCOMET, a zero-shot framework to discern knowledge present in language\nmodels specific to the provided visual input. We present baseline results of\nActionCOMET over the collected dataset and compare them with the performance of\nthe best existing VQA approaches.\n","authors":["Shailaja Keyur Sampat","Yezhou Yang","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2410.13662v1.pdf","comment":"15 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:2004.10796 by other authors"},{"id":"http://arxiv.org/abs/2410.13651v1","updated":"2024-10-17T15:16:10Z","published":"2024-10-17T15:16:10Z","title":"Help Me Identify: Is an LLM+VQA System All We Need to Identify Visual\n  Concepts?","summary":"  An ability to learn about new objects from a small amount of visual data and\nproduce convincing linguistic justification about the presence/absence of\ncertain concepts (that collectively compose the object) in novel scenarios is\nan important characteristic of human cognition. This is possible due to\nabstraction of attributes/properties that an object is composed of e.g. an\nobject `bird' can be identified by the presence of a beak, feathers, legs,\nwings, etc. Inspired by this aspect of human reasoning, in this work, we\npresent a zero-shot framework for fine-grained visual concept learning by\nleveraging large language model and Visual Question Answering (VQA) system.\nSpecifically, we prompt GPT-3 to obtain a rich linguistic description of visual\nobjects in the dataset. We convert the obtained concept descriptions into a set\nof binary questions. We pose these questions along with the query image to a\nVQA system and aggregate the answers to determine the presence or absence of an\nobject in the test images. Our experiments demonstrate comparable performance\nwith existing zero-shot visual classification methods and few-shot concept\nlearning approaches, without substantial computational overhead, yet being\nfully explainable from the reasoning perspective.\n","authors":["Shailaja Keyur Sampat","Maitreya Patel","Yezhou Yang","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2410.13651v1.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.03471v3","updated":"2024-10-17T15:12:44Z","published":"2024-07-03T19:36:33Z","title":"Learning Action and Reasoning-Centric Image Editing from Videos and\n  Simulations","summary":"  An image editing model should be able to perform diverse edits, ranging from\nobject replacement, changing attributes or style, to performing actions or\nmovement, which require many forms of reasoning. Current general\ninstruction-guided editing models have significant shortcomings with action and\nreasoning-centric edits. Object, attribute or stylistic changes can be learned\nfrom visually static datasets. On the other hand, high-quality data for action\nand reasoning-centric edits is scarce and has to come from entirely different\nsources that cover e.g. physical dynamics, temporality and spatial reasoning.\nTo this end, we meticulously curate the AURORA Dataset\n(Action-Reasoning-Object-Attribute), a collection of high-quality training\ndata, human-annotated and curated from videos and simulation engines. We focus\non a key aspect of quality training data: triplets (source image, prompt,\ntarget image) contain a single meaningful visual change described by the\nprompt, i.e., truly minimal changes between source and target images. To\ndemonstrate the value of our dataset, we evaluate an AURORA-finetuned model on\na new expert-curated benchmark (AURORA-Bench) covering 8 diverse editing tasks.\nOur model significantly outperforms previous editing models as judged by human\nraters. For automatic evaluations, we find important flaws in previous metrics\nand caution their use for semantically hard editing tasks. Instead, we propose\na new automatic metric that focuses on discriminative understanding. We hope\nthat our efforts : (1) curating a quality training dataset and an evaluation\nbenchmark, (2) developing critical evaluations, and (3) releasing a\nstate-of-the-art model, will fuel further progress on general image editing.\n","authors":["Benno Krojer","Dheeraj Vattikonda","Luis Lara","Varun Jampani","Eva Portelance","Christopher Pal","Siva Reddy"],"pdf_url":"https://arxiv.org/pdf/2407.03471v3.pdf","comment":"NeurIPS 2024 (Dataset & Benchmarks)"},{"id":"http://arxiv.org/abs/2410.09913v2","updated":"2024-10-17T15:08:08Z","published":"2024-10-13T16:40:48Z","title":"Stratified Domain Adaptation: A Progressive Self-Training Approach for\n  Scene Text Recognition","summary":"  Unsupervised domain adaptation (UDA) has become increasingly prevalent in\nscene text recognition (STR), especially where training and testing data reside\nin different domains. The efficacy of existing UDA approaches tends to degrade\nwhen there is a large gap between the source and target domains. To deal with\nthis problem, gradually shifting or progressively learning to shift from domain\nto domain is the key issue. In this paper, we introduce the Stratified Domain\nAdaptation (StrDA) approach, which examines the gradual escalation of the\ndomain gap for the learning process. The objective is to partition the training\ndata into subsets so that the progressively self-trained model can adapt to\ngradual changes. We stratify the training data by evaluating the proximity of\neach data sample to both the source and target domains. We propose a novel\nmethod for employing domain discriminators to estimate the out-of-distribution\nand domain discriminative levels of data samples. Extensive experiments on\nbenchmark scene-text datasets show that our approach significantly improves the\nperformance of baseline (source-trained) STR models.\n","authors":["Kha Nhat Le","Hoang-Tuan Nguyen","Hung Tien Tran","Thanh Duc Ngo"],"pdf_url":"https://arxiv.org/pdf/2410.09913v2.pdf","comment":"15 pages, 12 figures, 5 tables, include supplementary materials"},{"id":"http://arxiv.org/abs/2407.04952v2","updated":"2024-10-17T14:58:53Z","published":"2024-07-06T04:06:55Z","title":"Granular Privacy Control for Geolocation with Vision Language Models","summary":"  Vision Language Models (VLMs) are rapidly advancing in their capability to\nanswer information-seeking questions. As these models are widely deployed in\nconsumer applications, they could lead to new privacy risks due to emergent\nabilities to identify people in photos, geolocate images, etc. As we\ndemonstrate, somewhat surprisingly, current open-source and proprietary VLMs\nare very capable image geolocators, making widespread geolocation with VLMs an\nimmediate privacy risk, rather than merely a theoretical future concern. As a\nfirst step to address this challenge, we develop a new benchmark, GPTGeoChat,\nto test the ability of VLMs to moderate geolocation dialogues with users. We\ncollect a set of 1,000 image geolocation conversations between in-house\nannotators and GPT-4v, which are annotated with the granularity of location\ninformation revealed at each turn. Using this new dataset, we evaluate the\nability of various VLMs to moderate GPT-4v geolocation conversations by\ndetermining when too much location information has been revealed. We find that\ncustom fine-tuned models perform on par with prompted API-based models when\nidentifying leaked location information at the country or city level; however,\nfine-tuning on supervised data appears to be needed to accurately moderate\nfiner granularities, such as the name of a restaurant or building.\n","authors":["Ethan Mendes","Yang Chen","James Hays","Sauvik Das","Wei Xu","Alan Ritter"],"pdf_url":"https://arxiv.org/pdf/2407.04952v2.pdf","comment":"Accepted to EMNLP 2024 main conference"},{"id":"http://arxiv.org/abs/2410.13621v1","updated":"2024-10-17T14:55:09Z","published":"2024-10-17T14:55:09Z","title":"Enhanced Prompt-leveraged Weakly Supervised Cancer Segmentation based on\n  Segment Anything","summary":"  This work proposes a novel approach beyond supervised learning for effective\npathological image analysis, addressing the challenge of limited robust labeled\ndata. Pathological diagnosis of diseases like cancer has conventionally relied\non the evaluation of morphological features by physicians and pathologists.\nHowever, recent advancements in compute-aided diagnosis (CAD) systems are\ngaining significant attention as diagnostic support tools. Although the\nadvancement of deep learning has improved CAD significantly, segmentation\nmodels typically require large pixel-level annotated dataset, and such labeling\nis expensive. Existing studies not based on supervised approaches still\nstruggle with limited generalization, and no practical approach has emerged\nyet. To address this issue, we present a weakly supervised semantic\nsegmentation (WSSS) model by combining class activation map and Segment\nAnything Model (SAM)-based pseudo-labeling. For effective pretraining, we adopt\nthe SAM-a foundation model that is pretrained on large datasets and operates in\nzero-shot configurations using only coarse prompts. The proposed approach\ntransfer enhanced Attention Dropout Layer's knowledge to SAM, thereby\ngenerating pseudo-labels. To demonstrate the superiority of the proposed\nmethod, experimental studies are conducted on histopathological breast cancer\ndatasets. The proposed method outperformed other WSSS methods across three\ndatasets, demonstrating its efficiency by achieving this with only 12GB of GPU\nmemory during training. Our code is available at :\nhttps://github.com/QI-NemoSong/EPLC-SAM\n","authors":["Joonhyeon Song","Seohwan Yun","Seongho Yoon","Joohyeok Kim","Sangmin Lee"],"pdf_url":"https://arxiv.org/pdf/2410.13621v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.13618v1","updated":"2024-10-17T14:51:17Z","published":"2024-10-17T14:51:17Z","title":"LoLDU: Low-Rank Adaptation via Lower-Diag-Upper Decomposition for\n  Parameter-Efficient Fine-Tuning","summary":"  The rapid growth of model scale has necessitated substantial computational\nresources for fine-tuning. Existing approach such as Low-Rank Adaptation (LoRA)\nhas sought to address the problem of handling the large updated parameters in\nfull fine-tuning. However, LoRA utilize random initialization and optimization\nof low-rank matrices to approximate updated weights, which can result in\nsuboptimal convergence and an accuracy gap compared to full fine-tuning. To\naddress these issues, we propose LoLDU, a Parameter-Efficient Fine-Tuning\n(PEFT) approach that significantly reduces trainable parameters by 2600 times\ncompared to regular PEFT methods while maintaining comparable performance.\nLoLDU leverages Lower-Diag-Upper Decomposition (LDU) to initialize low-rank\nmatrices for faster convergence and orthogonality. We focus on optimizing the\ndiagonal matrix for scaling transformations. To the best of our knowledge,\nLoLDU has the fewest parameters among all PEFT approaches. We conducted\nextensive experiments across 4 instruction-following datasets, 6 natural\nlanguage understanding (NLU) datasets, 8 image classification datasets, and\nimage generation datasets with multiple model types (LLaMA2, RoBERTa, ViT, and\nStable Diffusion), providing a comprehensive and detailed analysis. Our\nopen-source code can be accessed at\n\\href{https://github.com/SKDDJ/LoLDU}{https://github.com/SKDDJ/LoLDU}.\n","authors":["Yiming Shi","Jiwei Wei","Yujia Wu","Ran Ran","Chengwei Sun","Shiyuan He","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2410.13618v1.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.13616v1","updated":"2024-10-17T14:49:37Z","published":"2024-10-17T14:49:37Z","title":"Spatiotemporal Object Detection for Improved Aerial Vehicle Detection in\n  Traffic Monitoring","summary":"  This work presents advancements in multi-class vehicle detection using UAV\ncameras through the development of spatiotemporal object detection models. The\nstudy introduces a Spatio-Temporal Vehicle Detection Dataset (STVD) containing\n6, 600 annotated sequential frame images captured by UAVs, enabling\ncomprehensive training and evaluation of algorithms for holistic spatiotemporal\nperception. A YOLO-based object detection algorithm is enhanced to incorporate\ntemporal dynamics, resulting in improved performance over single frame models.\nThe integration of attention mechanisms into spatiotemporal models is shown to\nfurther enhance performance. Experimental validation demonstrates significant\nprogress, with the best spatiotemporal model exhibiting a 16.22% improvement\nover single frame models, while it is demonstrated that attention mechanisms\nhold the potential for additional performance gains.\n","authors":["Kristina Telegraph","Christos Kyrkou"],"pdf_url":"https://arxiv.org/pdf/2410.13616v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2410.13615v1","updated":"2024-10-17T14:47:53Z","published":"2024-10-17T14:47:53Z","title":"Material Fingerprinting: Identifying and Predicting Perceptual\n  Attributes of Material Appearance","summary":"  The world is abundant with diverse materials, each possessing unique surface\nappearances that play a crucial role in our daily perception and understanding\nof their properties. Despite advancements in technology enabling the capture\nand realistic reproduction of material appearances for visualization and\nquality control, the interoperability of material property information across\nvarious measurement representations and software platforms remains a complex\nchallenge. A key to overcoming this challenge lies in the automatic\nidentification of materials' perceptual features, enabling intuitive\ndifferentiation of properties stored in disparate material data\nrepresentations. We reasoned that for many practical purposes, a compact\nrepresentation of the perceptual appearance is more useful than an exhaustive\nphysical description.This paper introduces a novel approach to material\nidentification by encoding perceptual features obtained from dynamic visual\nstimuli. We conducted a psychophysical experiment to select and validate 16\nparticularly significant perceptual attributes obtained from videos of 347\nmaterials. We then gathered attribute ratings from over twenty participants for\neach material, creating a 'material fingerprint' that encodes the unique\nperceptual properties of each material. Finally, we trained a multi-layer\nperceptron model to predict the relationship between statistical and deep\nlearning image features and their corresponding perceptual properties. We\ndemonstrate the model's performance in material retrieval and filtering\naccording to individual attributes. This model represents a significant step\ntowards simplifying the sharing and understanding of material properties in\ndiverse digital environments regardless of their digital representation,\nenhancing both the accuracy and efficiency of material identification.\n","authors":["Jiri Filip","Filip Dechterenko","Filipp Schmidt","Jiri Lukavsky","Veronika Vilimovska","Jan Kotera","Roland W. Fleming"],"pdf_url":"https://arxiv.org/pdf/2410.13615v1.pdf","comment":"14 pages, 12 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.13613v1","updated":"2024-10-17T14:47:08Z","published":"2024-10-17T14:47:08Z","title":"MEGA: Memory-Efficient 4D Gaussian Splatting for Dynamic Scenes","summary":"  4D Gaussian Splatting (4DGS) has recently emerged as a promising technique\nfor capturing complex dynamic 3D scenes with high fidelity. It utilizes a 4D\nGaussian representation and a GPU-friendly rasterizer, enabling rapid rendering\nspeeds. Despite its advantages, 4DGS faces significant challenges, notably the\nrequirement of millions of 4D Gaussians, each with extensive associated\nattributes, leading to substantial memory and storage cost. This paper\nintroduces a memory-efficient framework for 4DGS. We streamline the color\nattribute by decomposing it into a per-Gaussian direct color component with\nonly 3 parameters and a shared lightweight alternating current color predictor.\nThis approach eliminates the need for spherical harmonics coefficients, which\ntypically involve up to 144 parameters in classic 4DGS, thereby creating a\nmemory-efficient 4D Gaussian representation. Furthermore, we introduce an\nentropy-constrained Gaussian deformation technique that uses a deformation\nfield to expand the action range of each Gaussian and integrates an\nopacity-based entropy loss to limit the number of Gaussians, thus forcing our\nmodel to use as few Gaussians as possible to fit a dynamic scene well. With\nsimple half-precision storage and zip compression, our framework achieves a\nstorage reduction by approximately 190$\\times$ and 125$\\times$ on the\nTechnicolor and Neural 3D Video datasets, respectively, compared to the\noriginal 4DGS. Meanwhile, it maintains comparable rendering speeds and scene\nrepresentation quality, setting a new standard in the field.\n","authors":["Xinjie Zhang","Zhening Liu","Yifan Zhang","Xingtong Ge","Dailan He","Tongda Xu","Yan Wang","Zehong Lin","Shuicheng Yan","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.13613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13611v1","updated":"2024-10-17T14:46:34Z","published":"2024-10-17T14:46:34Z","title":"H2OVL-Mississippi Vision Language Models Technical Report","summary":"  Smaller vision-language models (VLMs) are becoming increasingly important for\nprivacy-focused, on-device applications due to their ability to run efficiently\non consumer hardware for processing enterprise commercial documents and images.\nThese models require strong language understanding and visual capabilities to\nenhance human-machine interaction. To address this need, we present\nH2OVL-Mississippi, a pair of small VLMs trained on 37 million image-text pairs\nusing 240 hours of compute on 8 x H100 GPUs. H2OVL-Mississippi-0.8B is a tiny\nmodel with 0.8 billion parameters that specializes in text recognition,\nachieving state of the art performance on the Text Recognition portion of\nOCRBench and surpassing much larger models in this area. Additionally, we are\nreleasing H2OVL-Mississippi-2B, a 2 billion parameter model for general use\ncases, exhibiting highly competitive metrics across various academic\nbenchmarks. Both models build upon our prior work with H2O-Danube language\nmodels, extending their capabilities into the visual domain. We release them\nunder the Apache 2.0 license, making VLMs accessible to everyone, democratizing\ndocument AI and visual LLMs.\n","authors":["Shaikat Galib","Shanshan Wang","Guanshuo Xu","Pascal Pfeiffer","Ryan Chesler","Mark Landry","Sri Satish Ambati"],"pdf_url":"https://arxiv.org/pdf/2410.13611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13607v1","updated":"2024-10-17T14:43:07Z","published":"2024-10-17T14:43:07Z","title":"DN-4DGS: Denoised Deformable Network with Temporal-Spatial Aggregation\n  for Dynamic Scene Rendering","summary":"  Dynamic scenes rendering is an intriguing yet challenging problem. Although\ncurrent methods based on NeRF have achieved satisfactory performance, they\nstill can not reach real-time levels. Recently, 3D Gaussian Splatting (3DGS)\nhas gar?nered researchers attention due to their outstanding rendering quality\nand real?time speed. Therefore, a new paradigm has been proposed: defining a\ncanonical 3D gaussians and deforming it to individual frames in deformable\nfields. How?ever, since the coordinates of canonical 3D gaussians are filled\nwith noise, which can transfer noise into the deformable fields, and there is\ncurrently no method that adequately considers the aggregation of 4D\ninformation. Therefore, we pro?pose Denoised Deformable Network with\nTemporal-Spatial Aggregation for Dy?namic Scene Rendering (DN-4DGS).\nSpecifically, a Noise Suppression Strategy is introduced to change the\ndistribution of the coordinates of the canonical 3D gaussians and suppress\nnoise. Additionally, a Decoupled Temporal-Spatial Ag?gregation Module is\ndesigned to aggregate information from adjacent points and frames. Extensive\nexperiments on various real-world datasets demonstrate that our method achieves\nstate-of-the-art rendering quality under a real-time level.\n","authors":["Jiahao Lu","Jiacheng Deng","Ruijie Zhu","Yanzhe Liang","Wenfei Yang","Tianzhu Zhang","Xu Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.13607v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.13598v1","updated":"2024-10-17T14:31:02Z","published":"2024-10-17T14:31:02Z","title":"Let Me Finish My Sentence: Video Temporal Grounding with Holistic Text\n  Understanding","summary":"  Video Temporal Grounding (VTG) aims to identify visual frames in a video clip\nthat match text queries. Recent studies in VTG employ cross-attention to\ncorrelate visual frames and text queries as individual token sequences.\nHowever, these approaches overlook a crucial aspect of the problem: a holistic\nunderstanding of the query sentence. A model may capture correlations between\nindividual word tokens and arbitrary visual frames while possibly missing out\non the global meaning. To address this, we introduce two primary contributions:\n(1) a visual frame-level gate mechanism that incorporates holistic textual\ninformation, (2) cross-modal alignment loss to learn the fine-grained\ncorrelation between query and relevant frames. As a result, we regularize the\neffect of individual word tokens and suppress irrelevant visual frames. We\ndemonstrate that our method outperforms state-of-the-art approaches in VTG\nbenchmarks, indicating that holistic text understanding guides the model to\nfocus on the semantically important parts within the video.\n","authors":["Jongbhin Woo","Hyeonggon Ryu","Youngjoon Jang","Jae Won Cho","Joon Son Chung"],"pdf_url":"https://arxiv.org/pdf/2410.13598v1.pdf","comment":"Accepted by ACMMM 24"},{"id":"http://arxiv.org/abs/2410.13594v1","updated":"2024-10-17T14:28:11Z","published":"2024-10-17T14:28:11Z","title":"Deep-learning recognition and tracking of individual nanotubes in\n  low-contrast microscopy videos","summary":"  This study addresses the challenge of analyzing the growth kinetics of carbon\nnanotubes using in-situ homodyne polarization microscopy (HPM) by developing an\nautomated deep learning (DL) approach. A Mask-RCNN architecture, enhanced with\na ResNet-50 backbone, was employed to recognize and track individual nanotubes\nin microscopy videos, significantly improving the efficiency and\nreproducibility of kinetic data extraction. The method involves a series of\nvideo processing steps to enhance contrast and used differential treatment\ntechniques to manage low signal and fast kinetics. The DL model demonstrates\nconsistency with manual measurements and increased throughput, laying the\nfoundation for statistical studies of nanotube growth. The approach can be\nadapted for other types of in-situ microscopy studies, emphasizing the\nimportance of automation in high-throughput data acquisition for research on\nindividual nano-objects.\n","authors":["Vladimir Pimonov","Said Tahir","Vincent Jourdain"],"pdf_url":"https://arxiv.org/pdf/2410.13594v1.pdf","comment":"13 pages, 5 Figures, No supporting information included"},{"id":"http://arxiv.org/abs/2410.13585v1","updated":"2024-10-17T14:21:22Z","published":"2024-10-17T14:21:22Z","title":"Pseudo Dataset Generation for Out-of-Domain Multi-Camera View\n  Recommendation","summary":"  Multi-camera systems are indispensable in movies, TV shows, and other media.\nSelecting the appropriate camera at every timestamp has a decisive impact on\nproduction quality and audience preferences. Learning-based view recommendation\nframeworks can assist professionals in decision-making. However, they often\nstruggle outside of their training domains. The scarcity of labeled\nmulti-camera view recommendation datasets exacerbates the issue. Based on the\ninsight that many videos are edited from the original multi-camera videos, we\npropose transforming regular videos into pseudo-labeled multi-camera view\nrecommendation datasets. Promisingly, by training the model on pseudo-labeled\ndatasets stemming from videos in the target domain, we achieve a 68% relative\nimprovement in the model's accuracy in the target domain and bridge the\naccuracy gap between in-domain and never-before-seen domains.\n","authors":["Kuan-Ying Lee","Qian Zhou","Klara Nahrstedt"],"pdf_url":"https://arxiv.org/pdf/2410.13585v1.pdf","comment":"Accepted to VCIP 2024. Project page:\n  https://eric11220.github.io/publication/VCIP24/"},{"id":"http://arxiv.org/abs/2410.13582v1","updated":"2024-10-17T14:16:45Z","published":"2024-10-17T14:16:45Z","title":"Co-Segmentation without any Pixel-level Supervision with Application to\n  Large-Scale Sketch Classification","summary":"  This work proposes a novel method for object co-segmentation, i.e.\npixel-level localization of a common object in a set of images, that uses no\npixel-level supervision for training. Two pre-trained Vision Transformer (ViT)\nmodels are exploited: ImageNet classification-trained ViT, whose features are\nused to estimate rough object localization through intra-class token relevance,\nand a self-supervised DINO-ViT for intra-image token relevance. On recent\nchallenging benchmarks, the method achieves state-of-the-art performance among\nmethods trained with the same level of supervision (image labels) while being\ncompetitive with methods trained with pixel-level supervision (binary masks).\nThe benefits of the proposed co-segmentation method are further demonstrated in\nthe task of large-scale sketch recognition, that is, the classification of\nsketches into a wide range of categories. The limited amount of hand-drawn\nsketch training data is leveraged by exploiting readily available\nimage-level-annotated datasets of natural images containing a large number of\nclasses. To bridge the domain gap, the classifier is trained on a sketch-like\nproxy domain derived from edges detected on natural images. We show that sketch\nrecognition significantly benefits when the classifier is trained on\nsketch-like structures extracted from the co-segmented area rather than from\nthe full image. Code: https://github.com/nikosips/CBNC .\n","authors":["Nikolaos-Antonios Ypsilantis","Ondřej Chum"],"pdf_url":"https://arxiv.org/pdf/2410.13582v1.pdf","comment":"ACCV 2024 Main Paper + Supplementary (Appendix)"},{"id":"http://arxiv.org/abs/2402.06165v5","updated":"2024-10-17T14:10:16Z","published":"2024-02-09T03:48:20Z","title":"Learning Contrastive Feature Representations for Facial Action Unit\n  Detection","summary":"  Facial action unit (AU) detection has long encountered the challenge of\ndetecting subtle feature differences when AUs activate. Existing methods often\nrely on encoding pixel-level information of AUs, which not only encodes\nadditional redundant information but also leads to increased model complexity\nand limited generalizability. Additionally, the accuracy of AU detection is\nnegatively impacted by the class imbalance issue of each AU type, and the\npresence of noisy and false AU labels. In this paper, we introduce a novel\ncontrastive learning framework aimed for AU detection that incorporates both\nself-supervised and supervised signals, thereby enhancing the learning of\ndiscriminative features for accurate AU detection. To tackle the class\nimbalance issue, we employ a negative sample re-weighting strategy that adjusts\nthe step size of updating parameters for minority and majority class samples.\nMoreover, to address the challenges posed by noisy and false AU labels, we\nemploy a sampling technique that encompasses three distinct types of positive\nsample pairs. This enables us to inject self-supervised signals into the\nsupervised signal, effectively mitigating the adverse effects of noisy labels.\nOur experimental assessments, conducted on four widely-utilized benchmark\ndatasets (BP4D, DISFA, GFT and Aff-Wild2), underscore the superior performance\nof our approach compared to state-of-the-art methods of AU detection. Our code\nis available at \\url{https://github.com/Ziqiao-Shang/AUNCE}.\n","authors":["Ziqiao Shang","Bin Liu","Fengmao Lv","Fei Teng","Tianrui Li"],"pdf_url":"https://arxiv.org/pdf/2402.06165v5.pdf","comment":"35 pages, 18 figures, submitted to Pattern Recognition (PR)"},{"id":"http://arxiv.org/abs/2410.13571v1","updated":"2024-10-17T14:07:46Z","published":"2024-10-17T14:07:46Z","title":"DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving\n  Scene Representation","summary":"  Closed-loop simulation is essential for advancing end-to-end autonomous\ndriving systems. Contemporary sensor simulation methods, such as NeRF and 3DGS,\nrely predominantly on conditions closely aligned with training data\ndistributions, which are largely confined to forward-driving scenarios.\nConsequently, these methods face limitations when rendering complex maneuvers\n(e.g., lane change, acceleration, deceleration). Recent advancements in\nautonomous-driving world models have demonstrated the potential to generate\ndiverse driving videos. However, these approaches remain constrained to 2D\nvideo generation, inherently lacking the spatiotemporal coherence required to\ncapture intricacies of dynamic driving environments. In this paper, we\nintroduce \\textit{DriveDreamer4D}, which enhances 4D driving scene\nrepresentation leveraging world model priors. Specifically, we utilize the\nworld model as a data machine to synthesize novel trajectory videos based on\nreal-world driving data. Notably, we explicitly leverage structured conditions\nto control the spatial-temporal consistency of foreground and background\nelements, thus the generated data adheres closely to traffic constraints. To\nour knowledge, \\textit{DriveDreamer4D} is the first to utilize video generation\nmodels for improving 4D reconstruction in driving scenarios. Experimental\nresults reveal that \\textit{DriveDreamer4D} significantly enhances generation\nquality under novel trajectory views, achieving a relative improvement in FID\nby 24.5\\%, 39.0\\%, and 10.5\\% compared to PVG, $\\text{S}^3$Gaussian, and\nDeformable-GS. Moreover, \\textit{DriveDreamer4D} markedly enhances the\nspatiotemporal coherence of driving agents, which is verified by a\ncomprehensive user study and the relative increases of 20.3\\%, 42.0\\%, and\n13.7\\% in the NTA-IoU metric.\n","authors":["Guosheng Zhao","Chaojun Ni","Xiaofeng Wang","Zheng Zhu","Guan Huang","Xinze Chen","Boyuan Wang","Youyi Zhang","Wenjun Mei","Xingang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13571v1.pdf","comment":"https://drivedreamer4d.github.io"},{"id":"http://arxiv.org/abs/2410.13570v1","updated":"2024-10-17T14:05:41Z","published":"2024-10-17T14:05:41Z","title":"RGB to Hyperspectral: Spectral Reconstruction for Enhanced Surgical\n  Imaging","summary":"  This study investigates the reconstruction of hyperspectral signatures from\nRGB data to enhance surgical imaging, utilizing the publicly available\nHeiPorSPECTRAL dataset from porcine surgery and an in-house neurosurgery\ndataset. Various architectures based on convolutional neural networks (CNNs)\nand transformer models are evaluated using comprehensive metrics. Transformer\nmodels exhibit superior performance in terms of RMSE, SAM, PSNR and SSIM by\neffectively integrating spatial information to predict accurate spectral\nprofiles, encompassing both visible and extended spectral ranges. Qualitative\nassessments demonstrate the capability to predict spectral profiles critical\nfor informed surgical decision-making during procedures. Challenges associated\nwith capturing both the visible and extended hyperspectral ranges are\nhighlighted using the MAE, emphasizing the complexities involved. The findings\nopen up the new research direction of hyperspectral reconstruction for surgical\napplications and clinical use cases in real-time surgical environments.\n","authors":["Tobias Czempiel","Alfie Roddan","Maria Leiloglou","Zepeng Hu","Kevin O'Neill","Giulio Anichini","Danail Stoyanov","Daniel Elson"],"pdf_url":"https://arxiv.org/pdf/2410.13570v1.pdf","comment":"10 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.13567v1","updated":"2024-10-17T14:04:02Z","published":"2024-10-17T14:04:02Z","title":"CCUP: A Controllable Synthetic Data Generation Pipeline for Pretraining\n  Cloth-Changing Person Re-Identification Models","summary":"  Cloth-changing person re-identification (CC-ReID), also known as Long-Term\nPerson Re-Identification (LT-ReID) is a critical and challenging research topic\nin computer vision that has recently garnered significant attention. However,\ndue to the high cost of constructing CC-ReID data, the existing data-driven\nmodels are hard to train efficiently on limited data, causing overfitting\nissue. To address this challenge, we propose a low-cost and efficient pipeline\nfor generating controllable and high-quality synthetic data simulating the\nsurveillance of real scenarios specific to the CC-ReID task. Particularly, we\nconstruct a new self-annotated CC-ReID dataset named Cloth-Changing Unreal\nPerson (CCUP), containing 6,000 IDs, 1,179,976 images, 100 cameras, and 26.5\noutfits per individual. Based on this large-scale dataset, we introduce an\neffective and scalable pretrain-finetune framework for enhancing the\ngeneralization capabilities of the traditional CC-ReID models. The extensive\nexperiments demonstrate that two typical models namely TransReID and FIRe^2,\nwhen integrated into our framework, outperform other state-of-the-art models\nafter pretraining on CCUP and finetuning on the benchmarks such as PRCC,\nVC-Clothes and NKUP. The CCUP is available at:\nhttps://github.com/yjzhao1019/CCUP.\n","authors":["Yujian Zhao","Chengru Wu","Yinong Xu","Xuanzheng Du","Ruiyu Li","Guanglin Niu"],"pdf_url":"https://arxiv.org/pdf/2410.13567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08551v2","updated":"2024-10-17T14:04:01Z","published":"2024-10-11T06:04:30Z","title":"Context-Aware Full Body Anonymization using Text-to-Image Diffusion\n  Models","summary":"  Anonymization plays a key role in protecting sensible information of\nindividuals in real world datasets. Self-driving cars for example need high\nresolution facial features to track people and their viewing direction to\npredict future behaviour and react accordingly. In order to protect people's\nprivacy whilst keeping important features in the dataset, it is important to\nreplace the full body of a person with a highly detailed anonymized one. In\ncontrast to doing face anonymization, full body replacement decreases the\nability of recognizing people by their hairstyle or clothes. In this paper, we\npropose a workflow for full body person anonymization utilizing Stable\nDiffusion as a generative backend. Text-to-image diffusion models, like Stable\nDiffusion, OpenAI's DALL-E or Midjourney, have become very popular in recent\ntime, being able to create photorealistic images from a single text prompt. We\nshow that our method outperforms state-of-the art anonymization pipelines with\nrespect to image quality, resolution, Inception Score (IS) and Frechet\nInception Distance (FID). Additionally, our method is invariant with respect to\nthe image generator and thus able to be used with the latest models available.\n","authors":["Pascal Zwick","Kevin Roesch","Marvin Klemp","Oliver Bringmann"],"pdf_url":"https://arxiv.org/pdf/2410.08551v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13566v1","updated":"2024-10-17T14:03:53Z","published":"2024-10-17T14:03:53Z","title":"360U-Former: HDR Illumination Estimation with Panoramic Adapted Vision\n  Transformers","summary":"  Recent illumination estimation methods have focused on enhancing the\nresolution and improving the quality and diversity of the generated textures.\nHowever, few have explored tailoring the neural network architecture to the\nEquirectangular Panorama (ERP) format utilised in image-based lighting.\nConsequently, high dynamic range images (HDRI) results usually exhibit a seam\nat the side borders and textures or objects that are warped at the poles. To\naddress this shortcoming we propose a novel architecture, 360U-Former, based on\na U-Net style Vision-Transformer which leverages the work of PanoSWIN, an\nadapted shifted window attention tailored to the ERP format. To the best of our\nknowledge, this is the first purely Vision-Transformer model used in the field\nof illumination estimation. We train 360U-Former as a GAN to generate HDRI from\na limited field of view low dynamic range image (LDRI). We evaluate our method\nusing current illumination estimation evaluation protocols and datasets,\ndemonstrating that our approach outperforms existing and state-of-the-art\nmethods without the artefacts typically associated with the use of the ERP\nformat.\n","authors":["Jack Hilliard","Adrian Hilton","Jean-Yves Guillemaut"],"pdf_url":"https://arxiv.org/pdf/2410.13566v1.pdf","comment":"Accepted at AIM Workshop 2024 at ECCV 2024, 18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.13564v1","updated":"2024-10-17T14:00:41Z","published":"2024-10-17T14:00:41Z","title":"Generative Location Modeling for Spatially Aware Object Insertion","summary":"  Generative models have become a powerful tool for image editing tasks,\nincluding object insertion. However, these methods often lack spatial\nawareness, generating objects with unrealistic locations and scales, or\nunintentionally altering the scene background. A key challenge lies in\nmaintaining visual coherence, which requires both a geometrically suitable\nobject location and a high-quality image edit. In this paper, we focus on the\nformer, creating a location model dedicated to identifying realistic object\nlocations. Specifically, we train an autoregressive model that generates\nbounding box coordinates, conditioned on the background image and the desired\nobject class. This formulation allows to effectively handle sparse placement\nannotations and to incorporate implausible locations into a preference dataset\nby performing direct preference optimization. Our extensive experiments\ndemonstrate that our generative location model, when paired with an inpainting\nmethod, substantially outperforms state-of-the-art instruction-tuned models and\nlocation modeling baselines in object insertion tasks, delivering accurate and\nvisually coherent results.\n","authors":["Jooyeol Yun","Davide Abati","Mohamed Omran","Jaegul Choo","Amirhossein Habibian","Auke Wiggers"],"pdf_url":"https://arxiv.org/pdf/2410.13564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13532v1","updated":"2024-10-17T13:20:20Z","published":"2024-10-17T13:20:20Z","title":"RemoteDet-Mamba: A Hybrid Mamba-CNN Network for Multi-modal Object\n  Detection in Remote Sensing Images","summary":"  Unmanned aerial vehicle (UAV) remote sensing is widely applied in fields such\nas emergency response, owing to its advantages of rapid information acquisition\nand low cost. However, due to the effects of shooting distance and imaging\nmechanisms, the objects in the images present challenges such as small size,\ndense distribution, and low inter-class differentiation. To this end, we\npropose a multimodal remote sensing detection network that employs a\nquad-directional selective scanning fusion strategy called RemoteDet-Mamba.\nRemoteDet-Mamba simultaneously facilitates the learning of single-modal local\nfeatures and the integration of patch-level global features across modalities,\nenhancing the distinguishability for small objects and utilizing local\ninformation to improve discrimination between different classes. Additionally,\nthe use of Mamba's serial processing significantly increases detection speed.\nExperimental results on the DroneVehicle dataset demonstrate the effectiveness\nof RemoteDet-Mamba, which achieves superior detection accuracy compared to\nstate-of-the-art methods while maintaining computational efficiency and\nparameter count.\n","authors":["Kejun Ren","Xin Wu","Lianming Xu","Li Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13530v1","updated":"2024-10-17T13:19:32Z","published":"2024-10-17T13:19:32Z","title":"L3DG: Latent 3D Gaussian Diffusion","summary":"  We propose L3DG, the first approach for generative 3D modeling of 3D\nGaussians through a latent 3D Gaussian diffusion formulation. This enables\neffective generative 3D modeling, scaling to generation of entire room-scale\nscenes which can be very efficiently rendered. To enable effective synthesis of\n3D Gaussians, we propose a latent diffusion formulation, operating in a\ncompressed latent space of 3D Gaussians. This compressed latent space is\nlearned by a vector-quantized variational autoencoder (VQ-VAE), for which we\nemploy a sparse convolutional architecture to efficiently operate on room-scale\nscenes. This way, the complexity of the costly generation process via diffusion\nis substantially reduced, allowing higher detail on object-level generation, as\nwell as scalability to large scenes. By leveraging the 3D Gaussian\nrepresentation, the generated scenes can be rendered from arbitrary viewpoints\nin real-time. We demonstrate that our approach significantly improves visual\nquality over prior work on unconditional object-level radiance field synthesis\nand showcase its applicability to room-scale scene generation.\n","authors":["Barbara Roessle","Norman Müller","Lorenzo Porzi","Samuel Rota Bulò","Peter Kontschieder","Angela Dai","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2410.13530v1.pdf","comment":"SIGGRAPH Asia 2024, project page:\n  https://barbararoessle.github.io/l3dg , video: https://youtu.be/UHEEiXCYeLU"},{"id":"http://arxiv.org/abs/2410.13526v1","updated":"2024-10-17T13:14:25Z","published":"2024-10-17T13:14:25Z","title":"Generative Adversarial Synthesis of Radar Point Cloud Scenes","summary":"  For the validation and verification of automotive radars, datasets of\nrealistic traffic scenarios are required, which, how ever, are laborious to\nacquire. In this paper, we introduce radar scene synthesis using GANs as an\nalternative to the real dataset acquisition and simulation-based approaches. We\ntrain a PointNet++ based GAN model to generate realistic radar point cloud\nscenes and use a binary classifier to evaluate the performance of scenes\ngenerated using this model against a test set of real scenes. We demonstrate\nthat our GAN model achieves similar performance (~87%) to the real scenes test\nset.\n","authors":["Muhammad Saad Nawaz","Thomas Dallmann","Torsten Schoen","Dirk Heberling"],"pdf_url":"https://arxiv.org/pdf/2410.13526v1.pdf","comment":"ICMIM 2024; 7th IEEE MTT Conference"},{"id":"http://arxiv.org/abs/2410.13523v1","updated":"2024-10-17T13:11:07Z","published":"2024-10-17T13:11:07Z","title":"Can Medical Vision-Language Pre-training Succeed with Purely Synthetic\n  Data?","summary":"  Medical Vision-Language Pre-training (MedVLP) has made significant progress\nin enabling zero-shot tasks for medical image understanding. However, training\nMedVLP models typically requires large-scale datasets with paired, high-quality\nimage-text data, which are scarce in the medical domain. Recent advancements in\nLarge Language Models (LLMs) and diffusion models have made it possible to\ngenerate large-scale synthetic image-text pairs. This raises the question: *Can\nMedVLP succeed using purely synthetic data?* To address this, we use\noff-the-shelf generative models to create synthetic radiology reports and\npaired Chest X-ray (CXR) images, and propose an automated pipeline to build a\ndiverse, high-quality synthetic dataset, enabling a rigorous study that\nisolates model and training settings, focusing entirely from the data\nperspective. Our results show that MedVLP models trained *exclusively on\nsynthetic data* outperform those trained on real data by **3.8%** in averaged\nAUC on zero-shot classification. Moreover, using a combination of synthetic and\nreal data leads to a further improvement of **9.07%**. Additionally, MedVLP\nmodels trained on synthetic or mixed data consistently outperform those trained\non real data in zero-shot grounding, as well as in fine-tuned classification\nand segmentation tasks. Our analysis suggests MedVLP trained on well-designed\nsynthetic data can outperform models trained on real datasets, which may be\nlimited by low-quality samples and long-tailed distributions.\n","authors":["Che Liu","Zhongwei Wan","Haozhe Wang","Yinda Chen","Talha Qaiser","Chen Jin","Fariba Yousefi","Nikolay Burlutskiy","Rossella Arcucci"],"pdf_url":"https://arxiv.org/pdf/2410.13523v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2406.03857v2","updated":"2024-10-17T13:08:13Z","published":"2024-06-06T08:42:36Z","title":"MuJo: Multimodal Joint Feature Space Learning for Human Activity\n  Recognition","summary":"  Human Activity Recognition (HAR) is a longstanding problem in AI with\napplications in a broad range of areas, including healthcare, sports and\nfitness, security, and more. The performance of HAR in real-world settings is\nstrongly dependent on the type and quality of the input signal that can be\nacquired. Given an unobstructed, high-quality camera view of a scene, computer\nvision systems, in particular in conjunction with foundation models, can today\nfairly reliably distinguish complex activities. On the other hand, recognition\nusing modalities such as wearable sensors (which are often more broadly\navailable, e.g., in mobile phones and smartwatches) is a more difficult\nproblem, as the signals often contain less information and labeled training\ndata is more difficult to acquire. To alleviate the need for labeled data, we\nintroduce our comprehensive Fitness Multimodal Activity Dataset (FiMAD) in this\nwork, which can be used with the proposed pre-training method MuJo (Multimodal\nJoint Feature Space Learning) to enhance HAR performance across various\nmodalities. FiMAD was created using YouTube fitness videos and contains\nparallel video, language, pose, and simulated IMU sensor data. MuJo utilizes\nthis dataset to learn a joint feature space for these modalities. We show that\nclassifiers pre-trained on FiMAD can increase the performance on real HAR\ndatasets such as MM-Fit, MyoGym, MotionSense, and MHEALTH. For instance, on\nMM-Fit, we achieve an Macro F1-Score of up to 0.855 when fine-tuning on only 2%\nof the training data and 0.942 when utilizing the full training set for\nclassification tasks. We have compared our approach to other self-supervised\nones and showed that, unlike them, ours can consistently improve on the\nbaseline network performance as well as provide a better data-efficiency.\n","authors":["Stefan Gerd Fritsch","Cennet Oguz","Vitor Fortes Rey","Lala Ray","Maximilian Kiefer-Emmanouilidis","Paul Lukowicz"],"pdf_url":"https://arxiv.org/pdf/2406.03857v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13510v1","updated":"2024-10-17T12:56:52Z","published":"2024-10-17T12:56:52Z","title":"GeoCoder: Solving Geometry Problems by Generating Modular Code through\n  Vision-Language Models","summary":"  Geometry problem-solving demands advanced reasoning abilities to process\nmultimodal inputs and employ mathematical knowledge effectively.\nVision-language models (VLMs) have made significant progress in various\nmultimodal tasks. Yet, they still struggle with geometry problems and are\nsignificantly limited by their inability to perform mathematical operations not\nseen during pre-training, such as calculating the cosine of an arbitrary angle,\nand by difficulties in correctly applying relevant geometry formulas. To\novercome these challenges, we present GeoCoder, which leverages modular\ncode-finetuning to generate and execute code using a predefined geometry\nfunction library. By executing the code, we achieve accurate and deterministic\ncalculations, contrasting the stochastic nature of autoregressive token\nprediction, while the function library minimizes errors in formula usage. We\nalso propose a multimodal retrieval-augmented variant of GeoCoder, named\nRAG-GeoCoder, which incorporates a non-parametric memory module for retrieving\nfunctions from the geometry library, thereby reducing reliance on parametric\nmemory. Our modular code-finetuning approach enhances the geometric reasoning\ncapabilities of VLMs, yielding an average improvement of over 16% across\nvarious question complexities on the GeomVerse dataset compared to other\nfinetuning methods.\n","authors":["Aditya Sharma","Aman Dalmia","Mehran Kazemi","Amal Zouaq","Christopher J. Pal"],"pdf_url":"https://arxiv.org/pdf/2410.13510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12686v2","updated":"2024-10-17T12:52:30Z","published":"2024-10-16T15:48:28Z","title":"Automatic Mapping of Anatomical Landmarks from Free-Text Using Large\n  Language Models: Insights from Llama-2","summary":"  Anatomical landmarks are vital in medical imaging for navigation and anomaly\ndetection. Modern large language models (LLMs), like Llama-2, offer promise for\nautomating the mapping of these landmarks in free-text radiology reports to\ncorresponding positions in image data. Recent studies propose LLMs may develop\ncoherent representations of generative processes. Motivated by these insights,\nwe investigated whether LLMs accurately represent the spatial positions of\nanatomical landmarks. Through experiments with Llama-2 models, we found that\nthey can linearly represent anatomical landmarks in space with considerable\nrobustness to different prompts. These results underscore the potential of LLMs\nto enhance the efficiency and accuracy of medical imaging workflows.\n","authors":["Mohamad Abdi","Gerardo Hermosillo Valadez","Halid Ziya Yerebakan"],"pdf_url":"https://arxiv.org/pdf/2410.12686v2.pdf","comment":"6 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2410.13500v1","updated":"2024-10-17T12:46:26Z","published":"2024-10-17T12:46:26Z","title":"SAda-Net: A Self-Supervised Adaptive Stereo Estimation CNN For Remote\n  Sensing Image Data","summary":"  Stereo estimation has made many advancements in recent years with the\nintroduction of deep-learning. However the traditional supervised approach to\ndeep-learning requires the creation of accurate and plentiful ground-truth\ndata, which is expensive to create and not available in many situations. This\nis especially true for remote sensing applications, where there is an excess of\navailable data without proper ground truth. To tackle this problem, we propose\na self-supervised CNN with self-improving adaptive abilities. In the first\niteration, the created disparity map is inaccurate and noisy. Leveraging the\nleft-right consistency check, we get a sparse but more accurate disparity map\nwhich is used as an initial pseudo ground-truth. This pseudo ground-truth is\nthen adapted and updated after every epoch in the training step of the network.\nWe use the sum of inconsistent points in order to track the network\nconvergence. The code for our method is publicly available at:\nhttps://github.com/thedodo/SAda-Net}{https://github.com/thedodo/SAda-Net\n","authors":["Dominik Hirner","Friedrich Fraundorfer"],"pdf_url":"https://arxiv.org/pdf/2410.13500v1.pdf","comment":"Will be presented at ICPR2024 in December 2024 in Kolkata, India"},{"id":"http://arxiv.org/abs/2410.13486v1","updated":"2024-10-17T12:31:37Z","published":"2024-10-17T12:31:37Z","title":"SemSim: Revisiting Weak-to-Strong Consistency from a Semantic Similarity\n  Perspective for Semi-supervised Medical Image Segmentation","summary":"  Semi-supervised learning (SSL) for medical image segmentation is a\nchallenging yet highly practical task, which reduces reliance on large-scale\nlabeled dataset by leveraging unlabeled samples. Among SSL techniques, the\nweak-to-strong consistency framework, popularized by FixMatch, has emerged as a\nstate-of-the-art method in classification tasks. Notably, such a simple\npipeline has also shown competitive performance in medical image segmentation.\nHowever, two key limitations still persist, impeding its efficient adaptation:\n(1) the neglect of contextual dependencies results in inconsistent predictions\nfor similar semantic features, leading to incomplete object segmentation; (2)\nthe lack of exploitation of semantic similarity between labeled and unlabeled\ndata induces considerable class-distribution discrepancy. To address these\nlimitations, we propose a novel semi-supervised framework based on FixMatch,\nnamed SemSim, powered by two appealing designs from semantic similarity\nperspective: (1) rectifying pixel-wise prediction by reasoning about the\nintra-image pair-wise affinity map, thus integrating contextual dependencies\nexplicitly into the final prediction; (2) bridging labeled and unlabeled data\nvia a feature querying mechanism for compact class representation learning,\nwhich fully considers cross-image anatomical similarities. As the reliable\nsemantic similarity extraction depends on robust features, we further introduce\nan effective spatial-aware fusion module (SFM) to explore distinctive\ninformation from multiple scales. Extensive experiments show that SemSim yields\nconsistent improvements over the state-of-the-art methods across three public\nsegmentation benchmarks.\n","authors":["Shiao Xie","Hongyi Wang","Ziwei Niu","Hao Sun","Shuyi Ouyang","Yen-Wei Chen","Lanfen Lin"],"pdf_url":"https://arxiv.org/pdf/2410.13486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07961v3","updated":"2024-10-17T12:23:54Z","published":"2024-09-12T11:42:40Z","title":"Estimating Atmospheric Variables from Digital Typhoon Satellite Images\n  via Conditional Denoising Diffusion Models","summary":"  This study explores the application of diffusion models in the field of\ntyphoons, predicting multiple ERA5 meteorological variables simultaneously from\nDigital Typhoon satellite images. The focus of this study is taken to be\nTaiwan, an area very vulnerable to typhoons. By comparing the performance of\nConditional Denoising Diffusion Probability Model (CDDPM) with Convolutional\nNeural Networks (CNN) and Squeeze-and-Excitation Networks (SENet), results\nsuggest that the CDDPM performs best in generating accurate and realistic\nmeteorological data. Specifically, CDDPM achieved a PSNR of 32.807, which is\napproximately 7.9% higher than CNN and 5.5% higher than SENet. Furthermore,\nCDDPM recorded an RMSE of 0.032, showing a 11.1% improvement over CNN and 8.6%\nimprovement over SENet. A key application of this research can be for\nimputation purposes in missing meteorological datasets and generate additional\nhigh-quality meteorological data using satellite images. It is hoped that the\nresults of this analysis will enable more robust and detailed forecasting,\nreducing the impact of severe weather events on vulnerable regions. Code\naccessible at https://github.com/TammyLing/Typhoon-forecasting.\n","authors":["Zhangyue Ling","Pritthijit Nath","César Quilodrán-Casas"],"pdf_url":"https://arxiv.org/pdf/2409.07961v3.pdf","comment":"Accepted for spotlight presentation at the NeurIPS 2024 workshop on\n  Tackling Climate Change with Machine Learning. 8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.08845v3","updated":"2024-10-17T12:11:36Z","published":"2024-06-13T06:09:22Z","title":"Rethinking Human Evaluation Protocol for Text-to-Video Models: Enhancing\n  Reliability,Reproducibility, and Practicality","summary":"  Recent text-to-video (T2V) technology advancements, as demonstrated by models\nsuch as Gen2, Pika, and Sora, have significantly broadened its applicability\nand popularity. Despite these strides, evaluating these models poses\nsubstantial challenges. Primarily, due to the limitations inherent in automatic\nmetrics, manual evaluation is often considered a superior method for assessing\nT2V generation. However, existing manual evaluation protocols face\nreproducibility, reliability, and practicality issues. To address these\nchallenges, this paper introduces the Text-to-Video Human Evaluation (T2VHE)\nprotocol, a comprehensive and standardized protocol for T2V models. The T2VHE\nprotocol includes well-defined metrics, thorough annotator training, and an\neffective dynamic evaluation module. Experimental results demonstrate that this\nprotocol not only ensures high-quality annotations but can also reduce\nevaluation costs by nearly 50\\%. We will open-source the entire setup of the\nT2VHE protocol, including the complete protocol workflow, the dynamic\nevaluation component details, and the annotation interface code. This will help\ncommunities establish more sophisticated human assessment protocols.\n","authors":["Tianle Zhang","Langtian Ma","Yuchen Yan","Yuchen Zhang","Kai Wang","Yue Yang","Ziyao Guo","Wenqi Shao","Yang You","Yu Qiao","Ping Luo","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.08845v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13472v1","updated":"2024-10-17T12:02:29Z","published":"2024-10-17T12:02:29Z","title":"Day-Night Adaptation: An Innovative Source-free Adaptation Framework for\n  Medical Image Segmentation","summary":"  Distribution shifts widely exist in medical images acquired from different\nmedical centers, hindering the deployment of semantic segmentation models\ntrained on data from one center (source domain) to another (target domain).\nWhile unsupervised domain adaptation (UDA) has shown significant promise in\nmitigating these shifts, it poses privacy risks due to sharing data between\ncenters. To facilitate adaptation while preserving data privacy, source-free\ndomain adaptation (SFDA) and test-time adaptation (TTA) have emerged as\neffective paradigms, relying solely on target domain data. However, the\nscenarios currently addressed by SFDA and TTA are limited, making them less\nsuitable for clinical applications. In a more realistic clinical scenario, the\npre-trained model is deployed in a medical centre to assist with clinical tasks\nduring the day and rest at night. During the daytime process, TTA can be\nemployed to enhance inference performance. During the nighttime process, after\ncollecting the test data from the day, the model can be fine-tuned utilizing\nSFDA to further adapt to the target domain. With above insights, we propose a\nnovel adaptation framework called Day-Night Adaptation (DyNA). This framework\nadapts the model to the target domain through day-night loops without requiring\naccess to source data. Specifically, we implement distinct adaptation\nstrategies for daytime and nighttime to better meet the demands of clinical\nsettings. During the daytime, model parameters are frozen, and a specific\nlow-frequency prompt is trained for each test sample. Additionally, we\nconstruct a memory bank for prompt initialization and develop a warm-up\nmechanism to enhance prompt training. During nighttime, we integrate a global\nstudent model into the traditional teacher-student self-training paradigm to\nfine-tune the model while ensuring training stability...\n","authors":["Ziyang Chen","Yiwen Ye","Yongsheng Pan","Yong Xia"],"pdf_url":"https://arxiv.org/pdf/2410.13472v1.pdf","comment":"10 pages, 4 figures, 6 tables"},{"id":"http://arxiv.org/abs/2410.13471v1","updated":"2024-10-17T11:59:39Z","published":"2024-10-17T11:59:39Z","title":"SiamSeg: Self-Training with Contrastive Learning for Unsupervised Domain\n  Adaptation in Remote Sensing","summary":"  Semantic segmentation of remote sensing (RS) images is a challenging task\nwith significant potential across various applications. Deep learning,\nespecially supervised learning with large-scale labeled datasets, has greatly\nadvanced this field. However, acquiring high-quality labeled data is expensive\nand time-consuming. Moreover, variations in ground sampling distance (GSD),\nimaging equipment, and geographic diversity contribute to domain shifts between\ndatasets, which pose significant challenges to models trained solely on source\ndomain data, leading to poor cross-domain performance. Domain shift is\nwell-known for undermining a model's generalization ability in the target\ndomain. To address this, unsupervised domain adaptation (UDA) has emerged as a\npromising solution, enabling models to learn from unlabeled target domain data\nwhile training on labeled source domain data. Recent advancements, particularly\nin self-supervised learning via pseudo-label generation, have shown potential\nin mitigating domain discrepancies. Strategies combining source and target\ndomain images with their true and pseudo labels for self-supervised training\nhave been effective in addressing domain bias. Despite progress in computer\nvision, the application of pseudo-labeling methods to RS image segmentation\nremains underexplored.\n","authors":["Bin Wang","Fei Deng","Shuang Wang","Wen Luo","Zhixuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.13471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13465v1","updated":"2024-10-17T11:51:12Z","published":"2024-10-17T11:51:12Z","title":"Object Pose Estimation Using Implicit Representation For Transparent\n  Objects","summary":"  Object pose estimation is a prominent task in computer vision. The object\npose gives the orientation and translation of the object in real-world space,\nwhich allows various applications such as manipulation, augmented reality, etc.\nVarious objects exhibit different properties with light, such as reflections,\nabsorption, etc. This makes it challenging to understand the object's structure\nin RGB and depth channels. Recent research has been moving toward\nlearning-based methods, which provide a more flexible and generalizable\napproach to object pose estimation utilizing deep learning. One such approach\nis the render-and-compare method, which renders the object from multiple views\nand compares it against the given 2D image, which often requires an object\nrepresentation in the form of a CAD model. We reason that the synthetic texture\nof the CAD model may not be ideal for rendering and comparing operations. We\nshowed that if the object is represented as an implicit (neural) representation\nin the form of Neural Radiance Field (NeRF), it exhibits a more realistic\nrendering of the actual scene and retains the crucial spatial features, which\nmakes the comparison more versatile. We evaluated our NeRF implementation of\nthe render-and-compare method on transparent datasets and found that it\nsurpassed the current state-of-the-art results.\n","authors":["Varun Burde","Artem Moroz","Vit Zeman","Pavel Burget"],"pdf_url":"https://arxiv.org/pdf/2410.13465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09250v2","updated":"2024-10-17T11:46:45Z","published":"2024-06-13T15:55:04Z","title":"MirrorCheck: Efficient Adversarial Defense for Vision-Language Models","summary":"  Vision-Language Models (VLMs) are becoming increasingly vulnerable to\nadversarial attacks as various novel attack strategies are being proposed\nagainst these models. While existing defenses excel in unimodal contexts, they\ncurrently fall short in safeguarding VLMs against adversarial threats. To\nmitigate this vulnerability, we propose a novel, yet elegantly simple approach\nfor detecting adversarial samples in VLMs. Our method leverages Text-to-Image\n(T2I) models to generate images based on captions produced by target VLMs.\nSubsequently, we calculate the similarities of the embeddings of both input and\ngenerated images in the feature space to identify adversarial samples.\nEmpirical evaluations conducted on different datasets validate the efficacy of\nour approach, outperforming baseline methods adapted from image classification\ndomains. Furthermore, we extend our methodology to classification tasks,\nshowcasing its adaptability and model-agnostic nature. Theoretical analyses and\nempirical findings also show the resilience of our approach against adaptive\nattacks, positioning it as an excellent defense mechanism for real-world\ndeployment against adversarial threats.\n","authors":["Samar Fares","Klea Ziu","Toluwani Aremu","Nikita Durasov","Martin Takáč","Pascal Fua","Karthik Nandakumar","Ivan Laptev"],"pdf_url":"https://arxiv.org/pdf/2406.09250v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16807v2","updated":"2024-10-17T11:46:18Z","published":"2024-06-24T17:19:34Z","title":"Beyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback\n  for Text-to-Image Generation","summary":"  Human feedback plays a critical role in learning and refining reward models\nfor text-to-image generation, but the optimal form the feedback should take for\nlearning an accurate reward function has not been conclusively established.\nThis paper investigates the effectiveness of fine-grained feedback which\ncaptures nuanced distinctions in image quality and prompt-alignment, compared\nto traditional coarse-grained feedback (for example, thumbs up/down or ranking\nbetween a set of options). While fine-grained feedback holds promise,\nparticularly for systems catering to diverse societal preferences, we show that\ndemonstrating its superiority to coarse-grained feedback is not automatic.\nThrough experiments on real and synthetic preference data, we surface the\ncomplexities of building effective models due to the interplay of model choice,\nfeedback type, and the alignment between human judgment and computational\ninterpretation. We identify key challenges in eliciting and utilizing\nfine-grained feedback, prompting a reassessment of its assumed benefits and\npracticality. Our findings -- e.g., that fine-grained feedback can lead to\nworse models for a fixed budget, in some settings; however, in controlled\nsettings with known attributes, fine grained rewards can indeed be more helpful\n-- call for careful consideration of feedback attributes and potentially beckon\nnovel modeling approaches to appropriately unlock the potential value of\nfine-grained feedback in-the-wild.\n","authors":["Katherine M. Collins","Najoung Kim","Yonatan Bitton","Verena Rieser","Shayegan Omidshafiei","Yushi Hu","Sherol Chen","Senjuti Dutta","Minsuk Chang","Kimin Lee","Youwei Liang","Georgina Evans","Sahil Singla","Gang Li","Adrian Weller","Junfeng He","Deepak Ramachandran","Krishnamurthy Dj Dvijotham"],"pdf_url":"https://arxiv.org/pdf/2406.16807v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01522v3","updated":"2024-10-17T11:33:09Z","published":"2023-12-03T22:44:04Z","title":"G2D: From Global to Dense Radiography Representation Learning via\n  Vision-Language Pre-training","summary":"  Recently, medical vision-language pre-training (VLP) has reached substantial\nprogress to learn global visual representation from medical images and their\npaired radiology reports. However, medical imaging tasks in real world usually\nrequire finer granularity in visual features. These tasks include visual\nlocalization tasks (e.g., semantic segmentation, object detection) and visual\ngrounding task. Yet, current medical VLP methods face challenges in learning\nthese fine-grained features, as they primarily focus on brute-force alignment\nbetween image patches and individual text tokens for local visual feature\nlearning, which is suboptimal for downstream dense prediction tasks. In this\nwork, we propose a new VLP framework, named \\textbf{G}lobal to \\textbf{D}ense\nlevel representation learning (G2D) that achieves significantly improved\ngranularity and more accurate grounding for the learned features, compared to\nexisting medical VLP approaches. In particular, G2D learns dense and\nsemantically-grounded image representations via a pseudo segmentation task\nparallel with the global vision-language alignment. Notably, generating pseudo\nsegmentation targets does not incur extra trainable parameters: they are\nobtained on the fly during VLP with a parameter-free processor. G2D achieves\nsuperior performance across 6 medical imaging tasks and 25 diseases,\nparticularly in semantic segmentation, which necessitates fine-grained,\nsemantically-grounded image features. In this task, G2D surpasses peer models\neven when fine-tuned with just 1\\% of the training data, compared to the 100\\%\nused by these models. The code will be released upon acceptance.\n","authors":["Che Liu","Cheng Ouyang","Sibo Cheng","Anand Shah","Wenjia Bai","Rossella Arcucci"],"pdf_url":"https://arxiv.org/pdf/2312.01522v3.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2410.13453v1","updated":"2024-10-17T11:26:10Z","published":"2024-10-17T11:26:10Z","title":"Augmentation Policy Generation for Image Classification Using Large\n  Language Models","summary":"  Automated data augmentation methods have significantly improved the\nperformance and generalization capability of deep learning models in image\nclassification. Yet, most state-of-the-art methods are optimized on common\nbenchmark datasets, limiting their applicability to more diverse or\ndomain-specific data, such as medical datasets. In this paper, we propose a\nstrategy that uses large language models to automatically generate efficient\naugmentation policies, customized to fit the specific characteristics of any\ndataset and model architecture. The proposed method iteratively interacts with\nan LLM to obtain and refine the augmentation policies on model performance\nfeedback, creating a dataset-agnostic data augmentation pipeline. The proposed\nmethod was evaluated on medical imaging datasets, showing a clear improvement\nover state-of-the-art methods. The proposed approach offers an adaptive and\nscalable solution. Although it increases computational cost, it significantly\nboosts model robustness, automates the process, and minimizes the need for\nhuman involvement during model development.\n","authors":["Ant Duru","Alptekin Temizel"],"pdf_url":"https://arxiv.org/pdf/2410.13453v1.pdf","comment":"5 pages, 2 figures, 4 tables, submitted for consideration to the\n  International Workshop on Computational Intelligence for Multimedia\n  Understanding (IWCIM), ISCAS 2025"},{"id":"http://arxiv.org/abs/2410.09747v2","updated":"2024-10-17T11:14:37Z","published":"2024-10-13T06:53:58Z","title":"t-READi: Transformer-Powered Robust and Efficient Multimodal Inference\n  for Autonomous Driving","summary":"  Given the wide adoption of multimodal sensors (e.g., camera, lidar, radar) by\nautonomous vehicles (AVs), deep analytics to fuse their outputs for a robust\nperception become imperative. However, existing fusion methods often make two\nassumptions rarely holding in practice: i) similar data distributions for all\ninputs and ii) constant availability for all sensors. Because, for example,\nlidars have various resolutions and failures of radars may occur, such\nvariability often results in significant performance degradation in fusion. To\nthis end, we present tREADi, an adaptive inference system that accommodates the\nvariability of multimodal sensory data and thus enables robust and efficient\nperception. t-READi identifies variation-sensitive yet structure-specific model\nparameters; it then adapts only these parameters while keeping the rest intact.\nt-READi also leverages a cross-modality contrastive learning method to\ncompensate for the loss from missing modalities. Both functions are implemented\nto maintain compatibility with existing multimodal deep fusion methods. The\nextensive experiments evidently demonstrate that compared with the status quo\napproaches, t-READi not only improves the average inference accuracy by more\nthan 6% but also reduces the inference latency by almost 15x with the cost of\nonly 5% extra memory overhead in the worst case under realistic data and modal\nvariations.\n","authors":["Pengfei Hu","Yuhang Qian","Tianyue Zheng","Ang Li","Zhe Chen","Yue Gao","Xiuzhen Cheng","Jun Luo"],"pdf_url":"https://arxiv.org/pdf/2410.09747v2.pdf","comment":"14 pages, 16 figures"},{"id":"http://arxiv.org/abs/2410.13439v1","updated":"2024-10-17T11:12:55Z","published":"2024-10-17T11:12:55Z","title":"Similarity-Dissimilarity Loss with Supervised Contrastive Learning for\n  Multi-label Classification","summary":"  Supervised contrastive learning has been explored in making use of label\ninformation for multi-label classification, but determining positive samples in\nmulti-label scenario remains challenging. Previous studies have examined\nstrategies for identifying positive samples, considering label overlap\nproportion between anchors and samples. However, they ignore various relations\nbetween given anchors and samples, as well as how to dynamically adjust the\nweights in contrastive loss functions based on different relations, leading to\ngreat ambiguity. In this paper, we introduce five distinct relations between\nmulti-label samples and propose a Similarity-Dissimilarity Loss with\ncontrastive learning for multi-label classification. Our loss function\nre-weights the loss by computing the similarity and dissimilarity between\npositive samples and a given anchor based on the introduced relations. We\nmainly conduct experiments for multi-label text classification on MIMIC\ndatasets, then further extend the evaluation on MS-COCO. The Experimental\nresults show that our proposed loss effectively improves the performance on all\nencoders under supervised contrastive learning paradigm, demonstrating its\neffectiveness and robustness.\n","authors":["Guangming Huang","Yunfei Long","Cunjin Luo","Sheng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.13439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13437v1","updated":"2024-10-17T11:07:05Z","published":"2024-10-17T11:07:05Z","title":"Temporal-Enhanced Multimodal Transformer for Referring Multi-Object\n  Tracking and Segmentation","summary":"  Referring multi-object tracking (RMOT) is an emerging cross-modal task that\naims to locate an arbitrary number of target objects and maintain their\nidentities referred by a language expression in a video. This intricate task\ninvolves the reasoning of linguistic and visual modalities, along with the\ntemporal association of target objects. However, the seminal work employs only\nloose feature fusion and overlooks the utilization of long-term information on\ntracked objects. In this study, we introduce a compact Transformer-based\nmethod, termed TenRMOT. We conduct feature fusion at both encoding and decoding\nstages to fully exploit the advantages of Transformer architecture.\nSpecifically, we incrementally perform cross-modal fusion layer-by-layer during\nthe encoding phase. In the decoding phase, we utilize language-guided queries\nto probe memory features for accurate prediction of the desired objects.\nMoreover, we introduce a query update module that explicitly leverages temporal\nprior information of the tracked objects to enhance the consistency of their\ntrajectories. In addition, we introduce a novel task called Referring\nMulti-Object Tracking and Segmentation (RMOTS) and construct a new dataset\nnamed Ref-KITTI Segmentation. Our dataset consists of 18 videos with 818\nexpressions, and each expression averages 10.7 masks, which poses a greater\nchallenge compared to the typical single mask in most existing referring video\nsegmentation datasets. TenRMOT demonstrates superior performance on both the\nreferring multi-object tracking and the segmentation tasks.\n","authors":["Changcheng Xiao","Qiong Cao","Yujie Zhong","Xiang Zhang","Tao Wang","Canqun Yang","Long Lan"],"pdf_url":"https://arxiv.org/pdf/2410.13437v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14271v2","updated":"2024-10-17T11:06:26Z","published":"2024-05-23T07:48:19Z","title":"Fine-grained Image-to-LiDAR Contrastive Distillation with Visual\n  Foundation Models","summary":"  Contrastive image-to-LiDAR knowledge transfer, commonly used for learning 3D\nrepresentations with synchronized images and point clouds, often faces a\nself-conflict dilemma. This issue arises as contrastive losses unintentionally\ndissociate features of unmatched points and pixels that share semantic labels,\ncompromising the integrity of learned representations. To overcome this, we\nharness Visual Foundation Models (VFMs), which have revolutionized the\nacquisition of pixel-level semantics, to enhance 3D representation learning.\nSpecifically, we utilize off-the-shelf VFMs to generate semantic labels for\nweakly-supervised pixel-to-point contrastive distillation. Additionally, we\nemploy von Mises-Fisher distributions to structure the feature space, ensuring\nsemantic embeddings within the same class remain consistent across varying\ninputs. Furthermore, we adapt sampling probabilities of points to address\nimbalances in spatial distribution and category frequency, promoting\ncomprehensive and balanced learning. Extensive experiments demonstrate that our\napproach mitigates the challenges posed by traditional methods and consistently\nsurpasses existing image-to-LiDAR contrastive distillation methods in\ndownstream tasks. The source code is available at\n\\href{https://github.com/Eaphan/OLIVINE.}{\\color{black}https://github.com/Eaphan/OLIVINE}.\n","authors":["Yifan Zhang","Junhui Hou"],"pdf_url":"https://arxiv.org/pdf/2405.14271v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.13427v1","updated":"2024-10-17T10:51:08Z","published":"2024-10-17T10:51:08Z","title":"Unsupervised Skull Segmentation via Contrastive MR-to-CT Modality\n  Translation","summary":"  The skull segmentation from CT scans can be seen as an already solved\nproblem. However, in MR this task has a significantly greater complexity due to\nthe presence of soft tissues rather than bones. Capturing the bone structures\nfrom MR images of the head, where the main visualization objective is the\nbrain, is very demanding. The attempts that make use of skull stripping seem to\nnot be well suited for this task and fail to work in many cases. On the other\nhand, supervised approaches require costly and time-consuming skull\nannotations. To overcome the difficulties we propose a fully unsupervised\napproach, where we do not perform the segmentation directly on MR images, but\nwe rather perform a synthetic CT data generation via MR-to-CT translation and\nperform the segmentation there. We address many issues associated with\nunsupervised skull segmentation including the unpaired nature of MR and CT\ndatasets (contrastive learning), low resolution and poor quality\n(super-resolution), and generalization capabilities. The research has a\nsignificant value for downstream tasks requiring skull segmentation from MR\nvolumes such as craniectomy or surgery planning and can be seen as an important\nstep towards the utilization of synthetic data in medical imaging.\n","authors":["Kamil Kwarciak","Mateusz Daniol","Daria Hemmerling","Marek Wodzinski"],"pdf_url":"https://arxiv.org/pdf/2410.13427v1.pdf","comment":"16 pages, 5 figures, ACCV 2024 - GAISynMeD Workshop"},{"id":"http://arxiv.org/abs/2410.13421v1","updated":"2024-10-17T10:43:43Z","published":"2024-10-17T10:43:43Z","title":"Performance of Gaussian Mixture Model Classifiers on Embedded Feature\n  Spaces","summary":"  Data embeddings with CLIP and ImageBind provide powerful features for the\nanalysis of multimedia and/or multimodal data. We assess their performance here\nfor classification using a Gaussian Mixture models (GMMs) based layer as an\nalternative to the standard Softmax layer. GMMs based classifiers have recently\nbeen shown to have interesting performances as part of deep learning pipelines\ntrained end-to-end. Our first contribution is to investigate GMM based\nclassification performance taking advantage of the embedded spaces CLIP and\nImageBind. Our second contribution is in proposing our own GMM based classifier\nwith a lower parameters count than previously proposed. Our findings are, that\nin most cases, on these tested embedded spaces, one gaussian component in the\nGMMs is often enough for capturing each class, and we hypothesize that this may\nbe due to the contrastive loss used for training these embedded spaces that\nnaturally concentrates features together for each class. We also observed that\nImageBind often provides better performance than CLIP for classification of\nimage datasets even when these embedded spaces are compressed using PCA.\n","authors":["Jeremy Chopin","Rozenn Dahyot"],"pdf_url":"https://arxiv.org/pdf/2410.13421v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2308.02935v4","updated":"2024-10-17T09:53:57Z","published":"2023-08-05T18:32:49Z","title":"Bias Behind the Wheel: Fairness Testing of Autonomous Driving Systems","summary":"  This paper conducts fairness testing of automated pedestrian detection, a\ncrucial but under-explored issue in autonomous driving systems. We evaluate\neight state-of-the-art deep learning-based pedestrian detectors across\ndemographic groups on large-scale real-world datasets. To enable thorough\nfairness testing, we provide extensive annotations for the datasets, resulting\nin 8,311 images with 16,070 gender labels, 20,115 age labels, and 3,513 skin\ntone labels. Our findings reveal significant fairness issues, particularly\nrelated to age. The proportion of undetected children is 20.14% higher compared\nto adults. Furthermore, we explore how various driving scenarios affect the\nfairness of pedestrian detectors. We find that pedestrian detectors demonstrate\nsignificant gender biases during night time, potentially exacerbating the\nprevalent societal issue of female safety concerns during nighttime out.\nMoreover, we observe that pedestrian detectors can demonstrate both enhanced\nfairness and superior performance under specific driving conditions, which\nchallenges the fairness-performance trade-off theory widely acknowledged in the\nfairness literature. We publicly release the code, data, and results to support\nfuture research on fairness in autonomous driving.\n","authors":["Xinyue Li","Zhenpeng Chen","Jie M. Zhang","Federica Sarro","Ying Zhang","Xuanzhe Liu"],"pdf_url":"https://arxiv.org/pdf/2308.02935v4.pdf","comment":"Accepted by ACM Transactions on Software Engineering and Methodology\n  (TOSEM)"},{"id":"http://arxiv.org/abs/2410.13384v1","updated":"2024-10-17T09:36:52Z","published":"2024-10-17T09:36:52Z","title":"RescueADI: Adaptive Disaster Interpretation in Remote Sensing Images\n  with Autonomous Agents","summary":"  Current methods for disaster scene interpretation in remote sensing images\n(RSIs) mostly focus on isolated tasks such as segmentation, detection, or\nvisual question-answering (VQA). However, current interpretation methods often\nfail at tasks that require the combination of multiple perception methods and\nspecialized tools. To fill this gap, this paper introduces Adaptive Disaster\nInterpretation (ADI), a novel task designed to solve requests by planning and\nexecuting multiple sequentially correlative interpretation tasks to provide a\ncomprehensive analysis of disaster scenes. To facilitate research and\napplication in this area, we present a new dataset named RescueADI, which\ncontains high-resolution RSIs with annotations for three connected aspects:\nplanning, perception, and recognition. The dataset includes 4,044 RSIs, 16,949\nsemantic masks, 14,483 object bounding boxes, and 13,424 interpretation\nrequests across nine challenging request types. Moreover, we propose a new\ndisaster interpretation method employing autonomous agents driven by large\nlanguage models (LLMs) for task planning and execution, proving its efficacy in\nhandling complex disaster interpretations. The proposed agent-based method\nsolves various complex interpretation requests such as counting, area\ncalculation, and path-finding without human intervention, which traditional\nsingle-task approaches cannot handle effectively. Experimental results on\nRescueADI demonstrate the feasibility of the proposed task and show that our\nmethod achieves an accuracy 9% higher than existing VQA methods, highlighting\nits advantages over conventional disaster interpretation approaches. The\ndataset will be publicly available.\n","authors":["Zhuoran Liu","Danpei Zhao","Bo Yuan"],"pdf_url":"https://arxiv.org/pdf/2410.13384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13383v1","updated":"2024-10-17T09:36:19Z","published":"2024-10-17T09:36:19Z","title":"Railway LiDAR semantic segmentation based on intelligent semi-automated\n  data annotation","summary":"  Automated vehicles rely on an accurate and robust perception of the\nenvironment. Similarly to automated cars, highly automated trains require an\nenvironmental perception. Although there is a lot of research based on either\ncamera or LiDAR sensors in the automotive domain, very few contributions for\nthis task exist yet for automated trains. Additionally, no public dataset or\ndescribed approach for a 3D LiDAR semantic segmentation in the railway\nenvironment exists yet. Thus, we propose an approach for a point-wise 3D\nsemantic segmentation based on the 2DPass network architecture using scans and\nimages jointly. In addition, we present a semi-automated intelligent data\nannotation approach, which we use to efficiently and accurately label the\nrequired dataset recorded on a railway track in Germany. To improve performance\ndespite a still small number of labeled scans, we apply an active learning\napproach to intelligently select scans for the training dataset. Our\ncontributions are threefold: We annotate rail data including camera and LiDAR\ndata from the railway environment, transfer label the raw LiDAR point clouds\nusing an image segmentation network, and train a state-of-the-art 3D LiDAR\nsemantic segmentation network efficiently leveraging active learning. The\ntrained network achieves good segmentation results with a mean IoU of 71.48% of\n9 classes.\n","authors":["Florian Wulff","Bernd Schaeufele","Julian Pfeifer","Ilja Radusch"],"pdf_url":"https://arxiv.org/pdf/2410.13383v1.pdf","comment":"This article has been accepted for publication in the IEEE VTC Fall\n  2024"},{"id":"http://arxiv.org/abs/2410.13371v1","updated":"2024-10-17T09:23:30Z","published":"2024-10-17T09:23:30Z","title":"Accurate Checkerboard Corner Detection under Defoucs","summary":"  Camera calibration is a critical process in 3D vision, im pacting\napplications in autonomous driving, robotics, ar chitecture, and so on. This\npaper focuses on enhancing feature extraction for chessboard corner detection,\na key step in calibration. We analyze existing methods, high lighting their\nlimitations and propose a novel sub-pixel refinement approach based on\nsymmetry, which signifi cantly improves accuracy for visible light cameras. Un\nlike prior symmetry based method that assume a contin uous physical pattern,\nour approach accounts for abrupt changes in visible light camera images and\ndefocus ef fects. We introduce a simplified objective function that reduces\ncomputation time and mitigates overfitting risks. Furthermore, we derive an\nexplicit expression for the pixel value of a blurred edge, providing insights\ninto the relationship between pixel value and center intensity. Our method\ndemonstrates superior performance, achiev ing substantial accuracy improvements\nover existing tech niques, particularly in the context of visible light cam era\ncalibration. Our code is available from https:\n//github.com/spdfghi/Accurate-Checkerboard Corner-Detection-under-Defoucs.git.\n","authors":["Zezhun Shi"],"pdf_url":"https://arxiv.org/pdf/2410.13371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13370v1","updated":"2024-10-17T09:22:53Z","published":"2024-10-17T09:22:53Z","title":"MagicTailor: Component-Controllable Personalization in Text-to-Image\n  Diffusion Models","summary":"  Recent advancements in text-to-image (T2I) diffusion models have enabled the\ncreation of high-quality images from text prompts, but they still struggle to\ngenerate images with precise control over specific visual concepts. Existing\napproaches can replicate a given concept by learning from reference images, yet\nthey lack the flexibility for fine-grained customization of the individual\ncomponent within the concept. In this paper, we introduce\ncomponent-controllable personalization, a novel task that pushes the boundaries\nof T2I models by allowing users to reconfigure specific components when\npersonalizing visual concepts. This task is particularly challenging due to two\nprimary obstacles: semantic pollution, where unwanted visual elements corrupt\nthe personalized concept, and semantic imbalance, which causes disproportionate\nlearning of the concept and component. To overcome these challenges, we design\nMagicTailor, an innovative framework that leverages Dynamic Masked Degradation\n(DM-Deg) to dynamically perturb undesired visual semantics and Dual-Stream\nBalancing (DS-Bal) to establish a balanced learning paradigm for desired visual\nsemantics. Extensive comparisons, ablations, and analyses demonstrate that\nMagicTailor not only excels in this challenging task but also holds significant\npromise for practical applications, paving the way for more nuanced and\ncreative image generation.\n","authors":["Donghao Zhou","Jiancheng Huang","Jinbin Bai","Jiaze Wang","Hao Chen","Guangyong Chen","Xiaowei Hu","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2410.13370v1.pdf","comment":"Project page: https://correr-zhou.github.io/MagicTailor"},{"id":"http://arxiv.org/abs/2410.13360v1","updated":"2024-10-17T09:10:26Z","published":"2024-10-17T09:10:26Z","title":"Remember, Retrieve and Generate: Understanding Infinite Visual Concepts\n  as Your Personalized Assistant","summary":"  The development of large language models (LLMs) has significantly enhanced\nthe capabilities of multimodal LLMs (MLLMs) as general assistants. However,\nlack of user-specific knowledge still restricts their application in human's\ndaily life. In this paper, we introduce the Retrieval Augmented Personalization\n(RAP) framework for MLLMs' personalization. Starting from a general MLLM, we\nturn it into a personalized assistant in three steps. (a) Remember: We design a\nkey-value database to store user-related information, e.g., user's name, avatar\nand other attributes. (b) Retrieve: When the user initiates a conversation, RAP\nwill retrieve relevant information from the database using a multimodal\nretriever. (c) Generate: The input query and retrieved concepts' information\nare fed into MLLMs to generate personalized, knowledge-augmented responses.\nUnlike previous methods, RAP allows real-time concept editing via updating the\nexternal database. To further improve generation quality and alignment with\nuser-specific information, we design a pipeline for data collection and create\na specialized dataset for personalized training of MLLMs. Based on the dataset,\nwe train a series of MLLMs as personalized multimodal assistants. By\npretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual\nconcepts without additional finetuning. Our models demonstrate outstanding\nflexibility and generation quality across a variety of tasks, such as\npersonalized image captioning, question answering and visual recognition. The\ncode, data and models are available at https://github.com/Hoar012/RAP-MLLM.\n","authors":["Haoran Hao","Jiaming Han","Changsheng Li","Yu-Feng Li","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2410.13360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13355v1","updated":"2024-10-17T09:05:15Z","published":"2024-10-17T09:05:15Z","title":"Self-Supervised Scene Flow Estimation with Point-Voxel Fusion and\n  Surface Representation","summary":"  Scene flow estimation aims to generate the 3D motion field of points between\ntwo consecutive frames of point clouds, which has wide applications in various\nfields. Existing point-based methods ignore the irregularity of point clouds\nand have difficulty capturing long-range dependencies due to the inefficiency\nof point-level computation. Voxel-based methods suffer from the loss of detail\ninformation. In this paper, we propose a point-voxel fusion method, where we\nutilize a voxel branch based on sparse grid attention and the shifted window\nstrategy to capture long-range dependencies and a point branch to capture\nfine-grained features to compensate for the information loss in the voxel\nbranch. In addition, since xyz coordinates are difficult to describe the\ngeometric structure of complex 3D objects in the scene, we explicitly encode\nthe local surface information of the point cloud through the umbrella surface\nfeature extraction (USFE) module. We verify the effectiveness of our method by\nconducting experiments on the Flyingthings3D and KITTI datasets. Our method\noutperforms all other self-supervised methods and achieves highly competitive\nresults compared to fully supervised methods. We achieve improvements in all\nmetrics, especially EPE, which is reduced by 8.51% and 10.52% on the KITTIo and\nKITTIs datasets, respectively.\n","authors":["Xuezhi Xiang","Xi Wang","Lei Zhang","Denis Ombati","Himaloy Himu","Xiantong Zhen"],"pdf_url":"https://arxiv.org/pdf/2410.13355v1.pdf","comment":"The paper is under consideration at 2025 IEEE International\n  Conference on Acoustics, Speech, and Signal Processing (ICASSP 2025)"},{"id":"http://arxiv.org/abs/2410.13349v1","updated":"2024-10-17T09:00:29Z","published":"2024-10-17T09:00:29Z","title":"GlossyGS: Inverse Rendering of Glossy Objects with 3D Gaussian Splatting","summary":"  Reconstructing objects from posed images is a crucial and complex task in\ncomputer graphics and computer vision. While NeRF-based neural reconstruction\nmethods have exhibited impressive reconstruction ability, they tend to be\ntime-comsuming. Recent strategies have adopted 3D Gaussian Splatting (3D-GS)\nfor inverse rendering, which have led to quick and effective outcomes. However,\nthese techniques generally have difficulty in producing believable geometries\nand materials for glossy objects, a challenge that stems from the inherent\nambiguities of inverse rendering. To address this, we introduce GlossyGS, an\ninnovative 3D-GS-based inverse rendering framework that aims to precisely\nreconstruct the geometry and materials of glossy objects by integrating\nmaterial priors. The key idea is the use of micro-facet geometry segmentation\nprior, which helps to reduce the intrinsic ambiguities and improve the\ndecomposition of geometries and materials. Additionally, we introduce a normal\nmap prefiltering strategy to more accurately simulate the normal distribution\nof reflective surfaces. These strategies are integrated into a hybrid geometry\nand material representation that employs both explicit and implicit methods to\ndepict glossy objects. We demonstrate through quantitative analysis and\nqualitative visualization that the proposed method is effective to reconstruct\nhigh-fidelity geometries and materials of glossy objects, and performs\nfavorably against state-of-the-arts.\n","authors":["Shuichang Lai","Letian Huang","Jie Guo","Kai Cheng","Bowen Pan","Xiaoxiao Long","Jiangjing Lyu","Chengfei Lv","Yanwen Guo"],"pdf_url":"https://arxiv.org/pdf/2410.13349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11553v4","updated":"2024-10-17T08:58:47Z","published":"2024-08-21T12:04:32Z","title":"AnyDesign: Versatile Area Fashion Editing via Mask-Free Diffusion","summary":"  Fashion image editing aims to modify a person's appearance based on a given\ninstruction. Existing methods require auxiliary tools like segmenters and\nkeypoint extractors, lacking a flexible and unified framework. Moreover, these\nmethods are limited in the variety of clothing types they can handle, as most\ndatasets focus on people in clean backgrounds and only include generic garments\nsuch as tops, pants, and dresses. These limitations restrict their\napplicability in real-world scenarios. In this paper, we first extend an\nexisting dataset for human generation to include a wider range of apparel and\nmore complex backgrounds. This extended dataset features people wearing diverse\nitems such as tops, pants, dresses, skirts, headwear, scarves, shoes, socks,\nand bags. Additionally, we propose AnyDesign, a diffusion-based method that\nenables mask-free editing on versatile areas. Users can simply input a human\nimage along with a corresponding prompt in either text or image format. Our\napproach incorporates Fashion DiT, equipped with a Fashion-Guidance Attention\n(FGA) module designed to fuse explicit apparel types and CLIP-encoded apparel\nfeatures. Both Qualitative and quantitative experiments demonstrate that our\nmethod delivers high-quality fashion editing and outperforms contemporary\ntext-guided fashion editing methods.\n","authors":["Yunfang Niu","Lingxiang Wu","Dong Yi","Jie Peng","Ning Jiang","Haiying Wu","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2408.11553v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02278v3","updated":"2024-10-17T08:57:50Z","published":"2023-04-05T07:50:16Z","title":"SCMM: Calibrating Cross-modal Representations for Text-Based Person\n  Search","summary":"  Text-Based Person Search (TBPS) is a crucial task that enables accurate\nretrieval of target individuals from large-scale galleries with only given\ntextual caption. For cross-modal TBPS tasks, it is critical to obtain\nwell-distributed representation in the common embedding space to reduce the\ninter-modal gap. Furthermore, learning detailed image-text correspondences is\nessential to discriminate similar targets and enable fine-grained search. To\naddress these challenges, we present a simple yet effective method named Sew\nCalibration and Masked Modeling (SCMM) that calibrates cross-modal\nrepresentations by learning compact and well-aligned embeddings. SCMM is\ndistinguished by two novel losses to provide fine-grained cross-modal\nrepresentations: 1) a Sew calibration loss that takes the quality of textual\ncaptions as guidance and aligns features between image and text modalities, and\n2) a Masked Caption Modeling (MCM) loss that leverages a masked caption\nprediction task to establish detailed and generic relationships between textual\nand visual parts. The dual-pronged strategy refines feature alignment and\nenriches cross-modal correspondences, enabling the accurate distinction of\nsimilar individuals. Consequently, its streamlined dual-encoder architecture\navoids complex branches and interactions and facilitates high-speed inference\nsuitable for real-time requirements. Consequently, high-speed inference is\nachieved, which is essential for resource-limited applications often demanding\nreal-time processing. Extensive experiments on three popular TBPS benchmarks\ndemonstrate the superiority of SCMM, achieving top results with 73.81%, 74.25%,\nand 57.35% Rank-1 accuracy on CUHK-PEDES, ICFG-PEDES, and RSTPReID,\nrespectively. We hope SCMM's scalable and cost-effective design will serve as a\nstrong baseline and facilitate future research in this field.\n","authors":["Jing Liu","Donglai Wei","Yang Liu","Sipeng Zhang","Tong Yang","Victor C. M. Leung"],"pdf_url":"https://arxiv.org/pdf/2304.02278v3.pdf","comment":"This version of manuscript is under IEEE TMM review"},{"id":"http://arxiv.org/abs/2410.03320v2","updated":"2024-10-17T08:43:29Z","published":"2024-10-04T11:14:31Z","title":"Lost in Tracking: Uncertainty-guided Cardiac Cine MRI Segmentation at\n  Right Ventricle Base","summary":"  Accurate biventricular segmentation of cardiac magnetic resonance (CMR) cine\nimages is essential for the clinical evaluation of heart function. However,\ncompared to left ventricle (LV), right ventricle (RV) segmentation is still\nmore challenging and less reproducible. Degenerate performance frequently\noccurs at the RV base, where the in-plane anatomical structures are complex\n(with atria, valve, and aorta) and vary due to the strong interplanar motion.\nIn this work, we propose to address the currently unsolved issues in CMR\nsegmentation, specifically at the RV base, with two strategies: first, we\ncomplemented the public resource by reannotating the RV base in the ACDC\ndataset, with refined delineation of the right ventricle outflow tract (RVOT),\nunder the guidance of an expert cardiologist. Second, we proposed a novel dual\nencoder U-Net architecture that leverages temporal incoherence to inform the\nsegmentation when interplanar motions occur. The inter-planar motion is\ncharacterized by loss-of-tracking, via Bayesian uncertainty of a\nmotion-tracking model. Our experiments showed that our method significantly\nimproved RV base segmentation taking into account temporal incoherence.\nFurthermore, we investigated the reproducibility of deep learning-based\nsegmentation and showed that the combination of consistent annotation and loss\nof tracking could enhance the reproducibility of RV segmentation, potentially\nfacilitating a large number of clinical studies focusing on RV.\n","authors":["Yidong Zhao","Yi Zhang","Orlando Simonetti","Yuchi Han","Qian Tao"],"pdf_url":"https://arxiv.org/pdf/2410.03320v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11781v2","updated":"2024-10-17T08:28:50Z","published":"2024-07-16T14:38:13Z","title":"Sliding Gaussian ball adaptive growth (SlingBAG): point cloud-based\n  iterative algorithm for large-scale 3D photoacoustic imaging","summary":"  Large-scale photoacoustic (PA) 3D imaging has become increasingly important\nfor both clinical and pre-clinical applications. Limited by resource and\napplication constrains, only sparsely-distributed transducer arrays can be\napplied, which necessitates advanced image reconstruction algorithms to\novercome artifacts caused by using back-projection algorithm. However, high\ncomputing memory consumption of traditional iterative algorithms for\nlarge-scale 3D cases is practically unacceptable. Here, we propose a point\ncloud-based iterative algorithm that reduces memory consumption by several\norders, wherein a 3D photoacoustic scene is modeled as a series of\nGaussian-distributed spherical sources. During the iterative reconstruction\nprocess, the properties of each Gaussian source, including peak intensities,\nstandard deviations and means are stored in form of point cloud, then\ncontinuously optimized and adaptively undergoing destroying, splitting, and\nduplication along the gradient direction, thus manifesting the sliding ball\nadaptive growth effect. This method, named the sliding Gaussian ball adaptive\ngrowth (SlingBAG) algorithm, enables high-quality 3D large-scale PA\nreconstruction with fast iteration and extremely less memory usage. We\nvalidated SlingBAG algorithm in both simulation study and in vivo animal\nexperiments.\n","authors":["Shuang Li","Yibing Wang","Jian Gao","Chulhong Kim","Seongwook Choi","Yu Zhang","Qian Chen","Yao Yao","Changhui Li"],"pdf_url":"https://arxiv.org/pdf/2407.11781v2.pdf","comment":"Added SlingBAG reconstruction of rat kidney and rat liver results;\n  updated methods; added references"},{"id":"http://arxiv.org/abs/2308.14409v2","updated":"2024-10-17T08:25:06Z","published":"2023-08-28T08:47:06Z","title":"Steerable Conditional Diffusion for Out-of-Distribution Adaptation in\n  Medical Image Reconstruction","summary":"  Denoising diffusion models have emerged as the go-to generative framework for\nsolving inverse problems in imaging. A critical concern regarding these models\nis their performance on out-of-distribution tasks, which remains an\nunder-explored challenge. Using a diffusion model on an out-of-distribution\ndataset, realistic reconstructions can be generated, but with hallucinating\nimage features that are uniquely present in the training dataset. To address\nthis discrepancy during train-test time and improve reconstruction accuracy, we\nintroduce a novel sampling framework called Steerable Conditional Diffusion.\nSpecifically, this framework adapts the diffusion model, concurrently with\nimage reconstruction, based solely on the information provided by the available\nmeasurement. Utilising our proposed method, we achieve substantial enhancements\nin out-of-distribution performance across diverse imaging modalities, advancing\nthe robust deployment of denoising diffusion models in real-world applications.\n","authors":["Riccardo Barbano","Alexander Denker","Hyungjin Chung","Tae Hoon Roh","Simon Arridge","Peter Maass","Bangti Jin","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2308.14409v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13321v1","updated":"2024-10-17T08:24:27Z","published":"2024-10-17T08:24:27Z","title":"Mitigating Hallucinations in Large Vision-Language Models via\n  Summary-Guided Decoding","summary":"  Large Vision-Language Models (LVLMs) demonstrate impressive capabilities in\ngenerating detailed and coherent responses from visual inputs. However, they\nare prone to generate hallucinations due to an over-reliance on language\npriors. To address this issue, we investigate the language priors in LVLMs and\nmake two key observations: (1) Even when predicting the tokens associated with\nimage-related part-of-speech (POS), models increasingly rely on linguistic\npriors as the token sequences grow, thereby amplifying hallucinations. (2)\nMethods that directly calibrate LVLM's output distribution to mitigate language\npriors can lead to a degradation in text quality or even exacerbate\nhallucinations. Based on these findings, we propose a novel method,\nSummary-Guided Decoding (SGD). This method naturally encourages the model to\nfocus more on image information by reducing the text context through summaries,\nwhile controlling only the image-related POS tokens to maintain text quality.\nThrough experiments, we demonstrate that SGD achieves state-of-the-art\nperformance on object hallucination benchmarks. Furthermore, in terms of the\ntrade-off between precision and recall, SGD achieves Pareto optimality among\nthe existing methods. Lastly, we observe that although existing methods\nstruggle to balance the reduction of object hallucinations with maintaining\ntext quality, SGD demonstrates robustness in handling this challenge.\n","authors":["Kyungmin Min","Minbeom Kim","Kang-il Lee","Dongryeol Lee","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2410.13321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13320v1","updated":"2024-10-17T08:23:09Z","published":"2024-10-17T08:23:09Z","title":"Inadequate contrast ratio of road markings as an indicator for ADAS\n  failure","summary":"  Road markings were reported as critical road safety features, equally needed\nfor both human drivers and for machine vision technologies utilised by advanced\ndriver assistance systems (ADAS) and in driving automation. Visibility of road\nmarkings is achieved because of their colour contrasting with the roadway\nsurface. During recent testing of an open-source camera-based ADAS under\nseveral visibility conditions (day, night, rain, glare), significant failures\nin trajectory planning were recorded and quantified. Consistently, better ADAS\nreliability under poor visibility conditions was achieved with Type II road\nmarkings (i.e. structured markings, facilitating moisture drainage) as compared\nto Type I road marking (i.e. flat lines). To further understand these failures,\nanalysis of contrast ratio of road markings, which the tested ADAS was\ndetecting for traffic lane recognition, was performed. The highest contrast\nratio (greater than 0.5, calculated per Michelson equation) was measured at\nnight in the absence of confounding factors, with statistically significant\ndifference of 0.1 in favour of Type II road markings over Type I. Under\ndaylight conditions, contrast ratio was reduced, with slightly higher values\nmeasured with Type I. The presence of rain or wet roads caused the\ndeterioration of the contrast ratio, with Type II road markings exhibiting\nsignificantly higher contrast ratio than Type I, even though the values were\nlow (less than 0.1). These findings matched the output of the ADAS related to\ntraffic lane detection and underlined the importance of road marking\nvisibility. Inadequate lane recognition by ADAS was associated with very low\ncontrast ratio of road markings indeed. Importantly, specific minimum contrast\nratio value could not be found, which was due to the complexity of ADAS\nalgorithms...\n","authors":["Novel Certad","Cristina Olaverri-Monreal","Friedrich Wiesinger","Tomasz E. Burghardt"],"pdf_url":"https://arxiv.org/pdf/2410.13320v1.pdf","comment":"IRF World Congress 2024"},{"id":"http://arxiv.org/abs/2410.10929v2","updated":"2024-10-17T08:10:53Z","published":"2024-10-14T16:35:27Z","title":"ASTM :Autonomous Smart Traffic Management System Using Artificial\n  Intelligence CNN and LSTM","summary":"  In the modern world, the development of Artificial Intelligence (AI) has\ncontributed to improvements in various areas, including automation, computer\nvision, fraud detection, and more. AI can be leveraged to enhance the\nefficiency of Autonomous Smart Traffic Management (ASTM) systems and reduce\ntraffic congestion rates. This paper presents an Autonomous Smart Traffic\nManagement (STM) system that uses AI to improve traffic flow rates. The system\nemploys the YOLO V5 Convolutional Neural Network to detect vehicles in traffic\nmanagement images. Additionally, it predicts the number of vehicles for the\nnext 12 hours using a Recurrent Neural Network with Long Short-Term Memory\n(RNN-LSTM). The Smart Traffic Management Cycle Length Analysis manages the\ntraffic cycle length based on these vehicle predictions, aided by AI. From the\nresults of the RNN-LSTM model for predicting vehicle numbers over the next 12\nhours, we observe that the model predicts traffic with a Mean Squared Error\n(MSE) of 4.521 vehicles and a Root Mean Squared Error (RMSE) of 2.232 vehicles.\nAfter simulating the STM system in the CARLA simulation environment, we found\nthat the Traffic Management Congestion Flow Rate with ASTM (21 vehicles per\nminute) is 50\\% higher than the rate without STM (around 15 vehicles per\nminute). Additionally, the Traffic Management Vehicle Pass Delay with STM (5\nseconds per vehicle) is 70\\% lower than without STM (around 12 seconds per\nvehicle). These results demonstrate that the STM system using AI can increase\ntraffic flow by 50\\% and reduce vehicle pass delays by 70\\%.\n","authors":["Christofel Rio Goenawan"],"pdf_url":"https://arxiv.org/pdf/2410.10929v2.pdf","comment":"In process to IEEE Intelligent Vehicle Symposium 2025"},{"id":"http://arxiv.org/abs/2410.13314v1","updated":"2024-10-17T08:10:41Z","published":"2024-10-17T08:10:41Z","title":"Precipitation Nowcasting Using Diffusion Transformer with Causal\n  Attention","summary":"  Short-term precipitation forecasting remains challenging due to the\ndifficulty in capturing long-term spatiotemporal dependencies. Current deep\nlearning methods fall short in establishing effective dependencies between\nconditions and forecast results, while also lacking interpretability. To\naddress this issue, we propose a Precipitation Nowcasting Using Diffusion\nTransformer with Causal Attention model. Our model leverages Transformer and\ncombines causal attention mechanisms to establish spatiotemporal queries\nbetween conditional information (causes) and forecast results (results). This\ndesign enables the model to effectively capture long-term dependencies,\nallowing forecast results to maintain strong causal relationships with input\nconditions over a wide range of time and space. We explore four variants of\nspatiotemporal information interactions for DTCA, demonstrating that global\nspatiotemporal labeling interactions yield the best performance. In addition,\nwe introduce a Channel-To-Batch shift operation to further enhance the model's\nability to represent complex rainfall dynamics. We conducted experiments on two\ndatasets. Compared to state-of-the-art U-Net-based methods, our approach\nimproved the CSI (Critical Success Index) for predicting heavy precipitation by\napproximately 15% and 8% respectively, achieving state-of-the-art performance.\n","authors":["ChaoRong Li","XuDong Ling","YiLan Xue","Wenjie Luo","LiHong Zhu","FengQing Qin","Yaodong Zhou","Yuanyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2410.13314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02097v3","updated":"2024-10-17T08:09:37Z","published":"2024-09-03T17:54:39Z","title":"LinFusion: 1 GPU, 1 Minute, 16K Image","summary":"  Modern diffusion models, particularly those utilizing a Transformer-based\nUNet for denoising, rely heavily on self-attention operations to manage complex\nspatial relationships, thus achieving impressive generation performance.\nHowever, this existing paradigm faces significant challenges in generating\nhigh-resolution visual content due to its quadratic time and memory complexity\nwith respect to the number of spatial tokens. To address this limitation, we\naim at a novel linear attention mechanism as an alternative in this paper.\nSpecifically, we begin our exploration from recently introduced models with\nlinear complexity, e.g., Mamba2, RWKV6, Gated Linear Attention, etc, and\nidentify two key features--attention normalization and non-causal\ninference--that enhance high-resolution visual generation performance. Building\non these insights, we introduce a generalized linear attention paradigm, which\nserves as a low-rank approximation of a wide spectrum of popular linear token\nmixers. To save the training cost and better leverage pre-trained models, we\ninitialize our models and distill the knowledge from pre-trained\nStableDiffusion (SD). We find that the distilled model, termed LinFusion,\nachieves performance on par with or superior to the original SD after only\nmodest training, while significantly reducing time and memory complexity.\nExtensive experiments on SD-v1.5, SD-v2.1, and SD-XL demonstrate that LinFusion\nenables satisfactory and efficient zero-shot cross-resolution generation,\naccommodating ultra-resolution images like 16K on a single GPU. Moreover, it is\nhighly compatible with pre-trained SD components and pipelines, such as\nControlNet, IP-Adapter, DemoFusion, DistriFusion, etc, requiring no adaptation\nefforts. Codes are available at https://github.com/Huage001/LinFusion.\n","authors":["Songhua Liu","Weihao Yu","Zhenxiong Tan","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2409.02097v3.pdf","comment":"Work in Progress. Codes are available at\n  https://github.com/Huage001/LinFusion"},{"id":"http://arxiv.org/abs/2410.13311v1","updated":"2024-10-17T08:09:28Z","published":"2024-10-17T08:09:28Z","title":"Enhancing Dataset Distillation via Label Inconsistency Elimination and\n  Learning Pattern Refinement","summary":"  Dataset Distillation (DD) seeks to create a condensed dataset that, when used\nto train a model, enables the model to achieve performance similar to that of a\nmodel trained on the entire original dataset. It relieves the model training\nfrom processing massive data and thus reduces the computation resources,\nstorage, and time costs. This paper illustrates our solution that ranks 1st in\nthe ECCV-2024 Data Distillation Challenge (track 1). Our solution, Modified\nDifficulty-Aligned Trajectory Matching (M-DATM), introduces two key\nmodifications to the original state-of-the-art method DATM: (1) the soft labels\nlearned by DATM do not achieve one-to-one correspondence with the counterparts\ngenerated by the official evaluation script, so we remove the soft labels\ntechnique to alleviate such inconsistency; (2) since the removal of soft labels\nmakes it harder for the synthetic dataset to learn late trajectory information,\nparticularly on Tiny ImageNet, we reduce the matching range, allowing the\nsynthetic data to concentrate more on the easier patterns. In the final\nevaluation, our M-DATM achieved accuracies of 0.4061 and 0.1831 on the\nCIFAR-100 and Tiny ImageNet datasets, ranking 1st in the Fixed Images Per Class\n(IPC) Track.\n","authors":["Chuhao Zhou","Chenxi Jiang","Yi Xie","Haozhi Cao","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2410.13311v1.pdf","comment":"ECCV 2024 Dataset Distillation Challenge"},{"id":"http://arxiv.org/abs/2410.13305v1","updated":"2024-10-17T08:05:02Z","published":"2024-10-17T08:05:02Z","title":"Reference-Based Post-OCR Processing with LLM for Diacritic Languages","summary":"  Extracting fine-grained OCR text from aged documents in diacritic languages\nremains challenging due to unexpected artifacts, time-induced degradation, and\nlack of datasets. While standalone spell correction approaches have been\nproposed, they show limited performance for historical documents due to\nnumerous possible OCR error combinations and differences between modern and\nclassical corpus distributions. We propose a method utilizing available\ncontent-focused ebooks as a reference base to correct imperfect OCR-generated\ntext, supported by large language models. This technique generates\nhigh-precision pseudo-page-to-page labels for diacritic languages, where small\nstrokes pose significant challenges in historical conditions. The pipeline\neliminates various types of noise from aged documents and addresses issues such\nas missing characters, words, and disordered sequences. Our post-processing\nmethod, which generated a large OCR dataset of classical Vietnamese books,\nachieved a mean grading score of 8.72 on a 10-point scale. This outperformed\nthe state-of-the-art transformer-based Vietnamese spell correction model, which\nscored 7.03 when evaluated on a sampled subset of the dataset. We also trained\na baseline OCR model to assess and compare it with well-known engines.\nExperimental results demonstrate the strength of our baseline model compared to\nwidely used open-source solutions. The resulting dataset will be released\npublicly to support future studies.\n","authors":["Thao Do"],"pdf_url":"https://arxiv.org/pdf/2410.13305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12489v2","updated":"2024-10-17T08:03:34Z","published":"2024-10-16T12:09:38Z","title":"Synthetic Augmentation for Anatomical Landmark Localization using DDPMs","summary":"  Deep learning techniques for anatomical landmark localization (ALL) have\nshown great success, but their reliance on large annotated datasets remains a\nproblem due to the tedious and costly nature of medical data acquisition and\nannotation. While traditional data augmentation, variational autoencoders\n(VAEs), and generative adversarial networks (GANs) have already been used to\nsynthetically expand medical datasets, diffusion-based generative models have\nrecently started to gain attention for their ability to generate high-quality\nsynthetic images. In this study, we explore the use of denoising diffusion\nprobabilistic models (DDPMs) for generating medical images and their\ncorresponding heatmaps of landmarks to enhance the training of a supervised\ndeep learning model for ALL. Our novel approach involves a DDPM with a\n2-channel input, incorporating both the original medical image and its heatmap\nof annotated landmarks. We also propose a novel way to assess the quality of\nthe generated images using a Markov Random Field (MRF) model for landmark\nmatching and a Statistical Shape Model (SSM) to check landmark plausibility,\nbefore we evaluate the DDPM-augmented dataset in the context of an ALL task\ninvolving hand X-Rays.\n","authors":["Arnela Hadzic","Lea Bogensperger","Simon Johannes Joham","Martin Urschler"],"pdf_url":"https://arxiv.org/pdf/2410.12489v2.pdf","comment":"Accepted for the SASHIMI workshop of MICCAI 2024"},{"id":"http://arxiv.org/abs/2312.05349v2","updated":"2024-10-17T07:53:51Z","published":"2023-12-08T20:12:26Z","title":"PixLore: A Dataset-driven Approach to Rich Image Captioning","summary":"  In the domain of vision-language integration, generating detailed image\ncaptions poses a significant challenge due to the lack of curated and rich\ndatasets. This study introduces PixLore, a novel method that leverages Querying\nTransformers through the fine-tuning of the BLIP-2 model using the LoRa method\non a standard commercial GPU. The followed approach, which involves training on\na carefully assembled dataset from state-of-the-art Computer Vision models\ncombined and augmented by ChatGPT, addresses the question of whether intricate\nimage understanding can be achieved with an ensemble of smaller-scale models,\nreferred to as Knowledge Stitching. Comparative evaluations against major\nmodels such as GPT-4 and Google Bard demonstrate that PixLore-2.7B, despite\nhaving considerably fewer parameters, is rated higher than the existing\nState-of-the-Art models in over half of the assessments. Precisely, PixLore\noutperform Bard and BLIP-2, which score approximately 35.18% and 27.98% lower\nthan PixLore in the task of image captioning. This research not only presents a\ngroundbreaking approach but also highlights the importance of well-curated\ndatasets in enhancing the performance of smaller models.\n","authors":["Diego Bonilla"],"pdf_url":"https://arxiv.org/pdf/2312.05349v2.pdf","comment":"Paper in preprint pending of publication"},{"id":"http://arxiv.org/abs/2410.13295v1","updated":"2024-10-17T07:49:23Z","published":"2024-10-17T07:49:23Z","title":"PiLocNet: Physics-informed neural network on 3D localization with\n  rotating point spread function","summary":"  For the 3D localization problem using point spread function (PSF)\nengineering, we propose a novel enhancement of our previously introduced\nlocalization neural network, LocNet. The improved network is a physics-informed\nneural network (PINN) that we call PiLocNet. Previous works on the localization\nproblem may be categorized separately into model-based optimization and neural\nnetwork approaches. Our PiLocNet combines the unique strengths of both\napproaches by incorporating forward-model-based information into the network\nvia a data-fitting loss term that constrains the neural network to yield\nresults that are physically sensible. We additionally incorporate certain\nregularization terms from the variational method, which further improves the\nrobustness of the network in the presence of image noise, as we show for the\nPoisson and Gaussian noise models. This framework accords interpretability to\nthe neural network, and the results we obtain show its superiority. Although\nthe paper focuses on the use of single-lobe rotating PSF to encode the full 3D\nsource location, we expect the method to be widely applicable to other PSFs and\nimaging problems that are constrained by known forward processes.\n","authors":["Mingda Lu","Zitian Ao","Chao Wang","Sudhakar Prasad","Raymond H. Chan"],"pdf_url":"https://arxiv.org/pdf/2410.13295v1.pdf","comment":"25 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.13294v1","updated":"2024-10-17T07:47:41Z","published":"2024-10-17T07:47:41Z","title":"LESS: Label-Efficient and Single-Stage Referring 3D Segmentation","summary":"  Referring 3D Segmentation is a visual-language task that segments all points\nof the specified object from a 3D point cloud described by a sentence of query.\nPrevious works perform a two-stage paradigm, first conducting language-agnostic\ninstance segmentation then matching with given text query. However, the\nsemantic concepts from text query and visual cues are separately interacted\nduring the training, and both instance and semantic labels for each object are\nrequired, which is time consuming and human-labor intensive. To mitigate these\nissues, we propose a novel Referring 3D Segmentation pipeline, Label-Efficient\nand Single-Stage, dubbed LESS, which is only under the supervision of efficient\nbinary mask. Specifically, we design a Point-Word Cross-Modal Alignment module\nfor aligning the fine-grained features of points and textual embedding. Query\nMask Predictor module and Query-Sentence Alignment module are introduced for\ncoarse-grained alignment between masks and query. Furthermore, we propose an\narea regularization loss, which coarsely reduces irrelevant background\npredictions on a large scale. Besides, a point-to-point contrastive loss is\nproposed concentrating on distinguishing points with subtly similar features.\nThrough extensive experiments, we achieve state-of-the-art performance on\nScanRefer dataset by surpassing the previous methods about 3.7% mIoU using only\nbinary labels.\n","authors":["Xuexun Liu","Xiaoxu Xu","Jinlong Li","Qiudan Zhang","Xu Wang","Nicu Sebe","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2410.13294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02240v3","updated":"2024-10-17T07:46:59Z","published":"2024-10-03T06:25:53Z","title":"SCA: Highly Efficient Semantic-Consistent Unrestricted Adversarial\n  Attack","summary":"  Deep neural network based systems deployed in sensitive environments are\nvulnerable to adversarial attacks. Unrestricted adversarial attacks typically\nmanipulate the semantic content of an image (e.g., color or texture) to create\nadversarial examples that are both effective and photorealistic. Recent works\nhave utilized the diffusion inversion process to map images into a latent\nspace, where high-level semantics are manipulated by introducing perturbations.\nHowever, they often results in substantial semantic distortions in the denoised\noutput and suffers from low efficiency. In this study, we propose a novel\nframework called Semantic-Consistent Unrestricted Adversarial Attacks (SCA),\nwhich employs an inversion method to extract edit-friendly noise maps and\nutilizes Multimodal Large Language Model (MLLM) to provide semantic guidance\nthroughout the process. Under the condition of rich semantic information\nprovided by MLLM, we perform the DDPM denoising process of each step using a\nseries of edit-friendly noise maps, and leverage DPM Solver++ to accelerate\nthis process, enabling efficient sampling with semantic consistency. Compared\nto existing methods, our framework enables the efficient generation of\nadversarial examples that exhibit minimal discernible semantic changes.\nConsequently, we for the first time introduce Semantic-Consistent Adversarial\nExamples (SCAE). Extensive experiments and visualizations have demonstrated the\nhigh efficiency of SCA, particularly in being on average 12 times faster than\nthe state-of-the-art attacks. Our research can further draw attention to the\nsecurity of multimedia information.\n","authors":["Zihao Pan","Weibin Wu","Yuhang Cao","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.02240v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13285v1","updated":"2024-10-17T07:30:20Z","published":"2024-10-17T07:30:20Z","title":"Composing Novel Classes: A Concept-Driven Approach to Generalized\n  Category Discovery","summary":"  We tackle the generalized category discovery (GCD) problem, which aims to\ndiscover novel classes in unlabeled datasets by leveraging the knowledge of\nknown classes. Previous works utilize the known class knowledge through shared\nrepresentation spaces. Despite their progress, our analysis experiments show\nthat novel classes can achieve impressive clustering results on the feature\nspace of a known class pre-trained model, suggesting that existing methods may\nnot fully utilize known class knowledge. To address it, we introduce a novel\nconcept learning framework for GCD, named ConceptGCD, that categorizes concepts\ninto two types: derivable and underivable from known class concepts, and adopts\na stage-wise learning strategy to learn them separately. Specifically, our\nframework first extracts known class concepts by a known class pre-trained\nmodel and then produces derivable concepts from them by a generator layer with\na covariance-augmented loss. Subsequently, we expand the generator layer to\nlearn underivable concepts in a balanced manner ensured by a concept score\nnormalization strategy and integrate a contrastive loss to preserve previously\nlearned concepts. Extensive experiments on various benchmark datasets\ndemonstrate the superiority of our approach over the previous state-of-the-art\nmethods. Code will be available soon.\n","authors":["Chuyu Zhang","Peiyan Gu","Xueyang Yu","Xuming He"],"pdf_url":"https://arxiv.org/pdf/2410.13285v1.pdf","comment":"Underreview. The first two authors contribute equally"},{"id":"http://arxiv.org/abs/2404.06666v3","updated":"2024-10-17T07:28:23Z","published":"2024-04-10T00:26:08Z","title":"SafeGen: Mitigating Sexually Explicit Content Generation in\n  Text-to-Image Models","summary":"  Text-to-image (T2I) models, such as Stable Diffusion, have exhibited\nremarkable performance in generating high-quality images from text descriptions\nin recent years. However, text-to-image models may be tricked into generating\nnot-safe-for-work (NSFW) content, particularly in sexually explicit scenarios.\nExisting countermeasures mostly focus on filtering inappropriate inputs and\noutputs, or suppressing improper text embeddings, which can block sexually\nexplicit content (e.g., naked) but may still be vulnerable to adversarial\nprompts -- inputs that appear innocent but are ill-intended. In this paper, we\npresent SafeGen, a framework to mitigate sexual content generation by\ntext-to-image models in a text-agnostic manner. The key idea is to eliminate\nexplicit visual representations from the model regardless of the text input. In\nthis way, the text-to-image model is resistant to adversarial prompts since\nsuch unsafe visual representations are obstructed from within. Extensive\nexperiments conducted on four datasets and large-scale user studies demonstrate\nSafeGen's effectiveness in mitigating sexually explicit content generation\nwhile preserving the high-fidelity of benign images. SafeGen outperforms eight\nstate-of-the-art baseline methods and achieves 99.4% sexual content removal\nperformance. Furthermore, our constructed benchmark of adversarial prompts\nprovides a basis for future development and evaluation of anti-NSFW-generation\nmethods.\n","authors":["Xinfeng Li","Yuchen Yang","Jiangyi Deng","Chen Yan","Yanjiao Chen","Xiaoyu Ji","Wenyuan Xu"],"pdf_url":"https://arxiv.org/pdf/2404.06666v3.pdf","comment":"Accepted by ACM CCS 2024. Please cite this paper as \"Xinfeng Li,\n  Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu Ji, Wenyuan Xu.\n  SafeGen: Mitigating Sexually Explicit Content Generation in Text-to-Image\n  Models. In Proceedings of ACM Conference on Computer and Communications\n  Security (CCS), 2024.\""},{"id":"http://arxiv.org/abs/2410.12158v2","updated":"2024-10-17T07:15:32Z","published":"2024-10-16T01:38:59Z","title":"SAM-Guided Masked Token Prediction for 3D Scene Understanding","summary":"  Foundation models have significantly enhanced 2D task performance, and recent\nworks like Bridge3D have successfully applied these models to improve 3D scene\nunderstanding through knowledge distillation, marking considerable\nadvancements. Nonetheless, challenges such as the misalignment between 2D and\n3D representations and the persistent long-tail distribution in 3D datasets\nstill restrict the effectiveness of knowledge distillation from 2D to 3D using\nfoundation models. To tackle these issues, we introduce a novel SAM-guided\ntokenization method that seamlessly aligns 3D transformer structures with\nregion-level knowledge distillation, replacing the traditional KNN-based\ntokenization techniques. Additionally, we implement a group-balanced\nre-weighting strategy to effectively address the long-tail problem in knowledge\ndistillation. Furthermore, inspired by the recent success of masked feature\nprediction, our framework incorporates a two-stage masked token prediction\nprocess in which the student model predicts both the global embeddings and the\ntoken-wise local embeddings derived from the teacher models trained in the\nfirst stage. Our methodology has been validated across multiple datasets,\nincluding SUN RGB-D, ScanNet, and S3DIS, for tasks like 3D object detection and\nsemantic segmentation. The results demonstrate significant improvements over\ncurrent State-of-the-art self-supervised methods, establishing new benchmarks\nin this field.\n","authors":["Zhimin Chen","Liang Yang","Yingwei Li","Longlong Jing","Bing Li"],"pdf_url":"https://arxiv.org/pdf/2410.12158v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.06704v2","updated":"2024-10-17T07:14:12Z","published":"2024-09-10T17:59:55Z","title":"GeoCalib: Learning Single-image Calibration with Geometric Optimization","summary":"  From a single image, visual cues can help deduce intrinsic and extrinsic\ncamera parameters like the focal length and the gravity direction. This\nsingle-image calibration can benefit various downstream applications like image\nediting and 3D mapping. Current approaches to this problem are based on either\nclassical geometry with lines and vanishing points or on deep neural networks\ntrained end-to-end. The learned approaches are more robust but struggle to\ngeneralize to new environments and are less accurate than their classical\ncounterparts. We hypothesize that they lack the constraints that 3D geometry\nprovides. In this work, we introduce GeoCalib, a deep neural network that\nleverages universal rules of 3D geometry through an optimization process.\nGeoCalib is trained end-to-end to estimate camera parameters and learns to find\nuseful visual cues from the data. Experiments on various benchmarks show that\nGeoCalib is more robust and more accurate than existing classical and learned\napproaches. Its internal optimization estimates uncertainties, which help flag\nfailure cases and benefit downstream applications like visual localization. The\ncode and trained models are publicly available at\nhttps://github.com/cvg/GeoCalib.\n","authors":["Alexander Veicht","Paul-Edouard Sarlin","Philipp Lindenberger","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2409.06704v2.pdf","comment":"Presented at ECCV 2024"},{"id":"http://arxiv.org/abs/2410.13280v1","updated":"2024-10-17T07:13:00Z","published":"2024-10-17T07:13:00Z","title":"Hybrid bundle-adjusting 3D Gaussians for view consistent rendering with\n  pose optimization","summary":"  Novel view synthesis has made significant progress in the field of 3D\ncomputer vision. However, the rendering of view-consistent novel views from\nimperfect camera poses remains challenging. In this paper, we introduce a\nhybrid bundle-adjusting 3D Gaussians model that enables view-consistent\nrendering with pose optimization. This model jointly extract image-based and\nneural 3D representations to simultaneously generate view-consistent images and\ncamera poses within forward-facing scenes. The effective of our model is\ndemonstrated through extensive experiments conducted on both real and synthetic\ndatasets. These experiments clearly illustrate that our model can effectively\noptimize neural scene representations while simultaneously resolving\nsignificant camera pose misalignments. The source code is available at\nhttps://github.com/Bistu3DV/hybridBA.\n","authors":["Yanan Guo","Ying Xie","Ying Chang","Benkui Zhang","Bo Jia","Lin Cao"],"pdf_url":"https://arxiv.org/pdf/2410.13280v1.pdf","comment":"Photonics Asia 2024"},{"id":"http://arxiv.org/abs/2403.17898v2","updated":"2024-10-17T07:11:31Z","published":"2024-03-26T17:39:36Z","title":"Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D\n  Gaussians","summary":"  The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering\nfidelity and efficiency compared to NeRF-based neural scene representations.\nWhile demonstrating the potential for real-time rendering, 3D-GS encounters\nrendering bottlenecks in large scenes with complex details due to an excessive\nnumber of Gaussian primitives located within the viewing frustum. This\nlimitation is particularly noticeable in zoom-out views and can lead to\ninconsistent rendering speeds in scenes with varying details. Moreover, it\noften struggles to capture the corresponding level of details at different\nscales with its heuristic density control operation. Inspired by the\nLevel-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an\nLOD-structured 3D Gaussian approach supporting level-of-detail decomposition\nfor scene representation that contributes to the final rendering results. Our\nmodel dynamically selects the appropriate level from the set of\nmulti-resolution anchor points, ensuring consistent rendering performance with\nadaptive LOD adjustments while maintaining high-fidelity rendering results.\n","authors":["Kerui Ren","Lihan Jiang","Tao Lu","Mulin Yu","Linning Xu","Zhangkai Ni","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2403.17898v2.pdf","comment":"Project page: https://city-super.github.io/octree-gs/"},{"id":"http://arxiv.org/abs/2405.20141v3","updated":"2024-10-17T06:54:04Z","published":"2024-05-30T15:16:06Z","title":"OpenDAS: Open-Vocabulary Domain Adaptation for Segmentation","summary":"  Recently, Vision-Language Models (VLMs) have advanced segmentation techniques\nby shifting from the traditional segmentation of a closed-set of predefined\nobject classes to open-vocabulary segmentation (OVS), allowing users to segment\nnovel classes and concepts unseen during training of the segmentation model.\nHowever, this flexibility comes with a trade-off: fully-supervised closed-set\nmethods still outperform OVS methods on base classes, that is on classes on\nwhich they have been explicitly trained. This is due to the lack of\npixel-aligned training masks for VLMs (which are trained on image-caption\npairs), and the absence of domain-specific knowledge, such as autonomous\ndriving. Therefore, we propose the task of open-vocabulary domain adaptation to\ninfuse domain-specific knowledge into VLMs while preserving their\nopen-vocabulary nature. By doing so, we achieve improved performance in base\nand novel classes. Existing VLM adaptation methods improve performance on base\n(training) queries, but fail to fully preserve the open-set capabilities of\nVLMs on novel queries. To address this shortcoming, we combine\nparameter-efficient prompt tuning with a triplet-loss-based training strategy\nthat uses auxiliary negative queries. Notably, our approach is the only\nparameter-efficient method that consistently surpasses the original VLM on\nnovel classes. Our adapted VLMs can seamlessly be integrated into existing OVS\npipelines, e.g., improving OVSeg by +6.0% mIoU on ADE20K for open-vocabulary 2D\nsegmentation, and OpenMask3D by +4.1% AP on ScanNet++ Offices for\nopen-vocabulary 3D instance segmentation without other changes.\n","authors":["Gonca Yilmaz","Songyou Peng","Marc Pollefeys","Francis Engelmann","Hermann Blum"],"pdf_url":"https://arxiv.org/pdf/2405.20141v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13271v1","updated":"2024-10-17T06:51:10Z","published":"2024-10-17T06:51:10Z","title":"Inductive Gradient Adjustment For Spectral Bias In Implicit Neural\n  Representations","summary":"  Implicit Neural Representations (INRs), as a versatile representation\nparadigm, have achieved success in various computer vision tasks. Due to the\nspectral bias of the vanilla multi-layer perceptrons (MLPs), existing methods\nfocus on designing MLPs with sophisticated architectures or repurposing\ntraining techniques for highly accurate INRs. In this paper, we delve into the\nlinear dynamics model of MLPs and theoretically identify the empirical Neural\nTangent Kernel (eNTK) matrix as a reliable link between spectral bias and\ntraining dynamics. Based on eNTK matrix, we propose a practical inductive\ngradient adjustment method, which could purposefully improve the spectral bias\nvia inductive generalization of eNTK-based gradient transformation matrix. We\nevaluate our method on different INRs tasks with various INR architectures and\ncompare to existing training techniques. The superior representation\nperformance clearly validates the advantage of our proposed method. Armed with\nour gradient adjustment method, better INRs with more enhanced texture details\nand sharpened edges can be learned from data by tailored improvements on\nspectral bias.\n","authors":["Kexuan Shi","Hai Chen","Leheng Zhang","Shuhang Gu"],"pdf_url":"https://arxiv.org/pdf/2410.13271v1.pdf","comment":"28 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.13242v1","updated":"2024-10-17T05:53:13Z","published":"2024-10-17T05:53:13Z","title":"Fundus to Fluorescein Angiography Video Generation as a Retinal\n  Generative Foundation Model","summary":"  Fundus fluorescein angiography (FFA) is crucial for diagnosing and monitoring\nretinal vascular issues but is limited by its invasive nature and restricted\naccessibility compared to color fundus (CF) imaging. Existing methods that\nconvert CF images to FFA are confined to static image generation, missing the\ndynamic lesional changes. We introduce Fundus2Video, an autoregressive\ngenerative adversarial network (GAN) model that generates dynamic FFA videos\nfrom single CF images. Fundus2Video excels in video generation, achieving an\nFVD of 1497.12 and a PSNR of 11.77. Clinical experts have validated the\nfidelity of the generated videos. Additionally, the model's generator\ndemonstrates remarkable downstream transferability across ten external public\ndatasets, including blood vessel segmentation, retinal disease diagnosis,\nsystemic disease prediction, and multimodal retrieval, showcasing impressive\nzero-shot and few-shot capabilities. These findings position Fundus2Video as a\npowerful, non-invasive alternative to FFA exams and a versatile retinal\ngenerative foundation model that captures both static and temporal retinal\nfeatures, enabling the representation of complex inter-modality relationships.\n","authors":["Weiyi Zhang","Jiancheng Yang","Ruoyu Chen","Siyu Huang","Pusheng Xu","Xiaolan Chen","Shanfu Lu","Hongyu Cao","Mingguang He","Danli Shi"],"pdf_url":"https://arxiv.org/pdf/2410.13242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19554v3","updated":"2024-10-17T05:34:55Z","published":"2024-09-29T04:43:10Z","title":"Tri-Cam: Practical Eye Gaze Tracking via Camera Network","summary":"  As human eyes serve as conduits of rich information, unveiling emotions,\nintentions, and even aspects of an individual's health and overall well-being,\ngaze tracking also enables various human-computer interaction applications, as\nwell as insights in psychological and medical research. However, existing gaze\ntracking solutions fall short at handling free user movement, and also require\nlaborious user effort in system calibration. We introduce Tri-Cam, a practical\ndeep learning-based gaze tracking system using three affordable RGB webcams. It\nfeatures a split network structure for efficient training, as well as\ndesignated network designs to handle the separated gaze tracking tasks. Tri-Cam\nis also equipped with an implicit calibration module, which makes use of mouse\nclick opportunities to reduce calibration overhead on the user's end. We\nevaluate Tri-Cam against Tobii, the state-of-the-art commercial eye tracker,\nachieving comparable accuracy, while supporting a wider free movement area. In\nconclusion, Tri-Cam provides a user-friendly, affordable, and robust gaze\ntracking solution that could practically enable various applications.\n","authors":["Sikai Yang","Wan Du"],"pdf_url":"https://arxiv.org/pdf/2409.19554v3.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2410.13227v1","updated":"2024-10-17T05:27:44Z","published":"2024-10-17T05:27:44Z","title":"Latent Image and Video Resolution Prediction using Convolutional Neural\n  Networks","summary":"  This paper introduces a Video Quality Assessment (VQA) problem that has\nreceived little attention in the literature, called the latent resolution\nprediction problem. The problem arises when images or videos are upscaled from\ntheir native resolution and are reported as having a higher resolution than\ntheir native resolution. This paper formulates the problem, constructs a\ndataset for training and evaluation, and introduces several machine learning\nalgorithms, including two Convolutional Neural Networks (CNNs), to address this\nproblem. Experiments indicate that some proposed methods can predict the latent\nvideo resolution with about 95% accuracy.\n","authors":["Rittwika Kansabanik","Adrian Barbu"],"pdf_url":"https://arxiv.org/pdf/2410.13227v1.pdf","comment":"Submitted in ICIP conference"},{"id":"http://arxiv.org/abs/2409.19454v3","updated":"2024-10-17T05:23:57Z","published":"2024-09-28T20:40:18Z","title":"See Where You Read with Eye Gaze Tracking and Large Language Model","summary":"  Losing track of reading progress during line switching can be frustrating.\nEye gaze tracking technology offers a potential solution by highlighting read\nparagraphs, aiding users in avoiding wrong line switches. However, the gap\nbetween gaze tracking accuracy (2-3 cm) and text line spacing (3-5 mm) makes\ndirect application impractical. Existing methods leverage the linear reading\npattern but fail during jump reading. This paper presents a reading tracking\nand highlighting system that supports both linear and jump reading. Based on\nexperimental insights from the gaze nature study of 16 users, two gaze error\nmodels are designed to enable both jump reading detection and relocation. The\nsystem further leverages the large language model's contextual perception\ncapability in aiding reading tracking. A reading tracking domain-specific\nline-gaze alignment opportunity is also exploited to enable dynamic and\nfrequent calibration of the gaze results. Controlled experiments demonstrate\nreliable linear reading tracking, as well as 84% accuracy in tracking jump\nreading. Furthermore, real field tests with 18 volunteers demonstrated the\nsystem's effectiveness in tracking and highlighting read paragraphs, improving\nreading efficiency, and enhancing user experience.\n","authors":["Sikai Yang","Gang Yan","Wan Du"],"pdf_url":"https://arxiv.org/pdf/2409.19454v3.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2410.13195v1","updated":"2024-10-17T03:48:02Z","published":"2024-10-17T03:48:02Z","title":"UniG: Modelling Unitary 3D Gaussians for View-consistent 3D\n  Reconstruction","summary":"  In this work, we present UniG, a view-consistent 3D reconstruction and novel\nview synthesis model that generates a high-fidelity representation of 3D\nGaussians from sparse images. Existing 3D Gaussians-based methods usually\nregress Gaussians per-pixel of each view, create 3D Gaussians per view\nseparately, and merge them through point concatenation. Such a view-independent\nreconstruction approach often results in a view inconsistency issue, where the\npredicted positions of the same 3D point from different views may have\ndiscrepancies. To address this problem, we develop a DETR (DEtection\nTRansformer)-like framework, which treats 3D Gaussians as decoder queries and\nupdates their parameters layer by layer by performing multi-view\ncross-attention (MVDFA) over multiple input images. In this way, multiple views\nnaturally contribute to modeling a unitary representation of 3D Gaussians,\nthereby making 3D reconstruction more view-consistent. Moreover, as the number\nof 3D Gaussians used as decoder queries is irrespective of the number of input\nviews, allow an arbitrary number of input images without causing memory\nexplosion. Extensive experiments validate the advantages of our approach,\nshowcasing superior performance over existing methods quantitatively (improving\nPSNR by 4.2 dB when trained on Objaverse and tested on the GSO benchmark) and\nqualitatively.\n","authors":["Jiamin Wu","Kenkun Liu","Yukai Shi","Xiaoke Jiang","Yuan Yao","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.13195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13193v1","updated":"2024-10-17T03:42:06Z","published":"2024-10-17T03:42:06Z","title":"Golyadkin's Torment: Doppelgängers and Adversarial Vulnerability","summary":"  Many machine learning (ML) classifiers are claimed to outperform humans, but\nthey still make mistakes that humans do not. The most notorious examples of\nsuch mistakes are adversarial visual metamers. This paper aims to define and\ninvestigate the phenomenon of adversarial Doppelgangers (AD), which includes\nadversarial visual metamers, and to compare the performance and robustness of\nML classifiers to human performance.\n  We find that AD are inputs that are close to each other with respect to a\nperceptual metric defined in this paper. AD are qualitatively different from\nthe usual adversarial examples. The vast majority of classifiers are vulnerable\nto AD and robustness-accuracy trade-offs may not improve them. Some\nclassification problems may not admit any AD robust classifiers because the\nunderlying classes are ambiguous. We provide criteria that can be used to\ndetermine whether a classification problem is well defined or not; describe the\nstructure and attributes of an AD-robust classifier; introduce and explore the\nnotions of conceptual entropy and regions of conceptual ambiguity for\nclassifiers that are vulnerable to AD attacks, along with methods to bound the\nAD fooling rate of an attack. We define the notion of classifiers that exhibit\nhypersensitive behavior, that is, classifiers whose only mistakes are\nadversarial Doppelgangers. Improving the AD robustness of hyper-sensitive\nclassifiers is equivalent to improving accuracy. We identify conditions\nguaranteeing that all classifiers with sufficiently high accuracy are\nhyper-sensitive.\n  Our findings are aimed at significant improvements in the reliability and\nsecurity of machine learning systems.\n","authors":["George I. Kamberov"],"pdf_url":"https://arxiv.org/pdf/2410.13193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12424v5","updated":"2024-10-17T03:39:59Z","published":"2024-02-19T16:34:50Z","title":"Tables as Texts or Images: Evaluating the Table Reasoning Ability of\n  LLMs and MLLMs","summary":"  In this paper, we investigate the effectiveness of various LLMs in\ninterpreting tabular data through different prompting strategies and data\nformats. Our analyses extend across six benchmarks for table-related tasks such\nas question-answering and fact-checking. We introduce for the first time the\nassessment of LLMs' performance on image-based table representations.\nSpecifically, we compare five text-based and three image-based table\nrepresentations, demonstrating the role of representation and prompting on LLM\nperformance. Our study provides insights into the effective use of LLMs on\ntable-related tasks.\n","authors":["Naihao Deng","Zhenjie Sun","Ruiqi He","Aman Sikka","Yulong Chen","Lin Ma","Yue Zhang","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2402.12424v5.pdf","comment":"Accepted to ACL 2024 Findings; Naihao and Zhenjie contributed equally\n  to the project; Data available at:\n  https://github.com/dnaihao/Tables-as-Texts-or-Images"},{"id":"http://arxiv.org/abs/2309.14122v3","updated":"2024-10-17T03:34:21Z","published":"2023-09-25T13:20:15Z","title":"SurrogatePrompt: Bypassing the Safety Filter of Text-to-Image Models via\n  Substitution","summary":"  Advanced text-to-image models such as DALL$\\cdot$E 2 and Midjourney possess\nthe capacity to generate highly realistic images, raising significant concerns\nregarding the potential proliferation of unsafe content. This includes adult,\nviolent, or deceptive imagery of political figures. Despite claims of rigorous\nsafety mechanisms implemented in these models to restrict the generation of\nnot-safe-for-work (NSFW) content, we successfully devise and exhibit the first\nprompt attacks on Midjourney, resulting in the production of abundant\nphotorealistic NSFW images. We reveal the fundamental principles of such prompt\nattacks and suggest strategically substituting high-risk sections within a\nsuspect prompt to evade closed-source safety measures. Our novel framework,\nSurrogatePrompt, systematically generates attack prompts, utilizing large\nlanguage models, image-to-text, and image-to-image modules to automate attack\nprompt creation at scale. Evaluation results disclose an 88% success rate in\nbypassing Midjourney's proprietary safety filter with our attack prompts,\nleading to the generation of counterfeit images depicting political figures in\nviolent scenarios. Both subjective and objective assessments validate that the\nimages generated from our attack prompts present considerable safety hazards.\n","authors":["Zhongjie Ba","Jieming Zhong","Jiachen Lei","Peng Cheng","Qinglong Wang","Zhan Qin","Zhibo Wang","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2309.14122v3.pdf","comment":"To appear in the the 31st ACM Conference on Computer and\n  Communications Security (CCS)"},{"id":"http://arxiv.org/abs/2406.18572v2","updated":"2024-10-17T03:25:47Z","published":"2024-06-03T18:08:56Z","title":"GeoReasoner: Geo-localization with Reasoning in Street Views using a\n  Large Vision-Language Model","summary":"  This work tackles the problem of geo-localization with a new paradigm using a\nlarge vision-language model (LVLM) augmented with human inference knowledge. A\nprimary challenge here is the scarcity of data for training the LVLM - existing\nstreet-view datasets often contain numerous low-quality images lacking visual\nclues, and lack any reasoning inference. To address the data-quality issue, we\ndevise a CLIP-based network to quantify the degree of street-view images being\nlocatable, leading to the creation of a new dataset comprising highly locatable\nstreet views. To enhance reasoning inference, we integrate external knowledge\nobtained from real geo-localization games, tapping into valuable human\ninference capabilities. The data are utilized to train GeoReasoner, which\nundergoes fine-tuning through dedicated reasoning and location-tuning stages.\nQualitative and quantitative evaluations illustrate that GeoReasoner\noutperforms counterpart LVLMs by more than 25% at country-level and 38% at\ncity-level geo-localization tasks, and surpasses StreetCLIP performance while\nrequiring fewer training resources. The data and code are available at\nhttps://github.com/lingli1996/GeoReasoner.\n","authors":["Ling Li","Yu Ye","Bingchuan Jiang","Wei Zeng"],"pdf_url":"https://arxiv.org/pdf/2406.18572v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2410.07266v3","updated":"2024-10-17T03:25:01Z","published":"2024-10-09T01:39:26Z","title":"Spiking GS: Towards High-Accuracy and Low-Cost Surface Reconstruction\n  via Spiking Neuron-based Gaussian Splatting","summary":"  3D Gaussian Splatting is capable of reconstructing 3D scenes in minutes.\nDespite recent advances in improving surface reconstruction accuracy, the\nreconstructed results still exhibit bias and suffer from inefficiency in\nstorage and training. This paper provides a different observation on the cause\nof the inefficiency and the reconstruction bias, which is attributed to the\nintegration of the low-opacity parts (LOPs) of the generated Gaussians. We show\nthat LOPs consist of Gaussians with overall low-opacity (LOGs) and the\nlow-opacity tails (LOTs) of Gaussians. We propose Spiking GS to reduce such two\ntypes of LOPs by integrating spiking neurons into the Gaussian Splatting\npipeline. Specifically, we introduce global and local full-precision\nintegrate-and-fire spiking neurons to the opacity and representation function\nof flattened 3D Gaussians, respectively. Furthermore, we enhance the density\ncontrol strategy with spiking neurons' thresholds and a new criterion on the\nscale of Gaussians. Our method can represent more accurate reconstructed\nsurfaces at a lower cost. The supplementary material and code are available at\nhttps://github.com/zju-bmi-lab/SpikingGS.\n","authors":["Weixing Zhang","Zongrui Li","De Ma","Huajin Tang","Xudong Jiang","Qian Zheng","Gang Pan"],"pdf_url":"https://arxiv.org/pdf/2410.07266v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05771v2","updated":"2024-10-17T03:16:52Z","published":"2024-10-08T07:53:06Z","title":"Cefdet: Cognitive Effectiveness Network Based on Fuzzy Inference for\n  Action Detection","summary":"  Action detection and understanding provide the foundation for the generation\nand interaction of multimedia content. However, existing methods mainly focus\non constructing complex relational inference networks, overlooking the judgment\nof detection effectiveness. Moreover, these methods frequently generate\ndetection results with cognitive abnormalities. To solve the above problems,\nthis study proposes a cognitive effectiveness network based on fuzzy inference\n(Cefdet), which introduces the concept of \"cognition-based detection\" to\nsimulate human cognition. First, a fuzzy-driven cognitive effectiveness\nevaluation module (FCM) is established to introduce fuzzy inference into action\ndetection. FCM is combined with human action features to simulate the\ncognition-based detection process, which clearly locates the position of frames\nwith cognitive abnormalities. Then, a fuzzy cognitive update strategy (FCS) is\nproposed based on the FCM, which utilizes fuzzy logic to re-detect the\ncognition-based detection results and effectively update the results with\ncognitive abnormalities. Experimental results demonstrate that Cefdet exhibits\nsuperior performance against several mainstream algorithms on the public\ndatasets, validating its effectiveness and superiority. Code is available at\nhttps://github.com/12sakura/Cefdet.\n","authors":["Zhe Luo","Weina Fu","Shuai Liu","Saeed Anwar","Muhammad Saqib","Sambit Bakshi","Khan Muhammad"],"pdf_url":"https://arxiv.org/pdf/2410.05771v2.pdf","comment":"The paper has been accepted by ACM MM. If you find this work helpful,\n  please consider citing our paper. Zhe Luo, Weina Fu, Shuai Liu, Saeed Anwar,\n  Muhammad Saqib, Sambit Bakshi, Khan Muhammad (2024) Cefdet: Cognitive\n  Effectiveness Network Based on Fuzzy Inference for Action Detection, 32nd ACM\n  International Conference on Multimedia, online first, 10.1145/3664647.3681226"},{"id":"http://arxiv.org/abs/2401.15883v2","updated":"2024-10-17T03:00:14Z","published":"2024-01-29T04:35:48Z","title":"Model Supply Chain Poisoning: Backdooring Pre-trained Models via\n  Embedding Indistinguishability","summary":"  Pre-trained models (PTMs) are widely adopted across various downstream tasks\nin the machine learning supply chain. Adopting untrustworthy PTMs introduces\nsignificant security risks, where adversaries can poison the model supply chain\nby embedding hidden malicious behaviors (backdoors) into PTMs. However,\nexisting backdoor attacks to PTMs can only achieve partially task-agnostic and\nthe embedded backdoors are easily erased during the fine-tuning process. This\nmakes it challenging for the backdoors to persist and propagate through the\nsupply chain. In this paper, we propose a novel and severer backdoor attack,\nTransTroj, which enables the backdoors embedded in PTMs to efficiently transfer\nin the model supply chain. In particular, we first formalize this attack as an\nindistinguishability problem between poisoned and clean samples in the\nembedding space. We decompose embedding indistinguishability into pre- and\npost-indistinguishability, representing the similarity of the poisoned and\nreference embeddings before and after the attack. Then, we propose a two-stage\noptimization that separately optimizes triggers and victim PTMs to achieve\nembedding indistinguishability. We evaluate TransTroj on four PTMs and six\ndownstream tasks. Experimental results show that our method significantly\noutperforms SOTA task-agnostic backdoor attacks -- achieving nearly 100\\%\nattack success rate on most downstream tasks -- and demonstrates robustness\nunder various system settings. Our findings underscore the urgent need to\nsecure the model supply chain against such transferable backdoor attacks. The\ncode is available at https://github.com/haowang-cqu/TransTroj .\n","authors":["Hao Wang","Shangwei Guo","Jialing He","Hangcheng Liu","Tianwei Zhang","Tao Xiang"],"pdf_url":"https://arxiv.org/pdf/2401.15883v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13174v1","updated":"2024-10-17T02:57:35Z","published":"2024-10-17T02:57:35Z","title":"Scalable Drift Monitoring in Medical Imaging AI","summary":"  The integration of artificial intelligence (AI) into medical imaging has\nadvanced clinical diagnostics but poses challenges in managing model drift and\nensuring long-term reliability. To address these challenges, we develop MMC+,\nan enhanced framework for scalable drift monitoring, building upon the\nCheXstray framework that introduced real-time drift detection for medical\nimaging AI models using multi-modal data concordance. This work extends the\noriginal framework's methodologies, providing a more scalable and adaptable\nsolution for real-world healthcare settings and offers a reliable and\ncost-effective alternative to continuous performance monitoring addressing\nlimitations of both continuous and periodic monitoring methods. MMC+ introduces\ncritical improvements to the original framework, including more robust handling\nof diverse data streams, improved scalability with the integration of\nfoundation models like MedImageInsight for high-dimensional image embeddings\nwithout site-specific training, and the introduction of uncertainty bounds to\nbetter capture drift in dynamic clinical environments. Validated with\nreal-world data from Massachusetts General Hospital during the COVID-19\npandemic, MMC+ effectively detects significant data shifts and correlates them\nwith model performance changes. While not directly predicting performance\ndegradation, MMC+ serves as an early warning system, indicating when AI systems\nmay deviate from acceptable performance bounds and enabling timely\ninterventions. By emphasizing the importance of monitoring diverse data streams\nand evaluating data shifts alongside model performance, this work contributes\nto the broader adoption and integration of AI solutions in clinical settings.\n","authors":["Jameson Merkow","Felix J. Dorfner","Xiyu Yang","Alexander Ersoy","Giridhar Dasegowda","Mannudeep Kalra","Matthew P. Lungren","Christopher P. Bridge","Ivan Tarapov"],"pdf_url":"https://arxiv.org/pdf/2410.13174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05799v3","updated":"2024-10-17T02:41:16Z","published":"2024-10-08T08:33:47Z","title":"SeeClear: Semantic Distillation Enhances Pixel Condensation for Video\n  Super-Resolution","summary":"  Diffusion-based Video Super-Resolution (VSR) is renowned for generating\nperceptually realistic videos, yet it grapples with maintaining detail\nconsistency across frames due to stochastic fluctuations. The traditional\napproach of pixel-level alignment is ineffective for diffusion-processed frames\nbecause of iterative disruptions. To overcome this, we introduce SeeClear--a\nnovel VSR framework leveraging conditional video generation, orchestrated by\ninstance-centric and channel-wise semantic controls. This framework integrates\na Semantic Distiller and a Pixel Condenser, which synergize to extract and\nupscale semantic details from low-resolution frames. The Instance-Centric\nAlignment Module (InCAM) utilizes video-clip-wise tokens to dynamically relate\npixels within and across frames, enhancing coherency. Additionally, the\nChannel-wise Texture Aggregation Memory (CaTeGory) infuses extrinsic knowledge,\ncapitalizing on long-standing semantic textures. Our method also innovates the\nblurring diffusion process with the ResShift mechanism, finely balancing\nbetween sharpness and diffusion effects. Comprehensive experiments confirm our\nframework's advantage over state-of-the-art diffusion-based VSR techniques. The\ncode is available: https://github.com/Tang1705/SeeClear-NeurIPS24.\n","authors":["Qi Tang","Yao Zhao","Meiqin Liu","Chao Yao"],"pdf_url":"https://arxiv.org/pdf/2410.05799v3.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.06997v2","updated":"2024-10-17T02:36:38Z","published":"2024-10-09T15:44:34Z","title":"A Diffusion-based Xray2MRI Model: Generating Pseudo-MRI Volumes From one\n  Single X-ray","summary":"  Knee osteoarthritis (KOA) is a prevalent musculoskeletal disorder, and X-rays\nare commonly used for its diagnosis due to their cost-effectiveness. Magnetic\nResonance Imaging (MRI), on the other hand, offers detailed soft tissue\nvisualization and has become a valuable supplementary diagnostic tool for KOA.\nUnfortunately, the high cost and limited accessibility of MRI hinders its\nwidespread use, leaving many patients with KOA to rely solely on X-ray imaging.\nIn this study, we introduce a novel diffusion-based Xray2MRI model capable of\ngenerating pseudo-MRI volumes from a single X-ray image. In addition to using\nX-rays as conditional input, our model integrates target depth, KOA probability\ndistribution, and image intensity distribution modules to guide the synthesis\nprocess, ensuring that the generated corresponding slices accurately correspond\nto the anatomical structures. Experimental results demonstrate that by\nintegrating information from X-rays with additional input data, our proposed\napproach is capable of generating pseudo-MRI sequences that approximate real\nMRI scans. In addition, by increasing the number of inference steps, the model\nachieves effective interpolation, which further improves the continuity and\nsmoothness of the generated MRI sequences, representing a promising first\nattempt at cost-effective medical imaging solutions. This study is available on\nhttps://zwang78.github.io/.\n","authors":["Zhe Wang","Rachid Jennane","Aladine Chetouani","Yung Hsin Chen","Fabian Bauer","Mohamed Jarraya"],"pdf_url":"https://arxiv.org/pdf/2410.06997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11125v2","updated":"2024-10-17T02:34:39Z","published":"2024-10-14T22:24:11Z","title":"UAV3D: A Large-scale 3D Perception Benchmark for Unmanned Aerial\n  Vehicles","summary":"  Unmanned Aerial Vehicles (UAVs), equipped with cameras, are employed in\nnumerous applications, including aerial photography, surveillance, and\nagriculture. In these applications, robust object detection and tracking are\nessential for the effective deployment of UAVs. However, existing benchmarks\nfor UAV applications are mainly designed for traditional 2D perception tasks,\nrestricting the development of real-world applications that require a 3D\nunderstanding of the environment. Furthermore, despite recent advancements in\nsingle-UAV perception, limited views of a single UAV platform significantly\nconstrain its perception capabilities over long distances or in occluded areas.\nTo address these challenges, we introduce UAV3D, a benchmark designed to\nadvance research in both 3D and collaborative 3D perception tasks with UAVs.\nUAV3D comprises 1,000 scenes, each of which has 20 frames with fully annotated\n3D bounding boxes on vehicles. We provide the benchmark for four 3D perception\ntasks: single-UAV 3D object detection, single-UAV object tracking,\ncollaborative-UAV 3D object detection, and collaborative-UAV object tracking.\nOur dataset and code are available at\nhttps://huiyegit.github.io/UAV3D_Benchmark/.\n","authors":["Hui Ye","Rajshekhar Sunderraman","Shihao Ji"],"pdf_url":"https://arxiv.org/pdf/2410.11125v2.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.13156v1","updated":"2024-10-17T02:21:43Z","published":"2024-10-17T02:21:43Z","title":"FAMSeC: A Few-shot-sample-based General AI-generated Image Detection\n  Method","summary":"  The explosive growth of generative AI has saturated the internet with\nAI-generated images, raising security concerns and increasing the need for\nreliable detection methods. The primary requirement for such detection is\ngeneralizability, typically achieved by training on numerous fake images from\nvarious models. However, practical limitations, such as closed-source models\nand restricted access, often result in limited training samples. Therefore,\ntraining a general detector with few-shot samples is essential for modern\ndetection mechanisms. To address this challenge, we propose FAMSeC, a general\nAI-generated image detection method based on LoRA-based Forgery Awareness\nModule and Semantic feature-guided Contrastive learning strategy. To\neffectively learn from limited samples and prevent overfitting, we developed a\nForgery Awareness Module (FAM) based on LoRA, maintaining the generalization of\npre-trained features. Additionally, to cooperate with FAM, we designed a\nSemantic feature-guided Contrastive learning strategy (SeC), making the FAM\nfocus more on the differences between real/fake image than on the features of\nthe samples themselves. Experiments show that FAMSeC outperforms\nstate-of-the-art method, enhancing classification accuracy by 14.55% with just\n0.56% of the training samples.\n","authors":["Juncong Xu","Yang Yang","Han Fang","Honggu Liu","Weiming Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.13156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13147v1","updated":"2024-10-17T02:04:57Z","published":"2024-10-17T02:04:57Z","title":"Utilizing Large Language Models in An Iterative Paradigm with Domain\n  Feedback for Molecule Optimization","summary":"  Molecule optimization is a critical task in drug discovery to optimize\ndesired properties of a given molecule through chemical modification. Despite\nLarge Language Models (LLMs) holding the potential to efficiently simulate this\ntask by using natural language to direct the optimization, straightforwardly\nutilizing shows limited performance. In this work, we facilitate utilizing LLMs\nin an iterative paradigm by proposing a simple yet highly effective domain\nfeedback provider, namely $\\text{Re}^2$DF. In detail, $\\text{Re}^2$DF harnesses\nan external toolkit, RDKit, to handle the molecule hallucination, if the\nmodified molecule is chemically invalid. Otherwise, its desired properties are\ncomputed and compared to the original one, establishing reliable domain\nfeedback with correct direction and distance towards the objective, followed by\na retrieved example, to explicitly guide the LLM to refine the modified\nmolecule. We conduct experiments across both single- and multi-property\nobjectives with 2 thresholds, where $\\text{Re}^2$DF shows significant\nimprovements. Particularly, for 20 single-property objectives, $\\text{Re}^2$DF\nenhances the Hit ratio by 16.95\\% and 20.76\\% under loose and strict\nthresholds, respectively. For 32 multi-property objectives, $\\text{Re}^2$DF\nenhances the Hit ratio by 6.04\\% and 5.25\\%.\n","authors":["Khiem Le","Nitesh V. Chawla"],"pdf_url":"https://arxiv.org/pdf/2410.13147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13146v1","updated":"2024-10-17T02:03:27Z","published":"2024-10-17T02:03:27Z","title":"Mapping Bias in Vision Language Models: Signposts, Pitfalls, and the\n  Road Ahead","summary":"  As Vision Language Models (VLMs) gain widespread use, their fairness remains\nunder-explored. In this paper, we analyze demographic biases across five models\nand six datasets. We find that portrait datasets like UTKFace and CelebA are\nthe best tools for bias detection, finding gaps in performance and fairness\nbetween LLaVa and CLIP models. However, scene based datasets like PATA,\nVLStereoSet fail to be useful benchmarks for bias due to their construction. As\nfor pronoun based datasets like VisoGender, we receive mixed signals as only\nsome subsets of the data are useful in providing insights. To alleviate this\nproblem, we introduce a more difficult version of VisoGender to serve as a more\nrigorous evaluation. Based on these results, we call for more effective and\ncarefully designed datasets to ensure VLMs are both fair and reliable.\n","authors":["Kuleen Sasse","Shan Chen","Jackson Pond","Danielle Bitterman","John Osborne"],"pdf_url":"https://arxiv.org/pdf/2410.13146v1.pdf","comment":"Under Review at NAACL 2025"},{"id":"http://arxiv.org/abs/2407.05131v2","updated":"2024-10-17T01:55:28Z","published":"2024-07-06T16:45:07Z","title":"RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language\n  Models","summary":"  The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has\nenhanced medical diagnosis. However, current Med-LVLMs frequently encounter\nfactual issues, often generating responses that do not align with established\nmedical facts. Retrieval-Augmented Generation (RAG), which utilizes external\nknowledge, can improve the factual accuracy of these models but introduces two\nmajor challenges. First, limited retrieved contexts might not cover all\nnecessary information, while excessive retrieval can introduce irrelevant and\ninaccurate references, interfering with the model's generation. Second, in\ncases where the model originally responds correctly, applying RAG can lead to\nan over-reliance on retrieved contexts, resulting in incorrect answers. To\naddress these issues, we propose RULE, which consists of two components. First,\nwe introduce a provably effective strategy for controlling factuality risk\nthrough the calibrated selection of the number of retrieved contexts. Second,\nbased on samples where over-reliance on retrieved contexts led to errors, we\ncurate a preference dataset to fine-tune the model, balancing its dependence on\ninherent knowledge and retrieved contexts for generation. We demonstrate the\neffectiveness of RULE on medical VQA and report generation tasks across three\ndatasets, achieving an average improvement of 47.4% in factual accuracy. We\npublicly release our benchmark and code in\nhttps://github.com/richard-peng-xia/RULE.\n","authors":["Peng Xia","Kangyu Zhu","Haoran Li","Hongtu Zhu","Yun Li","Gang Li","Linjun Zhang","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2407.05131v2.pdf","comment":"EMNLP 2024 main"},{"id":"http://arxiv.org/abs/2410.13139v1","updated":"2024-10-17T01:51:58Z","published":"2024-10-17T01:51:58Z","title":"See Behind Walls in Real-time Using Aerial Drones and Augmented Reality","summary":"  This work presents ARD2, a framework that enables real-time through-wall\nsurveillance using two aerial drones and an augmented reality (AR) device. ARD2\nconsists of two main steps: target direction estimation and contour\nreconstruction. In the first stage, ARD2 leverages geometric relationships\nbetween the drones, the user, and the target to project the target's direction\nonto the user's AR display. In the second stage, images from the drones are\nsynthesized to reconstruct the target's contour, allowing the user to visualize\nthe target behind walls. Experimental results demonstrate the system's accuracy\nin both direction estimation and contour reconstruction.\n","authors":["Sikai Yang","Kang Yang","Yuning Chen","Fan Zhao","Wan Du"],"pdf_url":"https://arxiv.org/pdf/2410.13139v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2410.13136v1","updated":"2024-10-17T01:48:05Z","published":"2024-10-17T01:48:05Z","title":"Unlocking the Capabilities of Masked Generative Models for Image\n  Synthesis via Self-Guidance","summary":"  Masked generative models (MGMs) have shown impressive generative ability\nwhile providing an order of magnitude efficient sampling steps compared to\ncontinuous diffusion models. However, MGMs still underperform in image\nsynthesis compared to recent well-developed continuous diffusion models with\nsimilar size in terms of quality and diversity of generated samples. A key\nfactor in the performance of continuous diffusion models stems from the\nguidance methods, which enhance the sample quality at the expense of diversity.\nIn this paper, we extend these guidance methods to generalized guidance\nformulation for MGMs and propose a self-guidance sampling method, which leads\nto better generation quality. The proposed approach leverages an auxiliary task\nfor semantic smoothing in vector-quantized token space, analogous to the\nGaussian blur in continuous pixel space. Equipped with the parameter-efficient\nfine-tuning method and high-temperature sampling, MGMs with the proposed\nself-guidance achieve a superior quality-diversity trade-off, outperforming\nexisting sampling methods in MGMs with more efficient training and sampling\ncosts. Extensive experiments with the various sampling hyperparameters confirm\nthe effectiveness of the proposed self-guidance.\n","authors":["Jiwan Hur","Dong-Jae Lee","Gyojin Han","Jaehyun Choi","Yunho Jeon","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2410.13136v1.pdf","comment":"NeurIPS 2024. Code is available at:\n  https://github.com/JiwanHur/UnlockMGM"},{"id":"http://arxiv.org/abs/2312.06738v4","updated":"2024-10-17T01:30:33Z","published":"2023-12-11T17:53:45Z","title":"InstructAny2Pix: Flexible Visual Editing via Multimodal Instruction\n  Following","summary":"  The ability to provide fine-grained control for generating and editing visual\nimagery has profound implications for computer vision and its applications.\nPrevious works have explored extending controllability in two directions:\ninstruction tuning with text-based prompts and multi-modal conditioning.\nHowever, these works make one or more unnatural assumptions on the number\nand/or type of modality inputs used to express controllability. We propose\nInstructAny2Pix, a flexible multi-modal instruction-following system that\nenables users to edit an input image using instructions involving audio,\nimages, and text. InstructAny2Pix consists of three building blocks that\nfacilitate this capability: a multi-modal encoder that encodes different\nmodalities such as images and audio into a unified latent space, a diffusion\nmodel that learns to decode representations in this latent space into images,\nand a multi-modal LLM that can understand instructions involving multiple\nimages and audio pieces and generate a conditional embedding of the desired\noutput, which can be used by the diffusion decoder. Additionally, to facilitate\ntraining efficiency and improve generation quality, we include an additional\nrefinement prior module that enhances the visual quality of LLM outputs. These\ndesigns are critical to the performance of our system. We demonstrate that our\nsystem can perform a series of novel instruction-guided editing tasks. The code\nis available at https://github.com/jacklishufan/InstructAny2Pix.git\n","authors":["Shufan Li","Harkanwar Singh","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2312.06738v4.pdf","comment":"25 pages, 19 figures"},{"id":"http://arxiv.org/abs/2410.13122v1","updated":"2024-10-17T01:22:11Z","published":"2024-10-17T01:22:11Z","title":"Boosting Imperceptibility of Stable Diffusion-based Adversarial Examples\n  Generation with Momentum","summary":"  We propose a novel framework, Stable Diffusion-based Momentum Integrated\nAdversarial Examples (SD-MIAE), for generating adversarial examples that can\neffectively mislead neural network classifiers while maintaining visual\nimperceptibility and preserving the semantic similarity to the original class\nlabel. Our method leverages the text-to-image generation capabilities of the\nStable Diffusion model by manipulating token embeddings corresponding to the\nspecified class in its latent space. These token embeddings guide the\ngeneration of adversarial images that maintain high visual fidelity. The\nSD-MIAE framework consists of two phases: (1) an initial adversarial\noptimization phase that modifies token embeddings to produce misclassified yet\nnatural-looking images and (2) a momentum-based optimization phase that refines\nthe adversarial perturbations. By introducing momentum, our approach stabilizes\nthe optimization of perturbations across iterations, enhancing both the\nmisclassification rate and visual fidelity of the generated adversarial\nexamples. Experimental results demonstrate that SD-MIAE achieves a high\nmisclassification rate of 79%, improving by 35% over the state-of-the-art\nmethod while preserving the imperceptibility of adversarial perturbations and\nthe semantic similarity to the original class label, making it a practical\nmethod for robust adversarial evaluation.\n","authors":["Nashrah Haque","Xiang Li","Zhehui Chen","Yanzhao Wu","Lei Yu","Arun Iyengar","Wenqi Wei"],"pdf_url":"https://arxiv.org/pdf/2410.13122v1.pdf","comment":"10 pages, 12 figures. To be published in IEEE TPS 2024 Proceedings.\n  Code available on GitHub: https://github.com/nashrahhaque/SD-MIAE"},{"id":"http://arxiv.org/abs/2410.13121v1","updated":"2024-10-17T01:19:18Z","published":"2024-10-17T01:19:18Z","title":"Trust but Verify: Programmatic VLM Evaluation in the Wild","summary":"  Vision-Language Models (VLMs) often generate plausible but incorrect\nresponses to visual queries. However, reliably quantifying the effect of such\nhallucinations in free-form responses to open-ended queries is challenging as\nit requires visually verifying each claim within the response. We propose\nProgrammatic VLM Evaluation (PROVE), a new benchmarking paradigm for evaluating\nVLM responses to open-ended queries. To construct PROVE, we provide a large\nlanguage model (LLM) with a high-fidelity scene-graph representation\nconstructed from a hyper-detailed image caption, and prompt it to generate\ndiverse question-answer (QA) pairs, as well as programs that can be executed\nover the scene graph object to verify each QA pair. We thus construct a\nbenchmark of 10.5k challenging but visually grounded QA pairs. Next, to\nevaluate free-form model responses to queries in PROVE, we propose a\nprogrammatic evaluation strategy that measures both the helpfulness and\ntruthfulness of a response within a unified scene graph-based framework. We\nbenchmark the helpfulness-truthfulness trade-offs of a range of VLMs on PROVE,\nfinding that very few are in-fact able to achieve a good balance between the\ntwo. Project page: \\url{https://prove-explorer.netlify.app/}.\n","authors":["Viraj Prabhu","Senthil Purushwalkam","An Yan","Caiming Xiong","Ran Xu"],"pdf_url":"https://arxiv.org/pdf/2410.13121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06579v3","updated":"2024-10-17T01:17:25Z","published":"2024-06-04T13:52:54Z","title":"From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning\n  Tasks","summary":"  Large Vision Language Models (LVLMs) achieve great performance on\nvisual-language reasoning tasks, however, the black-box nature of LVLMs hinders\nin-depth research on the reasoning mechanism. As all images need to be\nconverted into image tokens to fit the input format of large language models\n(LLMs) along with natural language prompts, sequential visual representation is\nessential to the performance of LVLMs, and the information flow analysis\napproach can be an effective tool for determining interactions between these\nrepresentations. In this paper, we propose integrating attention analysis with\nLLaVA-CAM, concretely, attention scores highlight relevant regions during\nforward propagation, while LLaVA-CAM captures gradient changes through backward\npropagation, revealing key image features. By exploring the information flow\nfrom the perspective of visual representation contribution, we observe that it\ntends to converge in shallow layers but diversify in deeper layers. To validate\nour analysis, we conduct comprehensive experiments with truncation strategies\nacross various LVLMs for visual question answering and image captioning tasks,\nand experimental results not only verify our hypothesis but also reveal a\nconsistent pattern of information flow convergence in the corresponding layers,\nand the information flow cliff layer will be different due to different\ncontexts. The paper's source code can be accessed from\n\\url{https://github.com/zhangbaijin/From-Redundancy-to-Relevance}\n","authors":["Xiaofeng Zhang","Yihao Quan","Chen Shen","Xiaosong Yuan","Shaotian Yan","Liang Xie","Wenxiao Wang","Chaochen Gu","Hao Tang","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2406.06579v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10674v2","updated":"2024-10-17T00:51:28Z","published":"2024-03-15T20:49:43Z","title":"D-Net: Dynamic Large Kernel with Dynamic Feature Fusion for Volumetric\n  Medical Image Segmentation","summary":"  Hierarchical transformers have achieved significant success in medical image\nsegmentation due to their large receptive field and capabilities of effectively\nleveraging global long-range contextual information. Convolutional neural\nnetworks (CNNs) can also deliver a large receptive field by using large\nkernels, enabling them to achieve competitive performance with fewer model\nparameters. However, CNNs incorporated with large convolutional kernels remain\nconstrained in adaptively capturing multi-scale features from organs with large\nvariations in shape and size due to the employment of fixed-sized kernels.\nAdditionally, they are unable to utilize global contextual information\nefficiently. To address these limitations, we propose Dynamic Large Kernel\n(DLK) and Dynamic Feature Fusion (DFF) modules. The DLK module employs multiple\nlarge kernels with varying kernel sizes and dilation rates to capture\nmulti-scale features. Subsequently, a dynamic selection mechanism is utilized\nto adaptively highlight the most important spatial features based on global\ninformation. Additionally, the DFF module is proposed to adaptively fuse\nmulti-scale local feature maps based on their global information. We integrate\nDLK and DFF in a hierarchical transformer architecture to develop a novel\narchitecture, termed D-Net. D-Net is able to effectively utilize a multi-scale\nlarge receptive field and adaptively harness global contextual information.\nExtensive experimental results demonstrate that D-Net outperforms other\nstate-of-the-art models in the two volumetric segmentation tasks, including\nabdominal multi-organ segmentation and multi-modality brain tumor segmentation.\nOur code is available at https://github.com/sotiraslab/DLK.\n","authors":["Jin Yang","Peijie Qiu","Yichi Zhang","Daniel S. Marcus","Aristeidis Sotiras"],"pdf_url":"https://arxiv.org/pdf/2403.10674v2.pdf","comment":"18 pages, 8 figures, 9 tables"},{"id":"http://arxiv.org/abs/2306.04955v2","updated":"2024-10-17T00:27:09Z","published":"2023-06-08T06:02:39Z","title":"Degraded Polygons Raise Fundamental Questions of Neural Network\n  Perception","summary":"  It is well-known that modern computer vision systems often exhibit behaviors\nmisaligned with those of humans: from adversarial attacks to image corruptions,\ndeep learning vision models suffer in a variety of settings that humans capably\nhandle. In light of these phenomena, here we introduce another, orthogonal\nperspective studying the human-machine vision gap. We revisit the task of\nrecovering images under degradation, first introduced over 30 years ago in the\nRecognition-by-Components theory of human vision. Specifically, we study the\nperformance and behavior of neural networks on the seemingly simple task of\nclassifying regular polygons at varying orders of degradation along their\nperimeters. To this end, we implement the Automated Shape Recoverability Test\nfor rapidly generating large-scale datasets of perimeter-degraded regular\npolygons, modernizing the historically manual creation of image recoverability\nexperiments. We then investigate the capacity of neural networks to recognize\nand recover such degraded shapes when initialized with different priors.\nUltimately, we find that neural networks' behavior on this simple task\nconflicts with human behavior, raising a fundamental question of the robustness\nand learning capabilities of modern computer vision models.\n","authors":["Leonard Tang","Dan Ley"],"pdf_url":"https://arxiv.org/pdf/2306.04955v2.pdf","comment":"Accepted as a conference paper to NeurIPS 2023 (Datasets & Benchmarks\n  Track)"},{"id":"http://arxiv.org/abs/2410.13099v1","updated":"2024-10-17T00:05:05Z","published":"2024-10-17T00:05:05Z","title":"Adversarial Neural Networks in Medical Imaging Advancements and\n  Challenges in Semantic Segmentation","summary":"  Recent advancements in artificial intelligence (AI) have precipitated a\nparadigm shift in medical imaging, particularly revolutionizing the domain of\nbrain imaging. This paper systematically investigates the integration of deep\nlearning -- a principal branch of AI -- into the semantic segmentation of brain\nimages. Semantic segmentation serves as an indispensable technique for the\ndelineation of discrete anatomical structures and the identification of\npathological markers, essential for the diagnosis of complex neurological\ndisorders. Historically, the reliance on manual interpretation by radiologists,\nwhile noteworthy for its accuracy, is plagued by inherent subjectivity and\ninter-observer variability. This limitation becomes more pronounced with the\nexponential increase in imaging data, which traditional methods struggle to\nprocess efficiently and effectively. In response to these challenges, this\nstudy introduces the application of adversarial neural networks, a novel AI\napproach that not only automates but also refines the semantic segmentation\nprocess. By leveraging these advanced neural networks, our approach enhances\nthe precision of diagnostic outputs, reducing human error and increasing the\nthroughput of imaging data analysis. The paper provides a detailed discussion\non how adversarial neural networks facilitate a more robust, objective, and\nscalable solution, thereby significantly improving diagnostic accuracies in\nneurological evaluations. This exploration highlights the transformative impact\nof AI on medical imaging, setting a new benchmark for future research and\nclinical practice in neurology.\n","authors":["Houze Liu","Bo Zhang","Yanlin Xiang","Yuxiang Hu","Aoran Shen","Yang Lin"],"pdf_url":"https://arxiv.org/pdf/2410.13099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13807v1","updated":"2024-10-17T17:41:52Z","published":"2024-10-17T17:41:52Z","title":"ConsisSR: Delving Deep into Consistency in Diffusion-based Image\n  Super-Resolution","summary":"  Real-world image super-resolution (Real-ISR) aims at restoring high-quality\n(HQ) images from low-quality (LQ) inputs corrupted by unknown and complex\ndegradations. In particular, pretrained text-to-image (T2I) diffusion models\nprovide strong generative priors to reconstruct credible and intricate details.\nHowever, T2I generation focuses on semantic consistency while Real-ISR\nemphasizes pixel-level reconstruction, which hinders existing methods from\nfully exploiting diffusion priors. To address this challenge, we introduce\nConsisSR to handle both semantic and pixel-level consistency. Specifically,\ncompared to coarse-grained text prompts, we exploit the more powerful CLIP\nimage embedding and effectively leverage both modalities through our Hybrid\nPrompt Adapter (HPA) for semantic guidance. Secondly, we introduce Time-aware\nLatent Augmentation (TALA) to mitigate the inherent gap between T2I generation\nand Real-ISR consistency requirements. By randomly mixing LQ and HQ latent\ninputs, our model not only handle timestep-specific diffusion noise but also\nrefine the accumulated latent representations. Last but not least, our\nGAN-Embedding strategy employs the pretrained Real-ESRGAN model to refine the\ndiffusion start point. This accelerates the inference process to 10 steps while\npreserving sampling quality, in a training-free manner. Our method demonstrates\nstate-of-the-art performance among both full-scale and accelerated models. The\ncode will be made publicly available.\n","authors":["Junhao Gu","Peng-Tao Jiang","Hao Zhang","Mi Zhou","Jinwei Chen","Wenming Yang","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2410.13807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.02791v3","updated":"2024-10-17T16:11:28Z","published":"2021-07-06T17:58:35Z","title":"Depth-supervised NeRF: Fewer Views and Faster Training for Free","summary":"  A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting\nincorrect geometries when given an insufficient number of input views. One\npotential reason is that standard volumetric rendering does not enforce the\nconstraint that most of a scene's geometry consist of empty space and opaque\nsurfaces. We formalize the above assumption through DS-NeRF (Depth-supervised\nNeural Radiance Fields), a loss for learning radiance fields that takes\nadvantage of readily-available depth supervision. We leverage the fact that\ncurrent NeRF pipelines require images with known camera poses that are\ntypically estimated by running structure-from-motion (SFM). Crucially, SFM also\nproduces sparse 3D points that can be used as \"free\" depth supervision during\ntraining: we add a loss to encourage the distribution of a ray's terminating\ndepth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can\nrender better images given fewer training views while training 2-3x faster.\nFurther, we show that our loss is compatible with other recently proposed NeRF\nmethods, demonstrating that depth is a cheap and easily digestible supervisory\nsignal. And finally, we find that DS-NeRF can support other types of depth\nsupervision such as scanned depth sensors and RGB-D reconstruction outputs.\n","authors":["Kangle Deng","Andrew Liu","Jun-Yan Zhu","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2107.02791v3.pdf","comment":"Project page: http://www.cs.cmu.edu/~dsnerf/ GitHub:\n  https://github.com/dunbar12138/DSNeRF"},{"id":"http://arxiv.org/abs/2009.09231v2","updated":"2024-10-17T13:30:22Z","published":"2020-09-19T13:47:33Z","title":"Adversarial Exposure Attack on Diabetic Retinopathy Imagery Grading","summary":"  Diabetic Retinopathy (DR) is a leading cause of vision loss around the world.\nTo help diagnose it, numerous cutting-edge works have built powerful deep\nneural networks (DNNs) to automatically grade DR via retinal fundus images\n(RFIs). However, RFIs are commonly affected by camera exposure issues that may\nlead to incorrect grades. The mis-graded results can potentially pose high\nrisks to an aggravation of the condition. In this paper, we study this problem\nfrom the viewpoint of adversarial attacks. We identify and introduce a novel\nsolution to an entirely new task, termed as adversarial exposure attack, which\nis able to produce natural exposure images and mislead the state-of-the-art\nDNNs. We validate our proposed method on a real-world public DR dataset with\nthree DNNs, e.g., ResNet50, MobileNet, and EfficientNet, demonstrating that our\nmethod achieves high image quality and success rate in transferring the\nattacks. Our method reveals the potential threats to DNN-based automatic DR\ngrading and would benefit the development of exposure-robust DR grading methods\nin the future.\n","authors":["Yupeng Cheng","Qing Guo","Felix Juefei-Xu","Huazhu Fu","Shang-Wei Lin","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2009.09231v2.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.14089v1","updated":"2024-10-17T23:52:39Z","published":"2024-10-17T23:52:39Z","title":"MMAD-Purify: A Precision-Optimized Framework for Efficient and Scalable\n  Multi-Modal Attacks","summary":"  Neural networks have achieved remarkable performance across a wide range of\ntasks, yet they remain susceptible to adversarial perturbations, which pose\nsignificant risks in safety-critical applications. With the rise of\nmultimodality, diffusion models have emerged as powerful tools not only for\ngenerative tasks but also for various applications such as image editing,\ninpainting, and super-resolution. However, these models still lack robustness\ndue to limited research on attacking them to enhance their resilience.\nTraditional attack techniques, such as gradient-based adversarial attacks and\ndiffusion model-based methods, are hindered by computational inefficiencies and\nscalability issues due to their iterative nature. To address these challenges,\nwe introduce an innovative framework that leverages the distilled backbone of\ndiffusion models and incorporates a precision-optimized noise predictor to\nenhance the effectiveness of our attack framework. This approach not only\nenhances the attack's potency but also significantly reduces computational\ncosts. Our framework provides a cutting-edge solution for multi-modal\nadversarial attacks, ensuring reduced latency and the generation of\nhigh-fidelity adversarial examples with superior success rates. Furthermore, we\ndemonstrate that our framework achieves outstanding transferability and\nrobustness against purification defenses, outperforming existing gradient-based\nattack models in both effectiveness and efficiency.\n","authors":["Xinxin Liu","Zhongliang Guo","Siyuan Huang","Chun Pong Lau"],"pdf_url":"https://arxiv.org/pdf/2410.14089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14087v1","updated":"2024-10-17T23:37:58Z","published":"2024-10-17T23:37:58Z","title":"Your Interest, Your Summaries: Query-Focused Long Video Summarization","summary":"  Generating a concise and informative video summary from a long video is\nimportant, yet subjective due to varying scene importance. Users' ability to\nspecify scene importance through text queries enhances the relevance of such\nsummaries. This paper introduces an approach for query-focused video\nsummarization, aiming to align video summaries closely with user queries. To\nthis end, we propose the Fully Convolutional Sequence Network with Attention\n(FCSNA-QFVS), a novel approach designed for this task. Leveraging temporal\nconvolutional and attention mechanisms, our model effectively extracts and\nhighlights relevant content based on user-specified queries. Experimental\nvalidation on a benchmark dataset for query-focused video summarization\ndemonstrates the effectiveness of our approach.\n","authors":["Nirav Patel","Payal Prajapati","Maitrik Shah"],"pdf_url":"https://arxiv.org/pdf/2410.14087v1.pdf","comment":"To appear at the 18th International Conference on Control,\n  Automation, Robotics and Vision (ICARCV), December 2024, Dubai, UAE"},{"id":"http://arxiv.org/abs/2410.14084v1","updated":"2024-10-17T23:26:55Z","published":"2024-10-17T23:26:55Z","title":"Self Supervised Deep Learning for Robot Grasping","summary":"  Learning Based Robot Grasping currently involves the use of labeled data.\nThis approach has two major disadvantages. Firstly, labeling data for grasp\npoints and angles is a strenuous process, so the dataset remains limited.\nSecondly, human labeling is prone to bias due to semantics.\n  In order to solve these problems we propose a simpler self-supervised robotic\nsetup, that will train a Convolutional Neural Network (CNN). The robot will\nlabel and collect the data during the training process. The idea is to make a\nrobot that is less costly, small and easily maintainable in a lab setup. The\nrobot will be trained on a large data set for several hundred hours and then\nthe trained Neural Network can be mapped onto a larger grasping robot.\n","authors":["Danyal Saqib","Wajahat Hussain"],"pdf_url":"https://arxiv.org/pdf/2410.14084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14083v1","updated":"2024-10-17T23:23:48Z","published":"2024-10-17T23:23:48Z","title":"SAMReg: SAM-enabled Image Registration with ROI-based Correspondence","summary":"  This paper describes a new spatial correspondence representation based on\npaired regions-of-interest (ROIs), for medical image registration. The distinct\nproperties of the proposed ROI-based correspondence are discussed, in the\ncontext of potential benefits in clinical applications following image\nregistration, compared with alternative correspondence-representing approaches,\nsuch as those based on sampled displacements and spatial transformation\nfunctions. These benefits include a clear connection between learning-based\nimage registration and segmentation, which in turn motivates two cases of image\nregistration approaches using (pre-)trained segmentation networks. Based on the\nsegment anything model (SAM), a vision foundation model for segmentation, we\ndevelop a new registration algorithm SAMReg, which does not require any\ntraining (or training data), gradient-based fine-tuning or prompt engineering.\nThe proposed SAMReg models are evaluated across five real-world applications,\nincluding intra-subject registration tasks with cardiac MR and lung CT,\nchallenging inter-subject registration scenarios with prostate MR and retinal\nimaging, and an additional evaluation with a non-clinical example with aerial\nimage registration. The proposed methods outperform both intensity-based\niterative algorithms and DDF-predicting learning-based networks across tested\nmetrics including Dice and target registration errors on anatomical structures,\nand further demonstrates competitive performance compared to weakly-supervised\nregistration approaches that rely on fully-segmented training data. Open source\ncode and examples are available at: https://github.com/sqhuang0103/SAMReg.git.\n","authors":["Shiqi Huang","Tingfa Xu","Ziyi Shen","Shaheer Ullah Saeed","Wen Yan","Dean Barratt","Yipeng Hu"],"pdf_url":"https://arxiv.org/pdf/2410.14083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10551v3","updated":"2024-10-17T23:07:02Z","published":"2024-10-14T14:32:05Z","title":"Preserving Cardiac Integrity: A Topology-Infused Approach to Whole Heart\n  Segmentation","summary":"  Whole heart segmentation (WHS) supports cardiovascular disease (CVD)\ndiagnosis, disease monitoring, treatment planning, and prognosis. Deep learning\nhas become the most widely used method for WHS applications in recent years.\nHowever, segmentation of whole-heart structures faces numerous challenges\nincluding heart shape variability during the cardiac cycle, clinical artifacts\nlike motion and poor contrast-to-noise ratio, domain shifts in multi-center\ndata, and the distinct modalities of CT and MRI. To address these limitations\nand improve segmentation quality, this paper introduces a new\ntopology-preserving module that is integrated into deep neural networks. The\nimplementation achieves anatomically plausible segmentation by using learned\ntopology-preserving fields, which are based entirely on 3D convolution and are\ntherefore very effective for 3D voxel data. We incorporate natural constraints\nbetween structures into the end-to-end training and enrich the feature\nrepresentation of the neural network. The effectiveness of the proposed method\nis validated on an open-source medical heart dataset, specifically using the\nWHS++ data. The results demonstrate that the architecture performs\nexceptionally well, achieving a Dice coefficient of 0.939 during testing. This\nindicates full topology preservation for individual structures and\nsignificantly outperforms other baselines in preserving the overall scene\ntopology.\n","authors":["Chenyu Zhang","Wenxue Guan","Xiaodan Xing","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2410.10551v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09087v2","updated":"2024-10-17T23:02:17Z","published":"2024-06-13T13:13:17Z","title":"Suitability of KANs for Computer Vision: A preliminary investigation","summary":"  Kolmogorov-Arnold Networks (KANs) introduce a paradigm of neural modeling\nthat implements learnable functions on the edges of the networks, diverging\nfrom the traditional node-centric activations in neural networks. This work\nassesses the applicability and efficacy of KANs in visual modeling, focusing on\nfundamental recognition and segmentation tasks. We mainly analyze the\nperformance and efficiency of different network architectures built using KAN\nconcepts along with conventional building blocks of convolutional and linear\nlayers, enabling a comparative analysis with the conventional models. Our\nfindings are aimed at contributing to understanding the potential of KANs in\ncomputer vision, highlighting both their strengths and areas for further\nresearch. Our evaluation point toward the fact that while KAN-based\narchitectures perform in line with the original claims, it may often be\nimportant to employ more complex functions on the network edges to retain the\nperformance advantage of KANs on more complex visual data.\n","authors":["Basim Azam","Naveed Akhtar"],"pdf_url":"https://arxiv.org/pdf/2406.09087v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14072v1","updated":"2024-10-17T22:45:13Z","published":"2024-10-17T22:45:13Z","title":"Efficient Vision-Language Models by Summarizing Visual Tokens into\n  Compact Registers","summary":"  Recent advancements in vision-language models (VLMs) have expanded their\npotential for real-world applications, enabling these models to perform complex\nreasoning on images. In the widely used fully autoregressive transformer-based\nmodels like LLaVA, projected visual tokens are prepended to textual tokens.\nOftentimes, visual tokens are significantly more than prompt tokens, resulting\nin increased computational overhead during both training and inference. In this\npaper, we propose Visual Compact Token Registers (Victor), a method that\nreduces the number of visual tokens by summarizing them into a smaller set of\nregister tokens. Victor adds a few learnable register tokens after the visual\ntokens and summarizes the visual information into these registers using the\nfirst few layers in the language tower of VLMs. After these few layers, all\nvisual tokens are discarded, significantly improving computational efficiency\nfor both training and inference. Notably, our method is easy to implement and\nrequires a small number of new trainable parameters with minimal impact on\nmodel performance. In our experiment, with merely 8 visual registers--about 1%\nof the original tokens--Victor shows less than a 4% accuracy drop while\nreducing the total training time by 43% and boosting the inference throughput\nby 3.3X.\n","authors":["Yuxin Wen","Qingqing Cao","Qichen Fu","Sachin Mehta","Mahyar Najibi"],"pdf_url":"https://arxiv.org/pdf/2410.14072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14070v1","updated":"2024-10-17T22:36:52Z","published":"2024-10-17T22:36:52Z","title":"FaceSaliencyAug: Mitigating Geographic, Gender and Stereotypical Biases\n  via Saliency-Based Data Augmentation","summary":"  Geographical, gender and stereotypical biases in computer vision models pose\nsignificant challenges to their performance and fairness. {In this study, we\npresent an approach named FaceSaliencyAug aimed at addressing the gender bias\nin} {Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs).\nLeveraging the salient regions} { of faces detected by saliency, the propose\napproach mitigates geographical and stereotypical biases } {in the datasets.\nFaceSaliencyAug} randomly selects masks from a predefined search space and\napplies them to the salient region of face images, subsequently restoring the\noriginal image with masked salient region. {The proposed} augmentation strategy\nenhances data diversity, thereby improving model performance and debiasing\neffects. We quantify dataset diversity using Image Similarity Score (ISS)\nacross five datasets, including Flickr Faces HQ (FFHQ), WIKI, IMDB, Labelled\nFaces in the Wild (LFW), UTK Faces, and Diverse Dataset. The proposed approach\ndemonstrates superior diversity metrics, as evaluated by ISS-intra and\nISS-inter algorithms. Furthermore, we evaluate the effectiveness of our\napproach in mitigating gender bias on CEO, Engineer, Nurse, and School Teacher\ndatasets. We use the Image-Image Association Score (IIAS) to measure gender\nbias in these occupations. Our experiments reveal a reduction in gender bias\nfor both CNNs and ViTs, indicating the efficacy of our method in promoting\nfairness and inclusivity in computer vision models.\n","authors":["Teerath Kumar","Alessandra Mileo","Malika Bendechache"],"pdf_url":"https://arxiv.org/pdf/2410.14070v1.pdf","comment":"Accepted at Image Signal and Video processing"},{"id":"http://arxiv.org/abs/2410.14060v1","updated":"2024-10-17T22:06:34Z","published":"2024-10-17T22:06:34Z","title":"On Partial Prototype Collapse in the DINO Family of Self-Supervised\n  Methods","summary":"  A prominent self-supervised learning paradigm is to model the representations\nas clusters, or more generally as a mixture model. Learning to map the data\nsamples to compact representations and fitting the mixture model simultaneously\nleads to the representation collapse problem. Regularizing the distribution of\ndata points over the clusters is the prevalent strategy to avoid this issue.\nWhile this is sufficient to prevent full representation collapse, we show that\na partial prototype collapse problem still exists in the DINO family of\nmethods, that leads to significant redundancies in the prototypes. Such\nprototype redundancies serve as shortcuts for the method to achieve a marginal\nlatent class distribution that matches the prescribed prior. We show that by\nencouraging the model to use diverse prototypes, the partial prototype collapse\ncan be mitigated. Effective utilization of the prototypes enables the methods\nto learn more fine-grained clusters, encouraging more informative\nrepresentations. We demonstrate that this is especially beneficial when\npre-training on a long-tailed fine-grained dataset.\n","authors":["Hariprasath Govindarajan","Per Sidén","Jacob Roll","Fredrik Lindsten"],"pdf_url":"https://arxiv.org/pdf/2410.14060v1.pdf","comment":"First version of the paper appeared in OpenReview on 22 Sep 2023.\n  Accepted to BMVC 2024"},{"id":"http://arxiv.org/abs/2111.14259v5","updated":"2024-10-17T21:57:15Z","published":"2021-11-28T22:58:00Z","title":"Performance of a GPU- and Time-Efficient Pseudo 3D Network for Magnetic\n  Resonance Image Super-Resolution and Motion Artifact Reduction","summary":"  Shortening acquisition time and reducing motion artifacts are the most\ncritical challenges in magnetic resonance imaging (MRI). Deep learning-based\nimage restoration has emerged as a promising solution capable of generating\nhigh-resolution and motion-artifact-free MRI images from low-resolution images\nacquired with shortened acquisition times or from motion-artifact-corrupted\nimages. To facilitate clinical integration, a time- and GPU-efficient network\nwith reliable accuracy is essential. In this study, we adopted a unified 2D\ndeep learning framework for pseudo-3D MRI image super-resolution reconstruction\n(SRR) and motion artifact reduction (MAR). The optimal down-sampling factors to\noptimize the acquisition time in SRR were identified. Training for MAR was\nperformed using publicly available in vivo data, employing a novel standardized\nmethod to induce motion artifacts of varying severity in a controlled way. The\naccuracy of the network was evaluated through a pixel-wise uncertainty map, and\nperformance was benchmarked against state-of-the-art methods. The results\ndemonstrated that the down-sampling factor of 1x1x2 for x2 acceleration and\n2x2x2 for x4 acceleration was optimal. For SRR, the proposed TS-RCAN\noutperformed the 3D networks of mDCSRN and ReCNN, with an improvement of more\nthan 0.01 in SSIM and 1.5 dB in PSNR while reducing GPU load by up to and\ninference time by up to 90%. For MAR, TS-RCAN exceeded UNet's performance by up\nto 0.014 in SSIM and 1.48 dB in PSNR. Additionally, TS-RCAN provided\nuncertainty information, which can be used to estimate the quality of the\nreconstructed images. TS-RCAN has potential use for SRR and MAR in the clinical\nsetting.\n","authors":["Hao Li","Jianan Liu","Marianne Schell","Tao Huang","Arne Lauer","Katharina Schregel","Jessica Jesser","Dominik F Vollherbst","Martin Bendszus","Sabine Heiland","Tim Hilgenfeld"],"pdf_url":"https://arxiv.org/pdf/2111.14259v5.pdf","comment":"16 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.14050v1","updated":"2024-10-17T21:46:00Z","published":"2024-10-17T21:46:00Z","title":"Learning Multimodal Cues of Children's Uncertainty","summary":"  Understanding uncertainty plays a critical role in achieving common ground\n(Clark et al.,1983). This is especially important for multimodal AI systems\nthat collaborate with users to solve a problem or guide the user through a\nchallenging concept. In this work, for the first time, we present a dataset\nannotated in collaboration with developmental and cognitive psychologists for\nthe purpose of studying nonverbal cues of uncertainty. We then present an\nanalysis of the data, studying different roles of uncertainty and its\nrelationship with task difficulty and performance. Lastly, we present a\nmultimodal machine learning model that can predict uncertainty given a\nreal-time video clip of a participant, which we find improves upon a baseline\nmultimodal transformer model. This work informs research on cognitive\ncoordination between human-human and human-AI and has broad implications for\ngesture understanding and generation. The anonymized version of our data and\ncode will be publicly available upon the completion of the required consent\nforms and data sheets.\n","authors":["Qi Cheng","Mert İnan","Rahma Mbarki","Grace Grmek","Theresa Choi","Yiming Sun","Kimele Persaud","Jenny Wang","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2410.14050v1.pdf","comment":"SIGDIAL 2023"},{"id":"http://arxiv.org/abs/2410.14045v1","updated":"2024-10-17T21:37:40Z","published":"2024-10-17T21:37:40Z","title":"Human Action Anticipation: A Survey","summary":"  Predicting future human behavior is an increasingly popular topic in computer\nvision, driven by the interest in applications such as autonomous vehicles,\ndigital assistants and human-robot interactions. The literature on behavior\nprediction spans various tasks, including action anticipation, activity\nforecasting, intent prediction, goal prediction, and so on. Our survey aims to\ntie together this fragmented literature, covering recent technical innovations\nas well as the development of new large-scale datasets for model training and\nevaluation. We also summarize the widely-used metrics for different tasks and\nprovide a comprehensive performance comparison of existing approaches on eleven\naction anticipation datasets. This survey serves as not only a reference for\ncontemporary methodologies in action anticipation, but also a guideline for\nfuture research direction of this evolving landscape.\n","authors":["Bolin Lai","Sam Toyer","Tushar Nagarajan","Rohit Girdhar","Shengxin Zha","James M. Rehg","Kris Kitani","Kristen Grauman","Ruta Desai","Miao Liu"],"pdf_url":"https://arxiv.org/pdf/2410.14045v1.pdf","comment":"30 pages, 9 figures, 12 tables"},{"id":"http://arxiv.org/abs/2406.11579v2","updated":"2024-10-17T21:05:16Z","published":"2024-06-17T14:16:12Z","title":"Duoduo CLIP: Efficient 3D Understanding with Multi-View Images","summary":"  We introduce Duoduo CLIP, a model for 3D representation learning that learns\nshape encodings from multi-view images instead of point-clouds. The choice of\nmulti-view images allows us to leverage 2D priors from off-the-shelf CLIP\nmodels to facilitate fine-tuning with 3D data. Our approach not only shows\nbetter generalization compared to existing point cloud methods, but also\nreduces GPU requirements and training time. In addition, the model is modified\nwith cross-view attention to leverage information across multiple frames of the\nobject which further boosts performance. Notably, our model is permutation\ninvariant to the order of multi-view images while being pose-free. Compared to\nthe current SOTA point cloud method that requires 480 A100 hours to train 1\nbillion model parameters we only require 57 A5000 hours and 87 million\nparameters. Multi-view images also provide more flexibility including being\nable to encode objects with a variable number of images, and performance scales\nwhen more views are used. In contrast, point cloud based methods require an\nentire scan or model of the object. We showcase this flexibility with\nbenchmarks from images of real-world objects. Our model also achieves better\nperformance in more fine-grained text to shape retrieval, demonstrating better\ntext-and-shape alignment than point cloud based models.\n","authors":["Han-Hung Lee","Yiming Zhang","Angel X. Chang"],"pdf_url":"https://arxiv.org/pdf/2406.11579v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03665v2","updated":"2024-10-17T20:51:19Z","published":"2024-10-04T17:59:57Z","title":"Estimating Body and Hand Motion in an Ego-sensed World","summary":"  We present EgoAllo, a system for human motion estimation from a head-mounted\ndevice. Using only egocentric SLAM poses and images, EgoAllo guides sampling\nfrom a conditional diffusion model to estimate 3D body pose, height, and hand\nparameters that capture the wearer's actions in the allocentric coordinate\nframe of the scene. To achieve this, our key insight is in representation: we\npropose spatial and temporal invariance criteria for improving model\nperformance, from which we derive a head motion conditioning parameterization\nthat improves estimation by up to 18%. We also show how the bodies estimated by\nour system can improve the hands: the resulting kinematic and temporal\nconstraints result in over 40% lower hand estimation errors compared to noisy\nmonocular estimates. Project page: https://egoallo.github.io/\n","authors":["Brent Yi","Vickie Ye","Maya Zheng","Lea Müller","Georgios Pavlakos","Yi Ma","Jitendra Malik","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2410.03665v2.pdf","comment":"v2: fixed figures for Safari, typos"},{"id":"http://arxiv.org/abs/2410.14020v1","updated":"2024-10-17T20:46:13Z","published":"2024-10-17T20:46:13Z","title":"Segmentation of Pediatric Brain Tumors using a Radiologically informed,\n  Deep Learning Cascade","summary":"  Monitoring of Diffuse Intrinsic Pontine Glioma (DIPG) and Diffuse Midline\nGlioma (DMG) brain tumors in pediatric patients is key for assessment of\ntreatment response. Response Assessment in Pediatric Neuro-Oncology (RAPNO)\nguidelines recommend the volumetric measurement of these tumors using MRI.\nSegmentation challenges, such as the Brain Tumor Segmentation (BraTS)\nChallenge, promote development of automated approaches which are replicable,\ngeneralizable and accurate, to aid in these tasks. The current study presents a\nnovel adaptation of existing nnU-Net approaches for pediatric brain tumor\nsegmentation, submitted to the BraTS-PEDs 2024 challenge. We apply an adapted\nnnU-Net with hierarchical cascades to the segmentation task of the BraTS-PEDs\n2024 challenge. The residual encoder variant of nnU-Net, used as our baseline\nmodel, already provides high quality segmentations. We incorporate multiple\nchanges to the implementation of nnU-Net and devise a novel two-stage cascaded\nnnU-Net to segment the substructures of brain tumors from coarse to fine. Using\noutputs from the nnU-Net Residual Encoder (trained to segment CC, ED, ET and\nNET tumor labels from T1w, T1w-CE, T2w and T2-FLAIR MRI), these are passed to\ntwo additional models one classifying ET versus NET and a second classifying CC\nvs ED using cascade learning. We use radiological guidelines to steer which\nmulti parametric MRI (mpMRI) to use in these cascading models. Compared to a\ndefault nnU-Net and an ensembled nnU-net as baseline approaches, our novel\nmethod provides robust segmentations for the BraTS-PEDs 2024 challenge,\nachieving mean Dice scores of 0.657, 0.904, 0.703, and 0.967, and HD95 of 76.2,\n10.1, 111.0, and 12.3 for the ET, NET, CC and ED, respectively.\n","authors":["Timothy Mulvany","Daniel Griffiths-King","Jan Novak","Heather Rose"],"pdf_url":"https://arxiv.org/pdf/2410.14020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14017v1","updated":"2024-10-17T20:32:43Z","published":"2024-10-17T20:32:43Z","title":"Probabilistic U-Net with Kendall Shape Spaces for Geometry-Aware\n  Segmentations of Images","summary":"  One of the fundamental problems in computer vision is image segmentation, the\ntask of detecting distinct regions or objects in given images. Deep Neural\nNetworks (DNN) have been shown to be very effective in segmenting challenging\nimages, producing convincing segmentations. There is further need for\nprobabilistic DNNs that can reflect the uncertainties from the input images and\nthe models into the computed segmentations, in other words, new DNNs that can\ngenerate multiple plausible segmentations and their distributions depending on\nthe input or the model uncertainties. While there are existing probabilistic\nsegmentation models, many of them do not take into account the geometry or\nshape underlying the segmented regions. In this paper, we propose a\nprobabilistic image segmentation model that can incorporate the geometry of a\nsegmentation. Our proposed model builds on the Probabilistic U-Net of\n\\cite{kohl2018probabilistic} to generate probabilistic segmentations, i.e.\\!\nmultiple likely segmentations for an input image. Our model also adopts the\nKendall Shape Variational Auto-Encoder of \\cite{vadgama2023kendall} to encode a\nKendall shape space in the latent variable layers of the prior and posterior\nnetworks of the Probabilistic U-Net. Incorporating the shape space in this\nmanner leads to a more robust segmentation with spatially coherent regions,\nrespecting the underlying geometry in the input images.\n","authors":["Jiyoung Park","Günay Doğan"],"pdf_url":"https://arxiv.org/pdf/2410.14017v1.pdf","comment":"22 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.13989v1","updated":"2024-10-17T19:41:34Z","published":"2024-10-17T19:41:34Z","title":"Reproducibility study of \"LICO: Explainable Models with Language-Image\n  Consistency\"","summary":"  The growing reproducibility crisis in machine learning has brought forward a\nneed for careful examination of research findings. This paper investigates the\nclaims made by Lei et al. (2023) regarding their proposed method, LICO, for\nenhancing post-hoc interpretability techniques and improving image\nclassification performance. LICO leverages natural language supervision from a\nvision-language model to enrich feature representations and guide the learning\nprocess. We conduct a comprehensive reproducibility study, employing (Wide)\nResNets and established interpretability methods like Grad-CAM and RISE. We\nwere mostly unable to reproduce the authors' results. In particular, we did not\nfind that LICO consistently led to improved classification performance or\nimprovements in quantitative and qualitative measures of interpretability.\nThus, our findings highlight the importance of rigorous evaluation and\ntransparent reporting in interpretability research.\n","authors":["Luan Fletcher","Robert van der Klis","Martin Sedláček","Stefan Vasilev","Christos Athanasiadis"],"pdf_url":"https://arxiv.org/pdf/2410.13989v1.pdf","comment":"15 pages, 2 figures, Machine Learning Reproducibility Challenge 2024"},{"id":"http://arxiv.org/abs/2309.17329v3","updated":"2024-10-17T19:23:45Z","published":"2023-09-29T15:40:58Z","title":"Efficient Anatomical Labeling of Pulmonary Tree Structures via Deep\n  Point-Graph Representation-based Implicit Fields","summary":"  Pulmonary diseases rank prominently among the principal causes of death\nworldwide. Curing them will require, among other things, a better understanding\nof the complex 3D tree-shaped structures within the pulmonary system, such as\nairways, arteries, and veins. Traditional approaches using high-resolution\nimage stacks and standard CNNs on dense voxel grids face challenges in\ncomputational efficiency, limited resolution, local context, and inadequate\npreservation of shape topology. Our method addresses these issues by shifting\nfrom dense voxel to sparse point representation, offering better memory\nefficiency and global context utilization. However, the inherent sparsity in\npoint representation can lead to a loss of crucial connectivity in tree-shaped\nstructures. To mitigate this, we introduce graph learning on skeletonized\nstructures, incorporating differentiable feature fusion for improved topology\nand long-distance context capture. Furthermore, we employ an implicit function\nfor efficient conversion of sparse representations into dense reconstructions\nend-to-end. The proposed method not only delivers state-of-the-art performance\nin labeling accuracy, both overall and at key locations, but also enables\nefficient inference and the generation of closed surface shapes. Addressing\ndata scarcity in this field, we have also curated a comprehensive dataset to\nvalidate our approach. Data and code are available at\n\\url{https://github.com/M3DV/pulmonary-tree-labeling}.\n","authors":["Kangxian Xie","Jiancheng Yang","Donglai Wei","Ziqiao Weng","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2309.17329v3.pdf","comment":"Accepted by Medical Image Analysis"},{"id":"http://arxiv.org/abs/2406.01029v4","updated":"2024-10-17T19:05:50Z","published":"2024-06-03T06:24:55Z","title":"CYCLO: Cyclic Graph Transformer Approach to Multi-Object Relationship\n  Modeling in Aerial Videos","summary":"  Video scene graph generation (VidSGG) has emerged as a transformative\napproach to capturing and interpreting the intricate relationships among\nobjects and their temporal dynamics in video sequences. In this paper, we\nintroduce the new AeroEye dataset that focuses on multi-object relationship\nmodeling in aerial videos. Our AeroEye dataset features various drone scenes\nand includes a visually comprehensive and precise collection of predicates that\ncapture the intricate relationships and spatial arrangements among objects. To\nthis end, we propose the novel Cyclic Graph Transformer (CYCLO) approach that\nallows the model to capture both direct and long-range temporal dependencies by\ncontinuously updating the history of interactions in a circular manner. The\nproposed approach also allows one to handle sequences with inherent cyclical\npatterns and process object relationships in the correct sequential order.\nTherefore, it can effectively capture periodic and overlapping relationships\nwhile minimizing information loss. The extensive experiments on the AeroEye\ndataset demonstrate the effectiveness of the proposed CYCLO model,\ndemonstrating its potential to perform scene understanding on drone videos.\nFinally, the CYCLO method consistently achieves State-of-the-Art (SOTA) results\non two in-the-wild scene graph generation benchmarks, i.e., PVSG and ASPIRe.\n","authors":["Trong-Thuan Nguyen","Pha Nguyen","Xin Li","Jackson Cothren","Alper Yilmaz","Khoa Luu"],"pdf_url":"https://arxiv.org/pdf/2406.01029v4.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.13976v1","updated":"2024-10-17T19:02:31Z","published":"2024-10-17T19:02:31Z","title":"Debiasing Large Vision-Language Models by Ablating Protected Attribute\n  Representations","summary":"  Large Vision Language Models (LVLMs) such as LLaVA have demonstrated\nimpressive capabilities as general-purpose chatbots that can engage in\nconversations about a provided input image. However, their responses are\ninfluenced by societal biases present in their training datasets, leading to\nundesirable differences in how the model responds when presented with images\ndepicting people of different demographics. In this work, we propose a novel\ndebiasing framework for LVLMs by directly ablating biased attributes during\ntext generation to avoid generating text related to protected attributes, or\neven representing them internally. Our method requires no training and a\nrelatively small amount of representative biased outputs (~1000 samples). Our\nexperiments show that not only can we can minimize the propensity of LVLMs to\ngenerate text related to protected attributes, but we can even use synthetic\ndata to inform the ablation while retaining captioning performance on real data\nsuch as COCO. Furthermore, we find the resulting generations from a debiased\nLVLM exhibit similar accuracy as a baseline biased model, showing that\ndebiasing effects can be achieved without sacrificing model performance.\n","authors":["Neale Ratzlaff","Matthew Lyle Olson","Musashi Hinck","Shao-Yen Tseng","Vasudev Lal","Phillip Howard"],"pdf_url":"https://arxiv.org/pdf/2410.13976v1.pdf","comment":"NeurIPS workshop on SafeGenAI, 10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.13952v1","updated":"2024-10-17T18:22:50Z","published":"2024-10-17T18:22:50Z","title":"Satellite Streaming Video QoE Prediction: A Real-World Subjective\n  Database and Network-Level Prediction Models","summary":"  Demand for streaming services, including satellite, continues to exhibit\nunprecedented growth. Internet Service Providers find themselves at the\ncrossroads of technological advancements and rising customer expectations. To\nstay relevant and competitive, these ISPs must ensure their networks deliver\noptimal video streaming quality, a key determinant of user satisfaction.\nTowards this end, it is important to have accurate Quality of Experience\nprediction models in place. However, achieving robust performance by these\nmodels requires extensive data sets labeled by subjective opinion scores on\nvideos impaired by diverse playback disruptions. To bridge this data gap, we\nintroduce the LIVE-Viasat Real-World Satellite QoE Database. This database\nconsists of 179 videos recorded from real-world streaming services affected by\nvarious authentic distortion patterns. We also conducted a comprehensive\nsubjective study involving 54 participants, who contributed both\ncontinuous-time opinion scores and endpoint (retrospective) QoE scores. Our\nanalysis sheds light on various determinants influencing subjective QoE, such\nas stall events, spatial resolutions, bitrate, and certain network parameters.\nWe demonstrate the usefulness of this unique new resource by evaluating the\nefficacy of prevalent QoE-prediction models on it. We also created a new model\nthat maps the network parameters to predicted human perception scores, which\ncan be used by ISPs to optimize the video streaming quality of their networks.\nOur proposed model, which we call SatQA, is able to accurately predict QoE\nusing only network parameters, without any access to pixel data or\nvideo-specific metadata, estimated by Spearman's Rank Order Correlation\nCoefficient (SROCC), Pearson Linear Correlation Coefficient (PLCC), and Root\nMean Squared Error (RMSE), indicating high accuracy and reliability.\n","authors":["Bowen Chen","Zaixi Shang","Jae Won Chung","David Lerner","Werner Robitza","Rakesh Rao Ramachandra Rao","Alexander Raake","Alan C. Bovik"],"pdf_url":"https://arxiv.org/pdf/2410.13952v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.13765v1","updated":"2024-10-17T17:03:23Z","published":"2024-10-17T17:03:23Z","title":"Knowledge-Aware Query Expansion with Large Language Models for Textual\n  and Relational Retrieval","summary":"  Large language models (LLMs) have been used to generate query expansions\naugmenting original queries for improving information search. Recent studies\nalso explore providing LLMs with initial retrieval results to generate query\nexpansions more grounded to document corpus. However, these methods mostly\nfocus on enhancing textual similarities between search queries and target\ndocuments, overlooking document relations. For queries like \"Find me a highly\nrated camera for wildlife photography compatible with my Nikon F-Mount lenses\",\nexisting methods may generate expansions that are semantically similar but\nstructurally unrelated to user intents. To handle such semi-structured queries\nwith both textual and relational requirements, in this paper we propose a\nknowledge-aware query expansion framework, augmenting LLMs with structured\ndocument relations from knowledge graph (KG). To further address the limitation\nof entity-based scoring in existing KG-based methods, we leverage document\ntexts as rich KG node representations and use document-based relation filtering\nfor our Knowledge-Aware Retrieval (KAR). Extensive experiments on three\ndatasets of diverse domains show the advantages of our method compared against\nstate-of-the-art baselines on textual and relational semi-structured retrieval.\n","authors":["Yu Xia","Junda Wu","Sungchul Kim","Tong Yu","Ryan A. Rossi","Haoliang Wang","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2410.13765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13707v1","updated":"2024-10-17T16:07:51Z","published":"2024-10-17T16:07:51Z","title":"Disjointness Violations in Wikidata","summary":"  Disjointness checks are among the most important constraint checks in a\nknowledge base and can be used to help detect and correct incorrect statements\nand internal contradictions. Wikidata is a very large, community-managed\nknowledge base. Because of both its size and construction, Wikidata contains\nmany incorrect statements and internal contradictions. We analyze the current\nmodeling of disjointness on Wikidata, identify patterns that cause these\ndisjointness violations and categorize them. We use SPARQL queries to identify\neach ``culprit'' causing a disjointness violation and lay out formulas to\nidentify and fix conflicting information. We finally discuss how disjointness\ninformation could be better modeled and expanded in Wikidata in the future.\n","authors":["Ege Atacan Doğan","Peter F. Patel-Schneider"],"pdf_url":"https://arxiv.org/pdf/2410.13707v1.pdf","comment":"Sixth International Knowledge Graph and Semantic Web Conference"},{"id":"http://arxiv.org/abs/2410.13680v1","updated":"2024-10-17T15:40:09Z","published":"2024-10-17T15:40:09Z","title":"Pessimistic Evaluation","summary":"  Traditional evaluation of information access systems has focused primarily on\naverage utility across a set of information needs (information retrieval) or\nusers (recommender systems). In this work, we argue that evaluating only with\naverage metric measurements assumes utilitarian values not aligned with\ntraditions of information access based on equal access. We advocate for\npessimistic evaluation of information access systems focusing on worst case\nutility. These methods are (a) grounded in ethical and pragmatic concepts, (b)\ntheoretically complementary to existing robustness and fairness methods, and\n(c) empirically validated across a set of retrieval and recommendation tasks.\nThese results suggest that pessimistic evaluation should be included in\nexisting experimentation processes to better understand the behavior of\nsystems, especially when concerned with principles of social good.\n","authors":["Fernando Diaz"],"pdf_url":"https://arxiv.org/pdf/2410.13680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13604v1","updated":"2024-10-17T14:39:24Z","published":"2024-10-17T14:39:24Z","title":"Large Language Models as Narrative-Driven Recommenders","summary":"  Narrative-driven recommenders aim to provide personalized suggestions for\nuser requests expressed in free-form text such as \"I want to watch a thriller\nwith a mind-bending story, like Shutter Island.\" Although large language models\n(LLMs) have been shown to excel in processing general natural language queries,\ntheir effectiveness for handling such recommendation requests remains\nrelatively unexplored. To close this gap, we compare the performance of 38\nopen- and closed-source LLMs of various sizes, such as LLama 3.2 and GPT-4o, in\na movie recommendation setting. For this, we utilize a gold-standard,\ncrowdworker-annotated dataset of posts from reddit's movie suggestion community\nand employ various prompting strategies, including zero-shot, identity, and\nfew-shot prompting. Our findings demonstrate the ability of LLMs to generate\ncontextually relevant movie recommendations, significantly outperforming other\nstate-of-the-art approaches, such as doc2vec. While we find that closed-source\nand large-parameterized models generally perform best, medium-sized open-source\nmodels remain competitive, being only slightly outperformed by their more\ncomputationally expensive counterparts. Furthermore, we observe no significant\ndifferences across prompting strategies for most models, underscoring the\neffectiveness of simple approaches such as zero-shot prompting for\nnarrative-driven recommendations. Overall, this work offers valuable insights\nfor recommender system researchers as well as practitioners aiming to integrate\nLLMs into real-world recommendation tools.\n","authors":["Lukas Eberhard","Thorsten Ruprechter","Denis Helic"],"pdf_url":"https://arxiv.org/pdf/2410.13604v1.pdf","comment":"Under review; 19 pages"},{"id":"http://arxiv.org/abs/2410.13588v1","updated":"2024-10-17T14:22:57Z","published":"2024-10-17T14:22:57Z","title":"Cross-Domain Sequential Recommendation via Neural Process","summary":"  Cross-Domain Sequential Recommendation (CDSR) is a hot topic in\nsequence-based user interest modeling, which aims at utilizing a single model\nto predict the next items for different domains. To tackle the CDSR, many\nmethods are focused on domain overlapped users' behaviors fitting, which\nheavily relies on the same user's different-domain item sequences collaborating\nsignals to capture the synergy of cross-domain item-item correlation. Indeed,\nthese overlapped users occupy a small fraction of the entire user set only,\nwhich introduces a strong assumption that the small group of domain overlapped\nusers is enough to represent all domain user behavior characteristics. However,\nintuitively, such a suggestion is biased, and the insufficient learning\nparadigm in non-overlapped users will inevitably limit model performance.\nFurther, it is not trivial to model non-overlapped user behaviors in CDSR\nbecause there are no other domain behaviors to collaborate with, which causes\nthe observed single-domain users' behavior sequences to be hard to contribute\nto cross-domain knowledge mining. Considering such a phenomenon, we raise a\nchallenging and unexplored question: How to unleash the potential of\nnon-overlapped users' behaviors to empower CDSR?\n","authors":["Haipeng Li","Jiangxia Cao","Yiwen Gao","Yunhuai Liu","Shuchao Pang"],"pdf_url":"https://arxiv.org/pdf/2410.13588v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2409.20305v2","updated":"2024-10-17T13:19:08Z","published":"2024-09-30T14:04:27Z","title":"Mixed-Precision Embeddings for Large-Scale Recommendation Models","summary":"  Embedding techniques have become essential components of large databases in\nthe deep learning era. By encoding discrete entities, such as words, items, or\ngraph nodes, into continuous vector spaces, embeddings facilitate more\nefficient storage, retrieval, and processing in large databases. Especially in\nthe domain of recommender systems, millions of categorical features are encoded\nas unique embedding vectors, which facilitates the modeling of similarities and\ninteractions among features. However, numerous embedding vectors can result in\nsignificant storage overhead. In this paper, we aim to compress the embedding\ntable through quantization techniques. Given that features vary in importance\nlevels, we seek to identify an appropriate precision for each feature to\nbalance model accuracy and memory usage. To this end, we propose a novel\nembedding compression method, termed Mixed-Precision Embeddings (MPE).\nSpecifically, to reduce the size of the search space, we first group features\nby frequency and then search precision for each feature group. MPE further\nlearns the probability distribution over precision levels for each feature\ngroup, which can be used to identify the most suitable precision with a\nspecially designed sampling strategy. Extensive experiments on three public\ndatasets demonstrate that MPE significantly outperforms existing embedding\ncompression methods. Remarkably, MPE achieves about 200x compression on the\nCriteo dataset without comprising the prediction accuracy.\n","authors":["Shiwei Li","Zhuoqi Hu","Xing Tang","Haozhao Wang","Shijie Xu","Weihong Luo","Yuhua Li","Xiuqiang He","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2409.20305v2.pdf","comment":"under submision"},{"id":"http://arxiv.org/abs/2406.16350v2","updated":"2024-10-17T10:59:08Z","published":"2024-06-24T06:46:32Z","title":"A Survey on Intent-aware Recommender Systems","summary":"  Many modern online services feature personalized recommendations. A central\nchallenge when providing such recommendations is that the reason why an\nindividual user accesses the service may change from visit to visit or even\nduring an ongoing usage session. To be effective, a recommender system should\ntherefore aim to take the users' probable intent of using the service at a\ncertain point in time into account. In recent years, researchers have thus\nstarted to address this challenge by incorporating intent-awareness into\nrecommender systems. Correspondingly, a number of technical approaches were put\nforward, including diversification techniques, intent prediction models or\nlatent intent modeling approaches. In this paper, we survey and categorize\nexisting approaches to building the next generation of Intent-Aware Recommender\nSystems (IARS). Based on an analysis of current evaluation practices, we\noutline open gaps and possible future directions in this area, which in\nparticular include the consideration of additional interaction signals and\ncontextual information to further improve the effectiveness of such systems.\n","authors":["Dietmar Jannach","Markus Zanker"],"pdf_url":"https://arxiv.org/pdf/2406.16350v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13428v1","updated":"2024-10-17T10:51:34Z","published":"2024-10-17T10:51:34Z","title":"Generate and Instantiate What You Prefer: Text-Guided Diffusion for\n  Sequential Recommendation","summary":"  Recent advancements in generative recommendation systems, particularly in the\nrealm of sequential recommendation tasks, have shown promise in enhancing\ngeneralization to new items. Among these approaches, diffusion-based generative\nrecommendation has emerged as an effective tool, leveraging its ability to\ncapture data distributions and generate high-quality samples. Despite\neffectiveness, two primary challenges have been identified: 1) the lack of\nconsistent modeling of data distribution for oracle items; and 2) the\ndifficulty in scaling to more informative control signals beyond historical\ninteractions. These issues stem from the uninformative nature of ID embeddings,\nwhich necessitate random initialization and limit the incorporation of\nadditional control signals. To address these limitations, we propose iDreamRe }\nto involve more concrete prior knowledge to establish item embeddings,\nparticularly through detailed item text descriptions and advanced Text\nEmbedding Models (TEM). More importantly, by converting item descriptions into\nembeddings aligned with TEM, we enable the integration of intention\ninstructions as control signals to guide the generation of oracle items.\nExperimental results on four datasets demonstrate that iDreamRec not only\noutperforms existing diffusion-based generative recommenders but also\nfacilitates the incorporation of intention instructions for more precise and\neffective recommendation generation.\n","authors":["Guoqing Hu","Zhangyi Yang","Zhibo Cai","An Zhang","Xiang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13374v1","updated":"2024-10-17T09:24:40Z","published":"2024-10-17T09:24:40Z","title":"Context-aware adaptive personalised recommendation: a meta-hybrid","summary":"  Recommenders take place on a wide scale of e-commerce systems, reducing the\nproblem of information overload. The most common approach is to choose a\nrecommender used by the system to make predictions. However, users vary from\neach other; thus, a one-fits-all approach seems to be sub-optimal. In this\npaper, we propose a meta-hybrid recommender that uses machine learning to\npredict an optimal algorithm. In this way, the best-performing recommender is\nused for each specific session and user. This selection depends on contextual\nand preferential information collected about the user. We use standard\nMovieLens and The Movie DB datasets for offline evaluation. We show that based\non the proposed model, it is possible to predict which recommender will provide\nthe most precise recommendations to a user. The theoretical performance of our\nmeta-hybrid outperforms separate approaches by 20-50% in normalized Discounted\nGain and Root Mean Square Error metrics. However, it is hard to obtain the\noptimal performance based on widely-used standard information stored about\nusers.\n","authors":["Peter Tibensky","Michal Kompan"],"pdf_url":"https://arxiv.org/pdf/2410.13374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00800v2","updated":"2024-10-17T09:13:18Z","published":"2024-07-22T11:58:36Z","title":"Chatbot-Based Ontology Interaction Using Large Language Models and\n  Domain-Specific Standards","summary":"  The following contribution introduces a concept that employs Large Language\nModels (LLMs) and a chatbot interface to enhance SPARQL query generation for\nontologies, thereby facilitating intuitive access to formalized knowledge.\nUtilizing natural language inputs, the system converts user inquiries into\naccurate SPARQL queries that strictly query the factual content of the\nontology, effectively preventing misinformation or fabrication by the LLM. To\nenhance the quality and precision of outcomes, additional textual information\nfrom established domain-specific standards is integrated into the ontology for\nprecise descriptions of its concepts and relationships. An experimental study\nassesses the accuracy of generated SPARQL queries, revealing significant\nbenefits of using LLMs for querying ontologies and highlighting areas for\nfuture research.\n","authors":["Jonathan Reif","Tom Jeleniewski","Milapji Singh Gill","Felix Gehlhoff","Alexander Fay"],"pdf_url":"https://arxiv.org/pdf/2408.00800v2.pdf","comment":"\\c{opyright} 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2410.13326v1","updated":"2024-10-17T08:37:25Z","published":"2024-10-17T08:37:25Z","title":"Comparing the Utility, Preference, and Performance of Course Material\n  Search Functionality and Retrieval-Augmented Generation Large Language Model\n  (RAG-LLM) AI Chatbots in Information-Seeking Tasks","summary":"  Providing sufficient support for students requires substantial resources,\nespecially considering the growing enrollment numbers. Students need help in a\nvariety of tasks, ranging from information-seeking to requiring support with\ncourse assignments. To explore the utility of recent large language models\n(LLMs) as a support mechanism, we developed an LLM-powered AI chatbot that\naugments the answers that are produced with information from the course\nmaterials. To study the effect of the LLM-powered AI chatbot, we conducted a\nlab-based user study (N=14), in which the participants worked on tasks from a\nweb software development course. The participants were divided into two groups,\nwhere one of the groups first had access to the chatbot and then to a more\ntraditional search functionality, while another group started with the search\nfunctionality and was then given the chatbot. We assessed the participants'\nperformance and perceptions towards the chatbot and the search functionality\nand explored their preferences towards the support functionalities. Our\nfindings highlight that both support mechanisms are seen as useful and that\nsupport mechanisms work well for specific tasks, while less so for other tasks.\nWe also observe that students tended to prefer the second support mechanism\nmore, where students who were first given the chatbot tended to prefer the\nsearch functionality and vice versa.\n","authors":["Leonardo Pasquarelli","Charles Koutcheme","Arto Hellas"],"pdf_url":"https://arxiv.org/pdf/2410.13326v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.10025v2","updated":"2024-10-17T08:16:52Z","published":"2024-09-16T06:33:26Z","title":"DiffATR: Diffusion-based Generative Modeling for Audio-Text Retrieval","summary":"  Existing audio-text retrieval (ATR) methods are essentially discriminative\nmodels that aim to maximize the conditional likelihood, represented as\np(candidates|query). Nevertheless, this methodology fails to consider the\nintrinsic data distribution p(query), leading to difficulties in discerning\nout-of-distribution data. In this work, we attempt to tackle this constraint\nthrough a generative perspective and model the relationship between audio and\ntext as their joint probability p(candidates,query). To this end, we present a\ndiffusion-based ATR framework (DiffATR), which models ATR as an iterative\nprocedure that progressively generates joint distribution from noise.\nThroughout its training phase, DiffATR is optimized from both generative and\ndiscriminative viewpoints: the generator is refined through a generation loss,\nwhile the feature extractor benefits from a contrastive loss, thus combining\nthe merits of both methodologies. Experiments on the AudioCaps and Clotho\ndatasets with superior performances, verify the effectiveness of our approach.\nNotably, without any alterations, our DiffATR consistently exhibits strong\nperformance in out-of-domain retrieval settings.\n","authors":["Yifei Xin","Xuxin Cheng","Zhihong Zhu","Xusheng Yang","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2409.10025v2.pdf","comment":"Accepted by Interspeech2024"},{"id":"http://arxiv.org/abs/2410.13293v1","updated":"2024-10-17T07:46:49Z","published":"2024-10-17T07:46:49Z","title":"SBI-RAG: Enhancing Math Word Problem Solving for Students through\n  Schema-Based Instruction and Retrieval-Augmented Generation","summary":"  Many students struggle with math word problems (MWPs), often finding it\ndifficult to identify key information and select the appropriate mathematical\noperations.Schema-based instruction (SBI) is an evidence-based strategy that\nhelps students categorize problems based on their structure, improving\nproblem-solving accuracy. Building on this, we propose a Schema-Based\nInstruction Retrieval-Augmented Generation (SBI-RAG) framework that\nincorporates a large language model (LLM).Our approach emphasizes step-by-step\nreasoning by leveraging schemas to guide solution generation. We evaluate its\nperformance on the GSM8K dataset, comparing it with GPT-4 and GPT-3.5 Turbo,\nand introduce a \"reasoning score\" metric to assess solution quality. Our\nfindings suggest that SBI-RAG enhances reasoning clarity and problem-solving\naccuracy, potentially providing educational benefits for students\n","authors":["Prakhar Dixit","Tim Oates"],"pdf_url":"https://arxiv.org/pdf/2410.13293v1.pdf","comment":"Accepted to the 4th MATH-AI Workshop at NeurIPS'24"},{"id":"http://arxiv.org/abs/2410.13248v1","updated":"2024-10-17T06:15:00Z","published":"2024-10-17T06:15:00Z","title":"Disentangling Likes and Dislikes in Personalized Generative Explainable\n  Recommendation","summary":"  Recent research on explainable recommendation generally frames the task as a\nstandard text generation problem, and evaluates models simply based on the\ntextual similarity between the predicted and ground-truth explanations.\nHowever, this approach fails to consider one crucial aspect of the systems:\nwhether their outputs accurately reflect the users' (post-purchase) sentiments,\ni.e., whether and why they would like and/or dislike the recommended items. To\nshed light on this issue, we introduce new datasets and evaluation methods that\nfocus on the users' sentiments. Specifically, we construct the datasets by\nexplicitly extracting users' positive and negative opinions from their\npost-purchase reviews using an LLM, and propose to evaluate systems based on\nwhether the generated explanations 1) align well with the users' sentiments,\nand 2) accurately identify both positive and negative opinions of users on the\ntarget items. We benchmark several recent models on our datasets and\ndemonstrate that achieving strong performance on existing metrics does not\nensure that the generated explanations align well with the users' sentiments.\nLastly, we find that existing models can provide more sentiment-aware\nexplanations when the users' (predicted) ratings for the target items are\ndirectly fed into the models as input. We will release our code and datasets\nupon acceptance.\n","authors":["Ryotaro Shimizu","Takashi Wada","Yu Wang","Johannes Kruse","Sean O'Brien","Sai HtaungKham","Linxin Song","Yuya Yoshikawa","Yuki Saito","Fugee Tsung","Masayuki Goto","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2410.13248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13230v1","updated":"2024-10-17T05:33:50Z","published":"2024-10-17T05:33:50Z","title":"Starbucks: Improved Training for 2D Matryoshka Embeddings","summary":"  Effective approaches that can scale embedding model depth (i.e. layers) and\nembedding size allow for the creation of models that are highly scalable across\ndifferent computational resources and task requirements. While the recently\nproposed 2D Matryoshka training approach can efficiently produce a single\nembedding model such that its sub-layers and sub-dimensions can measure text\nsimilarity, its effectiveness is significantly worse than if smaller models\nwere trained separately. To address this issue, we propose Starbucks, a new\ntraining strategy for Matryoshka-like embedding models, which encompasses both\nthe fine-tuning and pre-training phases. For the fine-tuning phase, we discover\nthat, rather than sampling a random sub-layer and sub-dimensions for each\ntraining steps, providing a fixed list of layer-dimension pairs, from small\nsize to large sizes, and computing the loss across all pairs significantly\nimproves the effectiveness of 2D Matryoshka embedding models, bringing them on\npar with their separately trained counterparts. To further enhance performance,\nwe introduce a new pre-training strategy, which applies masked autoencoder\nlanguage modelling to sub-layers and sub-dimensions during pre-training,\nresulting in a stronger backbone for subsequent fine-tuning of the embedding\nmodel. Experimental results on both semantic text similarity and retrieval\nbenchmarks demonstrate that the proposed pre-training and fine-tuning\nstrategies significantly improved the effectiveness over 2D Matryoshka models,\nenabling Starbucks models to perform more efficiently and effectively than\nseparately trained models.\n","authors":["Shengyao Zhuang","Shuai Wang","Bevan Koopman","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2410.13230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13226v1","updated":"2024-10-17T05:17:01Z","published":"2024-10-17T05:17:01Z","title":"Research on Travel Route Planing Problems Based on Greedy Algorithm","summary":"  The greedy algorithm based route planning problem is a method of finding the\noptimal or near optimal route between a given starting and ending point. This\narticle first uses PCA method to reduce the dimensionality of urban evaluation\nindicators, extracts key principal components, and KMO and TOPSIS algorithms to\nreduce the dimensionality of the data. Secondly, for datasets that have not\npassed the KMO test, a comprehensive evaluation will be conducted using the\nentropy weight method and TOPSIS method. Finally, based on the greedy\nalgorithm, a route planning algorithm was proposed and optimized to provide\npersonalized route customization according to the different needs of tourists.\nWe also took into account the local travel efficiency, the time required to\nvisit tourist attractions, and necessary daily rest time to reduce costs and\navoid falling into the local optimal solution.\n","authors":["Yiquan Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13217v1","updated":"2024-10-17T04:48:06Z","published":"2024-10-17T04:48:06Z","title":"MixEHR-Nest: Identifying Subphenotypes within Electronic Health Records\n  through Hierarchical Guided-Topic Modeling","summary":"  Automatic subphenotyping from electronic health records (EHRs)provides\nnumerous opportunities to understand diseases with unique subgroups and enhance\npersonalized medicine for patients. However, existing machine learning\nalgorithms either focus on specific diseases for better interpretability or\nproduce coarse-grained phenotype topics without considering nuanced disease\npatterns. In this study, we propose a guided topic model, MixEHR-Nest, to infer\nsub-phenotype topics from thousands of disease using multi-modal EHR data.\nSpecifically, MixEHR-Nest detects multiple subtopics from each phenotype topic,\nwhose prior is guided by the expert-curated phenotype concepts such as\nPhenotype Codes (PheCodes) or Clinical Classification Software (CCS) codes. We\nevaluated MixEHR-Nest on two EHR datasets: (1) the MIMIC-III dataset consisting\nof over 38 thousand patients from intensive care unit (ICU) from Beth Israel\nDeaconess Medical Center (BIDMC) in Boston, USA; (2) the healthcare\nadministrative database PopHR, comprising 1.3 million patients from Montreal,\nCanada. Experimental results demonstrate that MixEHR-Nest can identify\nsubphenotypes with distinct patterns within each phenotype, which are\npredictive for disease progression and severity. Consequently, MixEHR-Nest\ndistinguishes between type 1 and type 2 diabetes by inferring subphenotypes\nusing CCS codes, which do not differentiate these two subtype concepts.\nAdditionally, MixEHR-Nest not only improved the prediction accuracy of\nshort-term mortality of ICU patients and initial insulin treatment in diabetic\npatients but also revealed the contributions of subphenotypes. For longitudinal\nanalysis, MixEHR-Nest identified subphenotypes of distinct age prevalence under\nthe same phenotypes, such as asthma, leukemia, epilepsy, and depression. The\nMixEHR-Nest software is available at GitHub:\nhttps://github.com/li-lab-mcgill/MixEHR-Nest.\n","authors":["Ruohan Wang","Zilong Wang","Ziyang Song","David Buckeridge","Yue Li"],"pdf_url":"https://arxiv.org/pdf/2410.13217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05666v7","updated":"2024-10-17T03:23:49Z","published":"2024-06-09T06:49:22Z","title":"Probability Distribution Learning: A theoretical framework for Deep\n  Learning","summary":"  This paper introduces probability distribution learning (PD learning), a\nnovel theoretical learning framework. Departing from the traditional\nstatistical learning framework, PD learning focuses on learning the underlying\nprobability distribution, which is modeled as a random variable within the\nprobability simplex. Within this framework, the learning error is decomposed\ninto uncertainty, estimation error, and the model's fitting error.\nSubsequently, we present the methodology for calculating uncertainty, along\nwith optimization strategies for both estimation error and fitting error. Given\nthat minimizing the fitting error typically constitutes a non-convex\noptimization problem, we introduce a standard loss function and the gradient\nstructural control (GSC) algorithm, and demonstrate that by employing this\nfunction, the optima of fitting error minimization can be approached by\nreducing the gradient norm and structural error. Furthermore, we apply the PD\nlearning framework to deep learning, elucidating the mechanisms by which\ntechniques such as random parameter initialization, over-parameterization,\nbias-variance trade-off, and dropout influence deep model training. Finally,\nexperimental results on various models validate the effectiveness of the\nproposed framework.\n","authors":["Binchuan Qi","Wei Gong","Li Li"],"pdf_url":"https://arxiv.org/pdf/2406.05666v7.pdf","comment":"arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors. arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors"},{"id":"http://arxiv.org/abs/2402.16872v2","updated":"2024-10-17T02:53:23Z","published":"2024-01-29T03:30:15Z","title":"NFT1000: A Cross-Modal Dataset for Non-Fungible Token Retrieval","summary":"  With the rise of \"Metaverse\" and \"Web 3.0\", Non-Fungible Token (NFT) has\nemerged as a kind of pivotal digital asset, garnering significant attention. By\nthe end of March 2024, more than 1.7 billion NFTs have been minted across\nvarious blockchain platforms. To effectively locate a desired NFT, conducting\nsearches within a vast array of NFTs is essential. The challenge in NFT\nretrieval is heightened due to the high degree of similarity among different\nNFTs, regarding regional and semantic aspects. In this paper, we will introduce\na benchmark dataset named \"NFT Top1000 Visual-Text Dataset\" (NFT1000),\ncontaining 7.56 million image-text pairs, and being collected from 1000 most\nfamous PFP1 NFT collections2 by sales volume on the Ethereum blockchain. Based\non this dataset and leveraging the CLIP series of pre-trained models as our\nfoundation, we propose the dynamic masking fine-tuning scheme. This innovative\napproach results in a 7.4\\% improvement in the top1 accuracy rate, while\nutilizing merely 13\\% of the total training data (0.79 million vs. 6.1\nmillion). We also propose a robust metric Comprehensive Variance Index (CVI) to\nassess the similarity and retrieval difficulty of visual-text pairs data. The\ndataset will be released as an open-source resource. For more details, please\nrefer to: https://github.com/ShuxunoO/NFT-Net.git.\n","authors":["Shuxun Wang","Yunfei Lei","Ziqi Zhang","Wei Liu","Haowei Liu","Li Yang","Wenjuan Li","Bing Li","Weiming Hu"],"pdf_url":"https://arxiv.org/pdf/2402.16872v2.pdf","comment":"11 pages,12figures to be published in ACM Multimedia 2024, see\n  https://openreview.net/forum?id=xUtNrKH8iB&noteId=xUtNrKH8iB"},{"id":"http://arxiv.org/abs/2408.01262v4","updated":"2024-10-17T02:20:47Z","published":"2024-08-02T13:35:11Z","title":"RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework","summary":"  Retrieval-Augmented Generation (RAG) is a powerful approach that enables\nlarge language models (LLMs) to incorporate external knowledge. However,\nevaluating the effectiveness of RAG systems in specialized scenarios remains\nchallenging due to the high costs of data construction and the lack of suitable\nevaluation metrics. This paper introduces RAGEval, a framework designed to\nassess RAG systems across diverse scenarios by generating high-quality\ndocuments, questions, answers, and references through a schema-based pipeline.\nWith a focus on factual accuracy, we propose three novel metrics Completeness,\nHallucination, and Irrelevance to rigorously evaluate LLM-generated responses.\nExperimental results show that RAGEval outperforms zero-shot and one-shot\nmethods in terms of clarity, safety, conformity, and richness of generated\nsamples. Furthermore, the use of LLMs for scoring the proposed metrics\ndemonstrates a high level of consistency with human evaluations. RAGEval\nestablishes a new paradigm for evaluating RAG systems in real-world\napplications.\n","authors":["Kunlun Zhu","Yifan Luo","Dingling Xu","Ruobing Wang","Shi Yu","Shuo Wang","Yukun Yan","Zhenghao Liu","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2408.01262v4.pdf","comment":"https://github.com/OpenBMB/RAGEval"},{"id":"http://arxiv.org/abs/2406.00944v2","updated":"2024-10-17T02:15:11Z","published":"2024-06-03T02:56:14Z","title":"A Theory for Token-Level Harmonization in Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance\nlarge language models (LLMs). Studies show that while RAG provides valuable\nexternal information (benefit), it may also mislead LLMs (detriment) with noisy\nor incorrect retrieved texts. Although many existing methods attempt to\npreserve benefit and avoid detriment, they lack a theoretical explanation for\nRAG. The benefit and detriment in the next token prediction of RAG remain a\nblack box that cannot be quantified or compared in an explainable manner, so\nexisting methods are data-driven, need additional utility evaluators or\npost-hoc. This paper takes the first step towards providing a theory to explain\nand trade off the benefit and detriment in RAG. First, we model RAG as the\nfusion between distribution of LLMs knowledge and distribution of retrieved\ntexts. Then, we formalize the trade-off between the value of external knowledge\n(benefit) and its potential risk of misleading LLMs (detriment) in next token\nprediction of RAG by distribution difference in this fusion. Finally, we prove\nthat the actual effect of RAG on the token, which is the comparison between\nbenefit and detriment, can be predicted without any training or accessing the\nutility of retrieval. Based on our theory, we propose a practical novel method,\nTok-RAG, which achieves collaborative generation between the pure LLM and RAG\nat token level to preserve benefit and avoid detriment. Experiments in\nreal-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\neffectiveness of our method and support our theoretical findings.\n","authors":["Shicheng Xu","Liang Pang","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.00944v2.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2410.13125v1","updated":"2024-10-17T01:27:57Z","published":"2024-10-17T01:27:57Z","title":"Transformers4NewsRec: A Transformer-based News Recommendation Framework","summary":"  Pre-trained transformer models have shown great promise in various natural\nlanguage processing tasks, including personalized news recommendations. To\nharness the power of these models, we introduce Transformers4NewsRec, a new\nPython framework built on the \\textbf{Transformers} library. This framework is\ndesigned to unify and compare the performance of various news recommendation\nmodels, including deep neural networks and graph-based models.\nTransformers4NewsRec offers flexibility in terms of model selection, data\npreprocessing, and evaluation, allowing both quantitative and qualitative\nanalysis.\n","authors":["Dairui Liu","Honghui Du","Boming Yang","Neil Hurley","Aonghus Lawlor","Irene Li","Derek Greene","Ruihai Dong"],"pdf_url":"https://arxiv.org/pdf/2410.13125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13118v1","updated":"2024-10-17T01:12:48Z","published":"2024-10-17T01:12:48Z","title":"Retrieval-Enhanced Named Entity Recognition","summary":"  When combined with In-Context Learning, a technique that enables models to\nadapt to new tasks by incorporating task-specific examples or demonstrations\ndirectly within the input prompt, autoregressive language models have achieved\ngood performance in a wide range of tasks and applications. However, this\ncombination has not been properly explored in the context of named entity\nrecognition, where the structure of this task poses unique challenges. We\npropose RENER (Retrieval-Enhanced Named Entity Recognition), a technique for\nnamed entity recognition using autoregressive language models based on\nIn-Context Learning and information retrieval techniques. When presented with\nan input text, RENER fetches similar examples from a dataset of training\nexamples that are used to enhance a language model to recognize named entities\nfrom this input text. RENER is modular and independent of the underlying\nlanguage model and information retrieval algorithms. Experimental results show\nthat in the CrossNER collection we achieve state-of-the-art performance with\nthe proposed technique and that information retrieval can increase the F-score\nby up to 11 percentage points.\n","authors":["Enzo Shiraishi","Raphael Y. de Camargo","Henrique L. P. Silva","Ronaldo C. Prati"],"pdf_url":"https://arxiv.org/pdf/2410.13118v1.pdf","comment":"13 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.13117v1","updated":"2024-10-17T01:02:04Z","published":"2024-10-17T01:02:04Z","title":"Preference Diffusion for Recommendation","summary":"  Recommender systems predict personalized item rankings based on user\npreference distributions derived from historical behavior data. Recently,\ndiffusion models (DMs) have gained attention in recommendation for their\nability to model complex distributions, yet current DM-based recommenders often\nrely on traditional objectives like mean squared error (MSE) or recommendation\nobjectives, which are not optimized for personalized ranking tasks or fail to\nfully leverage DM's generative potential. To address this, we propose\nPreferDiff, a tailored optimization objective for DM-based recommenders.\nPreferDiff transforms BPR into a log-likelihood ranking objective and\nintegrates multiple negative samples to better capture user preferences.\nSpecifically, we employ variational inference to handle the intractability\nthrough minimizing the variational upper bound and replaces MSE with cosine\nerror to improve alignment with recommendation tasks. Finally, we balance\nlearning generation and preference to enhance the training stability of DMs.\nPreferDiff offers three key benefits: it is the first personalized ranking loss\ndesigned specifically for DM-based recommenders and it improves ranking and\nfaster convergence by addressing hard negatives. We also prove that it is\ntheoretically connected to Direct Preference Optimization which indicates that\nit has the potential to align user preferences in DM-based recommenders via\ngenerative modeling. Extensive experiments across three benchmarks validate its\nsuperior recommendation performance and commendable general sequential\nrecommendation capabilities. Our codes are available at\n\\url{https://github.com/lswhim/PreferDiff}.\n","authors":["Shuo Liu","An Zhang","Guoqing Hu","Hong Qian","Tat-seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.13117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.16953v2","updated":"2024-10-17T22:47:50Z","published":"2022-10-30T21:26:07Z","title":"Improving Bilingual Lexicon Induction with Cross-Encoder Reranking","summary":"  Bilingual lexicon induction (BLI) with limited bilingual supervision is a\ncrucial yet challenging task in multilingual NLP. Current state-of-the-art BLI\nmethods rely on the induction of cross-lingual word embeddings (CLWEs) to\ncapture cross-lingual word similarities; such CLWEs are obtained 1) via\ntraditional static models (e.g., VecMap), or 2) by extracting type-level CLWEs\nfrom multilingual pretrained language models (mPLMs), or 3) through combining\nthe former two options. In this work, we propose a novel semi-supervised\npost-hoc reranking method termed BLICEr (BLI with Cross-Encoder Reranking),\napplicable to any precalculated CLWE space, which improves their BLI\ncapability. The key idea is to 'extract' cross-lingual lexical knowledge from\nmPLMs, and then combine it with the original CLWEs. This crucial step is done\nvia 1) creating a word similarity dataset, comprising positive word pairs\n(i.e., true translations) and hard negative pairs induced from the original\nCLWE space, and then 2) fine-tuning an mPLM (e.g., mBERT or XLM-R) in a\ncross-encoder manner to predict the similarity scores. At inference, we 3)\ncombine the similarity score from the original CLWE space with the score from\nthe BLI-tuned cross-encoder. BLICEr establishes new state-of-the-art results on\ntwo standard BLI benchmarks spanning a wide spectrum of diverse languages: it\nsubstantially outperforms a series of strong baselines across the board. We\nalso validate the robustness of BLICEr with different CLWEs.\n","authors":["Yaoyiran Li","Fangyu Liu","Ivan Vulić","Anna Korhonen"],"pdf_url":"https://arxiv.org/pdf/2210.16953v2.pdf","comment":"Findings of EMNLP 2022"},{"id":"http://arxiv.org/abs/2410.14066v1","updated":"2024-10-17T22:28:07Z","published":"2024-10-17T22:28:07Z","title":"Lightweight Correlation-Aware Table Compression","summary":"  The growing adoption of data lakes for managing relational data necessitates\nefficient, open storage formats that provide high scan performance and\ncompetitive compression ratios. While existing formats achieve fast scans\nthrough lightweight encoding techniques, they have reached a plateau in terms\nof minimizing storage footprint. Recently, correlation-aware compression\nschemes have been shown to reduce file sizes further. Yet, current approaches\neither incur significant scan overheads or require manual specification of\ncorrelations, limiting their practicability. We present $\\texttt{Virtual}$, a\nframework that integrates seamlessly with existing open formats to\nautomatically leverage data correlations, achieving substantial compression\ngains while having minimal scan performance overhead. Experiments on\n$\\texttt{data.gov}$ datasets show that $\\texttt{Virtual}$ reduces file sizes by\nup to 40% compared to Apache Parquet.\n","authors":["Mihail Stoian","Alexander van Renen","Jan Kobiolka","Ping-Lin Kuo","Josif Grabocka","Andreas Kipf"],"pdf_url":"https://arxiv.org/pdf/2410.14066v1.pdf","comment":"Third Table Representation Learning Workshop (TRL 2024)"},{"id":"http://arxiv.org/abs/2410.12123v2","updated":"2024-10-17T22:06:32Z","published":"2024-10-15T23:51:04Z","title":"The Moral Case for Using Language Model Agents for Recommendation","summary":"  Our information and communication environment has fallen short of the ideals\nthat networked global communication might have served. Identifying all the\ncauses of its pathologies is difficult, but existing recommender systems very\nlikely play a contributing role. In this paper, which draws on the normative\ntools of philosophy of computing, informed by empirical and technical insights\nfrom natural language processing and recommender systems, we make the moral\ncase for an alternative approach. We argue that existing recommenders\nincentivise mass surveillance, concentrate power, fall prey to narrow\nbehaviourism, and compromise user agency. Rather than just trying to avoid\nalgorithms entirely, or to make incremental improvements to the current\nparadigm, researchers and engineers should explore an alternative paradigm: the\nuse of language model (LM) agents to source and curate content that matches\nusers' preferences and values, expressed in natural language. The use of LM\nagents for recommendation poses its own challenges, including those related to\ncandidate generation, computational efficiency, preference modelling, and\nprompt injection. Nonetheless, if implemented successfully LM agents could:\nguide us through the digital public sphere without relying on mass\nsurveillance; shift power away from platforms towards users; optimise for what\nmatters instead of just for behavioural proxies; and scaffold our agency\ninstead of undermining it.\n","authors":["Seth Lazar","Luke Thorburn","Tian Jin","Luca Belli"],"pdf_url":"https://arxiv.org/pdf/2410.12123v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.08307v5","updated":"2024-10-17T21:50:37Z","published":"2022-03-15T22:51:22Z","title":"Improving Word Translation via Two-Stage Contrastive Learning","summary":"  Word translation or bilingual lexicon induction (BLI) is a key cross-lingual\ntask, aiming to bridge the lexical gap between different languages. In this\nwork, we propose a robust and effective two-stage contrastive learning\nframework for the BLI task. At Stage C1, we propose to refine standard\ncross-lingual linear maps between static word embeddings (WEs) via a\ncontrastive learning objective; we also show how to integrate it into the\nself-learning procedure for even more refined cross-lingual maps. In Stage C2,\nwe conduct BLI-oriented contrastive fine-tuning of mBERT, unlocking its word\ntranslation capability. We also show that static WEs induced from the\n`C2-tuned' mBERT complement static WEs from Stage C1. Comprehensive experiments\non standard BLI datasets for diverse languages and different experimental\nsetups demonstrate substantial gains achieved by our framework. While the BLI\nmethod from Stage C1 already yields substantial gains over all state-of-the-art\nBLI methods in our comparison, even stronger improvements are met with the full\ntwo-stage framework: e.g., we report gains for 112/112 BLI setups, spanning 28\nlanguage pairs.\n","authors":["Yaoyiran Li","Fangyu Liu","Nigel Collier","Anna Korhonen","Ivan Vulić"],"pdf_url":"https://arxiv.org/pdf/2203.08307v5.pdf","comment":"ACL 2022 Main"},{"id":"http://arxiv.org/abs/2410.14044v1","updated":"2024-10-17T21:37:08Z","published":"2024-10-17T21:37:08Z","title":"Best in Tau@LLMJudge: Criteria-Based Relevance Evaluation with Llama3","summary":"  Traditional evaluation of information retrieval (IR) systems relies on\nhuman-annotated relevance labels, which can be both biased and costly at scale.\nIn this context, large language models (LLMs) offer an alternative by allowing\nus to directly prompt them to assign relevance labels for passages associated\nwith each query. In this study, we explore alternative methods to directly\nprompt LLMs for assigned relevance labels, by exploring two hypotheses:\n  Hypothesis 1 assumes that it is helpful to break down \"relevance\" into\nspecific criteria - exactness, coverage, topicality, and contextual fit. We\nexplore different approaches that prompt large language models (LLMs) to obtain\ncriteria-level grades for all passages, and we consider various ways to\naggregate criteria-level grades into a relevance label. Hypothesis 2 assumes\nthat differences in linguistic style between queries and passages may\nnegatively impact the automatic relevance label prediction. We explore whether\nimprovements can be achieved by first synthesizing a summary of the passage in\nthe linguistic style of a query, and then using this summary in place of the\npassage to assess its relevance.\n  We include an empirical evaluation of our approaches based on data from the\nLLMJudge challenge run in Summer 2024, where our \"Four Prompts\" approach\nobtained the highest scores in Kendall's tau.\n","authors":["Naghmeh Farzi","Laura Dietz"],"pdf_url":"https://arxiv.org/pdf/2410.14044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14043v1","updated":"2024-10-17T21:35:55Z","published":"2024-10-17T21:35:55Z","title":"Efficient Retrieval of Temporal Event Sequences from Textual\n  Descriptions","summary":"  Retrieving temporal event sequences from textual descriptions is essential\nfor applications such as analyzing e-commerce behavior, monitoring social media\nactivities, and tracking criminal incidents. In this paper, we introduce\nTPP-LLM-Embedding, a unified model for efficiently embedding and retrieving\nevent sequences based on natural language descriptions. Built on the TPP-LLM\nframework, which integrates large language models with temporal point\nprocesses, our model encodes both event types and times, generating a\nsequence-level representation through pooling. Textual descriptions are\nembedded using the same architecture, ensuring a shared embedding space for\nboth sequences and descriptions. We optimize a contrastive loss based on\nsimilarity between these embeddings, bringing matching pairs closer and\nseparating non-matching ones. TPP-LLM-Embedding enables efficient retrieval and\ndemonstrates superior performance compared to baseline models across diverse\ndatasets.\n","authors":["Zefang Liu","Yinzhu Quan"],"pdf_url":"https://arxiv.org/pdf/2410.14043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15232v2","updated":"2024-10-17T20:43:22Z","published":"2024-08-27T17:50:03Z","title":"Into the Unknown Unknowns: Engaged Human Learning through Participation\n  in Language Model Agent Conversations","summary":"  While language model (LM)-powered chatbots and generative search engines\nexcel at answering concrete queries, discovering information in the terrain of\nunknown unknowns remains challenging for users. To emulate the common\neducational scenario where children/students learn by listening to and\nparticipating in conversations of their parents/teachers, we create\nCollaborative STORM (Co-STORM). Unlike QA systems that require users to ask all\nthe questions, Co-STORM lets users observe and occasionally steer the discourse\namong several LM agents. The agents ask questions on the user's behalf,\nallowing the user to discover unknown unknowns serendipitously. To facilitate\nuser interaction, Co-STORM assists users in tracking the discourse by\norganizing the uncovered information into a dynamic mind map, ultimately\ngenerating a comprehensive report as takeaways. For automatic evaluation, we\nconstruct the WildSeek dataset by collecting real information-seeking records\nwith user goals. Co-STORM outperforms baseline methods on both discourse trace\nand report quality. In a further human evaluation, 70% of participants prefer\nCo-STORM over a search engine, and 78% favor it over a RAG chatbot.\n","authors":["Yucheng Jiang","Yijia Shao","Dekun Ma","Sina J. Semnani","Monica S. Lam"],"pdf_url":"https://arxiv.org/pdf/2408.15232v2.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2307.08803v3","updated":"2024-10-17T19:09:13Z","published":"2023-07-17T19:38:40Z","title":"Mixed-initiative Query Rewriting in Conversational Passage Retrieval","summary":"  In this paper, we report our methods and experiments for the TREC\nConversational Assistance Track (CAsT) 2022. In this work, we aim to reproduce\nmulti-stage retrieval pipelines and explore one of the potential benefits of\ninvolving mixed-initiative interaction in conversational passage retrieval\nscenarios: reformulating raw queries. Before the first ranking stage of a\nmulti-stage retrieval pipeline, we propose a mixed-initiative query rewriting\nmodule, which achieves query rewriting based on the mixed-initiative\ninteraction between the users and the system, as the replacement for the neural\nrewriting method. Specifically, we design an algorithm to generate appropriate\nquestions related to the ambiguities in raw queries, and another algorithm to\nreformulate raw queries by parsing users' feedback and incorporating it into\nthe raw query. For the first ranking stage of our multi-stage pipelines, we\nadopt a sparse ranking function: BM25, and a dense retrieval method:\nTCT-ColBERT. For the second-ranking step, we adopt a pointwise reranker:\nMonoT5, and a pairwise reranker: DuoT5. Experiments on both TREC CAsT 2021 and\nTREC CAsT 2022 datasets show the effectiveness of our mixed-initiative-based\nquery rewriting (or query reformulation) method on improving retrieval\nperformance compared with two popular reformulators: a neural reformulator:\nCANARD-T5 and a rule-based reformulator: historical query reformulator(HQE).\n","authors":["Dayu Yang","Yue Zhang","Hui Fang"],"pdf_url":"https://arxiv.org/pdf/2307.08803v3.pdf","comment":"https://trec.nist.gov/pubs/trec31/papers/udel_fang.C.pdf"},{"id":"http://arxiv.org/abs/2404.11773v2","updated":"2024-10-17T18:59:18Z","published":"2024-04-17T21:56:27Z","title":"Behavior Alignment: A New Perspective of Evaluating LLM-based\n  Conversational Recommender Systems","summary":"  Large Language Models (LLMs) have demonstrated great potential in\nConversational Recommender Systems (CRS). However, the application of LLMs to\nCRS has exposed a notable discrepancy in behavior between LLM-based CRS and\nhuman recommenders: LLMs often appear inflexible and passive, frequently\nrushing to complete the recommendation task without sufficient inquiry.This\nbehavior discrepancy can lead to decreased accuracy in recommendations and\nlower user satisfaction. Despite its importance, existing studies in CRS lack a\nstudy about how to measure such behavior discrepancy. To fill this gap, we\npropose Behavior Alignment, a new evaluation metric to measure how well the\nrecommendation strategies made by a LLM-based CRS are consistent with human\nrecommenders'. Our experiment results show that the new metric is better\naligned with human preferences and can better differentiate how systems perform\nthan existing evaluation metrics. As Behavior Alignment requires explicit and\ncostly human annotations on the recommendation strategies, we also propose a\nclassification-based method to implicitly measure the Behavior Alignment based\non the responses. The evaluation results confirm the robustness of the method.\n","authors":["Dayu Yang","Fumian Chen","Hui Fang"],"pdf_url":"https://arxiv.org/pdf/2404.11773v2.pdf","comment":"Accepted by the 47th International ACM SIGIR Conference on Research\n  and Development in Information Retrieval"},{"id":"http://arxiv.org/abs/2307.09384v3","updated":"2024-10-17T18:53:54Z","published":"2023-07-18T16:05:25Z","title":"ZeQR: Zero-shot Query Reformulation for Conversational Search","summary":"  As the popularity of voice assistants continues to surge, conversational\nsearch has gained increased attention in Information Retrieval. However, data\nsparsity issues in conversational search significantly hinder the progress of\nsupervised conversational search methods. Consequently, researchers are\nfocusing more on zero-shot conversational search approaches. Nevertheless,\nexisting zero-shot methods face three primary limitations: they are not\nuniversally applicable to all retrievers, their effectiveness lacks sufficient\nexplainability, and they struggle to resolve common conversational ambiguities\ncaused by omission. To address these limitations, we introduce a novel\nZero-shot Query Reformulation (or Query Rewriting) (ZeQR) framework that\nreformulates queries based on previous dialogue contexts without requiring\nsupervision from conversational search data. Specifically, our framework\nutilizes language models designed for machine reading comprehension tasks to\nexplicitly resolve two common ambiguities: coreference and omission, in raw\nqueries. In comparison to existing zero-shot methods, our approach is\nuniversally applicable to any retriever without additional adaptation or\nindexing. It also provides greater explainability and effectively enhances\nquery intent understanding because ambiguities are explicitly and proactively\nresolved. Through extensive experiments on four TREC conversational datasets,\nwe demonstrate the effectiveness of our method, which consistently outperforms\nstate-of-the-art baselines.\n","authors":["Dayu Yang","Yue Zhang","Hui Fang"],"pdf_url":"https://arxiv.org/pdf/2307.09384v3.pdf","comment":"Accepted by the 9th ACM SIGIR International Conference on the Theory\n  of Information Retrieval"},{"id":"http://arxiv.org/abs/2310.01038v2","updated":"2024-10-17T18:35:41Z","published":"2023-10-02T09:30:11Z","title":"Dataset Condensation for Recommendation","summary":"  Training recommendation models on large datasets requires significant time\nand resources. It is desired to construct concise yet informative datasets for\nefficient training. Recent advances in dataset condensation show promise in\naddressing this problem by synthesizing small datasets. However, applying\nexisting methods of dataset condensation to recommendation has limitations: (1)\nthey fail to generate discrete user-item interactions, and (2) they could not\npreserve users' potential preferences. To address the limitations, we propose a\nlightweight condensation framework tailored for recommendation (DConRec),\nfocusing on condensing user-item historical interaction sets. Specifically, we\nmodel the discrete user-item interactions via a probabilistic approach and\ndesign a pre-augmentation module to incorporate the potential preferences of\nusers into the condensed datasets. While the substantial size of datasets leads\nto costly optimization, we propose a lightweight policy gradient estimation to\naccelerate the data synthesis. Experimental results on multiple real-world\ndatasets have demonstrated the effectiveness and efficiency of our framework.\nBesides, we provide a theoretical analysis of the provable convergence of\nDConRec. Our implementation is available at:\nhttps://github.com/JiahaoWuGit/DConRec.\n","authors":["Jiahao Wu","Wenqi Fan","Jingfan Chen","Shengcai Liu","Qijiong Liu","Rui He","Qing Li","Ke Tang"],"pdf_url":"https://arxiv.org/pdf/2310.01038v2.pdf","comment":"Accepted by IEEE TKDE. Also titled as \"Condensing Pre-augmented\n  Recommendation Data via Lightweight Policy Gradient Estimation\""},{"id":"http://arxiv.org/abs/2410.13959v1","updated":"2024-10-17T18:34:43Z","published":"2024-10-17T18:34:43Z","title":"FinQAPT: Empowering Financial Decisions with End-to-End LLM-driven\n  Question Answering Pipeline","summary":"  Financial decision-making hinges on the analysis of relevant information\nembedded in the enormous volume of documents in the financial domain. To\naddress this challenge, we developed FinQAPT, an end-to-end pipeline that\nstreamlines the identification of relevant financial reports based on a query,\nextracts pertinent context, and leverages Large Language Models (LLMs) to\nperform downstream tasks. To evaluate the pipeline, we experimented with\nvarious techniques to optimize the performance of each module using the FinQA\ndataset. We introduced a novel clustering-based negative sampling technique to\nenhance context extraction and a novel prompting method called Dynamic N-shot\nPrompting to boost the numerical question-answering capabilities of LLMs. At\nthe module level, we achieved state-of-the-art accuracy on FinQA, attaining an\naccuracy of 80.6\\%. However, at the pipeline level, we observed decreased\nperformance due to challenges in extracting relevant context from financial\nreports. We conducted a detailed error analysis of each module and the\nend-to-end pipeline, pinpointing specific challenges that must be addressed to\ndevelop a robust solution for handling complex financial tasks.\n","authors":["Kuldeep Singh","Simerjot Kaur","Charese Smiley"],"pdf_url":"https://arxiv.org/pdf/2410.13959v1.pdf","comment":"Accepted in ICAIF 2024, 8 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.13951v1","updated":"2024-10-17T18:22:42Z","published":"2024-10-17T18:22:42Z","title":"Identifying High Consideration E-Commerce Search Queries","summary":"  In e-commerce, high consideration search missions typically require careful\nand elaborate decision making, and involve a substantial research investment\nfrom customers. We consider the task of identifying High Consideration (HC)\nqueries. Identifying such queries enables e-commerce sites to better serve user\nneeds using targeted experiences such as curated QA widgets that help users\nreach purchase decisions. We explore the task by proposing an Engagement-based\nQuery Ranking (EQR) approach, focusing on query ranking to indicate potential\nengagement levels with query-related shopping knowledge content during product\nsearch. Unlike previous studies on predicting trends, EQR prioritizes\nquery-level features related to customer behavior, finance, and catalog\ninformation rather than popularity signals. We introduce an accurate and\nscalable method for EQR and present experimental results demonstrating its\neffectiveness. Offline experiments show strong ranking performance. Human\nevaluation shows a precision of 96% for HC queries identified by our model. The\nmodel was commercially deployed, and shown to outperform human-selected queries\nin terms of downstream customer impact, as measured through engagement.\n","authors":["Zhiyu Chen","Jason Choi","Besnik Fetahu","Shervin Malmasi"],"pdf_url":"https://arxiv.org/pdf/2410.13951v1.pdf","comment":"Accepted by EMNLP 2024 (Industry Track)"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2410.13863v1","updated":"2024-10-17T17:59:59Z","published":"2024-10-17T17:59:59Z","title":"Fluid: Scaling Autoregressive Text-to-image Generative Models with\n  Continuous Tokens","summary":"  Scaling up autoregressive models in vision has not proven as beneficial as in\nlarge language models. In this work, we investigate this scaling problem in the\ncontext of text-to-image generation, focusing on two critical factors: whether\nmodels use discrete or continuous tokens, and whether tokens are generated in a\nrandom or fixed raster order using BERT- or GPT-like transformer architectures.\nOur empirical results show that, while all models scale effectively in terms of\nvalidation loss, their evaluation performance -- measured by FID, GenEval\nscore, and visual quality -- follows different trends. Models based on\ncontinuous tokens achieve significantly better visual quality than those using\ndiscrete tokens. Furthermore, the generation order and attention mechanisms\nsignificantly affect the GenEval score: random-order models achieve notably\nbetter GenEval scores compared to raster-order models. Inspired by these\nfindings, we train Fluid, a random-order autoregressive model on continuous\ntokens. Fluid 10.5B model achieves a new state-of-the-art zero-shot FID of 6.16\non MS-COCO 30K, and 0.69 overall score on the GenEval benchmark. We hope our\nfindings and results will encourage future efforts to further bridge the\nscaling gap between vision and language models.\n","authors":["Lijie Fan","Tianhong Li","Siyang Qin","Yuanzhen Li","Chen Sun","Michael Rubinstein","Deqing Sun","Kaiming He","Yonglong Tian"],"pdf_url":"https://arxiv.org/pdf/2410.13863v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2410.13857v1","updated":"2024-10-17T17:59:35Z","published":"2024-10-17T17:59:35Z","title":"How Numerical Precision Affects Mathematical Reasoning Capabilities of\n  LLMs","summary":"  Despite the remarkable success of Transformer-based Large Language Models\n(LLMs) across various domains, understanding and enhancing their mathematical\ncapabilities remains a significant challenge. In this paper, we conduct a\nrigorous theoretical analysis of LLMs' mathematical abilities, with a specific\nfocus on their arithmetic performances. We identify numerical precision as a\nkey factor that influences their effectiveness in mathematical tasks. Our\nresults show that Transformers operating with low numerical precision fail to\naddress arithmetic tasks, such as iterated addition and integer multiplication,\nunless the model size grows super-polynomially with respect to the input\nlength. In contrast, Transformers with standard numerical precision can\nefficiently handle these tasks with significantly smaller model sizes. We\nfurther support our theoretical findings through empirical experiments that\nexplore the impact of varying numerical precision on arithmetic tasks,\nproviding valuable insights for improving the mathematical reasoning\ncapabilities of LLMs.\n","authors":["Guhao Feng","Kai Yang","Yuntian Gu","Xinyue Ai","Shengjie Luo","Jiacheng Sun","Di He","Zhenguo Li","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13855v1","updated":"2024-10-17T17:59:25Z","published":"2024-10-17T17:59:25Z","title":"Diffusing States and Matching Scores: A New Framework for Imitation\n  Learning","summary":"  Adversarial Imitation Learning is traditionally framed as a two-player\nzero-sum game between a learner and an adversarially chosen cost function, and\ncan therefore be thought of as the sequential generalization of a Generative\nAdversarial Network (GAN). A prominent example of this framework is Generative\nAdversarial Imitation Learning (GAIL). However, in recent years, diffusion\nmodels have emerged as a non-adversarial alternative to GANs that merely\nrequire training a score function via regression, yet produce generations of a\nhigher quality. In response, we investigate how to lift insights from diffusion\nmodeling to the sequential setting. We propose diffusing states and performing\nscore-matching along diffused states to measure the discrepancy between the\nexpert's and learner's states. Thus, our approach only requires training score\nfunctions to predict noises via standard regression, making it significantly\neasier and more stable to train than adversarial methods. Theoretically, we\nprove first- and second-order instance-dependent bounds with linear scaling in\nthe horizon, proving that our approach avoids the compounding errors that\nstymie offline approaches to imitation learning. Empirically, we show our\napproach outperforms GAN-style imitation learning baselines across various\ncontinuous control problems, including complex tasks like controlling humanoids\nto walk, sit, and crawl.\n","authors":["Runzhe Wu","Yiding Chen","Gokul Swamy","Kianté Brantley","Wen Sun"],"pdf_url":"https://arxiv.org/pdf/2410.13855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13853v1","updated":"2024-10-17T17:59:09Z","published":"2024-10-17T17:59:09Z","title":"AutoAL: Automated Active Learning with Differentiable Query Strategy\n  Search","summary":"  As deep learning continues to evolve, the need for data efficiency becomes\nincreasingly important. Considering labeling large datasets is both\ntime-consuming and expensive, active learning (AL) provides a promising\nsolution to this challenge by iteratively selecting the most informative\nsubsets of examples to train deep neural networks, thereby reducing the\nlabeling cost. However, the effectiveness of different AL algorithms can vary\nsignificantly across data scenarios, and determining which AL algorithm best\nfits a given task remains a challenging problem. This work presents the first\ndifferentiable AL strategy search method, named AutoAL, which is designed on\ntop of existing AL sampling strategies. AutoAL consists of two neural nets,\nnamed SearchNet and FitNet, which are optimized concurrently under a\ndifferentiable bi-level optimization framework. For any given task, SearchNet\nand FitNet are iteratively co-optimized using the labeled data, learning how\nwell a set of candidate AL algorithms perform on that task. With the optimal AL\nstrategies identified, SearchNet selects a small subset from the unlabeled pool\nfor querying their annotations, enabling efficient training of the task model.\nExperimental results demonstrate that AutoAL consistently achieves superior\naccuracy compared to all candidate AL algorithms and other selective AL\napproaches, showcasing its potential for adapting and integrating multiple\nexisting AL methods across diverse tasks and domains. Code will be available\nat: https://github.com/haizailache999/AutoAL.\n","authors":["Yifeng Wang","Xueying Zhan","Siyu Huang"],"pdf_url":"https://arxiv.org/pdf/2410.13853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13852v1","updated":"2024-10-17T17:59:03Z","published":"2024-10-17T17:59:03Z","title":"Retrospective Learning from Interactions","summary":"  Multi-turn interactions between large language models (LLMs) and users\nnaturally include implicit feedback signals. If an LLM responds in an\nunexpected way to an instruction, the user is likely to signal it by rephrasing\nthe request, expressing frustration, or pivoting to an alternative task. Such\nsignals are task-independent and occupy a relatively constrained subspace of\nlanguage, allowing the LLM to identify them even if it fails on the actual\ntask. This creates an avenue for continually learning from interactions without\nadditional annotations. We introduce ReSpect, a method to learn from such\nsignals in past interactions via retrospection. We deploy ReSpect in a new\nmultimodal interaction scenario, where humans instruct an LLM to solve an\nabstract reasoning task with a combinatorial solution space. Through thousands\nof interactions with humans, we show how ReSpect gradually improves task\ncompletion rate from 31% to 82%, all without any external annotation.\n","authors":["Zizhao Chen","Mustafa Omer Gul","Yiwei Chen","Gloria Geng","Anne Wu","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2410.13852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13850v1","updated":"2024-10-17T17:59:02Z","published":"2024-10-17T17:59:02Z","title":"Influence Functions for Scalable Data Attribution in Diffusion Models","summary":"  Diffusion models have led to significant advancements in generative\nmodelling. Yet their widespread adoption poses challenges regarding data\nattribution and interpretability. In this paper, we aim to help address such\nchallenges in diffusion models by developing an \\textit{influence functions}\nframework. Influence function-based data attribution methods approximate how a\nmodel's output would have changed if some training data were removed. In\nsupervised learning, this is usually used for predicting how the loss on a\nparticular example would change. For diffusion models, we focus on predicting\nthe change in the probability of generating a particular example via several\nproxy measurements. We show how to formulate influence functions for such\nquantities and how previously proposed methods can be interpreted as particular\ndesign choices in our framework. To ensure scalability of the Hessian\ncomputations in influence functions, we systematically develop K-FAC\napproximations based on generalised Gauss-Newton matrices specifically tailored\nto diffusion models. We recast previously proposed methods as specific design\nchoices in our framework and show that our recommended method outperforms\nprevious data attribution approaches on common evaluations, such as the Linear\nData-modelling Score (LDS) or retraining without top influences, without the\nneed for method-specific hyperparameter tuning.\n","authors":["Bruno Mlodozeniec","Runa Eschenhagen","Juhan Bae","Alexander Immer","David Krueger","Richard Turner"],"pdf_url":"https://arxiv.org/pdf/2410.13850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13849v1","updated":"2024-10-17T17:59:01Z","published":"2024-10-17T17:59:01Z","title":"From Gradient Clipping to Normalization for Heavy Tailed SGD","summary":"  Recent empirical evidence indicates that many machine learning applications\ninvolve heavy-tailed gradient noise, which challenges the standard assumptions\nof bounded variance in stochastic optimization. Gradient clipping has emerged\nas a popular tool to handle this heavy-tailed noise, as it achieves good\nperformance in this setting both theoretically and practically. However, our\ncurrent theoretical understanding of non-convex gradient clipping has three\nmain shortcomings. First, the theory hinges on large, increasing clipping\nthresholds, which are in stark contrast to the small constant clipping\nthresholds employed in practice. Second, clipping thresholds require knowledge\nof problem-dependent parameters to guarantee convergence. Lastly, even with\nthis knowledge, current sampling complexity upper bounds for the method are\nsub-optimal in nearly all parameters. To address these issues, we study\nconvergence of Normalized SGD (NSGD). First, we establish a parameter-free\nsample complexity for NSGD of\n$\\mathcal{O}\\left(\\varepsilon^{-\\frac{2p}{p-1}}\\right)$ to find an\n$\\varepsilon$-stationary point. Furthermore, we prove tightness of this result,\nby providing a matching algorithm-specific lower bound. In the setting where\nall problem parameters are known, we show this complexity is improved to\n$\\mathcal{O}\\left(\\varepsilon^{-\\frac{3p-2}{p-1}}\\right)$, matching the\npreviously known lower bound for all first-order methods in all problem\ndependent parameters. Finally, we establish high-probability convergence of\nNSGD with a mild logarithmic dependence on the failure probability. Our work\ncomplements the studies of gradient clipping under heavy tailed noise improving\nthe sample complexities of existing algorithms and offering an alternative\nmechanism to achieve high probability convergence.\n","authors":["Florian Hübler","Ilyas Fatkhullin","Niao He"],"pdf_url":"https://arxiv.org/pdf/2410.13849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08928v2","updated":"2024-10-17T17:58:53Z","published":"2024-10-11T15:53:24Z","title":"Towards Multilingual LLM Evaluation for European Languages","summary":"  The rise of Large Language Models (LLMs) has revolutionized natural language\nprocessing across numerous languages and tasks. However, evaluating LLM\nperformance in a consistent and meaningful way across multiple European\nlanguages remains challenging, especially due to the scarcity of\nlanguage-parallel multilingual benchmarks. We introduce a multilingual\nevaluation approach tailored for European languages. We employ translated\nversions of five widely-used benchmarks to assess the capabilities of 40 LLMs\nacross 21 European languages. Our contributions include examining the\neffectiveness of translated benchmarks, assessing the impact of different\ntranslation services, and offering a multilingual evaluation framework for LLMs\nthat includes newly created datasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC,\nEU20-TruthfulQA, and EU20-GSM8K. The benchmarks and results are made publicly\navailable to encourage further research in multilingual LLM evaluation.\n","authors":["Klaudia Thellmann","Bernhard Stadler","Michael Fromm","Jasper Schulze Buschhoff","Alex Jude","Fabio Barth","Johannes Leveling","Nicolas Flores-Herr","Joachim Köhler","René Jäkel","Mehdi Ali"],"pdf_url":"https://arxiv.org/pdf/2410.08928v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13846v1","updated":"2024-10-17T17:58:14Z","published":"2024-10-17T17:58:14Z","title":"SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction","summary":"  Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV.\n","authors":["Xuan Zhang","Cunxiao Du","Chao Du","Tianyu Pang","Wei Gao","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.13846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13841v1","updated":"2024-10-17T17:56:53Z","published":"2024-10-17T17:56:53Z","title":"A Unified View of Delta Parameter Editing in Post-Trained Large-Scale\n  Models","summary":"  Post-training has emerged as a crucial paradigm for adapting large-scale\npre-trained models to various tasks, whose effects are fully reflected by delta\nparameters (i.e., the disparity between post-trained and pre-trained\nparameters). While numerous studies have explored delta parameter properties\nvia operations like pruning, quantization, low-rank approximation, and\nextrapolation, a unified framework for systematically examining these\ncharacteristics has been lacking. In this paper, we propose a novel perspective\nbased on Riemann sum approximation of the loss function to elucidate delta\nparameter editing operations. Our analysis categorizes existing methods into\nthree classes based on their post-editing performance: competitive, decreased,\nand improved, explaining how they are expressed by the Riemann sum\napproximation term and how they alter the model performance. Extensive\nexperiments on both visual and language models, including ViT, LLaMA 3, Qwen 2,\nand Mistral, corroborate our theoretical findings. Furthermore, we introduce\nextensions to existing techniques like DARE and BitDelta, highlighting their\nlimitations in leveraging the properties of delta parameters and reorganizing\nthem into general expressions to enhance the applicability and effectiveness of\ndelta parameter editing in post-trained models.\n","authors":["Qiaoyu Tang","Le Yu","Bowen Yu","Hongyu Lin","Keming Lu","Yaojie Lu","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2410.13841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13837v1","updated":"2024-10-17T17:55:05Z","published":"2024-10-17T17:55:05Z","title":"ORSO: Accelerating Reward Design via Online Reward Selection and Policy\n  Optimization","summary":"  Reward shaping is a critical component in reinforcement learning (RL),\nparticularly for complex tasks where sparse rewards can hinder learning. While\nshaping rewards have been introduced to provide additional guidance, selecting\neffective shaping functions remains challenging and computationally expensive.\nThis paper introduces Online Reward Selection and Policy Optimization (ORSO), a\nnovel approach that frames shaping reward selection as an online model\nselection problem. ORSO employs principled exploration strategies to\nautomatically identify promising shaping reward functions without human\nintervention, balancing exploration and exploitation with provable regret\nguarantees. We demonstrate ORSO's effectiveness across various continuous\ncontrol tasks using the Isaac Gym simulator. Compared to traditional methods\nthat fully evaluate each shaping reward function, ORSO significantly improves\nsample efficiency, reduces computational time, and consistently identifies\nhigh-quality reward functions that produce policies comparable to those\ngenerated by domain experts through hand-engineered rewards.\n","authors":["Chen Bo Calvin Zhang","Zhang-Wei Hong","Aldo Pacchiano","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2410.13837v1.pdf","comment":"preprint, 35 pages, 23 figures"},{"id":"http://arxiv.org/abs/2410.13835v1","updated":"2024-10-17T17:54:06Z","published":"2024-10-17T17:54:06Z","title":"Active-Dormant Attention Heads: Mechanistically Demystifying\n  Extreme-Token Phenomena in LLMs","summary":"  Practitioners have consistently observed three puzzling phenomena in\ntransformer-based large language models (LLMs): attention sinks, value-state\ndrains, and residual-state peaks, collectively referred to as extreme-token\nphenomena. These phenomena are characterized by certain so-called \"sink tokens\"\nreceiving disproportionately high attention weights, exhibiting significantly\nsmaller value states, and having much larger residual-state norms than those of\nother tokens. These extreme tokens give rise to various challenges in LLM\ninference, quantization, and interpretability.\n  We elucidate the mechanisms behind extreme-token phenomena. First, we show\nthat these phenomena arise in very simple architectures -- transformers with\none to three layers -- trained on a toy model, the Bigram-Backcopy (BB) task.\nIn this setting, we identify an active-dormant mechanism, where attention heads\nbecome sinks for specific input domains while remaining non-sinks for others.\nOur theoretical analysis of the training dynamics reveals that these phenomena\nare driven by a mutual reinforcement mechanism. Building on these insights, we\npropose strategies to mitigate extreme-token phenomena during pretraining,\nincluding replacing softmax with ReLU and Adam with SGD. Next, we extend our\nanalysis to pretrained LLMs, including Llama and OLMo, showing that many\nattention heads exhibit a similar active-dormant mechanism as in the BB task,\nand that the mutual reinforcement mechanism also governs the emergence of\nextreme-token phenomena during LLM pretraining. Our results reveal that many of\nthe static and dynamic properties of extreme-token phenomena predicted by the\nBB task align with observations in pretrained LLMs.\n","authors":["Tianyu Guo","Druv Pai","Yu Bai","Jiantao Jiao","Michael I. Jordan","Song Mei"],"pdf_url":"https://arxiv.org/pdf/2410.13835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13831v1","updated":"2024-10-17T17:53:01Z","published":"2024-10-17T17:53:01Z","title":"The Disparate Benefits of Deep Ensembles","summary":"  Ensembles of Deep Neural Networks, Deep Ensembles, are widely used as a\nsimple way to boost predictive performance. However, their impact on\nalgorithmic fairness is not well understood yet. Algorithmic fairness\ninvestigates how a model's performance varies across different groups,\ntypically defined by protected attributes such as age, gender, or race. In this\nwork, we investigate the interplay between the performance gains from Deep\nEnsembles and fairness. Our analysis reveals that they unevenly favor different\ngroups in what we refer to as a disparate benefits effect. We empirically\ninvestigate this effect with Deep Ensembles applied to popular facial analysis\nand medical imaging datasets, where protected group attributes are given and\nfind that it occurs for multiple established group fairness metrics, including\nstatistical parity and equal opportunity. Furthermore, we identify the\nper-group difference in predictive diversity of ensemble members as the\npotential cause of the disparate benefits effect. Finally, we evaluate\ndifferent approaches to reduce unfairness due to the disparate benefits effect.\nOur findings show that post-processing is an effective method to mitigate this\nunfairness while preserving the improved performance of Deep Ensembles.\n","authors":["Kajetan Schweighofer","Adrian Arnaiz-Rodriguez","Sepp Hochreiter","Nuria Oliver"],"pdf_url":"https://arxiv.org/pdf/2410.13831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13828v1","updated":"2024-10-17T17:52:01Z","published":"2024-10-17T17:52:01Z","title":"A Common Pitfall of Margin-based Language Model Alignment: Gradient\n  Entanglement","summary":"  Reinforcement Learning from Human Feedback (RLHF) has become the predominant\napproach for language model (LM) alignment. At its core, RLHF uses a\nmargin-based loss for preference optimization, specifying ideal LM behavior\nonly by the difference between preferred and dispreferred responses. In this\npaper, we identify a common pitfall of margin-based methods -- the\nunder-specification of ideal LM behavior on preferred and dispreferred\nresponses individually, which leads to two unintended consequences as the\nmargin increases: (1) The probability of dispreferred (e.g., unsafe) responses\nmay increase, resulting in potential safety alignment failures. (2) The\nprobability of preferred responses may decrease, even when those responses are\nideal. We demystify the reasons behind these problematic behaviors:\nmargin-based losses couple the change in the preferred probability to the\ngradient of the dispreferred one, and vice versa, often preventing the\npreferred probability from increasing while the dispreferred one decreases, and\nthus causing a synchronized increase or decrease in both probabilities. We term\nthis effect, inherent in margin-based objectives, gradient entanglement.\nFormally, we derive conditions for general margin-based alignment objectives\nunder which gradient entanglement becomes concerning: the inner product of the\ngradients of preferred and dispreferred log-probabilities is large relative to\nthe individual gradient norms. We theoretically investigate why such inner\nproducts can be large when aligning language models and empirically validate\nour findings. Empirical implications of our framework extend to explaining\nimportant differences in the training dynamics of various preference\noptimization algorithms, and suggesting potential algorithm designs to mitigate\nthe under-specification issue of margin-based methods and thereby improving\nlanguage model alignment.\n","authors":["Hui Yuan","Yifan Zeng","Yue Wu","Huazheng Wang","Mengdi Wang","Liu Leqi"],"pdf_url":"https://arxiv.org/pdf/2410.13828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13826v1","updated":"2024-10-17T17:51:40Z","published":"2024-10-17T17:51:40Z","title":"Unearthing Skill-Level Insights for Understanding Trade-Offs of\n  Foundation Models","summary":"  With models getting stronger, evaluations have grown more complex, testing\nmultiple skills in one benchmark and even in the same instance at once.\nHowever, skill-wise performance is obscured when inspecting aggregate accuracy,\nunder-utilizing the rich signal modern benchmarks contain. We propose an\nautomatic approach to recover the underlying skills relevant for any evaluation\ninstance, by way of inspecting model-generated rationales. After validating the\nrelevance of rationale-parsed skills and inferring skills for $46$k instances\nover $12$ benchmarks, we observe many skills to be common across benchmarks,\nresulting in the curation of hundreds of skill-slices (i.e. sets of instances\ntesting a common skill). Inspecting accuracy over these slices yields novel\ninsights on model trade-offs: e.g., compared to GPT-4o and Claude 3.5 Sonnet,\non average, Gemini 1.5 Pro is $18\\%$ more accurate in \"computing molar mass\",\nbut $19\\%$ less accurate in \"applying constitutional law\", despite the overall\naccuracies of the three models differing by a mere $0.4\\%$. Furthermore, we\ndemonstrate the practical utility of our approach by showing that insights\nderived from skill slice analysis can generalize to held-out instances: when\nrouting each instance to the model strongest on the relevant skills, we see a\n$3\\%$ accuracy improvement over our $12$ dataset corpus. Our skill-slices and\nframework open a new avenue in model evaluation, leveraging skill-specific\nanalyses to unlock a more granular and actionable understanding of model\ncapabilities.\n","authors":["Mazda Moayeri","Vidhisha Balachandran","Varun Chandrasekaran","Safoora Yousefi","Thomas Fel","Soheil Feizi","Besmira Nushi","Neel Joshi","Vibhav Vineet"],"pdf_url":"https://arxiv.org/pdf/2410.13826v1.pdf","comment":"Code at: github.com/microsoft/skill-slice-insights"},{"id":"http://arxiv.org/abs/2407.16833v2","updated":"2024-10-17T17:51:19Z","published":"2024-07-23T20:51:52Z","title":"Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive\n  Study and Hybrid Approach","summary":"  Retrieval Augmented Generation (RAG) has been a powerful tool for Large\nLanguage Models (LLMs) to efficiently process overly lengthy contexts. However,\nrecent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to\nunderstand long contexts directly. We conduct a comprehensive comparison\nbetween RAG and long-context (LC) LLMs, aiming to leverage the strengths of\nboth. We benchmark RAG and LC across various public datasets using three latest\nLLMs. Results reveal that when resourced sufficiently, LC consistently\noutperforms RAG in terms of average performance. However, RAG's significantly\nlower cost remains a distinct advantage. Based on this observation, we propose\nSelf-Route, a simple yet effective method that routes queries to RAG or LC\nbased on model self-reflection. Self-Route significantly reduces the\ncomputation cost while maintaining a comparable performance to LC. Our findings\nprovide a guideline for long-context applications of LLMs using RAG and LC.\n","authors":["Zhuowan Li","Cheng Li","Mingyang Zhang","Qiaozhu Mei","Michael Bendersky"],"pdf_url":"https://arxiv.org/pdf/2407.16833v2.pdf","comment":"Accepted to EMNLP 2024 industry track"},{"id":"http://arxiv.org/abs/2410.13821v1","updated":"2024-10-17T17:47:54Z","published":"2024-10-17T17:47:54Z","title":"Artificial Kuramoto Oscillatory Neurons","summary":"  It has long been known in both neuroscience and AI that ``binding'' between\nneurons leads to a form of competitive learning where representations are\ncompressed in order to represent more abstract concepts in deeper layers of the\nnetwork. More recently, it was also hypothesized that dynamic (spatiotemporal)\nrepresentations play an important role in both neuroscience and AI. Building on\nthese ideas, we introduce Artificial Kuramoto Oscillatory Neurons (AKOrN) as a\ndynamical alternative to threshold units, which can be combined with arbitrary\nconnectivity designs such as fully connected, convolutional, or attentive\nmechanisms. Our generalized Kuramoto updates bind neurons together through\ntheir synchronization dynamics. We show that this idea provides performance\nimprovements across a wide spectrum of tasks such as unsupervised object\ndiscovery, adversarial robustness, calibrated uncertainty quantification, and\nreasoning. We believe that these empirical results show the importance of\nrethinking our assumptions at the most basic neuronal level of neural\nrepresentation, and in particular show the importance of dynamical\nrepresentations.\n","authors":["Takeru Miyato","Sindy Löwe","Andreas Geiger","Max Welling"],"pdf_url":"https://arxiv.org/pdf/2410.13821v1.pdf","comment":"Code: https://github.com/autonomousvision/akorn"},{"id":"http://arxiv.org/abs/2410.13816v1","updated":"2024-10-17T17:46:26Z","published":"2024-10-17T17:46:26Z","title":"Steering Your Generalists: Improving Robotic Foundation Models via Value\n  Guidance","summary":"  Large, general-purpose robotic policies trained on diverse demonstration\ndatasets have been shown to be remarkably effective both for controlling a\nvariety of robots in a range of different scenes, and for acquiring broad\nrepertoires of manipulation skills. However, the data that such policies are\ntrained on is generally of mixed quality -- not only are human-collected\ndemonstrations unlikely to perform the task perfectly, but the larger the\ndataset is, the harder it is to curate only the highest quality examples. It\nalso remains unclear how optimal data from one embodiment is for training on\nanother embodiment. In this paper, we present a general and broadly applicable\napproach that enhances the performance of such generalist robot policies at\ndeployment time by re-ranking their actions according to a value function\nlearned via offline RL. This approach, which we call Value-Guided Policy\nSteering (V-GPS), is compatible with a wide range of different generalist\npolicies, without needing to fine-tune or even access the weights of the\npolicy. We show that the same value function can improve the performance of\nfive different state-of-the-art policies with different architectures, even\nthough they were trained on distinct datasets, attaining consistent performance\nimprovement on multiple robotic platforms across a total of 12 tasks. Code and\nvideos can be found at: https://nakamotoo.github.io/V-GPS\n","authors":["Mitsuhiko Nakamoto","Oier Mees","Aviral Kumar","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2410.13816v1.pdf","comment":"Conference on Robot Learning (CoRL) 2024. Project Page:\n  https://nakamotoo.github.io/V-GPS"},{"id":"http://arxiv.org/abs/2404.11018v3","updated":"2024-10-17T17:45:09Z","published":"2024-04-17T02:49:26Z","title":"Many-Shot In-Context Learning","summary":"  Large language models (LLMs) excel at few-shot in-context learning (ICL) --\nlearning from a few examples provided in context at inference, without any\nweight updates. Newly expanded context windows allow us to investigate ICL with\nhundreds or thousands of examples -- the many-shot regime. Going from few-shot\nto many-shot, we observe significant performance gains across a wide variety of\ngenerative and discriminative tasks. While promising, many-shot ICL can be\nbottlenecked by the available amount of human-generated examples. To mitigate\nthis limitation, we explore two new settings: Reinforced and Unsupervised ICL.\nReinforced ICL uses model-generated chain-of-thought rationales in place of\nhuman examples. Unsupervised ICL removes rationales from the prompt altogether,\nand prompts the model only with domain-specific questions. We find that both\nReinforced and Unsupervised ICL can be quite effective in the many-shot regime,\nparticularly on complex reasoning tasks. Finally, we demonstrate that, unlike\nfew-shot learning, many-shot learning is effective at overriding pretraining\nbiases, can learn high-dimensional functions with numerical inputs, and\nperforms comparably to fine-tuning. We also find that inference cost increases\nlinearly in the many-shot regime, and frontier LLMs benefit from many-shot ICL\nto varying degrees. Our analysis also reveals the limitations of next-token\nprediction loss as an indicator of downstream ICL performance.\n","authors":["Rishabh Agarwal","Avi Singh","Lei M. Zhang","Bernd Bohnet","Luis Rosias","Stephanie Chan","Biao Zhang","Ankesh Anand","Zaheer Abbas","Azade Nova","John D. Co-Reyes","Eric Chu","Feryal Behbahani","Aleksandra Faust","Hugo Larochelle"],"pdf_url":"https://arxiv.org/pdf/2404.11018v3.pdf","comment":"NeurIPS (Spotlight)"},{"id":"http://arxiv.org/abs/2410.13812v1","updated":"2024-10-17T17:45:07Z","published":"2024-10-17T17:45:07Z","title":"Private Counterfactual Retrieval","summary":"  Transparency and explainability are two extremely important aspects to be\nconsidered when employing black-box machine learning models in high-stake\napplications. Providing counterfactual explanations is one way of catering this\nrequirement. However, this also poses a threat to the privacy of both the\ninstitution that is providing the explanation as well as the user who is\nrequesting it. In this work, we propose multiple schemes inspired by private\ninformation retrieval (PIR) techniques which ensure the \\emph{user's privacy}\nwhen retrieving counterfactual explanations. We present a scheme which\nretrieves the \\emph{exact} nearest neighbor counterfactual explanation from a\ndatabase of accepted points while achieving perfect (information-theoretic)\nprivacy for the user. While the scheme achieves perfect privacy for the user,\nsome leakage on the database is inevitable which we quantify using a mutual\ninformation based metric. Furthermore, we propose strategies to reduce this\nleakage to achieve an advanced degree of database privacy. We extend these\nschemes to incorporate user's preference on transforming their attributes, so\nthat a more actionable explanation can be received. Since our schemes rely on\nfinite field arithmetic, we empirically validate our schemes on real datasets\nto understand the trade-off between the accuracy and the finite field sizes.\n","authors":["Mohamed Nomeir","Pasan Dissanayake","Shreya Meel","Sanghamitra Dutta","Sennur Ulukus"],"pdf_url":"https://arxiv.org/pdf/2410.13812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.06615v2","updated":"2024-10-17T17:40:25Z","published":"2023-01-16T21:36:49Z","title":"Data-Driven Estimation of Heterogeneous Treatment Effects","summary":"  Estimating how a treatment affects different individuals, known as\nheterogeneous treatment effect estimation, is an important problem in empirical\nsciences. In the last few years, there has been a considerable interest in\nadapting machine learning algorithms to the problem of estimating heterogeneous\neffects from observational and experimental data. However, these algorithms\noften make strong assumptions about the observed features in the data and\nignore the structure of the underlying causal model, which can lead to biased\nestimation. At the same time, the underlying causal mechanism is rarely known\nin real-world datasets, making it hard to take it into consideration. In this\nwork, we provide a survey of state-of-the-art data-driven methods for\nheterogeneous treatment effect estimation using machine learning, broadly\ncategorizing them as methods that focus on counterfactual prediction and\nmethods that directly estimate the causal effect. We also provide an overview\nof a third category of methods which rely on structural causal models and learn\nthe model structure from data. Our empirical evaluation under various\nunderlying structural model mechanisms shows the advantages and deficiencies of\nexisting estimators and of the metrics for measuring their performance.\n","authors":["Christopher Tran","Keith Burghardt","Kristina Lerman","Elena Zheleva"],"pdf_url":"https://arxiv.org/pdf/2301.06615v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13802v1","updated":"2024-10-17T17:39:46Z","published":"2024-10-17T17:39:46Z","title":"Adversarial Testing as a Tool for Interpretability: Length-based\n  Overfitting of Elementary Functions in Transformers","summary":"  The Transformer model has a tendency to overfit various aspects of the\ntraining data, such as the overall sequence length. We study elementary string\nedit functions using a defined set of error indicators to interpret the\nbehaviour of the sequence-to-sequence Transformer. We show that generalization\nto shorter sequences is often possible, but confirm that longer sequences are\nhighly problematic, although partially correct answers are often obtained.\nAdditionally, we find that other structural characteristics of the sequences,\nsuch as subsegment length, may be equally important. We hypothesize that the\nmodels learn algorithmic aspects of the tasks simultaneously with structural\naspects but adhering to the structural aspects is unfortunately often preferred\nby Transformer when they come into conflict.\n","authors":["Patrik Zavoral","Dušan Variš","Ondřej Bojar"],"pdf_url":"https://arxiv.org/pdf/2410.13802v1.pdf","comment":"9 pages, 8 figures, 2 tables; to be published"},{"id":"http://arxiv.org/abs/2410.13799v1","updated":"2024-10-17T17:38:44Z","published":"2024-10-17T17:38:44Z","title":"Machine-Learning Analysis of Radiative Decays to Dark Matter at the LHC","summary":"  The search for weakly interacting matter particles (WIMPs) is one of the main\nobjectives of the High Luminosity Large Hadron Collider (HL-LHC). In this work\nwe use Machine Learning (ML) techniques to explore WIMP radiative decays into a\nDark Matter (DM) candidate in a supersymmetric framework. The minimal\nsupersymmetric WIMP sector includes the lightest neutralino that can provide\nthe observed DM relic density through its co-annihilation with the second\nlightest neutralino and lightest chargino. Moreover, the direct DM detection\ncross section rates fulfill current experimental bounds and provide discovery\ntargets for the same region of model parameters in which the radiative decay of\nthe second lightest neutralino into a photon and the lightest neutralino is\nenhanced. This strongly motivates the search for radiatively decaying\nneutralinos which, however, suffers from strong backgrounds. We investigate the\nLHC reach in the search for these radiatively decaying particles by means of\ncut-based and ML methods and estimate its discovery potential in this\nwell-motivated, new physics scenario.\n","authors":["Ernesto Arganda","Marcela Carena","Martín de los Rios","Andres D. Perez","Duncan Rocha","Rosa M. Sandá Seoane","Carlos E. M. Wagner"],"pdf_url":"https://arxiv.org/pdf/2410.13799v1.pdf","comment":"32 pages, 9 figures, 3 tables, 4 appendices"},{"id":"http://arxiv.org/abs/2410.13800v1","updated":"2024-10-17T17:38:44Z","published":"2024-10-17T17:38:44Z","title":"Discrete distributions are learnable from metastable samples","summary":"  Markov chain samplers designed to sample from multi-variable distributions\noften undesirably get stuck in specific regions of their state space. This\ncauses such samplers to approximately sample from a metastable distribution\nwhich is usually quite different from the desired, stationary distribution of\nthe chain. We show that single-variable conditionals of metastable\ndistributions of reversible Markov chain samplers that satisfy a strong\nmetastability condition are on average very close to those of the true\ndistribution. This holds even when the metastable distribution is far away from\nthe true model in terms of global metrics like Kullback-Leibler divergence or\ntotal variation distance. This property allows us to learn the true model using\na conditional likelihood based estimator, even when the samples come from a\nmetastable distribution concentrated in a small region of the state space.\nExplicit examples of such metastable states can be constructed from regions\nthat effectively bottleneck the probability flow and cause poor mixing of the\nMarkov chain. For specific cases of binary pairwise undirected graphical\nmodels, we extend our results to further rigorously show that data coming from\nmetastable states can be used to learn the parameters of the energy function\nand recover the structure of the model.\n","authors":["Abhijith Jayakumar","Andrey Y. Lokhov","Sidhant Misra","Marc Vuffray"],"pdf_url":"https://arxiv.org/pdf/2410.13800v1.pdf","comment":"Preliminary version, 26 pages"},{"id":"http://arxiv.org/abs/2410.13798v1","updated":"2024-10-17T17:38:24Z","published":"2024-10-17T17:38:24Z","title":"Learning Graph Quantized Tokenizers for Transformers","summary":"  Transformers serve as the backbone architectures of Foundational Models,\nwhere a domain-specific tokenizer helps them adapt to various domains. Graph\nTransformers (GTs) have recently emerged as a leading model in geometric deep\nlearning, outperforming Graph Neural Networks (GNNs) in various graph learning\ntasks. However, the development of tokenizers for graphs has lagged behind\nother modalities, with existing approaches relying on heuristics or GNNs\nco-trained with Transformers. To address this, we introduce GQT (\\textbf{G}raph\n\\textbf{Q}uantized \\textbf{T}okenizer), which decouples tokenizer training from\nTransformer training by leveraging multi-task graph self-supervised learning,\nyielding robust and generalizable graph tokens. Furthermore, the GQT utilizes\nResidual Vector Quantization (RVQ) to learn hierarchical discrete tokens,\nresulting in significantly reduced memory requirements and improved\ngeneralization capabilities. By combining the GQT with token modulation, a\nTransformer encoder achieves state-of-the-art performance on 16 out of 18\nbenchmarks, including large-scale homophilic and heterophilic datasets. The\ncode is available at: https://github.com/limei0307/graph-tokenizer\n","authors":["Limei Wang","Kaveh Hassani","Si Zhang","Dongqi Fu","Baichuan Yuan","Weilin Cong","Zhigang Hua","Hao Wu","Ning Yao","Bo Long"],"pdf_url":"https://arxiv.org/pdf/2410.13798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14180v2","updated":"2024-10-17T17:38:00Z","published":"2023-12-19T00:36:53Z","title":"Dynamic Topic Language Model on Heterogeneous Children's Mental Health\n  Clinical Notes","summary":"  Mental health diseases affect children's lives and well-beings which have\nreceived increased attention since the COVID-19 pandemic. Analyzing psychiatric\nclinical notes with topic models is critical to evaluating children's mental\nstatus over time. However, few topic models are built for longitudinal\nsettings, and most existing approaches fail to capture temporal trajectories\nfor each document. To address these challenges, we develop a dynamic topic\nmodel with consistent topics and individualized temporal dependencies on the\nevolving document metadata. Our model preserves the semantic meaning of\ndiscovered topics over time and incorporates heterogeneity among documents. In\nparticular, when documents can be categorized, we propose a classifier-free\napproach to maximize topic heterogeneity across different document groups. We\nalso present an efficient variational optimization procedure adapted for the\nmultistage longitudinal setting. In this case study, we apply our method to the\npsychiatric clinical notes from a large tertiary pediatric hospital in Southern\nCalifornia and achieve a 38% increase in the overall coherence of extracted\ntopics. Our real data analysis reveals that children tend to express more\nnegative emotions during state shutdowns and more positive when schools reopen.\nFurthermore, it suggests that sexual and gender minority (SGM) children display\nmore pronounced reactions to major COVID-19 events and a greater sensitivity to\nvaccine-related news than non-SGM children. This study examines children's\nmental health progression during the pandemic and offers clinicians valuable\ninsights to recognize disparities in children's mental health related to their\nsexual and gender identities.\n","authors":["Hanwen Ye","Tatiana Moreno","Adrianne Alpern","Louis Ehwerhemuepha","Annie Qu"],"pdf_url":"https://arxiv.org/pdf/2312.14180v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13794v1","updated":"2024-10-17T17:34:06Z","published":"2024-10-17T17:34:06Z","title":"Arbitrarily-Conditioned Multi-Functional Diffusion for Multi-Physics\n  Emulation","summary":"  Modern physics simulation often involves multiple functions of interests, and\ntraditional numerical approaches are known to be complex and computationally\ncostly. While machine learning-based surrogate models can offer significant\ncost reductions, most focus on a single task, such as forward prediction, and\ntypically lack uncertainty quantification -- an essential component in many\napplications. To overcome these limitations, we propose Arbitrarily-Conditioned\nMulti-Functional Diffusion (ACMFD), a versatile probabilistic surrogate model\nfor multi-physics emulation. ACMFD can perform a wide range of tasks within a\nsingle framework, including forward prediction, various inverse problems, and\nsimulating data for entire systems or subsets of quantities conditioned on\nothers. Specifically, we extend the standard Denoising Diffusion Probabilistic\nModel (DDPM) for multi-functional generation by modeling noise as Gaussian\nprocesses (GP). We then introduce an innovative denoising loss. The training\ninvolves randomly sampling the conditioned part and fitting the corresponding\npredicted noise to zero, enabling ACMFD to flexibly generate function values\nconditioned on any other functions or quantities. To enable efficient training\nand sampling, and to flexibly handle irregularly sampled data, we use GPs to\ninterpolate function samples onto a grid, inducing a Kronecker product\nstructure for efficient computation. We demonstrate the advantages of ACMFD\nacross several fundamental multi-physics systems.\n","authors":["Da Long","Zhitong Xu","Guang Yang","Akil Narayan","Shandian Zhe"],"pdf_url":"https://arxiv.org/pdf/2410.13794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13792v1","updated":"2024-10-17T17:32:35Z","published":"2024-10-17T17:32:35Z","title":"Analyzing Deep Transformer Models for Time Series Forecasting via\n  Manifold Learning","summary":"  Transformer models have consistently achieved remarkable results in various\ndomains such as natural language processing and computer vision. However,\ndespite ongoing research efforts to better understand these models, the field\nstill lacks a comprehensive understanding. This is particularly true for deep\ntime series forecasting methods, where analysis and understanding work is\nrelatively limited. Time series data, unlike image and text information, can be\nmore challenging to interpret and analyze. To address this, we approach the\nproblem from a manifold learning perspective, assuming that the latent\nrepresentations of time series forecasting models lie next to a low-dimensional\nmanifold. In our study, we focus on analyzing the geometric features of these\nlatent data manifolds, including intrinsic dimension and principal curvatures.\nOur findings reveal that deep transformer models exhibit similar geometric\nbehavior across layers, and these geometric features are correlated with model\nperformance. Additionally, we observe that untrained models initially have\ndifferent structures, but they rapidly converge during training. By leveraging\nour geometric analysis and differentiable tools, we can potentially design new\nand improved deep forecasting neural networks. This approach complements\nexisting analysis studies and contributes to a better understanding of\ntransformer models in the context of time series forecasting. Code is released\nat https://github.com/azencot-group/GATLM.\n","authors":["Ilya Kaufman","Omri Azencot"],"pdf_url":"https://arxiv.org/pdf/2410.13792v1.pdf","comment":"Accepted to TMLR 2024"},{"id":"http://arxiv.org/abs/2405.17882v2","updated":"2024-10-17T17:28:16Z","published":"2024-05-28T07:08:29Z","title":"Achieving Exponential Asymptotic Optimality in Average-Reward Restless\n  Bandits without Global Attractor Assumption","summary":"  We consider the infinite-horizon average-reward restless bandit problem. We\npropose a novel \\emph{two-set policy} that maintains two dynamic subsets of\narms: one subset of arms has a nearly optimal state distribution and takes\nactions according to an Optimal Local Control routine; the other subset of arms\nis driven towards the optimal state distribution and gradually merged into the\nfirst subset. We show that our two-set policy is asymptotically optimal with an\n$O(\\exp(-C N))$ optimality gap for an $N$-armed problem, under the mild\nassumptions of aperiodic-unichain, non-degeneracy, and local stability. Our\npolicy is the first to achieve \\emph{exponential asymptotic optimality} under\nthe above set of easy-to-verify assumptions, whereas prior work either requires\na strong \\emph{global attractor} assumption or only achieves an $O(1/\\sqrt{N})$\noptimality gap. We further discuss obstacles in weakening the assumptions by\ndemonstrating examples where exponential asymptotic optimality is not\nachievable when any of the three assumptions is violated. Notably, we prove a\nlower bound for a large class of locally unstable restless bandits, showing\nthat local stability is particularly fundamental for exponential asymptotic\noptimality. Finally, we use simulations to demonstrate that the two-set policy\noutperforms previous policies on certain RB problems and performs competitively\noverall.\n","authors":["Yige Hong","Qiaomin Xie","Yudong Chen","Weina Wang"],"pdf_url":"https://arxiv.org/pdf/2405.17882v2.pdf","comment":"55 pages, 4 figures. In this version we included simulations"},{"id":"http://arxiv.org/abs/2410.13782v1","updated":"2024-10-17T17:20:24Z","published":"2024-10-17T17:20:24Z","title":"DPLM-2: A Multimodal Diffusion Protein Language Model","summary":"  Proteins are essential macromolecules defined by their amino acid sequences,\nwhich determine their three-dimensional structures and, consequently, their\nfunctions in all living organisms. Therefore, generative protein modeling\nnecessitates a multimodal approach to simultaneously model, understand, and\ngenerate both sequences and structures. However, existing methods typically use\nseparate models for each modality, limiting their ability to capture the\nintricate relationships between sequence and structure. This results in\nsuboptimal performance in tasks that requires joint understanding and\ngeneration of both modalities. In this paper, we introduce DPLM-2, a multimodal\nprotein foundation model that extends discrete diffusion protein language model\n(DPLM) to accommodate both sequences and structures. To enable structural\nlearning with the language model, 3D coordinates are converted to discrete\ntokens using a lookup-free quantization-based tokenizer. By training on both\nexperimental and high-quality synthetic structures, DPLM-2 learns the joint\ndistribution of sequence and structure, as well as their marginals and\nconditionals. We also implement an efficient warm-up strategy to exploit the\nconnection between large-scale evolutionary data and structural inductive\nbiases from pre-trained sequence-based protein language models. Empirical\nevaluation shows that DPLM-2 can simultaneously generate highly compatible\namino acid sequences and their corresponding 3D structures eliminating the need\nfor a two-stage generation approach. Moreover, DPLM-2 demonstrates competitive\nperformance in various conditional generation tasks, including folding, inverse\nfolding, and scaffolding with multimodal motif inputs, as well as providing\nstructure-aware representations for predictive tasks.\n","authors":["Xinyou Wang","Zaixiang Zheng","Fei Ye","Dongyu Xue","Shujian Huang","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2410.13782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13780v1","updated":"2024-10-17T17:19:48Z","published":"2024-10-17T17:19:48Z","title":"Optimal Quantization for Matrix Multiplication","summary":"  Recent work in machine learning community proposed multiple methods for\nperforming lossy compression (quantization) of large matrices. This\nquantization is important for accelerating matrix multiplication (main\ncomponent of large language models), which is often bottlenecked by the speed\nof loading these matrices from memory. Unlike classical vector quantization and\nrate-distortion theory, the goal of these new compression algorithms is to be\nable to approximate not the matrices themselves, but their matrix product.\nSpecifically, given a pair of real matrices $A,B$ an encoder (compressor) is\napplied to each of them independently producing descriptions with $R$ bits per\nentry. These representations subsequently are used by the decoder to estimate\nmatrix product $A^\\top B$. In this work, we provide a non-asymptotic lower\nbound on the mean squared error of this approximation (as a function of rate\n$R$) for the case of matrices $A,B$ with iid Gaussian entries. Algorithmically,\nwe construct a universal quantizer based on nested lattices with an explicit\nguarantee of approximation error for any (non-random) pair of matrices $A$, $B$\nin terms of only Frobenius norms $\\|A\\|_F, \\|B\\|_F$ and $\\|A^\\top B\\|_F$. For\niid Gaussian matrices our quantizer achieves the lower bound and is, thus,\nasymptotically optimal. A practical low-complexity version of our quantizer\nachieves performance quite close to optimal. In information-theoretic terms we\nderive rate-distortion function for matrix multiplication of iid Gaussian\nmatrices.\n","authors":["Or Ordentlich","Yury Polyanskiy"],"pdf_url":"https://arxiv.org/pdf/2410.13780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13779v1","updated":"2024-10-17T17:18:30Z","published":"2024-10-17T17:18:30Z","title":"The Mystery of the Pathological Path-star Task for Language Models","summary":"  The recently introduced path-star task is a minimal task designed to\nexemplify limitations to the abilities of language models (Bachmann and\nNagarajan, 2024). It involves a path-star graph where multiple arms radiate\nfrom a single starting node and each node is unique. Given the start node and a\nspecified target node that ends an arm, the task is to generate the arm\ncontaining that target node. This is straightforward for a human but\nsurprisingly difficult for language models, which did not outperform the random\nbaseline. The authors hypothesized this is due to a deficiency in\nteacher-forcing and the next-token prediction paradigm.\n  We demonstrate the task is learnable using teacher-forcing in alternative\nsettings and that the issue is partially due to representation. We introduce a\nregularization method using structured samples of the same graph but with\ndiffering target nodes, improving results across a variety of model types. We\nprovide RASP proofs showing the task is theoretically solvable. Finally, we\nfind settings where an encoder-only model can consistently solve the task.\n","authors":["Arvid Frydenlund"],"pdf_url":"https://arxiv.org/pdf/2410.13779v1.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.13778v1","updated":"2024-10-17T17:17:38Z","published":"2024-10-17T17:17:38Z","title":"Change Detection in Multivariate data streams: Online Analysis with\n  Kernel-QuantTree","summary":"  We present Kernel-QuantTree Exponentially Weighted Moving Average (KQT-EWMA),\na non-parametric change-detection algorithm that combines the Kernel-QuantTree\n(KQT) histogram and the EWMA statistic to monitor multivariate data streams\nonline. The resulting monitoring scheme is very flexible, since histograms can\nbe used to model any stationary distribution, and practical, since the\ndistribution of test statistics does not depend on the distribution of\ndatastream in stationary conditions (non-parametric monitoring). KQT-EWMA\nenables controlling false alarms by operating at a pre-determined Average Run\nLength ($ARL_0$), which measures the expected number of stationary samples to\nbe monitored before triggering a false alarm. The latter peculiarity is in\ncontrast with most non-parametric change-detection tests, which rarely can\ncontrol the $ARL_0$ a priori. Our experiments on synthetic and real-world\ndatasets demonstrate that KQT-EWMA can control $ARL_0$ while achieving\ndetection delays comparable to or lower than state-of-the-art methods designed\nto work in the same conditions.\n","authors":["Michelangelo Olmo Nogara Notarianni","Filippo Leveni","Diego Stucchi","Luca Frittoli","Giacomo Boracchi"],"pdf_url":"https://arxiv.org/pdf/2410.13778v1.pdf","comment":"AALTD workshop at ECML 2024 (https://ecml-aaltd.github.io/aaltd2024/)"},{"id":"http://arxiv.org/abs/2410.13569v1","updated":"2024-10-17T17:17:09Z","published":"2024-10-17T17:17:09Z","title":"Representing Model Weights with Language using Tree Experts","summary":"  The increasing availability of public models begs the question: can we train\nneural networks that use other networks as input? This paper learns to\nrepresent models within a joint space that embeds both model weights and\nlanguage. However, machine learning on model weights is challenging as model\nweights often exhibit significant variation unrelated to the models' semantic\nproperties (nuisance variation). We identify a key property of real-world\nmodels: most public models belong to a small set of Model Trees, where all\nmodels within a tree are fine-tuned from a common ancestor (e.g., a foundation\nmodel). Importantly, we find that within each tree there is less nuisance\nvariation between models. For example, while classifying models according to\ntheir training dataset generally requires complex architectures, in our case,\neven a linear classifier trained on a single layer is often effective. While\neffective, linear layers are computationally expensive as model weights are\nvery high dimensional. To address this, we introduce Probing Experts (ProbeX),\na theoretically motivated, lightweight probing method. Notably, ProbeX is the\nfirst probing method designed to learn from the weights of just a single model\nlayer. We also construct and release a dataset that simulates the structure of\npublic model repositories. Our results show that ProbeX can effectively map the\nweights of large models into a shared weight-language embedding space.\nFurthermore, we demonstrate the impressive generalization of our method,\nachieving zero-shot model classification and retrieval.\n","authors":["Eliahu Horwitz","Bar Cavia","Jonathan Kahana","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2410.13569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13773v1","updated":"2024-10-17T17:11:33Z","published":"2024-10-17T17:11:33Z","title":"Enhancing Retail Sales Forecasting with Optimized Machine Learning\n  Models","summary":"  In retail sales forecasting, accurately predicting future sales is crucial\nfor inventory management and strategic planning. Traditional methods like LR\noften fall short due to the complexity of sales data, which includes\nseasonality and numerous product families. Recent advancements in machine\nlearning (ML) provide more robust alternatives. This research benefits from the\npower of ML, particularly Random Forest (RF), Gradient Boosting (GB), Support\nVector Regression (SVR), and XGBoost, to improve prediction accuracy. Despite\nadvancements, a significant gap exists in handling complex datasets with high\nseasonality and multiple product families. The proposed solution involves\nimplementing and optimizing a RF model, leveraging hyperparameter tuning\nthrough randomized search cross-validation. This approach addresses the\ncomplexities of the dataset, capturing intricate patterns that traditional\nmethods miss. The optimized RF model achieved an R-squared value of 0.945,\nsubstantially higher than the initial RF model and traditional LR, which had an\nR-squared of 0.531. The model reduced the root mean squared logarithmic error\n(RMSLE) to 1.172, demonstrating its superior predictive capability. The\noptimized RF model did better than cutting-edge models like Gradient Boosting\n(R-squared: 0.942), SVR (R-squared: 0.940), and XGBoost (R-squared: 0.939),\nwith more minor mean squared error (MSE) and mean absolute error (MAE) numbers.\nThe results demonstrate that the optimized RF model excels in forecasting\nretail sales, handling the datasets complexity with higher accuracy and\nreliability. This research highlights the importance of advanced ML techniques\nin predictive analytics, offering a significant improvement over traditional\nmethods and other contemporary models.\n","authors":["Priyam Ganguly","Isha Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2410.13773v1.pdf","comment":"IEEE 4th ICSES 2024"},{"id":"http://arxiv.org/abs/2410.13772v1","updated":"2024-10-17T17:09:56Z","published":"2024-10-17T17:09:56Z","title":"Is Prior-Free Black-Box Non-Stationary Reinforcement Learning Feasible?","summary":"  We study the problem of Non-Stationary Reinforcement Learning (NS-RL) without\nprior knowledge about the system's non-stationarity. A state-of-the-art,\nblack-box algorithm, known as MASTER, is considered, with a focus on\nidentifying the conditions under which it can achieve its stated goals.\nSpecifically, we prove that MASTER's non-stationarity detection mechanism is\nnot triggered for practical choices of horizon, leading to performance akin to\na random restarting algorithm. Moreover, we show that the regret bound for\nMASTER, while being order optimal, stays above the worst-case linear regret\nuntil unreasonably large values of the horizon. To validate these observations,\nMASTER is tested for the special case of piecewise stationary multi-armed\nbandits, along with methods that employ random restarting, and others that use\nquickest change detection to restart. A simple, order optimal random restarting\nalgorithm, that has prior knowledge of the non-stationarity is proposed as a\nbaseline. The behavior of the MASTER algorithm is validated in simulations, and\nit is shown that methods employing quickest change detection are more robust\nand consistently outperform MASTER and other random restarting approaches.\n","authors":["Argyrios Gerogiannis","Yu-Han Huang","Venugopal V. Veeravalli"],"pdf_url":"https://arxiv.org/pdf/2410.13772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13770v1","updated":"2024-10-17T17:08:39Z","published":"2024-10-17T17:08:39Z","title":"Probing the Latent Hierarchical Structure of Data via Diffusion Models","summary":"  High-dimensional data must be highly structured to be learnable. Although the\ncompositional and hierarchical nature of data is often put forward to explain\nlearnability, quantitative measurements establishing these properties are\nscarce. Likewise, accessing the latent variables underlying such a data\nstructure remains a challenge. In this work, we show that forward-backward\nexperiments in diffusion-based models, where data is noised and then denoised\nto generate new samples, are a promising tool to probe the latent structure of\ndata. We predict in simple hierarchical models that, in this process, changes\nin data occur by correlated chunks, with a length scale that diverges at a\nnoise level where a phase transition is known to take place. Remarkably, we\nconfirm this prediction in both text and image datasets using state-of-the-art\ndiffusion models. Our results show how latent variable changes manifest in the\ndata and establish how to measure these effects in real data using diffusion\nmodels.\n","authors":["Antonio Sclocchi","Alessandro Favero","Noam Itzhak Levi","Matthieu Wyart"],"pdf_url":"https://arxiv.org/pdf/2410.13770v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2405.11785v2","updated":"2024-10-17T17:00:37Z","published":"2024-05-20T05:08:55Z","title":"Guided Multi-objective Generative AI to Enhance Structure-based Drug\n  Design","summary":"  Generative AI has the potential to revolutionize drug discovery. Yet, despite\nrecent advances in deep learning, existing models cannot generate molecules\nthat satisfy all desired physicochemical properties. Herein, we describe\nIDOLpro, a generative chemistry AI combining diffusion with multi-objective\noptimization for structure-based drug design. Differentiable scoring functions\nguide the latent variables of the diffusion model to explore uncharted chemical\nspace and generate novel ligands in silico, optimizing a plurality of target\nphysicochemical properties. We demonstrate our platform's effectiveness by\ngenerating ligands with optimized binding affinity and synthetic accessibility\non two benchmark sets. IDOLpro produces ligands with binding affinities over\n10%-20% better than the next best state-of-the-art method on each test set,\nproducing more drug-like molecules with generally better synthetic\naccessibility scores than other methods. We do a head-to-head comparison of\nIDOLpro against a classic virtual screen of a large database of drug-like\nmolecules. We show that IDOLpro can generate molecules for a range of important\ndisease-related targets with better binding affinity and synthetic\naccessibility than any molecule found in the virtual screen while being over\n100x faster and less expensive to run. On a test set of experimental complexes,\nIDOLpro is the first to produce molecules with better binding affinities than\nexperimentally observed ligands. IDOLpro can accommodate other scoring\nfunctions (e.g. ADME-Tox) to accelerate hit-finding, hit-to-lead, and lead\noptimization for drug discovery.\n","authors":["Amit Kadan","Kevin Ryczko","Erika Lloyd","Adrian Roitberg","Takeshi Yamazaki"],"pdf_url":"https://arxiv.org/pdf/2405.11785v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19300v2","updated":"2024-10-17T16:59:19Z","published":"2023-10-30T06:35:31Z","title":"Stage-Aware Learning for Dynamic Treatments","summary":"  Recent advances in dynamic treatment regimes (DTRs) facilitate the search for\noptimal treatments, which are tailored to individuals' specific needs and able\nto maximize their expected clinical benefits. However, existing algorithms\nrelying on consistent trajectories, such as inverse probability weighting\nestimators (IPWEs), could suffer from insufficient sample size under optimal\ntreatments and a growing number of decision-making stages, particularly in the\ncontext of chronic diseases. To address these challenges, we propose a novel\nindividualized learning method which estimates the DTR with a focus on\nprioritizing alignment between the observed treatment trajectory and the one\nobtained by the optimal regime across decision stages. By relaxing the\nrestriction that the observed trajectory must be fully aligned with the optimal\ntreatments, our approach substantially improves the sample efficiency and\nstability of IPWE-based methods. In particular, the proposed learning scheme\nbuilds a more general framework which includes the popular outcome weighted\nlearning framework as a special case of ours. Moreover, we introduce the notion\nof stage importance scores along with an attention mechanism to explicitly\naccount for heterogeneity among decision stages. We establish the theoretical\nproperties of the proposed approach, including the Fisher consistency and\nfinite-sample performance bound. Empirically, we evaluate the proposed method\nin extensive simulated environments and a real case study for the COVID-19\npandemic.\n","authors":["Hanwen Ye","Wenzhuo Zhou","Ruoqing Zhu","Annie Qu"],"pdf_url":"https://arxiv.org/pdf/2310.19300v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13762v1","updated":"2024-10-17T16:56:04Z","published":"2024-10-17T16:56:04Z","title":"Virtual Sensing for Real-Time Degradation Monitoring of Nuclear Systems:\n  Leveraging DeepONet for Enhanced Sensing Coverage for Digital Twin-Enabling\n  Technology","summary":"  Effective real-time monitoring technique is crucial for detecting material\ndegradation and maintaining the structural integrity of nuclear systems to\nensure both safety and operational efficiency. Traditional physical sensor\nsystems face limitations such as installation challenges, high costs, and\ndifficulties in measuring critical parameters in hard-to-reach or harsh\nenvironments, often resulting in incomplete data coverage. Machine\nlearning-driven virtual sensors offer a promising solution by enhancing\nphysical sensor capabilities to monitor critical degradation indicators like\npressure, velocity, and turbulence. However, conventional machine learning\nmodels struggle with real-time monitoring due to the high-dimensional nature of\nreactor data and the need for frequent retraining. This paper explores the use\nof Deep Operator Networks (DeepONet) within a digital twin (DT) framework to\npredict key thermal-hydraulic parameters in the hot leg of an AP-1000\nPressurized Water Reactor (PWR). In this study, DeepONet is trained with\ndifferent operational conditions, which relaxes the requirement of continuous\nretraining, making it suitable for online and real-time prediction components\nfor DT. Our results show that DeepONet achieves accurate predictions with low\nmean squared error and relative L2 error and can make predictions on unknown\ndata 160,000 times faster than traditional finite element (FE) simulations.\nThis speed and accuracy make DeepONet a powerful tool for tracking conditions\nthat contribute to material degradation in real-time, enhancing reactor safety\nand longevity.\n","authors":["Raisa Bentay Hossain","Farid Ahmed","Kazuma Kobayashi","Seid Koric","Diab Abueidda","Syed Bahauddin Alam"],"pdf_url":"https://arxiv.org/pdf/2410.13762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13761v1","updated":"2024-10-17T16:56:01Z","published":"2024-10-17T16:56:01Z","title":"GDeR: Safeguarding Efficiency, Balancing, and Robustness via\n  Prototypical Graph Pruning","summary":"  Training high-quality deep models necessitates vast amounts of data,\nresulting in overwhelming computational and memory demands. Recently, data\npruning, distillation, and coreset selection have been developed to streamline\ndata volume by retaining, synthesizing, or selecting a small yet informative\nsubset from the full set. Among these methods, data pruning incurs the least\nadditional training cost and offers the most practical acceleration benefits.\nHowever, it is the most vulnerable, often suffering significant performance\ndegradation with imbalanced or biased data schema, thus raising concerns about\nits accuracy and reliability in on-device deployment. Therefore, there is a\nlooming need for a new data pruning paradigm that maintains the efficiency of\nprevious practices while ensuring balance and robustness. Unlike the fields of\ncomputer vision and natural language processing, where mature solutions have\nbeen developed to address these issues, graph neural networks (GNNs) continue\nto struggle with increasingly large-scale, imbalanced, and noisy datasets,\nlacking a unified dataset pruning solution. To achieve this, we introduce a\nnovel dynamic soft-pruning method, GDeR, designed to update the training\n``basket'' during the process using trainable prototypes. GDeR first constructs\na well-modeled graph embedding hypersphere and then samples\n\\textit{representative, balanced, and unbiased subsets} from this embedding\nspace, which achieves the goal we called Graph Training Debugging. Extensive\nexperiments on five datasets across three GNN backbones, demonstrate that GDeR\n(I) achieves or surpasses the performance of the full dataset with 30%~50%\nfewer training samples, (II) attains up to a 2.81x lossless training speedup,\nand (III) outperforms state-of-the-art pruning methods in imbalanced training\nand noisy training scenarios by 0.3%~4.3% and 3.6%~7.8%, respectively.\n","authors":["Guibin Zhang","Haonan Dong","Yuchen Zhang","Zhixun Li","Dingshuo Chen","Kai Wang","Tianlong Chen","Yuxuan Liang","Dawei Cheng","Kun Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13761v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.13756v1","updated":"2024-10-17T16:53:43Z","published":"2024-10-17T16:53:43Z","title":"CLIMB: Language-Guided Continual Learning for Task Planning with\n  Iterative Model Building","summary":"  Intelligent and reliable task planning is a core capability for generalized\nrobotics, requiring a descriptive domain representation that sufficiently\nmodels all object and state information for the scene. We present CLIMB, a\ncontinual learning framework for robot task planning that leverages foundation\nmodels and execution feedback to guide domain model construction. CLIMB can\nbuild a model from a natural language description, learn non-obvious predicates\nwhile solving tasks, and store that information for future problems. We\ndemonstrate the ability of CLIMB to improve performance in common planning\nenvironments compared to baseline methods. We also develop the BlocksWorld++\ndomain, a simulated environment with an easily usable real counterpart,\ntogether with a curriculum of tasks with progressing difficulty for evaluating\ncontinual learning. Additional details and demonstrations for this system can\nbe found at https://plan-with-climb.github.io/ .\n","authors":["Walker Byrnes","Miroslav Bogdanovic","Avi Balakirsky","Stephen Balakirsky","Animesh Garg"],"pdf_url":"https://arxiv.org/pdf/2410.13756v1.pdf","comment":"6 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.13754v1","updated":"2024-10-17T16:52:28Z","published":"2024-10-17T16:52:28Z","title":"MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures","summary":"  Perceiving and generating diverse modalities are crucial for AI models to\neffectively learn from and engage with real-world signals, necessitating\nreliable evaluations for their development. We identify two major issues in\ncurrent evaluations: (1) inconsistent standards, shaped by different\ncommunities with varying protocols and maturity levels; and (2) significant\nquery, grading, and generalization biases. To address these, we introduce\nMixEval-X, the first any-to-any real-world benchmark designed to optimize and\nstandardize evaluations across input and output modalities. We propose\nmulti-modal benchmark mixture and adaptation-rectification pipelines to\nreconstruct real-world task distributions, ensuring evaluations generalize\neffectively to real-world use cases. Extensive meta-evaluations show our\napproach effectively aligns benchmark samples with real-world task\ndistributions and the model rankings correlate strongly with that of\ncrowd-sourced real-world evaluations (up to 0.98). We provide comprehensive\nleaderboards to rerank existing models and organizations and offer insights to\nenhance understanding of multi-modal evaluations and inform future research.\n","authors":["Jinjie Ni","Yifan Song","Deepanway Ghosal","Bo Li","David Junhao Zhang","Xiang Yue","Fuzhao Xue","Zian Zheng","Kaichen Zhang","Mahir Shah","Kabir Jain","Yang You","Michael Shieh"],"pdf_url":"https://arxiv.org/pdf/2410.13754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13749v1","updated":"2024-10-17T16:48:51Z","published":"2024-10-17T16:48:51Z","title":"Supervised Kernel Thinning","summary":"  The kernel thinning algorithm of Dwivedi & Mackey (2024) provides a\nbetter-than-i.i.d. compression of a generic set of points. By generating\nhigh-fidelity coresets of size significantly smaller than the input points, KT\nis known to speed up unsupervised tasks like Monte Carlo integration,\nuncertainty quantification, and non-parametric hypothesis testing, with minimal\nloss in statistical accuracy. In this work, we generalize the KT algorithm to\nspeed up supervised learning problems involving kernel methods. Specifically,\nwe combine two classical algorithms--Nadaraya-Watson (NW) regression or kernel\nsmoothing, and kernel ridge regression (KRR)--with KT to provide a quadratic\nspeed-up in both training and inference times. We show how distribution\ncompression with KT in each setting reduces to constructing an appropriate\nkernel, and introduce the Kernel-Thinned NW and Kernel-Thinned KRR estimators.\nWe prove that KT-based regression estimators enjoy significantly superior\ncomputational efficiency over the full-data estimators and improved statistical\nefficiency over i.i.d. subsampling of the training data. En route, we also\nprovide a novel multiplicative error guarantee for compressing with KT. We\nvalidate our design choices with both simulations and real data experiments.\n","authors":["Albert Gong","Kyuseong Choi","Raaz Dwivedi"],"pdf_url":"https://arxiv.org/pdf/2410.13749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14015v2","updated":"2024-10-17T16:47:51Z","published":"2024-02-21T18:54:37Z","title":"Corrective Machine Unlearning","summary":"  Machine Learning models increasingly face data integrity challenges due to\nthe use of large-scale training datasets drawn from the Internet. We study what\nmodel developers can do if they detect that some data was manipulated or\nincorrect. Such manipulated data can cause adverse effects including\nvulnerability to backdoored samples, systemic biases, and reduced accuracy on\ncertain input domains. Realistically, all manipulated training samples cannot\nbe identified, and only a small, representative subset of the affected data can\nbe flagged.\n  We formalize Corrective Machine Unlearning as the problem of mitigating the\nimpact of data affected by unknown manipulations on a trained model, only\nhaving identified a subset of the corrupted data. We demonstrate that the\nproblem of corrective unlearning has significantly different requirements from\ntraditional privacy-oriented unlearning. We find most existing unlearning\nmethods, including retraining-from-scratch without the deletion set, require\nmost of the manipulated data to be identified for effective corrective\nunlearning. However, one approach, Selective Synaptic Dampening, achieves\nlimited success, unlearning adverse effects with just a small portion of the\nmanipulated samples in our setting, which shows encouraging signs for future\nprogress. We hope our work spurs research towards developing better methods for\ncorrective unlearning and offers practitioners a new strategy to handle data\nintegrity challenges arising from web-scale training. Code is available at\nhttps://github.com/drimpossible/corrective-unlearning-bench.\n","authors":["Shashwat Goel","Ameya Prabhu","Philip Torr","Ponnurangam Kumaraguru","Amartya Sanyal"],"pdf_url":"https://arxiv.org/pdf/2402.14015v2.pdf","comment":"Published in Transactions of Machine Learning Research (TMLR), 17\n  pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.01024v2","updated":"2024-10-17T16:45:16Z","published":"2024-10-01T19:33:39Z","title":"GPTreeO: An R package for continual regression with dividing local\n  Gaussian processes","summary":"  We introduce GPTreeO, a flexible R package for scalable Gaussian process (GP)\nregression, particularly tailored to continual learning problems. GPTreeO\nbuilds upon the Dividing Local Gaussian Processes (DLGP) algorithm, in which a\nbinary tree of local GP regressors is dynamically constructed using a continual\nstream of input data. In GPTreeO we extend the original DLGP algorithm by\nallowing continual optimisation of the GP hyperparameters, incorporating\nuncertainty calibration, and introducing new strategies for how the local\npartitions are created. Moreover, the modular code structure allows users to\ninterface their favourite GP library to perform the local GP regression in\nGPTreeO. The flexibility of GPTreeO gives the user fine-grained control of the\nbalance between computational speed, accuracy, stability and smoothness. We\nconduct a sensitivity analysis to show how GPTreeO's configurable features\nimpact the regression performance in a continual learning setting.\n","authors":["Timo Braun","Anders Kvellestad","Riccardo De Bin"],"pdf_url":"https://arxiv.org/pdf/2410.01024v2.pdf","comment":"Updated the bibliography, and is now equivalent to the journal\n  submission"},{"id":"http://arxiv.org/abs/2410.13746v1","updated":"2024-10-17T16:42:12Z","published":"2024-10-17T16:42:12Z","title":"Theory on Score-Mismatched Diffusion Models and Zero-Shot Conditional\n  Samplers","summary":"  The denoising diffusion model has recently emerged as a powerful generative\ntechnique, capable of transforming noise into meaningful data. While\ntheoretical convergence guarantees for diffusion models are well established\nwhen the target distribution aligns with the training distribution, practical\nscenarios often present mismatches. One common case is in zero-shot conditional\ndiffusion sampling, where the target conditional distribution is different from\nthe (unconditional) training distribution. These score-mismatched diffusion\nmodels remain largely unexplored from a theoretical perspective. In this paper,\nwe present the first performance guarantee with explicit dimensional\ndependencies for general score-mismatched diffusion samplers, focusing on\ntarget distributions with finite second moments. We show that score mismatches\nresult in an asymptotic distributional bias between the target and sampling\ndistributions, proportional to the accumulated mismatch between the target and\ntraining distributions. This result can be directly applied to zero-shot\nconditional samplers for any conditional model, irrespective of measurement\nnoise. Interestingly, the derived convergence upper bound offers useful\nguidance for designing a novel bias-optimal zero-shot sampler in linear\nconditional models that minimizes the asymptotic bias. For such bias-optimal\nsamplers, we further establish convergence guarantees with explicit\ndependencies on dimension and conditioning, applied to several interesting\ntarget distributions, including those with bounded support and Gaussian\nmixtures. Our findings are supported by numerical studies.\n","authors":["Yuchen Liang","Peizhong Ju","Yingbin Liang","Ness Shroff"],"pdf_url":"https://arxiv.org/pdf/2410.13746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13743v1","updated":"2024-10-17T16:39:53Z","published":"2024-10-17T16:39:53Z","title":"Single-Timescale Multi-Sequence Stochastic Approximation Without Fixed\n  Point Smoothness: Theories and Applications","summary":"  Stochastic approximation (SA) that involves multiple coupled sequences, known\nas multiple-sequence SA (MSSA), finds diverse applications in the fields of\nsignal processing and machine learning. However, existing theoretical\nunderstandings {of} MSSA are limited: the multi-timescale analysis implies a\nslow convergence rate, whereas the single-timescale analysis relies on a\nstringent fixed point smoothness assumption. This paper establishes tighter\nsingle-timescale analysis for MSSA, without assuming smoothness of the fixed\npoints. Our theoretical findings reveal that, when all involved operators are\nstrongly monotone, MSSA converges at a rate of $\\tilde{\\mathcal{O}}(K^{-1})$,\nwhere $K$ denotes the total number of iterations. In addition, when all\ninvolved operators are strongly monotone except for the main one, MSSA\nconverges at a rate of $\\mathcal{O}(K^{-\\frac{1}{2}})$. These theoretical\nfindings align with those established for single-sequence SA. Applying these\ntheoretical findings to bilevel optimization and communication-efficient\ndistributed learning offers relaxed assumptions and/or simpler algorithms with\nperformance guarantees, as validated by numerical experiments.\n","authors":["Yue Huang","Zhaoxian Wu","Shiqian Ma","Qing Ling"],"pdf_url":"https://arxiv.org/pdf/2410.13743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13738v1","updated":"2024-10-17T16:37:33Z","published":"2024-10-17T16:37:33Z","title":"Improved Convergence Rate for Diffusion Probabilistic Models","summary":"  Score-based diffusion models have achieved remarkable empirical performance\nin the field of machine learning and artificial intelligence for their ability\nto generate high-quality new data instances from complex distributions.\nImproving our understanding of diffusion models, including mainly convergence\nanalysis for such models, has attracted a lot of interests. Despite a lot of\ntheoretical attempts, there still exists significant gap between theory and\npractice. Towards to close this gap, we establish an iteration complexity at\nthe order of $d^{1/3}\\varepsilon^{-2/3}$, which is better than\n$d^{5/12}\\varepsilon^{-1}$, the best known complexity achieved before our work.\nThis convergence analysis is based on a randomized midpoint method, which is\nfirst proposed for log-concave sampling (Shen and Lee, 2019), and then extended\nto diffusion models by Gupta et al. (2024). Our theory accommodates\n$\\varepsilon$-accurate score estimates, and does not require log-concavity on\nthe target distribution. Moreover, the algorithm can also be parallelized to\nrun in only $O(\\log^2(d/\\varepsilon))$ parallel rounds in a similar way to\nprior works.\n","authors":["Gen Li","Yuchen Jiao"],"pdf_url":"https://arxiv.org/pdf/2410.13738v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2410.13735v1","updated":"2024-10-17T16:37:03Z","published":"2024-10-17T16:37:03Z","title":"Optimizing Probabilistic Conformal Prediction with Vectorized\n  Non-Conformity Scores","summary":"  Generative models have shown significant promise in critical domains such as\nmedical diagnosis, autonomous driving, and climate science, where reliable\ndecision-making hinges on accurate uncertainty quantification. While\nprobabilistic conformal prediction (PCP) offers a powerful framework for this\npurpose, its coverage efficiency -- the size of the uncertainty set -- is\nlimited when dealing with complex underlying distributions and a finite number\nof generated samples. In this paper, we propose a novel PCP framework that\nenhances efficiency by first vectorizing the non-conformity scores with ranked\nsamples and then optimizing the shape of the prediction set by varying the\nquantiles for samples at the same rank. Our method delivers valid coverage\nwhile producing discontinuous and more efficient prediction sets, making it\nparticularly suited for high-stakes applications. We demonstrate the\neffectiveness of our approach through experiments on both synthetic and\nreal-world datasets.\n","authors":["Minxing Zheng","Shixiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.13735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13732v1","updated":"2024-10-17T16:36:14Z","published":"2024-10-17T16:36:14Z","title":"Reducing the Transformer Architecture to a Minimum","summary":"  Transformers are a widespread and successful model architecture, particularly\nin Natural Language Processing (NLP) and Computer Vision (CV). The essential\ninnovation of this architecture is the Attention Mechanism, which solves the\nproblem of extracting relevant context information from long sequences in NLP\nand realistic scenes in CV. A classical neural network component, a Multi-Layer\nPerceptron (MLP), complements the attention mechanism. Its necessity is\nfrequently justified by its capability of modeling nonlinear relationships.\nHowever, the attention mechanism itself is nonlinear through its internal use\nof similarity measures. A possible hypothesis is that this nonlinearity is\nsufficient for modeling typical application problems. As the MLPs usually\ncontain the most trainable parameters of the whole model, their omission would\nsubstantially reduce the parameter set size. Further components can also be\nreorganized to reduce the number of parameters. Under some conditions, query\nand key matrices can be collapsed into a single matrix of the same size. The\nsame is true about value and projection matrices, which can also be omitted\nwithout eliminating the substance of the attention mechanism. Initially, the\nsimilarity measure was defined asymmetrically, with peculiar properties such as\nthat a token is possibly dissimilar to itself. A possible symmetric definition\nrequires only half of the parameters. We have laid the groundwork by testing\nwidespread CV benchmarks: MNIST and CIFAR-10. The tests have shown that\nsimplified transformer architectures (a) without MLP, (b) with collapsed\nmatrices, and (c) symmetric similarity matrices exhibit similar performance as\nthe original architecture, saving up to 90% of parameters without hurting the\nclassification performance.\n","authors":["Bernhard Bermeitinger","Tomas Hrycej","Massimo Pavone","Julianus Kath","Siegfried Handschuh"],"pdf_url":"https://arxiv.org/pdf/2410.13732v1.pdf","comment":"8 pages, to appear in KDIR2024"},{"id":"http://arxiv.org/abs/2403.08854v2","updated":"2024-10-17T16:23:42Z","published":"2024-03-13T18:00:01Z","title":"Moments of Clarity: Streamlining Latent Spaces in Machine Learning using\n  Moment Pooling","summary":"  Many machine learning applications involve learning a latent representation\nof data, which is often high-dimensional and difficult to directly interpret.\nIn this work, we propose \"Moment Pooling\", a natural extension of Deep Sets\nnetworks which drastically decrease latent space dimensionality of these\nnetworks while maintaining or even improving performance. Moment Pooling\ngeneralizes the summation in Deep Sets to arbitrary multivariate moments, which\nenables the model to achieve a much higher effective latent dimensionality for\na fixed latent dimension. We demonstrate Moment Pooling on the collider physics\ntask of quark/gluon jet classification by extending Energy Flow Networks (EFNs)\nto Moment EFNs. We find that Moment EFNs with latent dimensions as small as 1\nperform similarly to ordinary EFNs with higher latent dimension. This small\nlatent dimension allows for the internal representation to be directly\nvisualized and interpreted, which in turn enables the learned internal jet\nrepresentation to be extracted in closed form.\n","authors":["Rikab Gambhir","Athis Osathapan","Jesse Thaler"],"pdf_url":"https://arxiv.org/pdf/2403.08854v2.pdf","comment":"15+7 pages, 14 figures, 7 tables. Code available at\n  https://github.com/athiso/moment and https://github.com/rikab/MomentAnalysis;\n  v2: Updated to match journal version"},{"id":"http://arxiv.org/abs/2402.14877v2","updated":"2024-10-17T16:22:48Z","published":"2024-02-21T20:59:19Z","title":"Machine-learning prediction of tipping with applications to the Atlantic\n  Meridional Overturning Circulation","summary":"  Anticipating a tipping point, a transition from one stable steady state to\nanother, is a problem of broad relevance due to the ubiquity of the phenomenon\nin diverse fields. The steady-state nature of the dynamics about a tipping\npoint makes its prediction significantly more challenging than predicting other\ntypes of critical transitions from oscillatory or chaotic dynamics. Exploiting\nthe benefits of noise, we develop a general data-driven and machine-learning\napproach to predicting potential future tipping in nonautonomous dynamical\nsystems and validate the framework using examples from different fields. As an\napplication, we address the problem of predicting the potential collapse of the\nAtlantic Meridional Overturning Circulation (AMOC), possibly driven by\nclimate-induced changes in the freshwater input to the North Atlantic. Our\npredictions based on synthetic and currently available empirical data place a\npotential collapse window spanning from 2040 to 2065, in consistency with the\nresults in the current literature.\n","authors":["Shirin Panahi","Ling-Wei Kong","Mohammadamin Moradi","Zheng-Meng Zhai","Bryan Glaz","Mulugeta Haile","Ying-Cheng Lai"],"pdf_url":"https://arxiv.org/pdf/2402.14877v2.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.13720v1","updated":"2024-10-17T16:22:46Z","published":"2024-10-17T16:22:46Z","title":"Movie Gen: A Cast of Media Foundation Models","summary":"  We present Movie Gen, a cast of foundation models that generates\nhigh-quality, 1080p HD videos with different aspect ratios and synchronized\naudio. We also show additional capabilities such as precise instruction-based\nvideo editing and generation of personalized videos based on a user's image.\nOur models set a new state-of-the-art on multiple tasks: text-to-video\nsynthesis, video personalization, video editing, video-to-audio generation, and\ntext-to-audio generation. Our largest video generation model is a 30B parameter\ntransformer trained with a maximum context length of 73K video tokens,\ncorresponding to a generated video of 16 seconds at 16 frames-per-second. We\nshow multiple technical innovations and simplifications on the architecture,\nlatent spaces, training objectives and recipes, data curation, evaluation\nprotocols, parallelization techniques, and inference optimizations that allow\nus to reap the benefits of scaling pre-training data, model size, and training\ncompute for training large scale media generation models. We hope this paper\nhelps the research community to accelerate progress and innovation in media\ngeneration models. All videos from this paper are available at\nhttps://go.fb.me/MovieGenResearchVideos.\n","authors":["Adam Polyak","Amit Zohar","Andrew Brown","Andros Tjandra","Animesh Sinha","Ann Lee","Apoorv Vyas","Bowen Shi","Chih-Yao Ma","Ching-Yao Chuang","David Yan","Dhruv Choudhary","Dingkang Wang","Geet Sethi","Guan Pang","Haoyu Ma","Ishan Misra","Ji Hou","Jialiang Wang","Kiran Jagadeesh","Kunpeng Li","Luxin Zhang","Mannat Singh","Mary Williamson","Matt Le","Matthew Yu","Mitesh Kumar Singh","Peizhao Zhang","Peter Vajda","Quentin Duval","Rohit Girdhar","Roshan Sumbaly","Sai Saketh Rambhatla","Sam Tsai","Samaneh Azadi","Samyak Datta","Sanyuan Chen","Sean Bell","Sharadh Ramaswamy","Shelly Sheynin","Siddharth Bhattacharya","Simran Motwani","Tao Xu","Tianhe Li","Tingbo Hou","Wei-Ning Hsu","Xi Yin","Xiaoliang Dai","Yaniv Taigman","Yaqiao Luo","Yen-Cheng Liu","Yi-Chiao Wu","Yue Zhao","Yuval Kirstain","Zecheng He","Zijian He","Albert Pumarola","Ali Thabet","Artsiom Sanakoyeu","Arun Mallya","Baishan Guo","Boris Araya","Breena Kerr","Carleigh Wood","Ce Liu","Cen Peng","Dimitry Vengertsev","Edgar Schonfeld","Elliot Blanchard","Felix Juefei-Xu","Fraylie Nord","Jeff Liang","John Hoffman","Jonas Kohler","Kaolin Fire","Karthik Sivakumar","Lawrence Chen","Licheng Yu","Luya Gao","Markos Georgopoulos","Rashel Moritz","Sara K. Sampson","Shikai Li","Simone Parmeggiani","Steve Fine","Tara Fowler","Vladan Petrovic","Yuming Du"],"pdf_url":"https://arxiv.org/pdf/2410.13720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13714v1","updated":"2024-10-17T16:14:49Z","published":"2024-10-17T16:14:49Z","title":"Generation through the lens of learning theory","summary":"  We study generation through the lens of statistical learning theory. First,\nwe abstract and formalize the results of Gold [1967], Angluin [1979, 1980], and\nKleinberg and Mullainathan [2024] for language identification/generation in the\nlimit in terms of a binary hypothesis class defined over an abstract instance\nspace. Then, we formalize a different paradigm of generation studied by\nKleinberg and Mullainathan [2024], which we call ``uniform generation,\" and\nprovide a characterization of which hypothesis classes are uniformly\ngeneratable. As is standard in statistical learning theory, our\ncharacterization is in terms of the finiteness of a new combinatorial dimension\nwe call the Closure dimension. By doing so, we are able to compare\ngeneratability with predictability (captured via PAC and online learnability)\nand show that these two properties of hypothesis classes are\n\\emph{incompatible} - there are classes that are generatable but not\npredictable and vice versa.\n","authors":["Vinod Raman","Ambuj Tewari"],"pdf_url":"https://arxiv.org/pdf/2410.13714v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2410.13713v1","updated":"2024-10-17T16:12:55Z","published":"2024-10-17T16:12:55Z","title":"CrystalX: Ultra-Precision Crystal Structure Resolution and Error\n  Correction Using Deep Learning","summary":"  Atomic structure analysis of crystalline materials is a paramount endeavor in\nboth chemical and material sciences. This sophisticated technique necessitates\nnot only a solid foundation in crystallography but also a profound\ncomprehension of the intricacies of the accompanying software, posing a\nsignificant challenge in meeting the rigorous daily demands. For the first\ntime, we confront this challenge head-on by harnessing the power of deep\nlearning for ultra-precise structural analysis at the full-atom level. To\nvalidate the performance of the model, named CrystalX, we employed a vast\ndataset comprising over 50,000 X-ray diffraction measurements derived from\nauthentic experiments, demonstrating performance that is commensurate with\nhuman experts and adept at deciphering intricate geometric patterns.\nRemarkably, CrystalX revealed that even peer-reviewed publications can harbor\nerrors that are stealthy to human scrutiny, yet CrystalX adeptly rectifies\nthem. This deep learning model revolutionizes the time frame for crystal\nstructure analysis, slashing it down to seconds. It has already been\nsuccessfully applied in the structure analysis of newly discovered compounds in\nthe latest research without human intervention. Overall, CrystalX marks the\nbeginning of a new era in automating routine structural analysis within\nself-driving laboratories.\n","authors":["Kaipeng Zheng","Weiran Huang","Wanli Ouyang","Han-Sen Zhong","Yuqiang Li"],"pdf_url":"https://arxiv.org/pdf/2410.13713v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13709v1","updated":"2024-10-17T16:09:32Z","published":"2024-10-17T16:09:32Z","title":"On-device Federated Learning in Smartphones for Detecting Depression\n  from Reddit Posts","summary":"  Depression detection using deep learning models has been widely explored in\nprevious studies, especially due to the large amounts of data available from\nsocial media posts. These posts provide valuable information about individuals'\nmental health conditions and can be leveraged to train models and identify\npatterns in the data. However, distributed learning approaches have not been\nextensively explored in this domain. In this study, we adopt Federated Learning\n(FL) to facilitate decentralized training on smartphones while protecting user\ndata privacy. We train three neural network architectures--GRU, RNN, and LSTM\non Reddit posts to detect signs of depression and evaluate their performance\nunder heterogeneous FL settings. To optimize the training process, we leverage\na common tokenizer across all client devices, which reduces the computational\nload. Additionally, we analyze resource consumption and communication costs on\nsmartphones to assess their impact in a real-world FL environment. Our\nexperimental results demonstrate that the federated models achieve comparable\nperformance to the centralized models. This study highlights the potential of\nFL for decentralized mental health prediction by providing a secure and\nefficient model training process on edge devices.\n","authors":["Mustofa Ahmed","Abdul Muntakim","Nawrin Tabassum","Mohammad Asifur Rahim","Faisal Muhammad Shah"],"pdf_url":"https://arxiv.org/pdf/2410.13709v1.pdf","comment":"11 pages, 7 figures, Submitted to IEEE"},{"id":"http://arxiv.org/abs/2410.13708v1","updated":"2024-10-17T16:08:06Z","published":"2024-10-17T16:08:06Z","title":"On the Role of Attention Heads in Large Language Model Safety","summary":"  Large language models (LLMs) achieve state-of-the-art performance on multiple\nlanguage tasks, yet their safety guardrails can be circumvented, leading to\nharmful generations. In light of this, recent research on safety mechanisms has\nemerged, revealing that when safety representations or component are\nsuppressed, the safety capability of LLMs are compromised. However, existing\nresearch tends to overlook the safety impact of multi-head attention\nmechanisms, despite their crucial role in various model functionalities. Hence,\nin this paper, we aim to explore the connection between standard attention\nmechanisms and safety capability to fill this gap in the safety-related\nmechanistic interpretability. We propose a novel metric which tailored for\nmulti-head attention, the Safety Head ImPortant Score (Ships), to assess the\nindividual heads' contributions to model safety. Based on this, we generalize\nShips to the dataset level and further introduce the Safety Attention Head\nAttRibution Algorithm (Sahara) to attribute the critical safety attention heads\ninside the model. Our findings show that the special attention head has a\nsignificant impact on safety. Ablating a single safety head allows aligned\nmodel (e.g., Llama-2-7b-chat) to respond to 16 times more harmful queries,\nwhile only modifying 0.006% of the parameters, in contrast to the ~ 5%\nmodification required in previous studies. More importantly, we demonstrate\nthat attention heads primarily function as feature extractors for safety and\nmodels fine-tuned from the same base model exhibit overlapping safety heads\nthrough comprehensive experiments. Together, our attribution approach and\nfindings provide a novel perspective for unpacking the black box of safety\nmechanisms within large models.\n","authors":["Zhenhong Zhou","Haiyang Yu","Xinghua Zhang","Rongwu Xu","Fei Huang","Kun Wang","Yang Liu","Junfeng Fang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2410.13708v1.pdf","comment":"28 pages, 18 figures, 7 tables"},{"id":"http://arxiv.org/abs/2406.10322v2","updated":"2024-10-17T16:06:18Z","published":"2024-06-14T17:41:55Z","title":"LieRE: Generalizing Rotary Position Encodings","summary":"  While Rotary Position Embeddings (RoPE) for large language models have become\nwidely adopted, their application for other modalities has been slower. Here,\nwe introduce Lie group Relative position Encodings (LieRE) that goes beyond\nRoPE in supporting n-dimensional inputs. We evaluate the performance of LieRE\non 2D and 3D image classification tasks and observe that LieRE leads to marked\nrelative improvements in performance (up to 9.7% for 2D and up to 25.5% for\n3D), training efficiency (3.5x reduction), data efficiency (30%) compared to\nthe baselines of DeiT III, RoPE-Mixed and Vision-Llama.\nhttps://github.com/Stanford-AIMI/LieRE\n","authors":["Sophie Ostmeier","Brian Axelrod","Michael E. Moseley","Akshay Chaudhari","Curtis Langlotz"],"pdf_url":"https://arxiv.org/pdf/2406.10322v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13696v1","updated":"2024-10-17T16:03:43Z","published":"2024-10-17T16:03:43Z","title":"Efficient Function Placement in Virtual Networks: An Online Learning\n  Approach","summary":"  We propose a model for the virtual function placement problem and several\nnovel algorithms using ideas based on multi-armed bandits. We prove that these\nalgorithms learn the optimal placement policy rapidly, and their regret grows\nat a rate at most $O( N M \\sqrt{T\\ln T} )$ while respecting the feasibility\nconstraints with high probability. We show through numerical experiments that\nthose algorithms both have good practical performance and modest computational\ncomplexity. Using the proposed acceleration technique, they can be used to\nlearn in large networks where computational power is limited. Our experiments\nare fully reproducible, and the code is publicly available.\n","authors":["Wei Huang","Richard Combes","Hind Castel-Taleb","Badii Jouaber"],"pdf_url":"https://arxiv.org/pdf/2410.13696v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2406.16635v2","updated":"2024-10-17T15:45:10Z","published":"2024-06-24T13:41:08Z","title":"ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models","summary":"  The high power consumption and latency-sensitive deployments of large\nlanguage models (LLMs) have motivated efficiency techniques like quantization\nand sparsity. Contextual sparsity, where the sparsity pattern is\ninput-dependent, is crucial in LLMs because the permanent removal of attention\nheads or neurons from LLMs can significantly degrade accuracy. Prior work has\nattempted to model contextual sparsity using neural networks trained to predict\nactivation magnitudes, which can be used to dynamically prune structures with\nlow predicted activation magnitude. In this paper, we look beyond\nmagnitude-based pruning criteria to assess attention head and neuron importance\nin LLMs. We develop a novel predictor called ShadowLLM, which can shadow the\nLLM behavior and enforce better sparsity patterns, resulting in over 15%\nimprovement in end-to-end accuracy compared to prior methods. In addition,\nShadowLLM achieves up to a 20% speed-up over the state-of-the-art DejaVu\nframework. These enhancements are validated on Llama-2 and OPT models with up\nto 30 billion parameters. Our code is available at\n\\href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM}.\n","authors":["Yash Akhauri","Ahmed F AbouElhamayed","Jordan Dotzel","Zhiru Zhang","Alexander M Rush","Safeen Huda","Mohamed S Abdelfattah"],"pdf_url":"https://arxiv.org/pdf/2406.16635v2.pdf","comment":"Accepted to EMNLP 2024 (Main, Long Paper)"},{"id":"http://arxiv.org/abs/2402.13251v3","updated":"2024-10-17T15:45:06Z","published":"2024-02-20T18:59:00Z","title":"FlashTex: Fast Relightable Mesh Texturing with LightControlNet","summary":"  Manually creating textures for 3D meshes is time-consuming, even for expert\nvisual content creators. We propose a fast approach for automatically texturing\nan input 3D mesh based on a user-provided text prompt. Importantly, our\napproach disentangles lighting from surface material/reflectance in the\nresulting texture so that the mesh can be properly relit and rendered in any\nlighting environment. We introduce LightControlNet, a new text-to-image model\nbased on the ControlNet architecture, which allows the specification of the\ndesired lighting as a conditioning image to the model. Our text-to-texture\npipeline then constructs the texture in two stages. The first stage produces a\nsparse set of visually consistent reference views of the mesh using\nLightControlNet. The second stage applies a texture optimization based on Score\nDistillation Sampling (SDS) that works with LightControlNet to increase the\ntexture quality while disentangling surface material from lighting. Our\nalgorithm is significantly faster than previous text-to-texture methods, while\nproducing high-quality and relightable textures.\n","authors":["Kangle Deng","Timothy Omernick","Alexander Weiss","Deva Ramanan","Jun-Yan Zhu","Tinghui Zhou","Maneesh Agrawala"],"pdf_url":"https://arxiv.org/pdf/2402.13251v3.pdf","comment":"Project page: https://flashtex.github.io/"},{"id":"http://arxiv.org/abs/2410.13681v1","updated":"2024-10-17T15:41:06Z","published":"2024-10-17T15:41:06Z","title":"Ab initio nonparametric variable selection for scalable Symbolic\n  Regression with large $p$","summary":"  Symbolic regression (SR) is a powerful technique for discovering symbolic\nexpressions that characterize nonlinear relationships in data, gaining\nincreasing attention for its interpretability, compactness, and robustness.\nHowever, existing SR methods do not scale to datasets with a large number of\ninput variables (referred to as extreme-scale SR), which are common in modern\nscientific applications. This ``large $p$'' setting, often accompanied by\nmeasurement error, leads to slow performance of SR methods and overly complex\nexpressions that are difficult to interpret. To address this scalability\nchallenge, we propose a method called PAN+SR, which combines a key idea of ab\ninitio nonparametric variable selection with SR to efficiently pre-screen large\ninput spaces and reduce search complexity while maintaining accuracy. The use\nof nonparametric methods eliminates model misspecification, supporting a\nstrategy called parametric-assisted nonparametric (PAN). We also extend\nSRBench, an open-source benchmarking platform, by incorporating\nhigh-dimensional regression problems with various signal-to-noise ratios. Our\nresults demonstrate that PAN+SR consistently enhances the performance of 17\ncontemporary SR methods, enabling several to achieve state-of-the-art\nperformance on these challenging datasets.\n","authors":["Shengbin Ye","Meng Li"],"pdf_url":"https://arxiv.org/pdf/2410.13681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15355v4","updated":"2024-10-17T15:27:30Z","published":"2024-09-14T02:34:26Z","title":"Block-Attention for Efficient RAG","summary":"  We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.\n","authors":["East Sun","Yan Wang","Lan Tian"],"pdf_url":"https://arxiv.org/pdf/2409.15355v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00489v2","updated":"2024-10-17T15:20:23Z","published":"2024-03-30T23:07:58Z","title":"Prompt-SAW: Leveraging Relation-Aware Graphs for Textual Prompt\n  Compression","summary":"  Large Language Models (LLMs) have shown exceptional abilities for multiple\ndifferent natural language processing tasks. While prompting is a crucial tool\nfor LLM inference, we observe that there is a significant cost associated with\nexceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead\nto substandard results in terms of readability/interpretability of the\ncompressed prompt, with a detrimental impact on prompt utility. To address\nthis, we propose PromptSAW: Prompt compresSion via Relation AWare graphs, an\neffective strategy for prompt compression over task-agnostic and task-aware\nprompts. Prompt-SAW uses the prompt's textual information to build a graph and\nlater extracts key information elements in the graph to come up with the\ncompressed prompt. We also propose GSM8K-aug, i.e., an extended version of the\nexisting GSM8K benchmark for task-agnostic prompts in order to provide a\ncomprehensive evaluation platform. Experimental evaluation using benchmark\ndatasets shows that prompts compressed by Prompt-SAW are not only better in\nterms of readability, but they also outperform the best-performing baseline\nmodels by up to 10.1 and 77.1, respectively, for task-agnostic and task-aware\nsettings while compressing the original prompt text by 34.9 and 56.7.\n","authors":["Muhammad Asif Ali","Zhengping Li","Shu Yang","Keyuan Cheng","Yang Cao","Tianhao Huang","Guimin Hu","Weimin Lyu","Lijie Hu","Lu Yu","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2404.00489v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2410.12176v2","updated":"2024-10-17T15:18:31Z","published":"2024-10-16T02:44:36Z","title":"Expected Sliced Transport Plans","summary":"  The optimal transport (OT) problem has gained significant traction in modern\nmachine learning for its ability to: (1) provide versatile metrics, such as\nWasserstein distances and their variants, and (2) determine optimal couplings\nbetween probability measures. To reduce the computational complexity of OT\nsolvers, methods like entropic regularization and sliced optimal transport have\nbeen proposed. The sliced OT framework improves efficiency by comparing\none-dimensional projections (slices) of high-dimensional distributions.\nHowever, despite their computational efficiency, sliced-Wasserstein approaches\nlack a transportation plan between the input measures, limiting their use in\nscenarios requiring explicit coupling. In this paper, we address two key\nquestions: Can a transportation plan be constructed between two probability\nmeasures using the sliced transport framework? If so, can this plan be used to\ndefine a metric between the measures? We propose a \"lifting\" operation to\nextend one-dimensional optimal transport plans back to the original space of\nthe measures. By computing the expectation of these lifted plans, we derive a\nnew transportation plan, termed expected sliced transport (EST) plans. We prove\nthat using the EST plan to weight the sum of the individual Euclidean costs for\nmoving from one point to another results in a valid metric between the input\ndiscrete probability measures. We demonstrate the connection between our\napproach and the recently proposed min-SWGG, along with illustrative numerical\nexamples that support our theoretical findings.\n","authors":["Xinran Liu","Rocío Díaz Martín","Yikun Bai","Ashkan Shahbazi","Matthew Thorpe","Akram Aldroubi","Soheil Kolouri"],"pdf_url":"https://arxiv.org/pdf/2410.12176v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13645v1","updated":"2024-10-17T15:12:55Z","published":"2024-10-17T15:12:55Z","title":"Automated Model Discovery for Tensional Homeostasis: Constitutive\n  Machine Learning in Growth and Remodeling","summary":"  Soft biological tissues exhibit a tendency to maintain a preferred state of\ntensile stress, known as tensional homeostasis, which is restored even after\nexternal mechanical stimuli. This macroscopic behavior can be described using\nthe theory of kinematic growth, where the deformation gradient is\nmultiplicatively decomposed into an elastic part and a part related to growth\nand remodeling. Recently, the concept of homeostatic surfaces was introduced to\ndefine the state of homeostasis and the evolution equations for inelastic\ndeformations.\n  However, identifying the optimal model and material parameters to accurately\ncapture the macroscopic behavior of inelastic materials can only be\naccomplished with significant expertise, is often time-consuming, and prone to\nerror, regardless of the specific inelastic phenomenon. To address this\nchallenge, built-in physics machine learning algorithms offer significant\npotential.\n  In this work, we extend our inelastic Constitutive Artificial Neural Networks\n(iCANNs) by incorporating kinematic growth and homeostatic surfaces to discover\nthe scalar model equations, namely the Helmholtz free energy and the pseudo\npotential. The latter describes the state of homeostasis in a smeared sense. We\nevaluate the ability of the proposed network to learn from experimentally\nobtained tissue equivalent data at the material point level, assess its\npredictive accuracy beyond the training regime, and discuss its current\nlimitations when applied at the structural level.\n  Our source code, data, examples, and an implementation of the corresponding\nmaterial subroutine are made accessible to the public at\nhttps://doi.org/10.5281/zenodo.13946282.\n","authors":["Hagen Holthusen","Tim Brepols","Kevin Linka","Ellen Kuhl"],"pdf_url":"https://arxiv.org/pdf/2410.13645v1.pdf","comment":"46 pages, 12 figures, 5 tables"},{"id":"http://arxiv.org/abs/2410.13643v1","updated":"2024-10-17T15:10:13Z","published":"2024-10-17T15:10:13Z","title":"Fine-Tuning Discrete Diffusion Models via Reward Optimization with\n  Applications to DNA and Protein Design","summary":"  Recent studies have demonstrated the strong empirical performance of\ndiffusion models on discrete sequences across domains from natural language to\nbiological sequence generation. For example, in the protein inverse folding\ntask, conditional diffusion models have achieved impressive results in\ngenerating natural-like sequences that fold back into the original structure.\nHowever, practical design tasks often require not only modeling a conditional\ndistribution but also optimizing specific task objectives. For instance, we may\nprefer protein sequences with high stability. To address this, we consider the\nscenario where we have pre-trained discrete diffusion models that can generate\nnatural-like sequences, as well as reward models that map sequences to task\nobjectives. We then formulate the reward maximization problem within discrete\ndiffusion models, analogous to reinforcement learning (RL), while minimizing\nthe KL divergence against pretrained diffusion models to preserve naturalness.\nTo solve this RL problem, we propose a novel algorithm, DRAKES, that enables\ndirect backpropagation of rewards through entire trajectories generated by\ndiffusion models, by making the originally non-differentiable trajectories\ndifferentiable using the Gumbel-Softmax trick. Our theoretical analysis\nindicates that our approach can generate sequences that are both natural-like\nand yield high rewards. While similar tasks have been recently explored in\ndiffusion models for continuous domains, our work addresses unique algorithmic\nand theoretical challenges specific to discrete diffusion models, which arise\nfrom their foundation in continuous-time Markov chains rather than Brownian\nmotion. Finally, we demonstrate the effectiveness of DRAKES in generating DNA\nand protein sequences that optimize enhancer activity and protein stability,\nrespectively, important tasks for gene therapies and protein-based\ntherapeutics.\n","authors":["Chenyu Wang","Masatoshi Uehara","Yichun He","Amy Wang","Tommaso Biancalani","Avantika Lal","Tommi Jaakkola","Sergey Levine","Hanchen Wang","Aviv Regev"],"pdf_url":"https://arxiv.org/pdf/2410.13643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13640v1","updated":"2024-10-17T15:09:24Z","published":"2024-10-17T15:09:24Z","title":"Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation","summary":"  LLM self-evaluation relies on the LLM's own ability to estimate response\ncorrectness, which can greatly improve its deployment reliability. In this\nresearch track, we propose the Chain-of-Embedding (CoE) in the latent space to\nenable LLMs to perform output-free self-evaluation. CoE consists of all\nprogressive hidden states produced during the inference time, which can be\ntreated as the latent thinking path of LLMs. We find that when LLMs respond\ncorrectly and incorrectly, their CoE features differ, these discrepancies\nassist us in estimating LLM response correctness. Experiments in four diverse\ndomains and seven LLMs fully demonstrate the effectiveness of our method.\nMeanwhile, its label-free design intent without any training and\nmillisecond-level computational cost ensure real-time feedback in large-scale\nscenarios. More importantly, we provide interesting insights into LLM response\ncorrectness from the perspective of hidden state changes inside LLMs.\n","authors":["Yiming Wang","Pei Zhang","Baosong Yang","Derek F. Wong","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13640v1.pdf","comment":"33 pages, 18 figures, 12 tables"},{"id":"http://arxiv.org/abs/2410.13638v1","updated":"2024-10-17T15:08:21Z","published":"2024-10-17T15:08:21Z","title":"Scaling Wearable Foundation Models","summary":"  Wearable sensors have become ubiquitous thanks to a variety of health\ntracking features. The resulting continuous and longitudinal measurements from\neveryday life generate large volumes of data; however, making sense of these\nobservations for scientific and actionable insights is non-trivial. Inspired by\nthe empirical success of generative modeling, where large neural networks learn\npowerful representations from vast amounts of text, image, video, or audio\ndata, we investigate the scaling properties of sensor foundation models across\ncompute, data, and model size. Using a dataset of up to 40 million hours of\nin-situ heart rate, heart rate variability, electrodermal activity,\naccelerometer, skin temperature, and altimeter per-minute data from over\n165,000 people, we create LSM, a multimodal foundation model built on the\nlargest wearable-signals dataset with the most extensive range of sensor\nmodalities to date. Our results establish the scaling laws of LSM for tasks\nsuch as imputation, interpolation and extrapolation, both across time and\nsensor modalities. Moreover, we highlight how LSM enables sample-efficient\ndownstream learning for tasks like exercise and activity recognition.\n","authors":["Girish Narayanswamy","Xin Liu","Kumar Ayush","Yuzhe Yang","Xuhai Xu","Shun Liao","Jake Garrison","Shyam Tailor","Jake Sunshine","Yun Liu","Tim Althoff","Shrikanth Narayanan","Pushmeet Kohli","Jiening Zhan","Mark Malhotra","Shwetak Patel","Samy Abdel-Ghaffar","Daniel McDuff"],"pdf_url":"https://arxiv.org/pdf/2410.13638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13637v1","updated":"2024-10-17T15:07:56Z","published":"2024-10-17T15:07:56Z","title":"Normalizing self-supervised learning for provably reliable Change Point\n  Detection","summary":"  Change point detection (CPD) methods aim to identify abrupt shifts in the\ndistribution of input data streams. Accurate estimators for this task are\ncrucial across various real-world scenarios. Yet, traditional unsupervised CPD\ntechniques face significant limitations, often relying on strong assumptions or\nsuffering from low expressive power due to inherent model simplicity. In\ncontrast, representation learning methods overcome these drawbacks by offering\nflexibility and the ability to capture the full complexity of the data without\nimposing restrictive assumptions. However, these approaches are still emerging\nin the CPD field and lack robust theoretical foundations to ensure their\nreliability. Our work addresses this gap by integrating the expressive power of\nrepresentation learning with the groundedness of traditional CPD techniques. We\nadopt spectral normalization (SN) for deep representation learning in CPD tasks\nand prove that the embeddings after SN are highly informative for CPD. Our\nmethod significantly outperforms current state-of-the-art methods during the\ncomprehensive evaluation via three standard CPD datasets.\n","authors":["Alexandra Bazarova","Evgenia Romanenkova","Alexey Zaytsev"],"pdf_url":"https://arxiv.org/pdf/2410.13637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13611v1","updated":"2024-10-17T14:46:34Z","published":"2024-10-17T14:46:34Z","title":"H2OVL-Mississippi Vision Language Models Technical Report","summary":"  Smaller vision-language models (VLMs) are becoming increasingly important for\nprivacy-focused, on-device applications due to their ability to run efficiently\non consumer hardware for processing enterprise commercial documents and images.\nThese models require strong language understanding and visual capabilities to\nenhance human-machine interaction. To address this need, we present\nH2OVL-Mississippi, a pair of small VLMs trained on 37 million image-text pairs\nusing 240 hours of compute on 8 x H100 GPUs. H2OVL-Mississippi-0.8B is a tiny\nmodel with 0.8 billion parameters that specializes in text recognition,\nachieving state of the art performance on the Text Recognition portion of\nOCRBench and surpassing much larger models in this area. Additionally, we are\nreleasing H2OVL-Mississippi-2B, a 2 billion parameter model for general use\ncases, exhibiting highly competitive metrics across various academic\nbenchmarks. Both models build upon our prior work with H2O-Danube language\nmodels, extending their capabilities into the visual domain. We release them\nunder the Apache 2.0 license, making VLMs accessible to everyone, democratizing\ndocument AI and visual LLMs.\n","authors":["Shaikat Galib","Shanshan Wang","Guanshuo Xu","Pascal Pfeiffer","Ryan Chesler","Mark Landry","Sri Satish Ambati"],"pdf_url":"https://arxiv.org/pdf/2410.13611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13609v1","updated":"2024-10-17T14:45:56Z","published":"2024-10-17T14:45:56Z","title":"All models are wrong, some are useful: Model Selection with Limited\n  Labels","summary":"  With the multitude of pretrained models available thanks to the advancements\nin large-scale supervised and self-supervised learning, choosing the right\nmodel is becoming increasingly pivotal in the machine learning lifecycle.\nHowever, much like the training process, choosing the best pretrained\noff-the-shelf model for raw, unlabeled data is a labor-intensive task. To\novercome this, we introduce MODEL SELECTOR, a framework for label-efficient\nselection of pretrained classifiers. Given a pool of unlabeled target data,\nMODEL SELECTOR samples a small subset of highly informative examples for\nlabeling, in order to efficiently identify the best pretrained model for\ndeployment on this target dataset. Through extensive experiments, we\ndemonstrate that MODEL SELECTOR drastically reduces the need for labeled data\nwhile consistently picking the best or near-best performing model. Across 18\nmodel collections on 16 different datasets, comprising over 1,500 pretrained\nmodels, MODEL SELECTOR reduces the labeling cost by up to 94.15% to identify\nthe best model compared to the cost of the strongest baseline. Our results\nfurther highlight the robustness of MODEL SELECTOR in model selection, as it\nreduces the labeling cost by up to 72.41% when selecting a near-best model,\nwhose accuracy is only within 1% of the best model.\n","authors":["Patrik Okanovic","Andreas Kirsch","Jannes Kasper","Torsten Hoefler","Andreas Krause","Nezihe Merve Gürel"],"pdf_url":"https://arxiv.org/pdf/2410.13609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13605v1","updated":"2024-10-17T14:39:55Z","published":"2024-10-17T14:39:55Z","title":"Transformer-Based Approaches for Sensor-Based Human Activity\n  Recognition: Opportunities and Challenges","summary":"  Transformers have excelled in natural language processing and computer\nvision, paving their way to sensor-based Human Activity Recognition (HAR).\nPrevious studies show that transformers outperform their counterparts\nexclusively when they harness abundant data or employ compute-intensive\noptimization algorithms. However, neither of these scenarios is viable in\nsensor-based HAR due to the scarcity of data in this field and the frequent\nneed to perform training and inference on resource-constrained devices. Our\nextensive investigation into various implementations of transformer-based\nversus non-transformer-based HAR using wearable sensors, encompassing more than\n500 experiments, corroborates these concerns. We observe that transformer-based\nsolutions pose higher computational demands, consistently yield inferior\nperformance, and experience significant performance degradation when quantized\nto accommodate resource-constrained devices. Additionally, transformers\ndemonstrate lower robustness to adversarial attacks, posing a potential threat\nto user trust in HAR.\n","authors":["Clayton Souza Leite","Henry Mauranen","Aziza Zhanabatyrova","Yu Xiao"],"pdf_url":"https://arxiv.org/pdf/2410.13605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13602v1","updated":"2024-10-17T14:36:58Z","published":"2024-10-17T14:36:58Z","title":"Towards Satellite Non-IID Imagery: A Spectral Clustering-Assisted\n  Federated Learning Approach","summary":"  Low Earth orbit (LEO) satellites are capable of gathering abundant Earth\nobservation data (EOD) to enable different Internet of Things (IoT)\napplications. However, to accomplish an effective EOD processing mechanism, it\nis imperative to investigate: 1) the challenge of processing the observed data\nwithout transmitting those large-size data to the ground because the connection\nbetween the satellites and the ground stations is intermittent, and 2) the\nchallenge of processing the non-independent and identically distributed\n(non-IID) satellite data. In this paper, to cope with those challenges, we\npropose an orbit-based spectral clustering-assisted clustered federated\nself-knowledge distillation (OSC-FSKD) approach for each orbit of an LEO\nsatellite constellation, which retains the advantage of FL that the observed\ndata does not need to be sent to the ground. Specifically, we introduce\nnormalized Laplacian-based spectral clustering (NLSC) into federated learning\n(FL) to create clustered FL in each round to address the challenge resulting\nfrom non-IID data. Particularly, NLSC is adopted to dynamically group clients\ninto several clusters based on cosine similarities calculated by model updates.\nIn addition, self-knowledge distillation is utilized to construct each local\nclient, where the most recent updated local model is used to guide current\nlocal model training. Experiments demonstrate that the observation accuracy\nobtained by the proposed method is separately 1.01x, 2.15x, 1.10x, and 1.03x\nhigher than that of pFedSD, FedProx, FedAU, and FedALA approaches using the\nSAT4 dataset. The proposed method also shows superiority when using other\ndatasets.\n","authors":["Luyao Zou","Yu Min Park","Chu Myaet Thwal","Yan Kyaw Tun","Zhu Han","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2410.13602v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.13597v1","updated":"2024-10-17T14:30:27Z","published":"2024-10-17T14:30:27Z","title":"Text-Guided Multi-Property Molecular Optimization with a Diffusion\n  Language Model","summary":"  Molecular optimization (MO) is a crucial stage in drug discovery in which\ntask-oriented generated molecules are optimized to meet practical industrial\nrequirements. Existing mainstream MO approaches primarily utilize external\nproperty predictors to guide iterative property optimization. However, learning\nall molecular samples in the vast chemical space is unrealistic for predictors.\nAs a result, errors and noise are inevitably introduced during property\nprediction due to the nature of approximation. This leads to discrepancy\naccumulation, generalization reduction and suboptimal molecular candidates. In\nthis paper, we propose a text-guided multi-property molecular optimization\nmethod utilizing transformer-based diffusion language model (TransDLM).\nTransDLM leverages standardized chemical nomenclature as semantic\nrepresentations of molecules and implicitly embeds property requirements into\ntextual descriptions, thereby preventing error propagation during diffusion\nprocess. Guided by physically and chemically detailed textual descriptions,\nTransDLM samples and optimizes encoded source molecules, retaining core\nscaffolds of source molecules and ensuring structural similarities. Moreover,\nTransDLM enables simultaneous sampling of multiple molecules, making it ideal\nfor scalable, efficient large-scale optimization through distributed\ncomputation on web platforms. Furthermore, our approach surpasses\nstate-of-the-art methods in optimizing molecular structural similarity and\nenhancing chemical properties on the benchmark dataset. The code is available\nat: https://anonymous.4open.science/r/TransDLM-A901.\n","authors":["Yida Xiong","Kun Li","Weiwei Liu","Jia Wu","Bo Du","Shirui Pan","Wenbin Hu"],"pdf_url":"https://arxiv.org/pdf/2410.13597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13579v1","updated":"2024-10-17T14:12:57Z","published":"2024-10-17T14:12:57Z","title":"Towards Better Performance in Incomplete LDL: Addressing Data Imbalance","summary":"  Label Distribution Learning (LDL) is a novel machine learning paradigm that\naddresses the problem of label ambiguity and has found widespread applications.\nObtaining complete label distributions in real-world scenarios is challenging,\nwhich has led to the emergence of Incomplete Label Distribution Learning\n(InLDL). However, the existing InLDL methods overlook a crucial aspect of LDL\ndata: the inherent imbalance in label distributions. To address this\nlimitation, we propose \\textbf{Incomplete and Imbalance Label Distribution\nLearning (I\\(^2\\)LDL)}, a framework that simultaneously handles incomplete\nlabels and imbalanced label distributions. Our method decomposes the label\ndistribution matrix into a low-rank component for frequent labels and a sparse\ncomponent for rare labels, effectively capturing the structure of both head and\ntail labels. We optimize the model using the Alternating Direction Method of\nMultipliers (ADMM) and derive generalization error bounds via Rademacher\ncomplexity, providing strong theoretical guarantees. Extensive experiments on\n15 real-world datasets demonstrate the effectiveness and robustness of our\nproposed framework compared to existing InLDL methods.\n","authors":["Zhiqiang Kou","Haoyuan Xuan","Jing Wang","Yuheng Jia","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2410.13579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13577v1","updated":"2024-10-17T14:12:35Z","published":"2024-10-17T14:12:35Z","title":"Sample Compression Hypernetworks: From Generalization Bounds to\n  Meta-Learning","summary":"  Reconstruction functions are pivotal in sample compression theory, a\nframework for deriving tight generalization bounds. From a small sample of the\ntraining set (the compression set) and an optional stream of information (the\nmessage), they recover a predictor previously learned from the whole training\nset. While usually fixed, we propose to learn reconstruction functions. To\nfacilitate the optimization and increase the expressiveness of the message, we\nderive a new sample compression generalization bound for real-valued messages.\nFrom this theoretical analysis, we then present a new hypernetwork architecture\nthat outputs predictors with tight generalization guarantees when trained using\nan original meta-learning framework. The results of promising preliminary\nexperiments are then reported.\n","authors":["Benjamin Leblanc","Mathieu Bazinet","Nathaniel D'Amours","Alexandre Drouin","Pascal Germain"],"pdf_url":"https://arxiv.org/pdf/2410.13577v1.pdf","comment":"Accepted at the NeurIPS 2024 workshop on Compression in Machine\n  Learning"},{"id":"http://arxiv.org/abs/2402.06165v5","updated":"2024-10-17T14:10:16Z","published":"2024-02-09T03:48:20Z","title":"Learning Contrastive Feature Representations for Facial Action Unit\n  Detection","summary":"  Facial action unit (AU) detection has long encountered the challenge of\ndetecting subtle feature differences when AUs activate. Existing methods often\nrely on encoding pixel-level information of AUs, which not only encodes\nadditional redundant information but also leads to increased model complexity\nand limited generalizability. Additionally, the accuracy of AU detection is\nnegatively impacted by the class imbalance issue of each AU type, and the\npresence of noisy and false AU labels. In this paper, we introduce a novel\ncontrastive learning framework aimed for AU detection that incorporates both\nself-supervised and supervised signals, thereby enhancing the learning of\ndiscriminative features for accurate AU detection. To tackle the class\nimbalance issue, we employ a negative sample re-weighting strategy that adjusts\nthe step size of updating parameters for minority and majority class samples.\nMoreover, to address the challenges posed by noisy and false AU labels, we\nemploy a sampling technique that encompasses three distinct types of positive\nsample pairs. This enables us to inject self-supervised signals into the\nsupervised signal, effectively mitigating the adverse effects of noisy labels.\nOur experimental assessments, conducted on four widely-utilized benchmark\ndatasets (BP4D, DISFA, GFT and Aff-Wild2), underscore the superior performance\nof our approach compared to state-of-the-art methods of AU detection. Our code\nis available at \\url{https://github.com/Ziqiao-Shang/AUNCE}.\n","authors":["Ziqiao Shang","Bin Liu","Fengmao Lv","Fei Teng","Tianrui Li"],"pdf_url":"https://arxiv.org/pdf/2402.06165v5.pdf","comment":"35 pages, 18 figures, submitted to Pattern Recognition (PR)"},{"id":"http://arxiv.org/abs/2410.13563v1","updated":"2024-10-17T14:00:18Z","published":"2024-10-17T14:00:18Z","title":"Ornstein-Uhlenbeck Adaptation as a Mechanism for Learning in Brains and\n  Machines","summary":"  Learning is a fundamental property of intelligent systems, observed across\nbiological organisms and engineered systems. While modern intelligent systems\ntypically rely on gradient descent for learning, the need for exact gradients\nand complex information flow makes its implementation in biological and\nneuromorphic systems challenging. This has motivated the exploration of\nalternative learning mechanisms that can operate locally and do not rely on\nexact gradients. In this work, we introduce a novel approach that leverages\nnoise in the parameters of the system and global reinforcement signals. Using\nan Ornstein-Uhlenbeck process with adaptive dynamics, our method balances\nexploration and exploitation during learning, driven by deviations from error\npredictions, akin to reward prediction error. Operating in continuous time,\nOrstein-Uhlenbeck adaptation (OUA) is proposed as a general mechanism for\nlearning dynamic, time-evolving environments. We validate our approach across\ndiverse tasks, including supervised learning and reinforcement learning in\nfeedforward and recurrent systems. Additionally, we demonstrate that it can\nperform meta-learning, adjusting hyper-parameters autonomously. Our results\nindicate that OUA provides a viable alternative to traditional gradient-based\nmethods, with potential applications in neuromorphic computing. It also hints\nat a possible mechanism for noise-driven learning in the brain, where\nstochastic neurotransmitter release may guide synaptic adjustments.\n","authors":["Jesus Garcia Fernandez","Nasir Ahmad","Marcel van Gerven"],"pdf_url":"https://arxiv.org/pdf/2410.13563v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16710v3","updated":"2024-10-17T13:50:46Z","published":"2024-04-25T16:20:23Z","title":"LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding","summary":"  We present LayerSkip, an end-to-end solution to speed-up inference of large\nlanguage models (LLMs). First, during training we apply layer dropout, with low\ndropout rates for earlier layers and higher dropout rates for later layers, and\nan early exit loss where all transformer layers share the same exit. Second,\nduring inference, we show that this training recipe increases the accuracy of\nearly exit at earlier layers, without adding any auxiliary layers or modules to\nthe model. Third, we present a novel self-speculative decoding solution where\nwe exit at early layers and verify and correct with remaining layers of the\nmodel. Our proposed self-speculative decoding approach has less memory\nfootprint than other speculative decoding approaches and benefits from shared\ncompute and activations of the draft and verification stages. We run\nexperiments on different Llama model sizes on different types of training:\npretraining from scratch, continual pretraining, finetuning on specific data\ndomain, and finetuning on specific task. We implement our inference solution\nand show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x\non coding, and 2.0x on TOPv2 semantic parsing task. We open source our code and\ncheckpoints at https://github.com/facebookresearch/LayerSkip.\n","authors":["Mostafa Elhoushi","Akshat Shrivastava","Diana Liskovich","Basil Hosmer","Bram Wasti","Liangzhen Lai","Anas Mahmoud","Bilge Acun","Saurabh Agarwal","Ahmed Roman","Ahmed A Aly","Beidi Chen","Carole-Jean Wu"],"pdf_url":"https://arxiv.org/pdf/2404.16710v3.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2410.13548v1","updated":"2024-10-17T13:42:56Z","published":"2024-10-17T13:42:56Z","title":"Adaptive and oblivious statistical adversaries are equivalent","summary":"  We resolve a fundamental question about the ability to perform a statistical\ntask, such as learning, when an adversary corrupts the sample. Such adversaries\nare specified by the types of corruption they can make and their level of\nknowledge about the sample. The latter distinguishes between sample-adaptive\nadversaries which know the contents of the sample when choosing the corruption,\nand sample-oblivious adversaries, which do not. We prove that for all types of\ncorruptions, sample-adaptive and sample-oblivious adversaries are\n\\emph{equivalent} up to polynomial factors in the sample size. This resolves\nthe main open question introduced by \\cite{BLMT22} and further explored in\n\\cite{CHLLN23}.\n  Specifically, consider any algorithm $A$ that solves a statistical task even\nwhen a sample-oblivious adversary corrupts its input. We show that there is an\nalgorithm $A'$ that solves the same task when the corresponding sample-adaptive\nadversary corrupts its input. The construction of $A'$ is simple and maintains\nthe computational efficiency of $A$: It requests a polynomially larger sample\nthan $A$ uses and then runs $A$ on a uniformly random subsample.\n  One of our main technical tools is a new structural result relating two\ndistributions defined on sunflowers which may be of independent interest.\n","authors":["Guy Blanc","Gregory Valiant"],"pdf_url":"https://arxiv.org/pdf/2410.13548v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12294v2","updated":"2024-10-17T13:27:43Z","published":"2024-10-16T06:51:09Z","title":"LLM-based Cognitive Models of Students with Misconceptions","summary":"  Accurately modeling student cognition is crucial for developing effective\nAI-driven educational technologies. A key challenge is creating realistic\nstudent models that satisfy two essential properties: (1) accurately\nreplicating specific misconceptions, and (2) correctly solving problems where\nthese misconceptions are not applicable. This dual requirement reflects the\ncomplex nature of student understanding, where misconceptions coexist with\ncorrect knowledge. This paper investigates whether Large Language Models (LLMs)\ncan be instruction-tuned to meet this dual requirement and effectively simulate\nstudent thinking in algebra. We introduce MalAlgoPy, a novel Python library\nthat generates datasets reflecting authentic student solution patterns through\na graph-based representation of algebraic problem-solving. Utilizing MalAlgoPy,\nwe define and examine Cognitive Student Models (CSMs) - LLMs instruction tuned\nto faithfully emulate realistic student behavior. Our findings reveal that LLMs\ntrained on misconception examples can efficiently learn to replicate errors.\nHowever, the training diminishes the model's ability to solve problems\ncorrectly, particularly for problem types where the misconceptions are not\napplicable, thus failing to satisfy second property of CSMs. We demonstrate\nthat by carefully calibrating the ratio of correct to misconception examples in\nthe training data - sometimes as low as 0.25 - it is possible to develop CSMs\nthat satisfy both properties. Our insights enhance our understanding of\nAI-based student models and pave the way for effective adaptive learning\nsystems.\n","authors":["Shashank Sonkar","Xinghe Chen","Naiming Liu","Richard G. Baraniuk","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.12294v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13526v1","updated":"2024-10-17T13:14:25Z","published":"2024-10-17T13:14:25Z","title":"Generative Adversarial Synthesis of Radar Point Cloud Scenes","summary":"  For the validation and verification of automotive radars, datasets of\nrealistic traffic scenarios are required, which, how ever, are laborious to\nacquire. In this paper, we introduce radar scene synthesis using GANs as an\nalternative to the real dataset acquisition and simulation-based approaches. We\ntrain a PointNet++ based GAN model to generate realistic radar point cloud\nscenes and use a binary classifier to evaluate the performance of scenes\ngenerated using this model against a test set of real scenes. We demonstrate\nthat our GAN model achieves similar performance (~87%) to the real scenes test\nset.\n","authors":["Muhammad Saad Nawaz","Thomas Dallmann","Torsten Schoen","Dirk Heberling"],"pdf_url":"https://arxiv.org/pdf/2410.13526v1.pdf","comment":"ICMIM 2024; 7th IEEE MTT Conference"},{"id":"http://arxiv.org/abs/2406.03857v2","updated":"2024-10-17T13:08:13Z","published":"2024-06-06T08:42:36Z","title":"MuJo: Multimodal Joint Feature Space Learning for Human Activity\n  Recognition","summary":"  Human Activity Recognition (HAR) is a longstanding problem in AI with\napplications in a broad range of areas, including healthcare, sports and\nfitness, security, and more. The performance of HAR in real-world settings is\nstrongly dependent on the type and quality of the input signal that can be\nacquired. Given an unobstructed, high-quality camera view of a scene, computer\nvision systems, in particular in conjunction with foundation models, can today\nfairly reliably distinguish complex activities. On the other hand, recognition\nusing modalities such as wearable sensors (which are often more broadly\navailable, e.g., in mobile phones and smartwatches) is a more difficult\nproblem, as the signals often contain less information and labeled training\ndata is more difficult to acquire. To alleviate the need for labeled data, we\nintroduce our comprehensive Fitness Multimodal Activity Dataset (FiMAD) in this\nwork, which can be used with the proposed pre-training method MuJo (Multimodal\nJoint Feature Space Learning) to enhance HAR performance across various\nmodalities. FiMAD was created using YouTube fitness videos and contains\nparallel video, language, pose, and simulated IMU sensor data. MuJo utilizes\nthis dataset to learn a joint feature space for these modalities. We show that\nclassifiers pre-trained on FiMAD can increase the performance on real HAR\ndatasets such as MM-Fit, MyoGym, MotionSense, and MHEALTH. For instance, on\nMM-Fit, we achieve an Macro F1-Score of up to 0.855 when fine-tuning on only 2%\nof the training data and 0.942 when utilizing the full training set for\nclassification tasks. We have compared our approach to other self-supervised\nones and showed that, unlike them, ours can consistently improve on the\nbaseline network performance as well as provide a better data-efficiency.\n","authors":["Stefan Gerd Fritsch","Cennet Oguz","Vitor Fortes Rey","Lala Ray","Maximilian Kiefer-Emmanouilidis","Paul Lukowicz"],"pdf_url":"https://arxiv.org/pdf/2406.03857v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13516v1","updated":"2024-10-17T13:05:44Z","published":"2024-10-17T13:05:44Z","title":"PORTAL: Scalable Tabular Foundation Models via Content-Specific\n  Tokenization","summary":"  Self-supervised learning on tabular data seeks to apply advances from natural\nlanguage and image domains to the diverse domain of tables. However, current\ntechniques often struggle with integrating multi-domain data and require data\ncleaning or specific structural requirements, limiting the scalability of\npre-training datasets. We introduce PORTAL (Pretraining One-Row-at-a-Time for\nAll tabLes), a framework that handles various data modalities without the need\nfor cleaning or preprocessing. This simple yet powerful approach can be\neffectively pre-trained on online-collected datasets and fine-tuned to match\nstate-of-the-art methods on complex classification and regression tasks. This\nwork offers a practical advancement in self-supervised learning for large-scale\ntabular data.\n","authors":["Marco Spinaci","Marek Polewczyk","Johannes Hoffart","Markus C. Kohler","Sam Thelin","Tassilo Klein"],"pdf_url":"https://arxiv.org/pdf/2410.13516v1.pdf","comment":"Accepted at Table Representation Learning Workshop at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.13514v1","updated":"2024-10-17T13:02:06Z","published":"2024-10-17T13:02:06Z","title":"CERES: Critical-Event Reconstruction via Temporal Scene Graph Completion","summary":"  This paper proposes a method for on-demand scenario generation in simulation,\ngrounded on real-world data. Evaluating the behaviour of Autonomous Vehicles\n(AVs) in both safety-critical and regular scenarios is essential for assessing\ntheir robustness before real-world deployment. By integrating scenarios derived\nfrom real-world datasets into the simulation, we enhance the plausibility and\nvalidity of testing sets. This work introduces a novel approach that employs\ntemporal scene graphs to capture evolving spatiotemporal relationships among\nscene entities from a real-world dataset, enabling the generation of dynamic\nscenarios in simulation through Graph Neural Networks (GNNs). User-defined\naction and criticality conditioning are used to ensure flexible, tailored\nscenario creation. Our model significantly outperforms the benchmarks in\naccurately predicting links corresponding to the requested scenarios. We\nfurther evaluate the validity and compatibility of our generated scenarios in\nan off-the-shelf simulator.\n","authors":["Efimia Panagiotaki","Georgi Pramatarov","Lars Kunze","Daniele De Martini"],"pdf_url":"https://arxiv.org/pdf/2410.13514v1.pdf","comment":"7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.10905v2","updated":"2024-10-17T12:59:03Z","published":"2024-10-13T19:28:41Z","title":"Improving Generalization on the ProcGen Benchmark with Simple\n  Architectural Changes and Scale","summary":"  We demonstrate that recent advances in reinforcement learning (RL) combined\nwith simple architectural changes significantly improves generalization on the\nProcGen benchmark. These changes are frame stacking, replacing 2D convolutional\nlayers with 3D convolutional layers, and scaling up the number of convolutional\nkernels per layer. Experimental results using a single set of hyperparameters\nacross all environments show a 37.9\\% reduction in the optimality gap compared\nto the baseline (from 0.58 to 0.36). This performance matches or exceeds\ncurrent state-of-the-art methods. The proposed changes are largely orthogonal\nand therefore complementary to the existing approaches for improving\ngeneralization in RL, and our results suggest that further exploration in this\ndirection could yield substantial improvements in addressing generalization\nchallenges in deep reinforcement learning.\n","authors":["Andrew Jesson","Yiding Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.10905v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12686v2","updated":"2024-10-17T12:52:30Z","published":"2024-10-16T15:48:28Z","title":"Automatic Mapping of Anatomical Landmarks from Free-Text Using Large\n  Language Models: Insights from Llama-2","summary":"  Anatomical landmarks are vital in medical imaging for navigation and anomaly\ndetection. Modern large language models (LLMs), like Llama-2, offer promise for\nautomating the mapping of these landmarks in free-text radiology reports to\ncorresponding positions in image data. Recent studies propose LLMs may develop\ncoherent representations of generative processes. Motivated by these insights,\nwe investigated whether LLMs accurately represent the spatial positions of\nanatomical landmarks. Through experiments with Llama-2 models, we found that\nthey can linearly represent anatomical landmarks in space with considerable\nrobustness to different prompts. These results underscore the potential of LLMs\nto enhance the efficiency and accuracy of medical imaging workflows.\n","authors":["Mohamad Abdi","Gerardo Hermosillo Valadez","Halid Ziya Yerebakan"],"pdf_url":"https://arxiv.org/pdf/2410.12686v2.pdf","comment":"6 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2410.01186v2","updated":"2024-10-17T12:50:06Z","published":"2024-10-02T02:38:33Z","title":"Efficient PAC Learning of Halfspaces with Constant Malicious Noise Rate","summary":"  Understanding noise tolerance of learning algorithms under certain conditions\nis a central quest in learning theory. In this work, we study the problem of\ncomputationally efficient PAC learning of halfspaces in the presence of\nmalicious noise, where an adversary can corrupt both instances and labels of\ntraining samples. The best-known noise tolerance either depends on a target\nerror rate under distributional assumptions or on a margin parameter under\nlarge-margin conditions. In this work, we show that when both types of\nconditions are satisfied, it is possible to achieve {\\em constant} noise\ntolerance by minimizing a reweighted hinge loss. Our key ingredients include:\n1) an efficient algorithm that finds weights to control the gradient\ndeterioration from corrupted samples, and 2) a new analysis on the robustness\nof the hinge loss equipped with such weights.\n","authors":["Jie Shen","Xiaoyu Li"],"pdf_url":"https://arxiv.org/pdf/2410.01186v2.pdf","comment":"author list in contribution order"},{"id":"http://arxiv.org/abs/2410.13502v1","updated":"2024-10-17T12:48:14Z","published":"2024-10-17T12:48:14Z","title":"MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily\n  Complex Proofs","summary":"  Large language models (LLMs) can solve arithmetic word problems with high\naccuracy, but little is known about how well they generalize to problems that\nare more complex than the ones on which they have been trained. Empirical\ninvestigations of such questions are impeded by two major flaws of current\nevaluations: (i) much of the evaluation data is contaminated, in the sense that\nit has already been seen during training, and (ii) benchmark datasets do not\ncapture how problem proofs may be arbitrarily complex in various ways. As a\nstep towards addressing these issues, we present a framework for evaluating\nLLMs on problems that have arbitrarily complex arithmetic proofs, called\nMathGAP. MathGAP generates problems that follow fixed proof specifications --\nalong with chain-of-thought reasoning annotations -- enabling systematic\nstudies on generalization with respect to arithmetic proof complexity. We apply\nMathGAP to analyze how in-context learning interacts with generalization to\nproblems that have more complex proofs. We find that among the models tested,\nmost show a significant decrease in performance as proofs get deeper and wider.\nThis effect is more pronounced in complex, nonlinear proof structures, which\nare challenging even for GPT-4o. Surprisingly, providing in-context examples\nfrom the same distribution as the test set is not always beneficial for\nperformance. In particular, zero-shot prompting as well as demonstrating a\ndiverse range of examples that are less complex than the test data sometimes\nyield similar or higher accuracies.\n","authors":["Andreas Opedal","Haruki Shirakami","Bernhard Schölkopf","Abulhair Saparov","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.13502v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.13501v1","updated":"2024-10-17T12:47:31Z","published":"2024-10-17T12:47:31Z","title":"Integrating Large Language Models and Reinforcement Learning for\n  Non-Linear Reasoning","summary":"  Large Language Models (LLMs) were shown to struggle with long-term planning,\nwhich may be caused by the limited way in which they explore the space of\npossible solutions. We propose an architecture where a Reinforcement Learning\n(RL) Agent guides an LLM's space exploration: (1) the Agent has access to\ndomain-specific information, and can therefore make decisions about the quality\nof candidate solutions based on specific and relevant metrics, which were not\nexplicitly considered by the LLM's training objective; (2) the LLM can focus on\ngenerating immediate next steps, without the need for long-term planning. We\nallow non-linear reasoning by exploring alternative paths and backtracking. We\nevaluate this architecture on the program equivalence task, and compare it\nagainst Chain of Thought (CoT) and Tree of Thoughts (ToT). We assess both the\ndownstream task, denoting the binary classification, and the intermediate\nreasoning steps. Our approach compares positively against CoT and ToT.\n","authors":["Yoav Alon","Cristina David"],"pdf_url":"https://arxiv.org/pdf/2410.13501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13500v1","updated":"2024-10-17T12:46:26Z","published":"2024-10-17T12:46:26Z","title":"SAda-Net: A Self-Supervised Adaptive Stereo Estimation CNN For Remote\n  Sensing Image Data","summary":"  Stereo estimation has made many advancements in recent years with the\nintroduction of deep-learning. However the traditional supervised approach to\ndeep-learning requires the creation of accurate and plentiful ground-truth\ndata, which is expensive to create and not available in many situations. This\nis especially true for remote sensing applications, where there is an excess of\navailable data without proper ground truth. To tackle this problem, we propose\na self-supervised CNN with self-improving adaptive abilities. In the first\niteration, the created disparity map is inaccurate and noisy. Leveraging the\nleft-right consistency check, we get a sparse but more accurate disparity map\nwhich is used as an initial pseudo ground-truth. This pseudo ground-truth is\nthen adapted and updated after every epoch in the training step of the network.\nWe use the sum of inconsistent points in order to track the network\nconvergence. The code for our method is publicly available at:\nhttps://github.com/thedodo/SAda-Net}{https://github.com/thedodo/SAda-Net\n","authors":["Dominik Hirner","Friedrich Fraundorfer"],"pdf_url":"https://arxiv.org/pdf/2410.13500v1.pdf","comment":"Will be presented at ICPR2024 in December 2024 in Kolkata, India"},{"id":"http://arxiv.org/abs/2410.13498v1","updated":"2024-10-17T12:43:49Z","published":"2024-10-17T12:43:49Z","title":"Enhancing Text Generation in Joint NLG/NLU Learning Through Curriculum\n  Learning, Semi-Supervised Training, and Advanced Optimization Techniques","summary":"  Text generation is the automated process of producing written or spoken\nlanguage using computational methods. It involves generating coherent and\ncontextually relevant text based on predefined rules or learned patterns.\nHowever, challenges in text generation arise from maintaining coherence,\nensuring diversity and creativity, and avoiding biases or inappropriate\ncontent. This research paper developed a novel approach to improve text\ngeneration in the context of joint Natural Language Generation (NLG) and\nNatural Language Understanding (NLU) learning. The data is prepared by\ngathering and preprocessing annotated datasets, including cleaning,\ntokenization, stemming, and stop-word removal. Feature extraction techniques\nsuch as POS tagging, Bag of words, and Term Frequency-Inverse Document\nFrequency (TF-IDF) are applied. Transformer-based encoders and decoders,\ncapturing long range dependencies and improving source-target sequence\nmodelling. Pre-trained language models like Optimized BERT are incorporated,\nalong with a Hybrid Redfox Artificial Hummingbird Algorithm (HRAHA).\nReinforcement learning with policy gradient techniques, semi-supervised\ntraining, improved attention mechanisms, and differentiable approximations like\nstraight-through Gumbel SoftMax estimator are employed to fine-tune the models\nand handle complex linguistic tasks effectively. The proposed model is\nimplemented using Python.\n","authors":["Rahimanuddin Shaik","Katikela Sreeharsha Kishore"],"pdf_url":"https://arxiv.org/pdf/2410.13498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13493v1","updated":"2024-10-17T12:38:08Z","published":"2024-10-17T12:38:08Z","title":"Deep Reinforcement Learning for Online Optimal Execution Strategies","summary":"  This paper tackles the challenge of learning non-Markovian optimal execution\nstrategies in dynamic financial markets. We introduce a novel actor-critic\nalgorithm based on Deep Deterministic Policy Gradient (DDPG) to address this\nissue, with a focus on transient price impact modeled by a general decay\nkernel. Through numerical experiments with various decay kernels, we show that\nour algorithm successfully approximates the optimal execution strategy.\nAdditionally, the proposed algorithm demonstrates adaptability to evolving\nmarket conditions, where parameters fluctuate over time. Our findings also show\nthat modern reinforcement learning algorithms can provide a solution that\nreduces the need for frequent and inefficient human intervention in optimal\nexecution tasks.\n","authors":["Alessandro Micheli","Mélodie Monod"],"pdf_url":"https://arxiv.org/pdf/2410.13493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13490v1","updated":"2024-10-17T12:34:37Z","published":"2024-10-17T12:34:37Z","title":"Novelty-based Sample Reuse for Continuous Robotics Control","summary":"  In reinforcement learning, agents collect state information and rewards\nthrough environmental interactions, essential for policy refinement. This\nprocess is notably time-consuming, especially in complex robotic simulations\nand real-world applications. Traditional algorithms usually re-engage with the\nenvironment after processing a single batch of samples, thereby failing to\nfully capitalize on historical data. However, frequently observed states, with\nreliable value estimates, require minimal updates; in contrast, rare observed\nstates necessitate more intensive updates for achieving accurate value\nestimations. To address uneven sample utilization, we propose Novelty-guided\nSample Reuse (NSR). NSR provides extra updates for infrequent, novel states and\nskips additional updates for frequent states, maximizing sample use before\ninteracting with the environment again. Our experiments show that NSR improves\nthe convergence rate and success rate of algorithms without significantly\nincreasing time consumption. Our code is publicly available at\nhttps://github.com/ppksigs/NSR-DDPG-HER.\n","authors":["Ke Duan","Kai Yang","Houde Liu","Xueqian Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13488v1","updated":"2024-10-17T12:32:00Z","published":"2024-10-17T12:32:00Z","title":"Seeing Through VisualBERT: A Causal Adventure on Memetic Landscapes","summary":"  Detecting offensive memes is crucial, yet standard deep neural network\nsystems often remain opaque. Various input attribution-based methods attempt to\ninterpret their behavior, but they face challenges with implicitly offensive\nmemes and non-causal attributions. To address these issues, we propose a\nframework based on a Structural Causal Model (SCM). In this framework,\nVisualBERT is trained to predict the class of an input meme based on both meme\ninput and causal concepts, allowing for transparent interpretation. Our\nqualitative evaluation demonstrates the framework's effectiveness in\nunderstanding model behavior, particularly in determining whether the model was\nright due to the right reason, and in identifying reasons behind\nmisclassification. Additionally, quantitative analysis assesses the\nsignificance of proposed modelling choices, such as de-confounding, adversarial\nlearning, and dynamic routing, and compares them with input attribution\nmethods. Surprisingly, we find that input attribution methods do not guarantee\ncausality within our framework, raising questions about their reliability in\nsafety-critical applications. The project page is at:\nhttps://newcodevelop.github.io/causality_adventure/\n","authors":["Dibyanayan Bandyopadhyay","Mohammed Hasanuzzaman","Asif Ekbal"],"pdf_url":"https://arxiv.org/pdf/2410.13488v1.pdf","comment":"Accepted at EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2409.19431v2","updated":"2024-10-17T12:23:07Z","published":"2024-09-28T18:31:51Z","title":"Generalization Error of the Tilted Empirical Risk","summary":"  The generalization error (risk) of a supervised statistical learning\nalgorithm quantifies its prediction ability on previously unseen data. Inspired\nby exponential tilting, Li et al. (2021) proposed the tilted empirical risk as\na non-linear risk metric for machine learning applications such as\nclassification and regression problems. In this work, we examine the\ngeneralization error of the tilted empirical risk. In particular, we provide\nuniform and information-theoretic bounds on the tilted generalization error,\ndefined as the difference between the population risk and the tilted empirical\nrisk, with a convergence rate of $O(1/\\sqrt{n})$ where $n$ is the number of\ntraining samples. Furthermore, we study the solution to the KL-regularized\nexpected tilted empirical risk minimization problem and derive an upper bound\non the expected tilted generalization error with a convergence rate of\n$O(1/n)$.\n","authors":["Gholamali Aminian","Amir R. Asadi","Tian Li","Ahmad Beirami","Gesine Reinert","Samuel N. Cohen"],"pdf_url":"https://arxiv.org/pdf/2409.19431v2.pdf","comment":"New results are added"},{"id":"http://arxiv.org/abs/2407.18402v2","updated":"2024-10-17T12:19:26Z","published":"2024-07-25T21:33:54Z","title":"RECOVAR: Representation Covariances on Deep Latent Spaces for Seismic\n  Event Detection","summary":"  While modern deep learning methods have shown great promise in the problem of\nearthquake detection, the most successful methods so far have been based on\nsupervised learning, which requires large datasets with ground-truth labels.\nThe curation of such datasets is both time consuming and prone to systematic\nbiases, which result in difficulties with cross-dataset generalization,\nhindering general applicability. In this paper, we develop an unsupervised\nmethod for earthquake detection that learns to detect earthquakes from raw\nwaveforms, without access to ground truth labels. The performance is comparable\nto, and in some cases better than, some state-of-the-art supervised methods.\nMoreover, the method has strong \\emph{cross-dataset generalization}\nperformance. The algorithm utilizes deep autoencoders that learn to reproduce\nthe waveforms after a data-compressive bottleneck and uses a simple,\ncross-covariance-based triggering algorithm at the bottleneck for labeling. The\napproach has the potential to be useful for time series datasets from other\ndomains.\n","authors":["Onur Efe","Arkadas Ozakin"],"pdf_url":"https://arxiv.org/pdf/2407.18402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18392v3","updated":"2024-10-17T12:01:15Z","published":"2024-05-28T17:33:54Z","title":"Scaling Laws and Compute-Optimal Training Beyond Fixed Training\n  Durations","summary":"  Scale has become a main ingredient in obtaining strong machine learning\nmodels. As a result, understanding a model's scaling properties is key to\neffectively designing both the right training setup as well as future\ngenerations of architectures. In this work, we argue that scale and training\nresearch has been needlessly complex due to reliance on the cosine schedule,\nwhich prevents training across different lengths for the same model size. We\ninvestigate the training behavior of a direct alternative -- constant learning\nrate and cooldowns -- and find that it scales predictably and reliably similar\nto cosine. Additionally, we show that stochastic weight averaging yields\nimproved performance along the training trajectory, without additional training\ncosts, across different scales. Importantly, with these findings we demonstrate\nthat scaling experiments can be performed with significantly reduced compute\nand GPU hours by utilizing fewer but reusable training runs. Our code is\navailable at \\url{https://github.com/epfml/schedules-and-scaling/}.\n","authors":["Alexander Hägele","Elie Bakouch","Atli Kosson","Loubna Ben Allal","Leandro Von Werra","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2405.18392v3.pdf","comment":"Spotlight at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.13469v1","updated":"2024-10-17T11:56:33Z","published":"2024-10-17T11:56:33Z","title":"Interpreting Temporal Graph Neural Networks with Koopman Theory","summary":"  Spatiotemporal graph neural networks (STGNNs) have shown promising results in\nmany domains, from forecasting to epidemiology. However, understanding the\ndynamics learned by these models and explaining their behaviour is\nsignificantly more complex than for models dealing with static data. Inspired\nby Koopman theory, which allows a simpler description of intricate, nonlinear\ndynamical systems, we introduce an explainability approach for temporal graphs.\nWe present two methods to interpret the STGNN's decision process and identify\nthe most relevant spatial and temporal patterns in the input for the task at\nhand. The first relies on dynamic mode decomposition (DMD), a Koopman-inspired\ndimensionality reduction method. The second relies on sparse identification of\nnonlinear dynamics (SINDy), a popular method for discovering governing\nequations, which we use for the first time as a general tool for\nexplainability. We show how our methods can correctly identify interpretable\nfeatures such as infection times and infected nodes in the context of\ndissemination processes.\n","authors":["Michele Guerra","Simone Scardapane","Filippo Maria Bianchi"],"pdf_url":"https://arxiv.org/pdf/2410.13469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.10874v3","updated":"2024-10-17T11:50:03Z","published":"2023-01-25T23:47:34Z","title":"Recursive deep learning framework for forecasting the decadal world\n  economic outlook","summary":"  The gross domestic product (GDP) is the most widely used indicator in\nmacroeconomics and the main tool for measuring a country's economic output. Due\nto the diversity and complexity of the world economy, a wide range of models\nhave been used, but there are challenges in making decadal GDP forecasts given\nunexpected changes such as emergence of catastrophic world events including\npandemics and wars. Deep learning models are well suited for modelling temporal\nsequences and time series forecasting. In this paper, we develop a deep\nlearning framework to forecast the GDP growth rate of the world economy over a\ndecade. We use the Penn World Table as the data source featuring 13 countries\nprior to the COVID-19 pandemic, such as Australia, China, India, and the United\nStates. We present a recursive deep learning framework to predict the GDP\ngrowth rate in the next ten years. We test prominent deep learning models and\ncompare their results with traditional econometric models for selected\ndeveloped and developing countries. Our decadal forecasts reveal that that most\nof the developed countries would experience economic growth slowdown,\nstagnation and even recession within five years (2020-2024). Furthermore, our\nmodel forecasts show that only China, France, and India would experience stable\nGDP growth.\n","authors":["Tianyi Wang","Rodney Beard","John Hawkins","Rohitash Chandra"],"pdf_url":"https://arxiv.org/pdf/2301.10874v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09760v2","updated":"2024-10-17T11:49:06Z","published":"2024-10-13T07:36:45Z","title":"Targeted Vaccine: Safety Alignment for Large Language Models against\n  Harmful Fine-Tuning via Layer-wise Perturbation","summary":"  Harmful fine-tuning attack poses a serious threat to the online fine-tuning\nservice. Vaccine, a recent alignment-stage defense, applies uniform\nperturbation to all layers of embedding to make the model robust to the\nsimulated embedding drift. However, applying layer-wise uniform perturbation\nmay lead to excess perturbations for some particular safety-irrelevant layers,\nresulting in defense performance degradation and unnecessary memory\nconsumption. To address this limitation, we propose Targeted Vaccine\n(T-Vaccine), a memory-efficient safety alignment method that applies\nperturbation to only selected layers of the model. T-Vaccine follows two core\nsteps: First, it uses gradient norm as a statistical metric to identify the\nsafety-critical layers. Second, instead of applying uniform perturbation across\nall layers, T-Vaccine only applies perturbation to the safety-critical layers\nwhile keeping other layers frozen during training. Results show that T-Vaccine\noutperforms Vaccine in terms of both defense effectiveness and resource\nefficiency. Comparison with other defense baselines, e.g., RepNoise and TAR\nalso demonstrate the superiority of T-Vaccine. Notably, T-Vaccine is the first\ndefense that can address harmful fine-tuning issues for a 7B pre-trained models\ntrained on consumer GPUs with limited memory (e.g., RTX 4090). Our code is\navailable at https://github.com/Lslland/T-Vaccine.\n","authors":["Guozhi Liu","Weiwei Lin","Tiansheng Huang","Ruichao Mo","Qi Mu","Li Shen"],"pdf_url":"https://arxiv.org/pdf/2410.09760v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13463v1","updated":"2024-10-17T11:47:56Z","published":"2024-10-17T11:47:56Z","title":"Truncating Trajectories in Monte Carlo Policy Evaluation: an Adaptive\n  Approach","summary":"  Policy evaluation via Monte Carlo (MC) simulation is at the core of many MC\nReinforcement Learning (RL) algorithms (e.g., policy gradient methods). In this\ncontext, the designer of the learning system specifies an interaction budget\nthat the agent usually spends by collecting trajectories of fixed length within\na simulator. However, is this data collection strategy the best option? To\nanswer this question, in this paper, we propose as a quality index a surrogate\nof the mean squared error of a return estimator that uses trajectories of\ndifferent lengths, i.e., \\emph{truncated}. Specifically, this surrogate shows\nthe sub-optimality of the fixed-length trajectory schedule. Furthermore, it\nsuggests that adaptive data collection strategies that spend the available\nbudget sequentially can allocate a larger portion of transitions in timesteps\nin which more accurate sampling is required to reduce the error of the final\nestimate. Building on these findings, we present an adaptive algorithm called\nRobust and Iterative Data collection strategy Optimization (RIDO). The main\nintuition behind RIDO is to split the available interaction budget into\nmini-batches. At each round, the agent determines the most convenient schedule\nof trajectories that minimizes an empirical and robust version of the surrogate\nof the estimator's error. After discussing the theoretical properties of our\nmethod, we conclude by assessing its performance across multiple domains. Our\nresults show that RIDO can adapt its trajectory schedule toward timesteps where\nmore sampling is required to increase the quality of the final estimation.\n","authors":["Riccardo Poiani","Nicole Nobili","Alberto Maria Metelli","Marcello Restelli"],"pdf_url":"https://arxiv.org/pdf/2410.13463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09250v2","updated":"2024-10-17T11:46:45Z","published":"2024-06-13T15:55:04Z","title":"MirrorCheck: Efficient Adversarial Defense for Vision-Language Models","summary":"  Vision-Language Models (VLMs) are becoming increasingly vulnerable to\nadversarial attacks as various novel attack strategies are being proposed\nagainst these models. While existing defenses excel in unimodal contexts, they\ncurrently fall short in safeguarding VLMs against adversarial threats. To\nmitigate this vulnerability, we propose a novel, yet elegantly simple approach\nfor detecting adversarial samples in VLMs. Our method leverages Text-to-Image\n(T2I) models to generate images based on captions produced by target VLMs.\nSubsequently, we calculate the similarities of the embeddings of both input and\ngenerated images in the feature space to identify adversarial samples.\nEmpirical evaluations conducted on different datasets validate the efficacy of\nour approach, outperforming baseline methods adapted from image classification\ndomains. Furthermore, we extend our methodology to classification tasks,\nshowcasing its adaptability and model-agnostic nature. Theoretical analyses and\nempirical findings also show the resilience of our approach against adaptive\nattacks, positioning it as an excellent defense mechanism for real-world\ndeployment against adversarial threats.\n","authors":["Samar Fares","Klea Ziu","Toluwani Aremu","Nikita Durasov","Martin Takáč","Pascal Fua","Karthik Nandakumar","Ivan Laptev"],"pdf_url":"https://arxiv.org/pdf/2406.09250v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13461v1","updated":"2024-10-17T11:46:33Z","published":"2024-10-17T11:46:33Z","title":"Progressive Mixed-Precision Decoding for Efficient LLM Inference","summary":"  In spite of the great potential of large language models (LLMs) across\nvarious tasks, their deployment on resource-constrained devices remains\nchallenging due to their excessive computational and memory demands.\nQuantization has emerged as an effective solution by storing weights in reduced\nprecision. However, utilizing low precisions (i.e.~2/3-bit) to substantially\nalleviate the memory-boundedness of LLM decoding, still suffers from\nprohibitive performance drop. In this work, we argue that existing approaches\nfail to explore the diversity in computational patterns, redundancy, and\nsensitivity to approximations of the different phases of LLM inference,\nresorting to a uniform quantization policy throughout. Instead, we propose a\nnovel phase-aware method that selectively allocates precision during different\nphases of LLM inference, achieving both strong context extraction during\nprefill and efficient memory bandwidth utilization during decoding. To further\naddress the memory-boundedness of the decoding phase, we introduce Progressive\nMixed-Precision Decoding (PMPD), a technique that enables the gradual lowering\nof precision deeper in the generated sequence, together with a spectrum of\nprecision-switching schedulers that dynamically drive the precision-lowering\ndecisions in either task-adaptive or prompt-adaptive manner. Extensive\nevaluation across diverse language tasks shows that when targeting Nvidia GPUs,\nPMPD achieves 1.4$-$12.2$\\times$ speedup in matrix-vector multiplications over\nfp16 models, while when targeting an LLM-optimized NPU, our approach delivers a\nthroughput gain of 3.8$-$8.0$\\times$ over fp16 models and up to 1.54$\\times$\nover uniform quantization approaches while preserving the output quality.\n","authors":["Hao Mark Chen","Fuwen Tan","Alexandros Kouris","Royson Lee","Hongxiang Fan","Stylianos I. Venieris"],"pdf_url":"https://arxiv.org/pdf/2410.13461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16807v2","updated":"2024-10-17T11:46:18Z","published":"2024-06-24T17:19:34Z","title":"Beyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback\n  for Text-to-Image Generation","summary":"  Human feedback plays a critical role in learning and refining reward models\nfor text-to-image generation, but the optimal form the feedback should take for\nlearning an accurate reward function has not been conclusively established.\nThis paper investigates the effectiveness of fine-grained feedback which\ncaptures nuanced distinctions in image quality and prompt-alignment, compared\nto traditional coarse-grained feedback (for example, thumbs up/down or ranking\nbetween a set of options). While fine-grained feedback holds promise,\nparticularly for systems catering to diverse societal preferences, we show that\ndemonstrating its superiority to coarse-grained feedback is not automatic.\nThrough experiments on real and synthetic preference data, we surface the\ncomplexities of building effective models due to the interplay of model choice,\nfeedback type, and the alignment between human judgment and computational\ninterpretation. We identify key challenges in eliciting and utilizing\nfine-grained feedback, prompting a reassessment of its assumed benefits and\npracticality. Our findings -- e.g., that fine-grained feedback can lead to\nworse models for a fixed budget, in some settings; however, in controlled\nsettings with known attributes, fine grained rewards can indeed be more helpful\n-- call for careful consideration of feedback attributes and potentially beckon\nnovel modeling approaches to appropriately unlock the potential value of\nfine-grained feedback in-the-wild.\n","authors":["Katherine M. Collins","Najoung Kim","Yonatan Bitton","Verena Rieser","Shayegan Omidshafiei","Yushi Hu","Sherol Chen","Senjuti Dutta","Minsuk Chang","Kimin Lee","Youwei Liang","Georgina Evans","Sahil Singla","Gang Li","Adrian Weller","Junfeng He","Deepak Ramachandran","Krishnamurthy Dj Dvijotham"],"pdf_url":"https://arxiv.org/pdf/2406.16807v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13460v1","updated":"2024-10-17T11:43:16Z","published":"2024-10-17T11:43:16Z","title":"Breaking the Manual Annotation Bottleneck: Creating a Comprehensive\n  Legal Case Criticality Dataset through Semi-Automated Labeling","summary":"  Predicting case criticality helps legal professionals in the court system\nmanage large volumes of case law. This paper introduces the Criticality\nPrediction dataset, a new resource for evaluating the potential influence of\nSwiss Federal Supreme Court decisions on future jurisprudence. Unlike existing\napproaches that rely on resource-intensive manual annotations, we\nsemi-automatically derive labels leading to a much larger dataset than\notherwise possible. Our dataset features a two-tier labeling system: (1) the\nLD-Label, which identifies cases published as Leading Decisions (LD), and (2)\nthe Citation-Label, which ranks cases by their citation frequency and recency.\nThis allows for a more nuanced evaluation of case importance. We evaluate\nseveral multilingual models, including fine-tuned variants and large language\nmodels, and find that fine-tuned models consistently outperform zero-shot\nbaselines, demonstrating the need for task-specific adaptation. Our\ncontributions include the introduction of this task and the release of a\nmultilingual dataset to the research community.\n","authors":["Ronja Stern","Ken Kawamura","Matthias Stürmer","Ilias Chalkidis","Joel Niklaus"],"pdf_url":"https://arxiv.org/pdf/2410.13460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.03089v2","updated":"2024-10-17T11:38:11Z","published":"2024-05-06T00:58:23Z","title":"Structure-Preserving Network Compression Via Low-Rank Induced Training\n  Through Linear Layers Composition","summary":"  Deep Neural Networks (DNNs) have achieved remarkable success in addressing\nmany previously unsolvable tasks. However, the storage and computational\nrequirements associated with DNNs pose a challenge for deploying these trained\nmodels on resource-limited devices. Therefore, a plethora of compression and\npruning techniques have been proposed in recent years. Low-rank decomposition\ntechniques are among the approaches most utilized to address this problem.\nCompared to post-training compression, compression-promoted training is still\nunder-explored. In this paper, we present a theoretically-justified technique\ntermed Low-Rank Induced Training (LoRITa), that promotes low-rankness through\nthe composition of linear layers and compresses by using singular value\ntruncation. This is achieved without the need to change the structure at\ninference time or require constrained and/or additional optimization, other\nthan the standard weight decay regularization. Moreover, LoRITa eliminates the\nneed to (i) initialize with pre-trained models, (ii) specify rank selection\nprior to training, and (iii) compute SVD in each iteration. Our experimental\nresults (i) demonstrate the effectiveness of our approach using MNIST on Fully\nConnected Networks, CIFAR10 on Vision Transformers, and CIFAR10/100 and\nImageNet on Convolutional Neural Networks, and (ii) illustrate that we achieve\neither competitive or state-of-the-art results when compared to leading\nstructured pruning and low-rank training methods in terms of FLOPs and\nparameters drop. Our code is available at\n\\url{https://github.com/XitongSystem/LoRITa/tree/main}.\n","authors":["Xitong Zhang","Ismail R. Alkhouri","Rongrong Wang"],"pdf_url":"https://arxiv.org/pdf/2405.03089v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13456v1","updated":"2024-10-17T11:34:07Z","published":"2024-10-17T11:34:07Z","title":"Unlocking Legal Knowledge: A Multilingual Dataset for Judicial\n  Summarization in Switzerland","summary":"  Legal research is a time-consuming task that most lawyers face on a daily\nbasis. A large part of legal research entails looking up relevant caselaw and\nbringing it in relation to the case at hand. Lawyers heavily rely on summaries\n(also called headnotes) to find the right cases quickly. However, not all\ndecisions are annotated with headnotes and writing them is time-consuming.\nAutomated headnote creation has the potential to make hundreds of thousands of\ndecisions more accessible for legal research in Switzerland alone. To kickstart\nthis, we introduce the Swiss Leading Decision Summarization ( SLDS) dataset, a\nnovel cross-lingual resource featuring 18K court rulings from the Swiss Federal\nSupreme Court (SFSC), in German, French, and Italian, along with German\nheadnotes. We fine-tune and evaluate three mT5 variants, along with proprietary\nmodels. Our analysis highlights that while proprietary models perform well in\nzero-shot and one-shot settings, fine-tuned smaller models still provide a\nstrong competitive edge. We publicly release the dataset to facilitate further\nresearch in multilingual legal summarization and the development of assistive\ntechnologies for legal professionals\n","authors":["Luca Rolshoven","Vishvaksenan Rasiah","Srinanda Brügger Bose","Matthias Stürmer","Joel Niklaus"],"pdf_url":"https://arxiv.org/pdf/2410.13456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07900v2","updated":"2024-10-17T11:33:40Z","published":"2024-10-10T13:29:12Z","title":"CL3: A Collaborative Learning Framework for the Medical Data Ensuring\n  Data Privacy in the Hyperconnected Environment","summary":"  In a hyperconnected environment, medical institutions are particularly\nconcerned with data privacy when sharing and transmitting sensitive patient\ninformation due to the risk of data breaches, where malicious actors could\nintercept sensitive information. A collaborative learning framework, including\ntransfer, federated, and incremental learning, can generate efficient, secure,\nand scalable models while requiring less computation, maintaining patient data\nprivacy, and ensuring an up-to-date model. This study aims to address the\ndetection of COVID-19 using chest X-ray images through a proposed collaborative\nlearning framework called CL3. Initially, transfer learning is employed,\nleveraging knowledge from a pre-trained model as the starting global model.\nLocal models from different medical institutes are then integrated, and a new\nglobal model is constructed to adapt to any data drift observed in the local\nmodels. Additionally, incremental learning is considered, allowing continuous\nadaptation to new medical data without forgetting previously learned\ninformation. Experimental results demonstrate that the CL3 framework achieved a\nglobal accuracy of 89.99% when using Xception with a batch size of 16 after\nbeing trained for six federated communication rounds. A demo of the CL3\nframework is available at\nhttps://github.com/zavidparvez/CL3-Collaborative-Approach to ensure\nreproducibility.\n","authors":["Mohamamd Zavid Parvez","Rafiqul Islam","Md Zahidul Islam"],"pdf_url":"https://arxiv.org/pdf/2410.07900v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01522v3","updated":"2024-10-17T11:33:09Z","published":"2023-12-03T22:44:04Z","title":"G2D: From Global to Dense Radiography Representation Learning via\n  Vision-Language Pre-training","summary":"  Recently, medical vision-language pre-training (VLP) has reached substantial\nprogress to learn global visual representation from medical images and their\npaired radiology reports. However, medical imaging tasks in real world usually\nrequire finer granularity in visual features. These tasks include visual\nlocalization tasks (e.g., semantic segmentation, object detection) and visual\ngrounding task. Yet, current medical VLP methods face challenges in learning\nthese fine-grained features, as they primarily focus on brute-force alignment\nbetween image patches and individual text tokens for local visual feature\nlearning, which is suboptimal for downstream dense prediction tasks. In this\nwork, we propose a new VLP framework, named \\textbf{G}lobal to \\textbf{D}ense\nlevel representation learning (G2D) that achieves significantly improved\ngranularity and more accurate grounding for the learned features, compared to\nexisting medical VLP approaches. In particular, G2D learns dense and\nsemantically-grounded image representations via a pseudo segmentation task\nparallel with the global vision-language alignment. Notably, generating pseudo\nsegmentation targets does not incur extra trainable parameters: they are\nobtained on the fly during VLP with a parameter-free processor. G2D achieves\nsuperior performance across 6 medical imaging tasks and 25 diseases,\nparticularly in semantic segmentation, which necessitates fine-grained,\nsemantically-grounded image features. In this task, G2D surpasses peer models\neven when fine-tuned with just 1\\% of the training data, compared to the 100\\%\nused by these models. The code will be released upon acceptance.\n","authors":["Che Liu","Cheng Ouyang","Sibo Cheng","Anand Shah","Wenjia Bai","Rossella Arcucci"],"pdf_url":"https://arxiv.org/pdf/2312.01522v3.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2410.13448v1","updated":"2024-10-17T11:21:47Z","published":"2024-10-17T11:21:47Z","title":"Fast Estimation of Partial Dependence Functions using Trees","summary":"  Many existing interpretation methods are based on Partial Dependence (PD)\nfunctions that, for a pre-trained machine learning model, capture how a subset\nof the features affects the predictions by averaging over the remaining\nfeatures. Notable methods include Shapley additive explanations (SHAP) which\ncomputes feature contributions based on a game theoretical interpretation and\nPD plots (i.e., 1-dim PD functions) that capture average marginal main effects.\nRecent work has connected these approaches using a functional decomposition and\nargues that SHAP values can be misleading since they merge main and interaction\neffects into a single local effect. A major advantage of SHAP compared to other\nPD-based interpretations, however, has been the availability of fast estimation\ntechniques, such as \\texttt{TreeSHAP}. In this paper, we propose a new\ntree-based estimator, \\texttt{FastPD}, which efficiently estimates arbitrary PD\nfunctions. We show that \\texttt{FastPD} consistently estimates the desired\npopulation quantity -- in contrast to path-dependent \\texttt{TreeSHAP} which is\ninconsistent when features are correlated. For moderately deep trees,\n\\texttt{FastPD} improves the complexity of existing methods from quadratic to\nlinear in the number of observations. By estimating PD functions for arbitrary\nfeature subsets, \\texttt{FastPD} can be used to extract PD-based\ninterpretations such as SHAP, PD plots and higher order interaction effects.\n","authors":["Jinyang Liu","Tessa Steensgaard","Marvin N. Wright","Niklas Pfister","Munir Hiabu"],"pdf_url":"https://arxiv.org/pdf/2410.13448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13445v1","updated":"2024-10-17T11:19:44Z","published":"2024-10-17T11:19:44Z","title":"Parameter-efficient Adaptation of Multilingual Multimodal Models for\n  Low-resource ASR","summary":"  Automatic speech recognition (ASR) for low-resource languages remains a\nchallenge due to the scarcity of labeled training data. Parameter-efficient\nfine-tuning and text-only adaptation are two popular methods that have been\nused to address such low-resource settings. In this work, we investigate how\nthese techniques can be effectively combined using a multilingual multimodal\nmodel like SeamlessM4T. Multimodal models are able to leverage unlabeled text\nvia text-only adaptation with further parameter-efficient ASR fine-tuning, thus\nboosting ASR performance. We also show cross-lingual transfer from a\nhigh-resource language, achieving up to a relative 17% WER reduction over a\nbaseline in a zero-shot setting without any labeled speech.\n","authors":["Abhishek Gupta","Amruta Parulekar","Sameep Chattopadhyay","Preethi Jyothi"],"pdf_url":"https://arxiv.org/pdf/2410.13445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05966v2","updated":"2024-10-17T11:15:39Z","published":"2024-10-08T12:16:12Z","title":"FLOPS: Forward Learning with OPtimal Sampling","summary":"  Given the limitations of backpropagation, perturbation-based gradient\ncomputation methods have recently gained focus for learning with only forward\npasses, also referred to as queries. Conventional forward learning consumes\nenormous queries on each data point for accurate gradient estimation through\nMonte Carlo sampling, which hinders the scalability of those algorithms.\nHowever, not all data points deserve equal queries for gradient estimation. In\nthis paper, we study the problem of improving the forward learning efficiency\nfrom a novel perspective: how to reduce the gradient estimation variance with\nminimum cost? For this, we propose to allocate the optimal number of queries\nover each data in one batch during training to achieve a good balance between\nestimation accuracy and computational efficiency. Specifically, with a\nsimplified proxy objective and a reparameterization technique, we derive a\nnovel plug-and-play query allocator with minimal parameters. Theoretical\nresults are carried out to verify its optimality. We conduct extensive\nexperiments for fine-tuning Vision Transformers on various datasets and further\ndeploy the allocator to two black-box applications: prompt tuning and\nmultimodal alignment for foundation models. All findings demonstrate that our\nproposed allocator significantly enhances the scalability of forward-learning\nalgorithms, paving the way for real-world applications.\n","authors":["Tao Ren","Zishi Zhang","Jinyang Jiang","Guanghao Li","Zeliang Zhang","Mingqian Feng","Yijie Peng"],"pdf_url":"https://arxiv.org/pdf/2410.05966v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09747v2","updated":"2024-10-17T11:14:37Z","published":"2024-10-13T06:53:58Z","title":"t-READi: Transformer-Powered Robust and Efficient Multimodal Inference\n  for Autonomous Driving","summary":"  Given the wide adoption of multimodal sensors (e.g., camera, lidar, radar) by\nautonomous vehicles (AVs), deep analytics to fuse their outputs for a robust\nperception become imperative. However, existing fusion methods often make two\nassumptions rarely holding in practice: i) similar data distributions for all\ninputs and ii) constant availability for all sensors. Because, for example,\nlidars have various resolutions and failures of radars may occur, such\nvariability often results in significant performance degradation in fusion. To\nthis end, we present tREADi, an adaptive inference system that accommodates the\nvariability of multimodal sensory data and thus enables robust and efficient\nperception. t-READi identifies variation-sensitive yet structure-specific model\nparameters; it then adapts only these parameters while keeping the rest intact.\nt-READi also leverages a cross-modality contrastive learning method to\ncompensate for the loss from missing modalities. Both functions are implemented\nto maintain compatibility with existing multimodal deep fusion methods. The\nextensive experiments evidently demonstrate that compared with the status quo\napproaches, t-READi not only improves the average inference accuracy by more\nthan 6% but also reduces the inference latency by almost 15x with the cost of\nonly 5% extra memory overhead in the worst case under realistic data and modal\nvariations.\n","authors":["Pengfei Hu","Yuhang Qian","Tianyue Zheng","Ang Li","Zhe Chen","Yue Gao","Xiuzhen Cheng","Jun Luo"],"pdf_url":"https://arxiv.org/pdf/2410.09747v2.pdf","comment":"14 pages, 16 figures"},{"id":"http://arxiv.org/abs/2410.13439v1","updated":"2024-10-17T11:12:55Z","published":"2024-10-17T11:12:55Z","title":"Similarity-Dissimilarity Loss with Supervised Contrastive Learning for\n  Multi-label Classification","summary":"  Supervised contrastive learning has been explored in making use of label\ninformation for multi-label classification, but determining positive samples in\nmulti-label scenario remains challenging. Previous studies have examined\nstrategies for identifying positive samples, considering label overlap\nproportion between anchors and samples. However, they ignore various relations\nbetween given anchors and samples, as well as how to dynamically adjust the\nweights in contrastive loss functions based on different relations, leading to\ngreat ambiguity. In this paper, we introduce five distinct relations between\nmulti-label samples and propose a Similarity-Dissimilarity Loss with\ncontrastive learning for multi-label classification. Our loss function\nre-weights the loss by computing the similarity and dissimilarity between\npositive samples and a given anchor based on the introduced relations. We\nmainly conduct experiments for multi-label text classification on MIMIC\ndatasets, then further extend the evaluation on MS-COCO. The Experimental\nresults show that our proposed loss effectively improves the performance on all\nencoders under supervised contrastive learning paradigm, demonstrating its\neffectiveness and robustness.\n","authors":["Guangming Huang","Yunfei Long","Cunjin Luo","Sheng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.13439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13431v1","updated":"2024-10-17T10:54:55Z","published":"2024-10-17T10:54:55Z","title":"Solving Prior Distribution Mismatch in Diffusion Models via Optimal\n  Transport","summary":"  In recent years, the knowledge surrounding diffusion models(DMs) has grown\nsignificantly, though several theoretical gaps remain. Particularly noteworthy\nis prior error, defined as the discrepancy between the termination distribution\nof the forward process and the initial distribution of the reverse process. To\naddress these deficiencies, this paper explores the deeper relationship between\noptimal transport(OT) theory and DMs with discrete initial distribution.\nSpecifically, we demonstrate that the two stages of DMs fundamentally involve\ncomputing time-dependent OT. However, unavoidable prior error result in\ndeviation during the reverse process under quadratic transport cost. By proving\nthat as the diffusion termination time increases, the probability flow\nexponentially converges to the gradient of the solution to the classical\nMonge-Amp\\`ere equation, we establish a vital link between these fields.\nTherefore, static OT emerges as the most intrinsic single-step method for\nbridging this theoretical potential gap. Additionally, we apply these insights\nto accelerate sampling in both unconditional and conditional generation\nscenarios. Experimental results across multiple image datasets validate the\neffectiveness of our approach.\n","authors":["Zhanpeng Wang","Shenghao Li","Chen Wang","Shuting Cao","Na Lei","Zhongxuan Luo"],"pdf_url":"https://arxiv.org/pdf/2410.13431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19674v2","updated":"2024-10-17T10:51:50Z","published":"2023-05-31T09:15:39Z","title":"Online-to-PAC Conversions: Generalization Bounds via Regret Analysis","summary":"  We present a new framework for deriving bounds on the generalization bound of\nstatistical learning algorithms from the perspective of online learning.\nSpecifically, we construct an online learning game called the \"generalization\ngame\", where an online learner is trying to compete with a fixed statistical\nlearning algorithm in predicting the sequence of generalization gaps on a\ntraining set of i.i.d. data points. We establish a connection between the\nonline and statistical learning setting by showing that the existence of an\nonline learning algorithm with bounded regret in this game implies a bound on\nthe generalization error of the statistical learning algorithm, up to a\nmartingale concentration term that is independent of the complexity of the\nstatistical learning method. This technique allows us to recover several\nstandard generalization bounds including a range of PAC-Bayesian and\ninformation-theoretic guarantees, as well as generalizations thereof.\n","authors":["Gábor Lugosi","Gergely Neu"],"pdf_url":"https://arxiv.org/pdf/2305.19674v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13416v1","updated":"2024-10-17T10:35:18Z","published":"2024-10-17T10:35:18Z","title":"Partially Trained Graph Convolutional Networks Resist Oversmoothing","summary":"  In this work we investigate an observation made by Kipf \\& Welling, who\nsuggested that untrained GCNs can generate meaningful node embeddings. In\nparticular, we investigate the effect of training only a single layer of a GCN,\nwhile keeping the rest of the layers frozen. We propose a basis on which the\neffect of the untrained layers and their contribution to the generation of\nembeddings can be predicted. Moreover, we show that network width influences\nthe dissimilarity of node embeddings produced after the initial node features\npass through the untrained part of the model. Additionally, we establish a\nconnection between partially trained GCNs and oversmoothing, showing that they\nare capable of reducing it. We verify our theoretical results experimentally\nand show the benefits of using deep networks that resist oversmoothing, in a\n``cold start'' scenario, where there is a lack of feature information for\nunlabeled nodes.\n","authors":["Dimitrios Kelesis","Dimitris Fotakis","Georgios Paliouras"],"pdf_url":"https://arxiv.org/pdf/2410.13416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19523v2","updated":"2024-10-17T10:27:49Z","published":"2024-07-28T16:23:55Z","title":"Robust Fast Adaptation from Adversarially Explicit Task Distribution\n  Generation","summary":"  Meta-learning is a practical learning paradigm to transfer skills across\ntasks from a few examples. Nevertheless, the existence of task distribution\nshifts tends to weaken meta-learners' generalization capability, particularly\nwhen the task distribution is naively hand-crafted or based on simple priors\nthat fail to cover typical scenarios sufficiently. Here, we consider explicitly\ngenerative modeling task distributions placed over task identifiers and propose\nrobustifying fast adaptation from adversarial training. Our approach, which can\nbe interpreted as a model of a Stackelberg game, not only uncovers the task\nstructure during problem-solving from an explicit generative model but also\ntheoretically increases the adaptation robustness in worst cases. This work has\npractical implications, particularly in dealing with task distribution shifts\nin meta-learning, and contributes to theoretical insights in the field. Our\nmethod demonstrates its robustness in the presence of task subpopulation shifts\nand improved performance over SOTA baselines in extensive experiments. The\nproject is available at https://sites.google.com/view/ar-metalearn.\n","authors":["Cheems Wang","Yiqin Lv","Yixiu Mao","Yun Qu","Yi Xu","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2407.19523v2.pdf","comment":"The project is available at\n  https://sites.google.com/view/ar-metalearn"},{"id":"http://arxiv.org/abs/2410.13412v1","updated":"2024-10-17T10:21:28Z","published":"2024-10-17T10:21:28Z","title":"RAMPA: Robotic Augmented Reality for Machine Programming and Automation","summary":"  As robotics continue to enter various sectors beyond traditional industrial\napplications, the need for intuitive robot training and interaction systems\nbecomes increasingly more important. This paper introduces Robotic Augmented\nReality for Machine Programming (RAMPA), a system that utilizes the\ncapabilities of state-of-the-art and commercially available AR headsets, e.g.,\nMeta Quest 3, to facilitate the application of Programming from Demonstration\n(PfD) approaches on industrial robotic arms, such as Universal Robots UR10. Our\napproach enables in-situ data recording, visualization, and fine-tuning of\nskill demonstrations directly within the user's physical environment. RAMPA\naddresses critical challenges of PfD, such as safety concerns, programming\nbarriers, and the inefficiency of collecting demonstrations on the actual\nhardware. The performance of our system is evaluated against the traditional\nmethod of kinesthetic control in teaching three different robotic manipulation\ntasks and analyzed with quantitative metrics, measuring task performance and\ncompletion time, trajectory smoothness, system usability, user experience, and\ntask load using standardized surveys. Our findings indicate a substantial\nadvancement in how robotic tasks are taught and refined, promising improvements\nin operational safety, efficiency, and user engagement in robotic programming.\n","authors":["Fatih Dogangun","Serdar Bahar","Yigit Yildirim","Bora Toprak Temir","Emre Ugur","Mustafa Doga Dogan"],"pdf_url":"https://arxiv.org/pdf/2410.13412v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2408.09490v2","updated":"2024-10-17T10:15:38Z","published":"2024-08-18T14:10:34Z","title":"Leveraging Invariant Principle for Heterophilic Graph Structure\n  Distribution Shifts","summary":"  Heterophilic Graph Neural Networks (HGNNs) have shown promising results for\nsemi-supervised learning tasks on graphs. Notably, most real-world heterophilic\ngraphs are composed of a mixture of nodes with different neighbor patterns,\nexhibiting local node-level homophilic and heterophilic structures. However,\nexisting works are only devoted to designing better HGNN backbones or\narchitectures for node classification tasks on heterophilic and homophilic\ngraph benchmarks simultaneously, and their analyses of HGNN performance with\nrespect to nodes are only based on the determined data distribution without\nexploring the effect caused by this structural difference between training and\ntesting nodes. How to learn invariant node representations on heterophilic\ngraphs to handle this structure difference or distribution shifts remains\nunexplored. In this paper, we first discuss the limitations of previous\ngraph-based invariant learning methods from the perspective of data\naugmentation. Then, we propose \\textbf{HEI}, a framework capable of generating\ninvariant node representations through incorporating heterophily information to\ninfer latent environments without augmentation, which are then used for\ninvariant prediction, under heterophilic graph structure distribution shifts.\nWe theoretically show that our proposed method can achieve guaranteed\nperformance under heterophilic graph structure distribution shifts. Extensive\nexperiments on various benchmarks and backbones can also demonstrate the\neffectiveness of our method compared with existing state-of-the-art baselines.\n","authors":["Jinluan Yang","Zhengyu Chen","Teng Xiao","Wenqiao Zhang","Yong Lin","Kun Kuang"],"pdf_url":"https://arxiv.org/pdf/2408.09490v2.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.13408v1","updated":"2024-10-17T10:14:52Z","published":"2024-10-17T10:14:52Z","title":"MoR: Mixture of Ranks for Low-Rank Adaptation Tuning","summary":"  Low-Rank Adaptation (LoRA) drives research to align its performance with full\nfine-tuning. However, significant challenges remain: (1) Simply increasing the\nrank size of LoRA does not effectively capture high-rank information, which\nleads to a performance bottleneck.(2) MoE-style LoRA methods substantially\nincrease parameters and inference latency, contradicting the goals of efficient\nfine-tuning and ease of application. To address these challenges, we introduce\nMixture of Ranks (MoR), which learns rank-specific information for different\ntasks based on input and efficiently integrates multi-rank information. We\nfirstly propose a new framework that equates the integration of multiple LoRAs\nto expanding the rank of LoRA. Moreover, we hypothesize that low-rank LoRA\nalready captures sufficient intrinsic information, and MoR can derive high-rank\ninformation through mathematical transformations of the low-rank components.\nThus, MoR can reduces the learning difficulty of LoRA and enhances its\nmulti-task capabilities. MoR achieves impressive results, with MoR delivering a\n1.31\\% performance improvement while using only 93.93\\% of the parameters\ncompared to baseline methods.\n","authors":["Chuanyu Tang","Yilong Chen","Zhenyu Zhang","Junyuan Shang","Wenyuan Zhang","Yong Huang","Tingwen Liu"],"pdf_url":"https://arxiv.org/pdf/2410.13408v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.00510v2","updated":"2024-10-17T10:12:54Z","published":"2024-10-01T08:48:05Z","title":"Advancing RVFL networks: Robust classification with the HawkEye loss\n  function","summary":"  Random vector functional link (RVFL), a variant of single-layer feedforward\nneural network (SLFN), has garnered significant attention due to its lower\ncomputational cost and robustness to overfitting. Despite its advantages, the\nRVFL network's reliance on the square error loss function makes it highly\nsensitive to outliers and noise, leading to degraded model performance in\nreal-world applications. To remedy it, we propose the incorporation of the\nHawkEye loss (H-loss) function into the RVFL framework. The H-loss function\nfeatures nice mathematical properties, including smoothness and boundedness,\nwhile simultaneously incorporating an insensitive zone. Each characteristic\nbrings its own advantages: 1) Boundedness limits the impact of extreme errors,\nenhancing robustness against outliers; 2) Smoothness facilitates the use of\ngradient-based optimization algorithms, ensuring stable and efficient\nconvergence; and 3) The insensitive zone mitigates the effect of minor\ndiscrepancies and noise. Leveraging the H-loss function, we embed it into the\nRVFL framework and develop a novel robust RVFL model termed H-RVFL. Notably,\nthis work addresses a significant gap, as no bounded loss function has been\nincorporated into RVFL to date. The non-convex optimization of the proposed\nH-RVFL is effectively addressed by the Nesterov accelerated gradient (NAG)\nalgorithm, whose computational complexity is also discussed. The proposed\nH-RVFL model's effectiveness is validated through extensive experiments on $40$\nbenchmark datasets from UCI and KEEL repositories, with and without label\nnoise. The results highlight significant improvements in robustness and\nefficiency, establishing the H-RVFL model as a powerful tool for applications\nin noisy and outlier-prone environments.\n","authors":["Mushir Akhtar","Ritik Mishra","M. Tanveer","Mohd. Arshad"],"pdf_url":"https://arxiv.org/pdf/2410.00510v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13404v1","updated":"2024-10-17T10:01:22Z","published":"2024-10-17T10:01:22Z","title":"Predicting Breast Cancer Survival: A Survival Analysis Approach Using\n  Log Odds and Clinical Variables","summary":"  Breast cancer remains a significant global health challenge, with prognosis\nand treatment decisions largely dependent on clinical characteristics. Accurate\nprediction of patient outcomes is crucial for personalized treatment\nstrategies. This study employs survival analysis techniques, including Cox\nproportional hazards and parametric survival models, to enhance the prediction\nof the log odds of survival in breast cancer patients. Clinical variables such\nas tumor size, hormone receptor status, HER2 status, age, and treatment history\nwere analyzed to assess their impact on survival outcomes. Data from 1557\nbreast cancer patients were obtained from a publicly available dataset provided\nby the University College Hospital, Ibadan, Nigeria. This dataset was\npreprocessed and analyzed using both univariate and multivariate approaches to\nevaluate survival outcomes. Kaplan-Meier survival curves were generated to\nvisualize survival probabilities, while the Cox proportional hazards model\nidentified key risk factors influencing mortality. The results showed that\nolder age, larger tumor size, and HER2-positive status were significantly\nassociated with an increased risk of mortality. In contrast, estrogen receptor\npositivity and breast-conserving surgery were linked to better survival\noutcomes. The findings suggest that integrating these clinical variables into\npredictive models improvesthe accuracy of survival predictions, helping to\nidentify high-risk patients who may benefit from more aggressive interventions.\nThis study demonstrates the potential of survival analysis in optimizing breast\ncancer care, particularly in resource-limited settings. Future research should\nfocus on integrating genomic data and real-world clinical outcomes to further\nrefine these models.\n","authors":["Opeyemi Sheu Alamu","Bismar Jorge Gutierrez Choque","Syed Wajeeh Abbs Rizvi","Samah Badr Hammed","Isameldin Elamin Medani","Md Kamrul Siam","Waqar Ahmad Tahir"],"pdf_url":"https://arxiv.org/pdf/2410.13404v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2406.02447v2","updated":"2024-10-17T09:52:10Z","published":"2024-06-04T16:12:27Z","title":"Reducing Bias in Federated Class-Incremental Learning with Hierarchical\n  Generative Prototypes","summary":"  Federated Learning (FL) aims at unburdening the training of deep models by\ndistributing computation across multiple devices (clients) while safeguarding\ndata privacy. On top of that, Federated Continual Learning (FCL) also accounts\nfor data distribution evolving over time, mirroring the dynamic nature of\nreal-world environments. In this work, we shed light on the Incremental and\nFederated biases that naturally emerge in FCL. While the former is a known\nproblem in Continual Learning, stemming from the prioritization of recently\nintroduced classes, the latter (i.e., the bias towards local distributions)\nremains relatively unexplored. Our proposal constrains both biases in the last\nlayer by efficiently fine-tuning a pre-trained backbone using learnable\nprompts, resulting in clients that produce less biased representations and more\nbiased classifiers. Therefore, instead of solely relying on parameter\naggregation, we also leverage generative prototypes to effectively balance the\npredictions of the global model. Our method improves on the current State Of\nThe Art, providing an average increase of +7.9% in accuracy.\n","authors":["Riccardo Salami","Pietro Buzzega","Matteo Mosconi","Mattia Verasani","Simone Calderara"],"pdf_url":"https://arxiv.org/pdf/2406.02447v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07943v2","updated":"2024-10-17T09:48:06Z","published":"2024-05-13T17:18:08Z","title":"Decision Mamba Architectures","summary":"  Recent advancements in imitation learning have been largely fueled by the\nintegration of sequence models, which provide a structured flow of information\nto effectively mimic task behaviours. Currently, Decision Transformer (DT) and\nsubsequently, the Hierarchical Decision Transformer (HDT), presented\nTransformer-based approaches to learn task policies. Recently, the Mamba\narchitecture has shown to outperform Transformers across various task domains.\nIn this work, we introduce two novel methods, Decision Mamba (DM) and\nHierarchical Decision Mamba (HDM), aimed at enhancing the performance of the\nTransformer models. Through extensive experimentation across diverse\nenvironments such as OpenAI Gym and D4RL, leveraging varying demonstration data\nsets, we demonstrate the superiority of Mamba models over their Transformer\ncounterparts in a majority of tasks. Results show that DM outperforms other\nmethods in most settings. The code can be found at\nhttps://github.com/meowatthemoon/DecisionMamba.\n","authors":["André Correia","Luís A. Alexandre"],"pdf_url":"https://arxiv.org/pdf/2405.07943v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13390v1","updated":"2024-10-17T09:41:54Z","published":"2024-10-17T09:41:54Z","title":"A Self-Constructing Multi-Expert Fuzzy System for High-dimensional Data\n  Classification","summary":"  Fuzzy Neural Networks (FNNs) are effective machine learning models for\nclassification tasks, commonly based on the Takagi-Sugeno-Kang (TSK) fuzzy\nsystem. However, when faced with high-dimensional data, especially with noise,\nFNNs encounter challenges such as vanishing gradients, excessive fuzzy rules,\nand limited access to prior knowledge. To address these challenges, we propose\na novel fuzzy system, the Self-Constructing Multi-Expert Fuzzy System\n(SOME-FS). It combines two learning strategies: mixed structure learning and\nmulti-expert advanced learning. The former enables each base classifier to\neffectively determine its structure without requiring prior knowledge, while\nthe latter tackles the issue of vanishing gradients by enabling each rule to\nfocus on its local region, thereby enhancing the robustness of the fuzzy\nclassifiers. The overall ensemble architecture enhances the stability and\nprediction performance of the fuzzy system. Our experimental results\ndemonstrate that the proposed SOME-FS is effective in high-dimensional tabular\ndata, especially in dealing with uncertainty. Moreover, our stable rule mining\nprocess can identify concise and core rules learned by the SOME-FS.\n","authors":["Yingtao Ren","Yu-Cheng Chang","Thomas Do","Zehong Cao","Chin-Teng Lin"],"pdf_url":"https://arxiv.org/pdf/2410.13390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13381v1","updated":"2024-10-17T09:36:01Z","published":"2024-10-17T09:36:01Z","title":"Learning Counterfactual Distributions via Kernel Nearest Neighbors","summary":"  Consider a setting with multiple units (e.g., individuals, cohorts,\ngeographic locations) and outcomes (e.g., treatments, times, items), where the\ngoal is to learn a multivariate distribution for each unit-outcome entry, such\nas the distribution of a user's weekly spend and engagement under a specific\nmobile app version. A common challenge is the prevalence of missing not at\nrandom data, where observations are available only for certain unit-outcome\ncombinations and the observation availability can be correlated with the\nproperties of distributions themselves, i.e., there is unobserved confounding.\nAn additional challenge is that for any observed unit-outcome entry, we only\nhave a finite number of samples from the underlying distribution. We tackle\nthese two challenges by casting the problem into a novel distributional matrix\ncompletion framework and introduce a kernel based distributional generalization\nof nearest neighbors to estimate the underlying distributions. By leveraging\nmaximum mean discrepancies and a suitable factor model on the kernel mean\nembeddings of the underlying distributions, we establish consistent recovery of\nthe underlying distributions even when data is missing not at random and\npositivity constraints are violated. Furthermore, we demonstrate that our\nnearest neighbors approach is robust to heteroscedastic noise, provided we have\naccess to two or more measurements for the observed unit-outcome entries, a\nrobustness not present in prior works on nearest neighbors with single\nmeasurements.\n","authors":["Kyuseong Choi","Jacob Feitelberg","Anish Agarwal","Raaz Dwivedi"],"pdf_url":"https://arxiv.org/pdf/2410.13381v1.pdf","comment":"33 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.13376v1","updated":"2024-10-17T09:26:14Z","published":"2024-10-17T09:26:14Z","title":"Data-Augmented Predictive Deep Neural Network: Enhancing the\n  extrapolation capabilities of non-intrusive surrogate models","summary":"  Numerically solving a large parametric nonlinear dynamical system is\nchallenging due to its high complexity and the high computational costs. In\nrecent years, machine-learning-aided surrogates are being actively researched.\nHowever, many methods fail in accurately generalizing in the entire time\ninterval $[0, T]$, when the training data is available only in a training time\ninterval $[0, T_0]$, with $T_0<T$.\n  To improve the extrapolation capabilities of the surrogate models in the\nentire time domain, we propose a new deep learning framework, where kernel\ndynamic mode decomposition (KDMD) is employed to evolve the dynamics of the\nlatent space generated by the encoder part of a convolutional autoencoder\n(CAE). After adding the KDMD-decoder-extrapolated data into the original data\nset, we train the CAE along with a feed-forward deep neural network using the\naugmented data. The trained network can predict future states outside the\ntraining time interval at any out-of-training parameter samples. The proposed\nmethod is tested on two numerical examples: a FitzHugh-Nagumo model and a model\nof incompressible flow past a cylinder. Numerical results show accurate and\nfast prediction performance in both the time and the parameter domain.\n","authors":["Shuwen Sun","Lihong Feng","Peter Benner"],"pdf_url":"https://arxiv.org/pdf/2410.13376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13373v1","updated":"2024-10-17T09:23:53Z","published":"2024-10-17T09:23:53Z","title":"Addressing Heterogeneity and Heterophily in Graphs: A Heterogeneous\n  Heterophilic Spectral Graph Neural Network","summary":"  Graph Neural Networks (GNNs) have garnered significant scholarly attention\nfor their powerful capabilities in modeling graph structures. Despite this, two\nprimary challenges persist: heterogeneity and heterophily. Existing studies\noften address heterogeneous and heterophilic graphs separately, leaving a\nresearch gap in the understanding of heterogeneous heterophilic graphs-those\nthat feature diverse node or relation types with dissimilar connected nodes. To\naddress this gap, we investigate the application of spectral graph filters\nwithin heterogeneous graphs. Specifically, we propose a Heterogeneous\nHeterophilic Spectral Graph Neural Network (H2SGNN), which employs a\ndual-module approach: local independent filtering and global hybrid filtering.\nThe local independent filtering module applies polynomial filters to each\nsubgraph independently to adapt to different homophily, while the global hybrid\nfiltering module captures interactions across different subgraphs. Extensive\nempirical evaluations on four real-world datasets demonstrate the superiority\nof H2SGNN compared to state-of-the-art methods.\n","authors":["Kangkang Lu","Yanhua Yu","Zhiyong Huang","Jia Li","Yuling Wang","Meiyu Liang","Xiting Qin","Yimeng Ren","Tat-Seng Chua","Xidian Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13363v1","updated":"2024-10-17T09:15:09Z","published":"2024-10-17T09:15:09Z","title":"Statistical testing on generative AI anomaly detection tools in\n  Alzheimer's Disease diagnosis","summary":"  Alzheimer's Disease is challenging to diagnose due to our limited\nunderstanding of its mechanism and large heterogeneity among patients.\nNeurodegeneration is studied widely as a biomarker for clinical diagnosis,\nwhich can be measured from time series MRI progression. On the other hand,\ngenerative AI has shown promise in anomaly detection in medical imaging and\nused for tasks including tumor detection. However, testing the reliability of\nsuch data-driven methods is non-trivial due to the issue of double-dipping in\nhypothesis testing. In this work, we propose to solve this issue with selective\ninference and develop a reliable generative AI method for Alzheimer's\nprediction. We show that compared to traditional statistical methods with\nhighly inflated p-values, selective inference successfully controls the false\ndiscovery rate under the desired alpha level while retaining statistical power.\nIn practice, our pipeline could assist clinicians in Alzheimer's diagnosis and\nearly intervention.\n","authors":["Rosemary He","Ichiro Takeuchi"],"pdf_url":"https://arxiv.org/pdf/2410.13363v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13360v1","updated":"2024-10-17T09:10:26Z","published":"2024-10-17T09:10:26Z","title":"Remember, Retrieve and Generate: Understanding Infinite Visual Concepts\n  as Your Personalized Assistant","summary":"  The development of large language models (LLMs) has significantly enhanced\nthe capabilities of multimodal LLMs (MLLMs) as general assistants. However,\nlack of user-specific knowledge still restricts their application in human's\ndaily life. In this paper, we introduce the Retrieval Augmented Personalization\n(RAP) framework for MLLMs' personalization. Starting from a general MLLM, we\nturn it into a personalized assistant in three steps. (a) Remember: We design a\nkey-value database to store user-related information, e.g., user's name, avatar\nand other attributes. (b) Retrieve: When the user initiates a conversation, RAP\nwill retrieve relevant information from the database using a multimodal\nretriever. (c) Generate: The input query and retrieved concepts' information\nare fed into MLLMs to generate personalized, knowledge-augmented responses.\nUnlike previous methods, RAP allows real-time concept editing via updating the\nexternal database. To further improve generation quality and alignment with\nuser-specific information, we design a pipeline for data collection and create\na specialized dataset for personalized training of MLLMs. Based on the dataset,\nwe train a series of MLLMs as personalized multimodal assistants. By\npretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual\nconcepts without additional finetuning. Our models demonstrate outstanding\nflexibility and generation quality across a variety of tasks, such as\npersonalized image captioning, question answering and visual recognition. The\ncode, data and models are available at https://github.com/Hoar012/RAP-MLLM.\n","authors":["Haoran Hao","Jiaming Han","Changsheng Li","Yu-Feng Li","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2410.13360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13351v1","updated":"2024-10-17T09:02:28Z","published":"2024-10-17T09:02:28Z","title":"Representation Learning of Structured Data for Medical Foundation Models","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious domains, including healthcare. However, their ability to effectively\nrepresent structured non-textual data, such as the alphanumeric medical codes\nused in records like ICD-10 or SNOMED-CT, is limited and has been particularly\nexposed in recent research. This paper examines the challenges LLMs face in\nprocessing medical codes due to the shortcomings of current tokenization\nmethods. As a result, we introduce the UniStruct architecture to design a\nmultimodal medical foundation model of unstructured text and structured data,\nwhich addresses these challenges by adapting subword tokenization techniques\nspecifically for the structured medical codes. Our approach is validated\nthrough model pre-training on both an extensive internal medical database and a\npublic repository of structured medical records. Trained on over 1 billion\ntokens on the internal medical database, the proposed model achieves up to a\n23% improvement in evaluation metrics, with around 2% gain attributed to our\nproposed tokenization. Additionally, when evaluated on the EHRSHOT public\nbenchmark with a 1/1000 fraction of the pre-training data, the UniStruct model\nimproves performance on over 42% of the downstream tasks. Our approach not only\nenhances the representation and generalization capabilities of patient-centric\nmodels but also bridges a critical gap in representation learning models'\nability to handle complex structured medical data, alongside unstructured text.\n","authors":["Vijay Prakash Dwivedi","Viktor Schlegel","Andy T. Liu","Thanh-Tung Nguyen","Abhinav Ramesh Kashyap","Jeng Wei","Wei-Hsian Yin","Stefan Winkler","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2410.13351v1.pdf","comment":"NeurIPS 2024 Workshop on Unifying Representations in Neural Models\n  (UniReps 2024)"},{"id":"http://arxiv.org/abs/2405.17525v2","updated":"2024-10-17T08:56:51Z","published":"2024-05-27T14:23:30Z","title":"SmoothGNN: Smoothing-aware GNN for Unsupervised Node Anomaly Detection","summary":"  The smoothing issue in graph learning leads to indistinguishable node\nrepresentations, posing significant challenges for graph-related tasks.\nHowever, our experiments reveal that this problem can uncover underlying\nproperties of node anomaly detection (NAD) that previous research has missed.\nWe introduce Individual Smoothing Patterns (ISP) and Neighborhood Smoothing\nPatterns (NSP), which indicate that the representations of anomalous nodes are\nharder to smooth than those of normal ones. In addition, we explore the\ntheoretical implications of these patterns, demonstrating the potential\nbenefits of ISP and NSP for NAD tasks. Motivated by these findings, we propose\nSmoothGNN, a novel unsupervised NAD framework. First, we design a learning\ncomponent to explicitly capture ISP for detecting node anomalies. Second, we\ndesign a spectral graph neural network to implicitly learn ISP to enhance\ndetection. Third, we design an effective coefficient based on our findings that\nNSP can serve as coefficients for node representations, aiding in the\nidentification of anomalous nodes. Furthermore, we devise a novel anomaly\nmeasure to calculate loss functions and anomalous scores for nodes, reflecting\nthe properties of NAD using ISP and NSP. Extensive experiments on 9 real\ndatasets show that SmoothGNN outperforms the best rival by an average of 14.66%\nin AUC and 7.28% in Average Precision, with 75x running time speedup,\nvalidating the effectiveness and efficiency of our framework.\n","authors":["Xiangyu Dong","Xingyi Zhang","Yanni Sun","Lei Chen","Mingxuan Yuan","Sibo Wang"],"pdf_url":"https://arxiv.org/pdf/2405.17525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02057v2","updated":"2024-10-17T08:55:31Z","published":"2024-07-02T08:38:32Z","title":"HC-GLAD: Dual Hyperbolic Contrastive Learning for Unsupervised\n  Graph-Level Anomaly Detection","summary":"  Unsupervised graph-level anomaly detection (UGAD) has garnered increasing\nattention in recent years due to its significance. Most existing methods that\nrely on traditional GNNs mainly consider pairwise relationships between\nfirst-order neighbors, which is insufficient to capture the complex high-order\ndependencies often associated with anomalies. This limitation underscores the\nnecessity of exploring high-order node interactions in UGAD. In addition, most\nprevious works ignore the underlying properties (e.g., hierarchy and power-law\nstructure) which are common in real-world graph datasets and therefore are\nindispensable factors in the UGAD task. In this paper, we propose a novel Dual\nHyperbolic Contrastive Learning for Unsupervised Graph-Level Anomaly Detection\n(HC-GLAD in short). To exploit high-order node group information, we construct\nhypergraphs based on pre-designed gold motifs and subsequently perform\nhypergraph convolution. Furthermore, to preserve the hierarchy of real-world\ngraphs, we introduce hyperbolic geometry into this field and conduct both graph\nand hypergraph embedding learning in hyperbolic space with the hyperboloid\nmodel. To the best of our knowledge, this is the first work to simultaneously\napply hypergraph with node group information and hyperbolic geometry in this\nfield. Extensive experiments on 13 real-world datasets of different fields\ndemonstrate the superiority of HC-GLAD on the UGAD task. The code is available\nat https://github.com/Yali-F/HC-GLAD.\n","authors":["Yali Fu","Jindong Li","Jiahong Liu","Qianli Xing","Qi Wang","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2407.02057v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07979v2","updated":"2024-10-17T08:54:37Z","published":"2024-04-11T17:57:22Z","title":"LLoCO: Learning Long Contexts Offline","summary":"  Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco.\n","authors":["Sijun Tan","Xiuyu Li","Shishir Patil","Ziyang Wu","Tianjun Zhang","Kurt Keutzer","Joseph E. Gonzalez","Raluca Ada Popa"],"pdf_url":"https://arxiv.org/pdf/2404.07979v2.pdf","comment":"EMNLP 2024. The first two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2410.13343v1","updated":"2024-10-17T08:52:52Z","published":"2024-10-17T08:52:52Z","title":"Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges\n  in Large Language Models","summary":"  Large Language Models (LLMs) have shown remarkable capabilities in various\nnatural language processing tasks. However, LLMs may rely on dataset biases as\nshortcuts for prediction, which can significantly impair their robustness and\ngeneralization capabilities. This paper presents Shortcut Suite, a\ncomprehensive test suite designed to evaluate the impact of shortcuts on LLMs'\nperformance, incorporating six shortcut types, five evaluation metrics, and\nfour prompting strategies. Our extensive experiments yield several key\nfindings: 1) LLMs demonstrate varying reliance on shortcuts for downstream\ntasks, significantly impairing their performance. 2) Larger LLMs are more\nlikely to utilize shortcuts under zero-shot and few-shot in-context learning\nprompts. 3) Chain-of-thought prompting notably reduces shortcut reliance and\noutperforms other prompting strategies, while few-shot prompts generally\nunderperform compared to zero-shot prompts. 4) LLMs often exhibit\noverconfidence in their predictions, especially when dealing with datasets that\ncontain shortcuts. 5) LLMs generally have a lower explanation quality in\nshortcut-laden datasets, with errors falling into three types: distraction,\ndisguised comprehension, and logical fallacy. Our findings offer new insights\nfor evaluating robustness and generalization in LLMs and suggest potential\ndirections for mitigating the reliance on shortcuts. The code is available at\n\\url {https://github.com/yyhappier/ShortcutSuite.git}.\n","authors":["Yu Yuan","Lili Zhao","Kai Zhang","Guangting Zheng","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2410.13343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13341v1","updated":"2024-10-17T08:49:42Z","published":"2024-10-17T08:49:42Z","title":"Limits to scalable evaluation at the frontier: LLM as Judge won't beat\n  twice the data","summary":"  High quality annotations are increasingly a bottleneck in the explosively\ngrowing machine learning ecosystem. Scalable evaluation methods that avoid\ncostly annotation have therefore become an important research ambition. Many\nhope to use strong existing models in lieu of costly labels to provide cheap\nmodel evaluations. Unfortunately, this method of using models as judges\nintroduces biases, such as self-preferencing, that can distort model\ncomparisons. An emerging family of debiasing tools promises to fix these issues\nby using a few high quality labels to debias a large number of model judgments.\nIn this paper, we study how far such debiasing methods, in principle, can go.\nOur main result shows that when the judge is no more accurate than the\nevaluated model, no debiasing method can decrease the required amount of ground\ntruth labels by more than half. Our result speaks to the severe limitations of\nthe LLM-as-a-judge paradigm at the evaluation frontier where the goal is to\nassess newly released models that are possibly better than the judge. Through\nan empirical evaluation, we demonstrate that the sample size savings achievable\nin practice are even more modest than what our theoretical limit suggests.\nAlong the way, our work provides new observations about debiasing methods for\nmodel evaluation, and points out promising avenues for future work.\n","authors":["Florian E. Dorner","Vivian Y. Nastl","Moritz Hardt"],"pdf_url":"https://arxiv.org/pdf/2410.13341v1.pdf","comment":"22 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.13338v1","updated":"2024-10-17T08:48:52Z","published":"2024-10-17T08:48:52Z","title":"DiffImp: Efficient Diffusion Model for Probabilistic Time Series\n  Imputation with Bidirectional Mamba Backbone","summary":"  Probabilistic time series imputation has been widely applied in real-world\nscenarios due to its ability to estimate uncertainty of imputation results.\nMeanwhile, denoising diffusion probabilistic models (DDPMs) have achieved great\nsuccess in probabilistic time series imputation tasks with its power to model\ncomplex distributions. However, current DDPM-based probabilistic time series\nimputation methodologies are confronted with two types of challenges:\n1)~\\textit{~The backbone modules of the denoising parts are not capable of\nachieving sequence modeling with low time complexity.} 2)~\\textit{The\narchitecture of denoising modules can not handle the inter-variable and\nbidirectional dependencies in the time series imputation problem effectively.}\nTo address the first challenge, we integrate the computational efficient state\nspace model, namely Mamba, as the backbone denosing module for DDPMs. To tackle\nthe second challenge, we carefully devise several SSM-based blocks for\nbidirectional modeling and inter-variable relation understanding. Experimental\nresults demonstrate that our approach can achieve state-of-the-art time series\nimputation results on multiple datasets, different missing scenarios and\nmissing ratios.\n","authors":["Hongfan Gao","Wangmeng Shen","Xiangfei Qiu","Ronghui Xu","Jilin Hu","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2410.13338v1.pdf","comment":"25 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.13334v1","updated":"2024-10-17T08:46:09Z","published":"2024-10-17T08:46:09Z","title":"Do LLMs Have Political Correctness? Analyzing Ethical Biases and\n  Jailbreak Vulnerabilities in AI Systems","summary":"  Although large language models (LLMs) demonstrate impressive proficiency in\nvarious tasks, they present potential safety risks, such as `jailbreaks', where\nmalicious inputs can coerce LLMs into generating harmful content. To address\nthese issues, many LLM developers have implemented various safety measures to\nalign these models. This alignment involves several techniques, including data\nfiltering during pre-training, supervised fine-tuning, reinforcement learning\nfrom human feedback, and red-teaming exercises. These methods often introduce\ndeliberate and intentional biases similar to Political Correctness (PC) to\nensure the ethical behavior of LLMs. In this paper, we delve into the\nintentional biases injected into LLMs for safety purposes and examine methods\nto circumvent these safety alignment techniques. Notably, these intentional\nbiases result in a jailbreaking success rate in GPT-4o models that differs by\n20% between non-binary and cisgender keywords and by 16% between white and\nblack keywords, even when the other parts of the prompts are identical. We\nintroduce the concept of PCJailbreak, highlighting the inherent risks posed by\nthese safety-induced biases. Additionally, we propose an efficient defense\nmethod PCDefense, which prevents jailbreak attempts by injecting defense\nprompts prior to generation. PCDefense stands as an appealing alternative to\nGuard Models, such as Llama-Guard, that require additional inference cost after\ntext generation. Our findings emphasize the urgent need for LLM developers to\nadopt a more responsible approach when designing and implementing safety\nmeasures.\n","authors":["Isack Lee","Haebin Seong"],"pdf_url":"https://arxiv.org/pdf/2410.13334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03888v2","updated":"2024-10-17T08:45:44Z","published":"2024-07-04T12:26:31Z","title":"Continuous-time q-Learning for Jump-Diffusion Models under Tsallis\n  Entropy","summary":"  This paper studies the continuous-time reinforcement learning in\njump-diffusion models by featuring the q-learning (the continuous-time\ncounterpart of Q-learning) under Tsallis entropy regularization. Contrary to\nthe Shannon entropy, the general form of Tsallis entropy renders the optimal\npolicy not necessary a Gibbs measure, where the Lagrange and KKT multipliers\nnaturally arise from some constraints to ensure the learnt policy to be a\nprobability density function. As a consequence, the characterization of the\noptimal policy using the q-function also involves a Lagrange multiplier. In\nresponse, we establish the martingale characterization of the q-function under\nTsallis entropy and devise two q-learning algorithms depending on whether the\nLagrange multiplier can be derived explicitly or not. In the latter case, we\nneed to consider different parameterizations of the optimal q-function and the\noptimal policy and update them alternatively in an Actor-Critic manner. We also\nstudy two financial applications, namely, an optimal portfolio liquidation\nproblem and a non-LQ control problem. It is interesting to see therein that the\noptimal policies under the Tsallis entropy regularization can be characterized\nexplicitly, which are distributions concentrated on some compact support. The\nsatisfactory performance of our q-learning algorithms is illustrated in each\nexample.\n","authors":["Lijun Bo","Yijie Huang","Xiang Yu","Tingting Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.03888v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13331v1","updated":"2024-10-17T08:44:57Z","published":"2024-10-17T08:44:57Z","title":"Improving Discrete Optimisation Via Decoupled Straight-Through\n  Gumbel-Softmax","summary":"  Discrete representations play a crucial role in many deep learning\narchitectures, yet their non-differentiable nature poses significant challenges\nfor gradient-based optimization. To address this issue, various gradient\nestimators have been developed, including the Straight-Through Gumbel-Softmax\n(ST-GS) estimator, which combines the Straight-Through Estimator (STE) and the\nGumbel-based reparameterization trick. However, the performance of ST-GS is\nhighly sensitive to temperature, with its selection often compromising gradient\nfidelity. In this work, we propose a simple yet effective extension to ST-GS by\nemploying decoupled temperatures for forward and backward passes, which we\nrefer to as \"Decoupled ST-GS\". We show that our approach significantly enhances\nthe original ST-GS through extensive experiments across multiple tasks and\ndatasets. We further investigate the impact of our method on gradient fidelity\nfrom multiple perspectives, including the gradient gap and the bias-variance\ntrade-off of estimated gradients. Our findings contribute to the ongoing effort\nto improve discrete optimization in deep learning, offering a practical\nsolution that balances simplicity and effectiveness.\n","authors":["Rushi Shah","Mingyuan Yan","Michael Curtis Mozer","Dianbo Liu"],"pdf_url":"https://arxiv.org/pdf/2410.13331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19272v2","updated":"2024-10-17T08:36:58Z","published":"2024-06-27T15:38:37Z","title":"Stochastic Concept Bottleneck Models","summary":"  Concept Bottleneck Models (CBMs) have emerged as a promising interpretable\nmethod whose final prediction is based on intermediate, human-understandable\nconcepts rather than the raw input. Through time-consuming manual\ninterventions, a user can correct wrongly predicted concept values to enhance\nthe model's downstream performance. We propose Stochastic Concept Bottleneck\nModels (SCBMs), a novel approach that models concept dependencies. In SCBMs, a\nsingle-concept intervention affects all correlated concepts, thereby improving\nintervention effectiveness. Unlike previous approaches that model the concept\nrelations via an autoregressive structure, we introduce an explicit,\ndistributional parameterization that allows SCBMs to retain the CBMs' efficient\ntraining and inference procedure. Additionally, we leverage the\nparameterization to derive an effective intervention strategy based on the\nconfidence region. We show empirically on synthetic tabular and natural image\ndatasets that our approach improves intervention effectiveness significantly.\nNotably, we showcase the versatility and usability of SCBMs by examining a\nsetting with CLIP-inferred concepts, alleviating the need for manual concept\nannotations.\n","authors":["Moritz Vandenhirtz","Sonia Laguna","Ričards Marcinkevičs","Julia E. Vogt"],"pdf_url":"https://arxiv.org/pdf/2406.19272v2.pdf","comment":"Published at 38th Conference on Neural Information Processing Systems\n  (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2402.02249v2","updated":"2024-10-17T08:30:07Z","published":"2024-02-03T19:40:41Z","title":"Don't Label Twice: Quantity Beats Quality when Comparing Binary\n  Classifiers on a Budget","summary":"  We study how to best spend a budget of noisy labels to compare the accuracy\nof two binary classifiers. It's common practice to collect and aggregate\nmultiple noisy labels for a given data point into a less noisy label via a\nmajority vote. We prove a theorem that runs counter to conventional wisdom. If\nthe goal is to identify the better of two classifiers, we show it's best to\nspend the budget on collecting a single label for more samples. Our result\nfollows from a non-trivial application of Cram\\'er's theorem, a staple in the\ntheory of large deviations. We discuss the implications of our work for the\ndesign of machine learning benchmarks, where they overturn some time-honored\nrecommendations. In addition, our results provide sample size bounds superior\nto what follows from Hoeffding's bound.\n","authors":["Florian E. Dorner","Moritz Hardt"],"pdf_url":"https://arxiv.org/pdf/2402.02249v2.pdf","comment":"34 pages, 3 Figures, Published at ICML 2024"},{"id":"http://arxiv.org/abs/2308.14409v2","updated":"2024-10-17T08:25:06Z","published":"2023-08-28T08:47:06Z","title":"Steerable Conditional Diffusion for Out-of-Distribution Adaptation in\n  Medical Image Reconstruction","summary":"  Denoising diffusion models have emerged as the go-to generative framework for\nsolving inverse problems in imaging. A critical concern regarding these models\nis their performance on out-of-distribution tasks, which remains an\nunder-explored challenge. Using a diffusion model on an out-of-distribution\ndataset, realistic reconstructions can be generated, but with hallucinating\nimage features that are uniquely present in the training dataset. To address\nthis discrepancy during train-test time and improve reconstruction accuracy, we\nintroduce a novel sampling framework called Steerable Conditional Diffusion.\nSpecifically, this framework adapts the diffusion model, concurrently with\nimage reconstruction, based solely on the information provided by the available\nmeasurement. Utilising our proposed method, we achieve substantial enhancements\nin out-of-distribution performance across diverse imaging modalities, advancing\nthe robust deployment of denoising diffusion models in real-world applications.\n","authors":["Riccardo Barbano","Alexander Denker","Hyungjin Chung","Tae Hoon Roh","Simon Arridge","Peter Maass","Bangti Jin","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2308.14409v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11522v2","updated":"2024-10-17T08:18:14Z","published":"2024-10-15T11:48:31Z","title":"Leveraging LLM Embeddings for Cross Dataset Label Alignment and Zero\n  Shot Music Emotion Prediction","summary":"  In this work, we present a novel method for music emotion recognition that\nleverages Large Language Model (LLM) embeddings for label alignment across\nmultiple datasets and zero-shot prediction on novel categories. First, we\ncompute LLM embeddings for emotion labels and apply non-parametric clustering\nto group similar labels, across multiple datasets containing disjoint labels.\nWe use these cluster centers to map music features (MERT) to the LLM embedding\nspace. To further enhance the model, we introduce an alignment regularization\nthat enables dissociation of MERT embeddings from different clusters. This\nfurther enhances the model's ability to better adaptation to unseen datasets.\nWe demonstrate the effectiveness of our approach by performing zero-shot\ninference on a new dataset, showcasing its ability to generalize to unseen\nlabels without additional training.\n","authors":["Renhang Liu","Abhinaba Roy","Dorien Herremans"],"pdf_url":"https://arxiv.org/pdf/2410.11522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10721v2","updated":"2024-10-17T08:15:27Z","published":"2024-01-19T14:32:50Z","title":"Generative Model for Constructing Reaction Path from Initial to Final\n  States","summary":"  Mapping the chemical reaction pathways and their corresponding activation\nbarriers is a significant challenge in molecular simulation. Given the inherent\ncomplexities of 3D atomic geometries, even generating an initial guess of these\npaths can be difficult for humans. This paper presents an innovative approach\nthat utilizes neural networks to generate initial guesses for reaction pathways\nbased on the initial state and learning from a database of low-energy\ntransition paths. The proposed method is initiated by inputting the coordinates\nof the initial state, followed by progressive alterations to its structure.\nThis iterative process culminates in the generation of the guess reaction path\nand the coordinates of the final state. The method does not require one-the-fly\ncomputation of the actual potential energy surface, and is therefore\nfast-acting. The application of this geometry-based method extends to complex\nreaction pathways illustrated by organic reactions. Training was executed on\nthe Transition1x dataset of organic reaction pathways. The results revealed the\ngeneration of reactions that bore substantial similarities with the test set of\nchemical reaction paths. The method's flexibility allows for reactions to be\ngenerated either to conform to predetermined conditions or in a randomized\nmanner.\n","authors":["Akihide Hayashi","So Takamoto","Ju Li","Yuta Tsuboi","Daisuke Okanohara"],"pdf_url":"https://arxiv.org/pdf/2401.10721v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03260v2","updated":"2024-10-17T08:15:09Z","published":"2024-06-05T13:37:42Z","title":"Feature learning in finite-width Bayesian deep linear networks with\n  multiple outputs and convolutional layers","summary":"  Deep linear networks have been extensively studied, as they provide\nsimplified models of deep learning. However, little is known in the case of\nfinite-width architectures with multiple outputs and convolutional layers. In\nthis manuscript, we provide rigorous results for the statistics of functions\nimplemented by the aforementioned class of networks, thus moving closer to a\ncomplete characterization of feature learning in the Bayesian setting. Our\nresults include: (i) an exact and elementary non-asymptotic integral\nrepresentation for the joint prior distribution over the outputs, given in\nterms of a mixture of Gaussians; (ii) an analytical formula for the posterior\ndistribution in the case of squared error loss function (Gaussian likelihood);\n(iii) a quantitative description of the feature learning infinite-width regime,\nusing large deviation theory. From a physical perspective, deep architectures\nwith multiple outputs or convolutional layers represent different\nmanifestations of kernel shape renormalization, and our work provides a\ndictionary that translates this physics intuition and terminology into rigorous\nBayesian statistics.\n","authors":["Federico Bassetti","Marco Gherardi","Alessandro Ingrosso","Mauro Pastore","Pietro Rotondo"],"pdf_url":"https://arxiv.org/pdf/2406.03260v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.02791v3","updated":"2024-10-17T16:11:28Z","published":"2021-07-06T17:58:35Z","title":"Depth-supervised NeRF: Fewer Views and Faster Training for Free","summary":"  A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting\nincorrect geometries when given an insufficient number of input views. One\npotential reason is that standard volumetric rendering does not enforce the\nconstraint that most of a scene's geometry consist of empty space and opaque\nsurfaces. We formalize the above assumption through DS-NeRF (Depth-supervised\nNeural Radiance Fields), a loss for learning radiance fields that takes\nadvantage of readily-available depth supervision. We leverage the fact that\ncurrent NeRF pipelines require images with known camera poses that are\ntypically estimated by running structure-from-motion (SFM). Crucially, SFM also\nproduces sparse 3D points that can be used as \"free\" depth supervision during\ntraining: we add a loss to encourage the distribution of a ray's terminating\ndepth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can\nrender better images given fewer training views while training 2-3x faster.\nFurther, we show that our loss is compatible with other recently proposed NeRF\nmethods, demonstrating that depth is a cheap and easily digestible supervisory\nsignal. And finally, we find that DS-NeRF can support other types of depth\nsupervision such as scanned depth sensors and RGB-D reconstruction outputs.\n","authors":["Kangle Deng","Andrew Liu","Jun-Yan Zhu","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2107.02791v3.pdf","comment":"Project page: http://www.cs.cmu.edu/~dsnerf/ GitHub:\n  https://github.com/dunbar12138/DSNeRF"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.13754v1","updated":"2024-10-17T16:52:28Z","published":"2024-10-17T16:52:28Z","title":"MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures","summary":"  Perceiving and generating diverse modalities are crucial for AI models to\neffectively learn from and engage with real-world signals, necessitating\nreliable evaluations for their development. We identify two major issues in\ncurrent evaluations: (1) inconsistent standards, shaped by different\ncommunities with varying protocols and maturity levels; and (2) significant\nquery, grading, and generalization biases. To address these, we introduce\nMixEval-X, the first any-to-any real-world benchmark designed to optimize and\nstandardize evaluations across input and output modalities. We propose\nmulti-modal benchmark mixture and adaptation-rectification pipelines to\nreconstruct real-world task distributions, ensuring evaluations generalize\neffectively to real-world use cases. Extensive meta-evaluations show our\napproach effectively aligns benchmark samples with real-world task\ndistributions and the model rankings correlate strongly with that of\ncrowd-sourced real-world evaluations (up to 0.98). We provide comprehensive\nleaderboards to rerank existing models and organizations and offer insights to\nenhance understanding of multi-modal evaluations and inform future research.\n","authors":["Jinjie Ni","Yifan Song","Deepanway Ghosal","Bo Li","David Junhao Zhang","Xiang Yue","Fuzhao Xue","Zian Zheng","Kaichen Zhang","Mahir Shah","Kabir Jain","Yang You","Michael Shieh"],"pdf_url":"https://arxiv.org/pdf/2410.13754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13733v1","updated":"2024-10-17T16:36:38Z","published":"2024-10-17T16:36:38Z","title":"Improving Multi-modal Large Language Model through Boosting Vision\n  Capabilities","summary":"  We focus on improving the visual understanding capability for boosting the\nvision-language models. We propose \\textbf{Arcana}, a multiModal language\nmodel, which introduces two crucial techniques. First, we present Multimodal\nLoRA (MM-LoRA), a module designed to enhance the decoder. Unlike traditional\nlanguage-driven decoders, MM-LoRA consists of two parallel LoRAs -- one for\nvision and one for language -- each with its own parameters. This disentangled\nparameters design allows for more specialized learning in each modality and\nbetter integration of multimodal information. Second, we introduce the Query\nLadder adapter (QLadder) to improve the visual encoder. QLadder employs a\nlearnable ``\\textit{ladder}'' structure to deeply aggregates the intermediate\nrepresentations from the frozen pretrained visual encoder (e.g., CLIP image\nencoder). This enables the model to learn new and informative visual features,\nas well as remaining the powerful capabilities of the pretrained visual\nencoder. These techniques collectively enhance Arcana's visual perception\npower, enabling it to leverage improved visual information for more accurate\nand contextually relevant outputs across various multimodal scenarios.\nExtensive experiments and ablation studies demonstrate the effectiveness and\ngeneralization capability of our Arcana. The code and re-annotated data are\navailable at \\url{https://arcana-project-page.github.io}.\n","authors":["Yanpeng Sun","Huaxin Zhang","Qiang Chen","Xinyu Zhang","Nong Sang","Gang Zhang","Jingdong Wang","Zechao Li"],"pdf_url":"https://arxiv.org/pdf/2410.13733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12407v2","updated":"2024-10-17T15:59:34Z","published":"2024-10-16T09:42:29Z","title":"Beyond Coarse-Grained Matching in Video-Text Retrieval","summary":"  Video-text retrieval has seen significant advancements, yet the ability of\nmodels to discern subtle differences in captions still requires verification.\nIn this paper, we introduce a new approach for fine-grained evaluation. Our\napproach can be applied to existing datasets by automatically generating hard\nnegative test captions with subtle single-word variations across nouns, verbs,\nadjectives, adverbs, and prepositions. We perform comprehensive experiments\nusing four state-of-the-art models across two standard benchmarks (MSR-VTT and\nVATEX) and two specially curated datasets enriched with detailed descriptions\n(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our\nanalyses show that the current evaluation benchmarks fall short in detecting a\nmodel's ability to perceive subtle single-word differences, 2) our fine-grained\nevaluation highlights the difficulty models face in distinguishing such subtle\nvariations. To enhance fine-grained understanding, we propose a new baseline\nthat can be easily combined with current methods. Experiments on our\nfine-grained evaluations demonstrate that this approach enhances a model's\nability to understand fine-grained differences.\n","authors":["Aozhu Chen","Hazel Doughty","Xirong Li","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12407v2.pdf","comment":"Accepted to ACCV 2024"},{"id":"http://arxiv.org/abs/2410.13647v1","updated":"2024-10-17T15:13:26Z","published":"2024-10-17T15:13:26Z","title":"Multimodal growth and development assessment model","summary":"  With the development of social economy and the improvement of people's\nattention to health, the growth and development of children and adolescents has\nbecome an important indicator to measure the level of national health.\nTherefore, accurate and timely assessment of children's growth and development\nhas become increasingly important. At the same time, global health\ninequalities, especially child malnutrition and stunting in developing\ncountries, urgently require effective assessment tools to monitor and\nintervene. In recent years, the rapid development of technologies such as big\ndata, artificial intelligence, and cloud computing, and the cross-integration\nof multiple disciplines such as biomedicine, statistics, and computer science\nhave promoted the rapid development of large-scale models for growth and\ndevelopment assessment. However, there are still problems such as too single\nevaluation factors, inaccurate diagnostic results, and inability to give\naccurate and reasonable recommendations. The multi-modal growth and development\nassessment model uses the public data set of RSNA ( North American College of\nRadiology ) as the training set, and the data set of the Department of\nPediatrics of Huaibei People's Hospital as the open source test set. The\nembedded ICL module enables the model to quickly adapt and identify the tasks\nthat need to be done to ensure that under the premise of considering multiple\nevaluation factors, accurate diagnosis results and reasonable medical\nrecommendations are given, so as to provide solutions to the above problems and\npromote the development of the medical field.\n","authors":["Ying Li","Zichen Song","Zijie Gong","Sitan Huang","Jiewei Ge"],"pdf_url":"https://arxiv.org/pdf/2410.13647v1.pdf","comment":"7 Pages 7 Figures"},{"id":"http://arxiv.org/abs/2410.13419v1","updated":"2024-10-17T10:41:52Z","published":"2024-10-17T10:41:52Z","title":"MeloTrans: A Text to Symbolic Music Generation Model Following Human\n  Composition Habit","summary":"  At present, neural network models show powerful sequence prediction ability\nand are used in many automatic composition models. In comparison, the way\nhumans compose music is very different from it. Composers usually start by\ncreating musical motifs and then develop them into music through a series of\nrules. This process ensures that the music has a specific structure and\nchanging pattern. However, it is difficult for neural network models to learn\nthese composition rules from training data, which results in a lack of\nmusicality and diversity in the generated music. This paper posits that\nintegrating the learning capabilities of neural networks with human-derived\nknowledge may lead to better results. To archive this, we develop the\nPOP909$\\_$M dataset, the first to include labels for musical motifs and their\nvariants, providing a basis for mimicking human compositional habits. Building\non this, we propose MeloTrans, a text-to-music composition model that employs\nprinciples of motif development rules. Our experiments demonstrate that\nMeloTrans excels beyond existing music generation models and even surpasses\nLarge Language Models (LLMs) like ChatGPT-4. This highlights the importance of\nmerging human insights with neural network capabilities to achieve superior\nsymbolic music generation.\n","authors":["Yutian Wang","Wanyin Yang","Zhenrong Dai","Yilong Zhang","Kun Zhao","Hui Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07728v4","updated":"2024-10-17T09:54:06Z","published":"2024-07-10T15:00:08Z","title":"SaMoye: Zero-shot Singing Voice Conversion Model Based on Feature\n  Disentanglement and Enhancement","summary":"  Singing voice conversion (SVC) aims to convert a singer's voice to another\nsinger's from a reference audio while keeping the original semantics. However,\nexisting SVC methods can hardly perform zero-shot due to incomplete feature\ndisentanglement or dependence on the speaker look-up table. We propose the\nfirst open-source high-quality zero-shot SVC model SaMoye that can convert\nsinging to human and non-human timbre. SaMoye disentangles the singing voice's\nfeatures into content, timbre, and pitch features, where we combine multiple\nASR models and compress the content features to reduce timbre leaks. Besides,\nwe enhance the timbre features by unfreezing the speaker encoder and mixing the\nspeaker embedding with top-3 similar speakers. We also establish an\nunparalleled large-scale dataset to guarantee zero-shot performance, which\ncomprises more than 1,815 hours of pure singing voice and 6,367 speakers. We\nconduct objective and subjective experiments to find that SaMoye outperforms\nother models in zero-shot SVC tasks even under extreme conditions like\nconverting singing to animals' timbre. The code and weight of SaMoye are\navailable on https://github.com/CarlWangChina/SaMoye-SVC. The weights, code,\ndataset, and documents of SaMoye are publicly available on\n\\url{https://github.com/CarlWangChina/SaMoye-SVC}.\n","authors":["Zihao Wang","Le Ma","Yongsheng Feng","Xin Pan","Yuhang Jin","Kejun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.07728v4.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.13360v1","updated":"2024-10-17T09:10:26Z","published":"2024-10-17T09:10:26Z","title":"Remember, Retrieve and Generate: Understanding Infinite Visual Concepts\n  as Your Personalized Assistant","summary":"  The development of large language models (LLMs) has significantly enhanced\nthe capabilities of multimodal LLMs (MLLMs) as general assistants. However,\nlack of user-specific knowledge still restricts their application in human's\ndaily life. In this paper, we introduce the Retrieval Augmented Personalization\n(RAP) framework for MLLMs' personalization. Starting from a general MLLM, we\nturn it into a personalized assistant in three steps. (a) Remember: We design a\nkey-value database to store user-related information, e.g., user's name, avatar\nand other attributes. (b) Retrieve: When the user initiates a conversation, RAP\nwill retrieve relevant information from the database using a multimodal\nretriever. (c) Generate: The input query and retrieved concepts' information\nare fed into MLLMs to generate personalized, knowledge-augmented responses.\nUnlike previous methods, RAP allows real-time concept editing via updating the\nexternal database. To further improve generation quality and alignment with\nuser-specific information, we design a pipeline for data collection and create\na specialized dataset for personalized training of MLLMs. Based on the dataset,\nwe train a series of MLLMs as personalized multimodal assistants. By\npretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual\nconcepts without additional finetuning. Our models demonstrate outstanding\nflexibility and generation quality across a variety of tasks, such as\npersonalized image captioning, question answering and visual recognition. The\ncode, data and models are available at https://github.com/Hoar012/RAP-MLLM.\n","authors":["Haoran Hao","Jiaming Han","Changsheng Li","Yu-Feng Li","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2410.13360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11522v2","updated":"2024-10-17T08:18:14Z","published":"2024-10-15T11:48:31Z","title":"Leveraging LLM Embeddings for Cross Dataset Label Alignment and Zero\n  Shot Music Emotion Prediction","summary":"  In this work, we present a novel method for music emotion recognition that\nleverages Large Language Model (LLM) embeddings for label alignment across\nmultiple datasets and zero-shot prediction on novel categories. First, we\ncompute LLM embeddings for emotion labels and apply non-parametric clustering\nto group similar labels, across multiple datasets containing disjoint labels.\nWe use these cluster centers to map music features (MERT) to the LLM embedding\nspace. To further enhance the model, we introduce an alignment regularization\nthat enables dissociation of MERT embeddings from different clusters. This\nfurther enhances the model's ability to better adaptation to unseen datasets.\nWe demonstrate the effectiveness of our approach by performing zero-shot\ninference on a new dataset, showcasing its ability to generalize to unseen\nlabels without additional training.\n","authors":["Renhang Liu","Abhinaba Roy","Dorien Herremans"],"pdf_url":"https://arxiv.org/pdf/2410.11522v2.pdf","comment":null}]},"2024-10-18T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.14677v1","updated":"2024-10-18T17:59:57Z","published":"2024-10-18T17:59:57Z","title":"Are AI Detectors Good Enough? A Survey on Quality of Datasets With\n  Machine-Generated Texts","summary":"  The rapid development of autoregressive Large Language Models (LLMs) has\nsignificantly improved the quality of generated texts, necessitating reliable\nmachine-generated text detectors. A huge number of detectors and collections\nwith AI fragments have emerged, and several detection methods even showed\nrecognition quality up to 99.9% according to the target metrics in such\ncollections. However, the quality of such detectors tends to drop dramatically\nin the wild, posing a question: Are detectors actually highly trustworthy or do\ntheir high benchmark scores come from the poor quality of evaluation datasets?\nIn this paper, we emphasise the need for robust and qualitative methods for\nevaluating generated data to be secure against bias and low generalising\nability of future model. We present a systematic review of datasets from\ncompetitions dedicated to AI-generated content detection and propose methods\nfor evaluating the quality of datasets containing AI-generated fragments. In\naddition, we discuss the possibility of using high-quality generated data to\nachieve two goals: improving the training of detection models and improving the\ntraining datasets themselves. Our contribution aims to facilitate a better\nunderstanding of the dynamics between human and machine text, which will\nultimately support the integrity of information in an increasingly automated\nworld.\n","authors":["German Gritsai","Anastasia Voznyuk","Andrey Grabovoy","Yury Chekhovich"],"pdf_url":"https://arxiv.org/pdf/2410.14677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14676v1","updated":"2024-10-18T17:59:51Z","published":"2024-10-18T17:59:51Z","title":"SudoLM: Learning Access Control of Parametric Knowledge with\n  Authorization Alignment","summary":"  Existing preference alignment is a one-size-fits-all alignment mechanism,\nwhere the part of the large language model (LLM) parametric knowledge with\nnon-preferred features is uniformly blocked to all the users. However, this\npart of knowledge can be useful to advanced users whose expertise qualifies\nthem to handle these information. The one-size-fits-all alignment mechanism\nundermines LLM's utility for these qualified users. To address this problem, we\npropose SudoLM, a framework that lets LLMs learn access control over specific\nparametric knowledge for users with different credentials via authorization\nalignment. SudoLM allows authorized users to unlock their access to all the\nparametric knowledge with an assigned SUDO key while blocking access to\nnon-qualified users. Experiments on two application scenarios demonstrate that\nSudoLM effectively controls the user's access to the parametric knowledge and\nmaintains its general utility.\n","authors":["Qin Liu","Fei Wang","Chaowei Xiao","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14675v1","updated":"2024-10-18T17:59:47Z","published":"2024-10-18T17:59:47Z","title":"Enhancing Large Language Models' Situated Faithfulness to External\n  Contexts","summary":"  Large Language Models (LLMs) are often augmented with external information as\ncontexts, but this external information can sometimes be inaccurate or even\nintentionally misleading. We argue that robust LLMs should demonstrate situated\nfaithfulness, dynamically calibrating their trust in external information based\non their confidence in the internal knowledge and the external context. To\nbenchmark this capability, we evaluate LLMs across several QA datasets,\nincluding a newly created dataset called RedditQA featuring in-the-wild\nincorrect contexts sourced from Reddit posts. We show that when provided with\nboth correct and incorrect contexts, both open-source and proprietary models\ntend to overly rely on external information, regardless of its factual\naccuracy. To enhance situated faithfulness, we propose two approaches:\nSelf-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning\n(RCR). SCR enables models to self-access the confidence of external information\nrelative to their own internal knowledge to produce the most accurate answer.\nRCR, in contrast, extracts explicit confidence signals from the LLM and\ndetermines the final answer using predefined rules. Our results show that for\nLLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR\noutperforms RCR, achieving improvements of up to 24.2% over a direct input\naugmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR\noutperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct\nPreference Optimization (CR-DPO) method improves performance on both seen and\nunseen datasets, yielding an average improvement of 8.9% on Llama-3-8B. In\naddition to quantitative results, we offer insights into the relative strengths\nof SCR and RCR. Our findings highlight promising avenues for improving situated\nfaithfulness in LLMs. The data and code are released.\n","authors":["Yukun Huang","Sanxing Chen","Hongyi Cai","Bhuwan Dhingra"],"pdf_url":"https://arxiv.org/pdf/2410.14675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14669v1","updated":"2024-10-18T17:58:21Z","published":"2024-10-18T17:58:21Z","title":"NaturalBench: Evaluating Vision-Language Models on Natural Adversarial\n  Samples","summary":"  Vision-language models (VLMs) have made significant progress in recent\nvisual-question-answering (VQA) benchmarks that evaluate complex\nvisio-linguistic reasoning. However, are these models truly effective? In this\nwork, we show that VLMs still struggle with natural images and questions that\nhumans can easily answer, which we term natural adversarial samples. We also\nfind it surprisingly easy to generate these VQA samples from natural image-text\ncorpora using off-the-shelf models like CLIP and ChatGPT. We propose a\nsemi-automated approach to collect a new benchmark, NaturalBench, for reliably\nevaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a\n$\\textbf{vision-centric}$ design by pairing each question with two images that\nyield different answers, preventing blind solutions from answering without\nusing the images. This makes NaturalBench more challenging than previous\nbenchmarks that can be solved with commonsense priors. We evaluate 53\nstate-of-the-art VLMs on NaturalBench, showing that models like\nLLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o\nlag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is\nhard from two angles: (1) Compositionality: Solving NaturalBench requires\ndiverse visio-linguistic skills, including understanding attribute bindings,\nobject relationships, and advanced reasoning like logic and counting. To this\nend, unlike prior work that uses a single tag per sample, we tag each\nNaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2)\nBiases: NaturalBench exposes severe biases in VLMs, as models often choose the\nsame answer regardless of the image. Lastly, we apply our benchmark curation\nmethod to diverse data sources, including long captions (over 100 words) and\nnon-English languages like Chinese and Hindi, highlighting its potential for\ndynamic evaluations of VLMs.\n","authors":["Baiqi Li","Zhiqiu Lin","Wenxuan Peng","Jean de Dieu Nyandwi","Daniel Jiang","Zixian Ma","Simran Khanuja","Ranjay Krishna","Graham Neubig","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2410.14669v1.pdf","comment":"Accepted to NeurIPS 24; We open-source our dataset at:\n  https://huggingface.co/datasets/BaiqiL/NaturalBench; Project page at:\n  https://linzhiqiu.github.io/papers/naturalbench/"},{"id":"http://arxiv.org/abs/2410.14668v1","updated":"2024-10-18T17:57:40Z","published":"2024-10-18T17:57:40Z","title":"MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image\n  Description and Reasoning Steps","summary":"  Multimodal Chain of Thought (MCoT) is a popular prompting strategy for\nimproving the performance of multimodal large language models (MLLMs) across a\nrange of complex reasoning tasks. Despite its popularity, there is a notable\nabsence of automated methods for evaluating the quality of reasoning steps in\nMCoT. To address this gap, we propose Multimodal Chain-of-Thought Evaluation\n(MiCEval), a framework designed to assess the correctness of reasoning chains\nby evaluating the quality of both the description and each reasoning step. The\nevaluation of the description component focuses on the accuracy of the image\ndescriptions, while the reasoning step evaluates the quality of each step as it\nis conditionally generated based on the preceding steps. MiCEval is built upon\na fine-grained dataset with annotations that rate each step according to\ncorrectness, relevance, and informativeness. Extensive experiments on four\nstate-of-the-art MLLMs show that step-wise evaluations using MiCEval align more\nclosely with human judgments compared to existing methods based on cosine\nsimilarity or fine-tuning approaches. MiCEval datasets and code can be found in\nhttps://github.com/alenai97/MiCEval.\n","authors":["Xiongtao Zhou","Jie He","Lanyu Chen","jingyu li","Haojing Chen","Victor Gutierrez Basulto","Jeff Z. Pan","Hanjie Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14668v1.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2410.14666v1","updated":"2024-10-18T17:56:11Z","published":"2024-10-18T17:56:11Z","title":"DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie\n  Character-Aware Discourse Graph","summary":"  Summarizing movie screenplays presents a unique set of challenges compared to\nstandard document summarization. Screenplays are not only lengthy, but also\nfeature a complex interplay of characters, dialogues, and scenes, with numerous\ndirect and subtle relationships and contextual nuances that are difficult for\nmachine learning models to accurately capture and comprehend. Recent attempts\nat screenplay summarization focus on fine-tuning transformer-based pre-trained\nmodels, but these models often fall short in capturing long-term dependencies\nand latent relationships, and frequently encounter the \"lost in the middle\"\nissue. To address these challenges, we introduce DiscoGraMS, a novel resource\nthat represents movie scripts as a movie character-aware discourse graph (CaD\nGraph). This approach is well-suited for various downstream tasks, such as\nsummarization, question-answering, and salience detection. The model aims to\npreserve all salient information, offering a more comprehensive and faithful\nrepresentation of the screenplay's content. We further explore a baseline\nmethod that combines the CaD Graph with the corresponding movie script through\na late fusion of graph and text modalities, and we present very initial\npromising results.\n","authors":["Maitreya Prafulla Chitale","Uday Bindal","Rajakrishnan Rajkumar","Rahul Mishra"],"pdf_url":"https://arxiv.org/pdf/2410.14666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06331v2","updated":"2024-10-18T17:53:46Z","published":"2024-10-08T20:12:11Z","title":"Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing","summary":"  The locate-then-edit paradigm has shown significant promise for knowledge\nediting (KE) in Large Language Models (LLMs). While previous methods perform\nwell on single-hop fact recall tasks, they consistently struggle with multi-hop\nfactual recall tasks involving newly edited knowledge. In this paper,\nleveraging tools in mechanistic interpretability, we first identify that in\nmulti-hop tasks, LLMs tend to retrieve implicit subject knowledge from deeper\nMLP layers, unlike single-hop tasks, which rely on earlier layers. This\ndistinction explains the poor performance of current methods in multi-hop\nqueries, as they primarily focus on editing shallow layers, leaving deeper\nlayers unchanged. To address this, we propose IFMET, a novel locate-then-edit\nKE approach designed to edit both shallow and deep MLP layers. IFMET employs\nmulti-hop editing prompts and supplementary sets to locate and modify knowledge\nacross different reasoning stages. Experimental results demonstrate that IFMET\nsignificantly improves performance on multi-hop factual recall tasks,\neffectively overcoming the limitations of previous locate-then-edit methods.\n","authors":["Zhuoran Zhang","Yongxiang Li","Zijian Kan","Keyuan Cheng","Lijie Hu","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06331v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2410.14651v1","updated":"2024-10-18T17:47:11Z","published":"2024-10-18T17:47:11Z","title":"Real-time Fake News from Adversarial Feedback","summary":"  We show that existing evaluations for fake news detection based on\nconventional sources, such as claims on fact-checking websites, result in an\nincreasing accuracy over time for LLM-based detectors -- even after their\nknowledge cutoffs. This suggests that recent popular political claims, which\nform the majority of fake news on such sources, are easily classified using\nsurface-level shallow patterns. Instead, we argue that a proper fake news\ndetection dataset should test a model's ability to reason factually about the\ncurrent world by retrieving and reading related evidence. To this end, we\ndevelop a novel pipeline that leverages natural language feedback from a\nRAG-based detector to iteratively modify real-time news into deceptive fake\nnews that challenges LLMs. Our iterative rewrite decreases the binary\nclassification AUC by an absolute 17.5 percent for a strong RAG GPT-4o\ndetector. Our experiments reveal the important role of RAG in both detecting\nand generating fake news, as retrieval-free LLM detectors are vulnerable to\nunseen events and adversarial attacks, while feedback from RAG detection helps\ndiscover more deceitful patterns in fake news.\n","authors":["Sanxing Chen","Yukun Huang","Bhuwan Dhingra"],"pdf_url":"https://arxiv.org/pdf/2410.14651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14641v1","updated":"2024-10-18T17:41:19Z","published":"2024-10-18T17:41:19Z","title":"Distance between Relevant Information Pieces Causes Bias in Long-Context\n  LLMs","summary":"  Positional bias in large language models (LLMs) hinders their ability to\neffectively process long inputs. A prominent example is the \"lost in the\nmiddle\" phenomenon, where LLMs struggle to utilize relevant information\nsituated in the middle of the input. While prior research primarily focuses on\nsingle pieces of relevant information, real-world applications often involve\nmultiple relevant information pieces. To bridge this gap, we present\nLongPiBench, a benchmark designed to assess positional bias involving multiple\npieces of relevant information. Thorough experiments are conducted with five\ncommercial and six open-source models. These experiments reveal that while most\ncurrent models are robust against the \"lost in the middle\" issue, there exist\nsignificant biases related to the spacing of relevant information pieces. These\nfindings highlight the importance of evaluating and reducing positional biases\nto advance LLM's capabilities.\n","authors":["Runchu Tian","Yanghao Li","Yuepeng Fu","Siyang Deng","Qinyu Luo","Cheng Qian","Shuo Wang","Xin Cong","Zhong Zhang","Yesai Wu","Yankai Lin","Huadong Wang","Xiaojiang Liu"],"pdf_url":"https://arxiv.org/pdf/2410.14641v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2410.14635v1","updated":"2024-10-18T17:36:53Z","published":"2024-10-18T17:36:53Z","title":"GenEOL: Harnessing the Generative Power of LLMs for Training-Free\n  Sentence Embeddings","summary":"  Training-free embedding methods directly leverage pretrained large language\nmodels (LLMs) to embed text, bypassing the costly and complex procedure of\ncontrastive learning. Previous training-free embedding methods have mainly\nfocused on optimizing embedding prompts and have overlooked the benefits of\nutilizing the generative abilities of LLMs. We propose a novel method, GenEOL,\nwhich uses LLMs to generate diverse transformations of a sentence that preserve\nits meaning, and aggregates the resulting embeddings of these transformations\nto enhance the overall sentence embedding. GenEOL significantly outperforms the\nexisting training-free embedding methods by an average of 2.85 points across\nseveral LLMs on the sentence semantic text similarity (STS) benchmark. Our\nanalysis shows that GenEOL stabilizes representation quality across LLM layers\nand is robust to perturbations of embedding prompts. GenEOL also achieves\nnotable gains on multiple clustering, reranking and pair-classification tasks\nfrom the MTEB benchmark.\n","authors":["Raghuveer Thirukovalluru","Bhuwan Dhingra"],"pdf_url":"https://arxiv.org/pdf/2410.14635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14632v1","updated":"2024-10-18T17:32:22Z","published":"2024-10-18T17:32:22Z","title":"Diverging Preferences: When do Annotators Disagree and do Models Know?","summary":"  We examine diverging preferences in human-labeled preference datasets. We\ndevelop a taxonomy of disagreement sources spanning 10 categories across four\nhigh-level classes -- task underspecification, response style, refusals, and\nannotation errors. We find that the majority of disagreements are in opposition\nwith standard reward modeling approaches, which are designed with the\nassumption that annotator disagreement is noise. We then explore how these\nfindings impact two areas of LLM development: reward modeling and evaluation.\nIn our experiments, we demonstrate how standard reward modeling methods, like\nthe Bradley-Terry model, fail to differentiate whether a given preference\njudgment is the result of unanimous agreement among annotators or the majority\nopinion among diverging user preferences. We also find that these tendencies\nare also echoed by popular LLM-as-Judge evaluation methods, which consistently\nidentify a winning response in cases of diverging preferences. These findings\nhighlight remaining challenges in LLM evaluations, which are greatly influenced\nby divisive features like response style, and in developing pluralistically\naligned LLMs. To address these issues, we develop methods for identifying\ndiverging preferences to mitigate their influence on evaluation and training.\n","authors":["Michael JQ Zhang","Zhilin Wang","Jena D. Hwang","Yi Dong","Olivier Delalleau","Yejin Choi","Eunsol Choi","Xiang Ren","Valentina Pyatkin"],"pdf_url":"https://arxiv.org/pdf/2410.14632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07114v2","updated":"2024-10-18T17:30:04Z","published":"2024-09-19T19:48:31Z","title":"System 2 thinking in OpenAI's o1-preview model: Near-perfect performance\n  on a mathematics exam","summary":"  The processes underlying human cognition are often divided into System 1,\nwhich involves fast, intuitive thinking, and System 2, which involves slow,\ndeliberate reasoning. Previously, large language models were criticized for\nlacking the deeper, more analytical capabilities of System 2. In September\n2024, OpenAI introduced the o1 model series, designed to handle System 2-like\nreasoning. While OpenAI's benchmarks are promising, independent validation is\nstill needed. In this study, we tested the o1-preview model twice on the Dutch\n'Mathematics B' final exam. It scored a near-perfect 76 and 74 out of 76\npoints. For context, only 24 out of 16,414 students in the Netherlands achieved\na perfect score. By comparison, the GPT-4o model scored 66 and 62 out of 76,\nwell above the Dutch average of 40.63 points. Neither model had access to the\nexam figures. Since there was a risk of model contamination (i.e., the\nknowledge cutoff of o1-preview and GPT-4o was after the exam was published\nonline), we repeated the procedure with a new Mathematics B exam that was\npublished after the cutoff date. The results again indicated that o1-preview\nperformed strongly (97.8th percentile), which suggests that contamination was\nnot a factor. We also show that there is some variability in the output of\no1-preview, which means that sometimes there is 'luck' (the answer is correct)\nor 'bad luck' (the output has diverged into something that is incorrect). We\ndemonstrate that a self-consistency approach, where repeated prompts are given\nand the most common answer is selected, is a useful strategy for identifying\nthe correct answer. It is concluded that while OpenAI's new model series holds\ngreat potential, certain risks must be considered.\n","authors":["Joost de Winter","Dimitra Dodou","Yke Bauke Eisma"],"pdf_url":"https://arxiv.org/pdf/2410.07114v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14627v1","updated":"2024-10-18T17:29:56Z","published":"2024-10-18T17:29:56Z","title":"CELI: Controller-Embedded Language Model Interactions","summary":"  We introduce Controller-Embedded Language Model Interactions (CELI), a\nframework that integrates control logic directly within language model (LM)\nprompts, facilitating complex, multi-stage task execution. CELI addresses\nlimitations of existing prompt engineering and workflow optimization techniques\nby embedding control logic directly within the operational context of language\nmodels, enabling dynamic adaptation to evolving task requirements. Our\nframework transfers control from the traditional programming execution\nenvironment to the LMs, allowing them to autonomously manage computational\nworkflows while maintaining seamless interaction with external systems and\nfunctions. CELI supports arbitrary function calls with variable arguments,\nbridging the gap between LMs' adaptive reasoning capabilities and conventional\nsoftware paradigms' structured control mechanisms. To evaluate CELI's\nversatility and effectiveness, we conducted case studies in two distinct\ndomains: code generation (HumanEval benchmark) and multi-stage content\ngeneration (Wikipedia-style articles). The results demonstrate notable\nperformance improvements across a range of domains. CELI achieved a 4.9\npercentage point improvement over the best reported score of the baseline GPT-4\nmodel on the HumanEval code generation benchmark. In multi-stage content\ngeneration, 94.4% of CELI-produced Wikipedia-style articles met or exceeded\nfirst draft quality when optimally configured, with 44.4% achieving high\nquality. These outcomes underscore CELI's potential for optimizing AI-driven\nworkflows across diverse computational domains.\n","authors":["Jan-Samuel Wagner","Dave DeCaprio","Abishek Chiffon Muthu Raja","Jonathan M. Holman","Lauren K. Brady","Sky C. Cheung","Hosein Barzekar","Eric Yang","Mark Anthony Martinez II","David Soong","Sriram Sridhar","Han Si","Brandon W. Higgs","Hisham Hamadeh","Scott Ogden"],"pdf_url":"https://arxiv.org/pdf/2410.14627v1.pdf","comment":"26 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.14626v1","updated":"2024-10-18T17:27:38Z","published":"2024-10-18T17:27:38Z","title":"You Shall Know a Tool by the Traces it Leaves: The Predictability of\n  Sentiment Analysis Tools","summary":"  If sentiment analysis tools were valid classifiers, one would expect them to\nprovide comparable results for sentiment classification on different kinds of\ncorpora and for different languages. In line with results of previous studies\nwe show that sentiment analysis tools disagree on the same dataset. Going\nbeyond previous studies we show that the sentiment tool used for sentiment\nannotation can even be predicted from its outcome, revealing an algorithmic\nbias of sentiment analysis. Based on Twitter, Wikipedia and different news\ncorpora from the English, German and French languages, our classifiers separate\nsentiment tools with an averaged F1-score of 0.89 (for the English corpora). We\ntherefore warn against taking sentiment annotations as face value and argue for\nthe need of more and systematic NLP evaluation studies.\n","authors":["Daniel Baumartz","Mevlüt Bagci","Alexander Henlein","Maxim Konca","Andy Lücking","Alexander Mehler"],"pdf_url":"https://arxiv.org/pdf/2410.14626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10989v2","updated":"2024-10-18T17:21:17Z","published":"2024-10-14T18:17:01Z","title":"Liger Kernel: Efficient Triton Kernels for LLM Training","summary":"  Training Large Language Models (LLMs) efficiently at scale presents a\nformidable challenge, driven by their ever-increasing computational demands and\nthe need for enhanced performance. In this work, we introduce Liger-Kernel, an\nopen-sourced set of Triton kernels developed specifically for LLM training.\nWith kernel optimization techniques like kernel operation fusing and input\nchunking, our kernels achieve on average a 20% increase in training throughput\nand a 60% reduction in GPU memory usage for popular LLMs compared to\nHuggingFace implementations. In addition, Liger-Kernel is designed with\nmodularity, accessibility, and adaptability in mind, catering to both casual\nand expert users. Comprehensive benchmarks and integration tests are built in\nto ensure compatibility, performance, correctness, and convergence across\ndiverse computing environments and model architectures.\n  The source code is available under a permissive license at:\ngithub.com/linkedin/Liger-Kernel.\n","authors":["Pin-Lun Hsu","Yun Dai","Vignesh Kothapalli","Qingquan Song","Shao Tang","Siyu Zhu","Steven Shimizu","Shivam Sahni","Haowen Ning","Yanning Chen"],"pdf_url":"https://arxiv.org/pdf/2410.10989v2.pdf","comment":"17 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.02525v3","updated":"2024-10-18T17:18:24Z","published":"2024-10-03T14:33:34Z","title":"Contextual Document Embeddings","summary":"  Dense document embeddings are central to neural retrieval. The dominant\nparadigm is to train and construct embeddings by running encoders directly on\nindividual documents. In this work, we argue that these embeddings, while\neffective, are implicitly out-of-context for targeted use cases of retrieval,\nand that a contextualized document embedding should take into account both the\ndocument and neighboring documents in context - analogous to contextualized\nword embeddings. We propose two complementary methods for contextualized\ndocument embeddings: first, an alternative contrastive learning objective that\nexplicitly incorporates the document neighbors into the intra-batch contextual\nloss; second, a new contextual architecture that explicitly encodes neighbor\ndocument information into the encoded representation. Results show that both\nmethods achieve better performance than biencoders in several settings, with\ndifferences especially pronounced out-of-domain. We achieve state-of-the-art\nresults on the MTEB benchmark with no hard negative mining, score distillation,\ndataset-specific instructions, intra-GPU example-sharing, or extremely large\nbatch sizes. Our method can be applied to improve performance on any\ncontrastive learning dataset and any biencoder.\n","authors":["John X. Morris","Alexander M. Rush"],"pdf_url":"https://arxiv.org/pdf/2410.02525v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10101v2","updated":"2024-10-18T17:15:09Z","published":"2024-10-14T02:41:01Z","title":"Learning Linear Attention in Polynomial Time","summary":"  Previous research has explored the computational expressivity of Transformer\nmodels in simulating Boolean circuits or Turing machines. However, the\nlearnability of these simulators from observational data has remained an open\nquestion. Our study addresses this gap by providing the first polynomial-time\nlearnability results (specifically strong, agnostic PAC learning) for\nsingle-layer Transformers with linear attention. We show that linear attention\nmay be viewed as a linear predictor in a suitably defined RKHS. As a\nconsequence, the problem of learning any linear transformer may be converted\ninto the problem of learning an ordinary linear predictor in an expanded\nfeature space, and any such predictor may be converted back into a multiheaded\nlinear transformer. Moving to generalization, we show how to efficiently\nidentify training datasets for which every empirical risk minimizer is\nequivalent (up to trivial symmetries) to the linear Transformer that generated\nthe data, thereby guaranteeing the learned model will correctly generalize\nacross all inputs. Finally, we provide examples of computations expressible via\nlinear attention and therefore polynomial-time learnable, including associative\nmemories, finite automata, and a class of Universal Turing Machine (UTMs) with\npolynomially bounded computation histories. We empirically validate our\ntheoretical findings on three tasks: learning random linear attention networks,\nkey--value associations, and learning to execute finite automata. Our findings\nbridge a critical gap between theoretical expressivity and learnability of\nTransformers, and show that flexible and general models of computation are\nefficiently learnable.\n","authors":["Morris Yau","Ekin Akyürek","Jiayuan Mao","Joshua B. Tenenbaum","Stefanie Jegelka","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2410.10101v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06402v2","updated":"2024-10-18T17:10:05Z","published":"2024-03-11T03:28:13Z","title":"One size doesn't fit all: Predicting the Number of Examples for\n  In-Context Learning","summary":"  In-context learning (ICL) refers to the process of adding a small number of\nlocalized examples (ones that are semantically similar to the input) from a\ntraining set of labelled data to an LLM's prompt with an objective to\neffectively control the generative process seeking to improve the downstream\ntask performance. Existing ICL approaches use an identical number of examples\n(a pre-configured hyper-parameter) for each data instance. Our work alleviates\nthe limitations of this 'one fits all' approach by dynamically predicting the\nnumber of examples for each data instance to be used in few-shot inference with\nLLMs. In particular, we employ a multi-label classifier, the parameters of\nwhich are fitted using a training set, where the label for each instance in the\ntraining set indicates if using a specific value of k (number of most similar\nexamples from 0 up to a maximum value) leads to correct k-shot downstream\npredictions. Our experiments on a number of text classification benchmarks show\nthat AICL substantially outperforms standard ICL by up to 17%.\n","authors":["Manish Chandra","Debasis Ganguly","Iadh Ounis"],"pdf_url":"https://arxiv.org/pdf/2403.06402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14609v1","updated":"2024-10-18T17:03:17Z","published":"2024-10-18T17:03:17Z","title":"DiSCo Meets LLMs: A Unified Approach for Sparse Retrieval and Contextual\n  Distillation in Conversational Search","summary":"  Conversational Search (CS) is the task of retrieving relevant documents from\na corpus within a conversational context, combining retrieval with\nconversational context modeling. With the explosion of Large Language Models\n(LLMs), the CS field has seen major improvements with LLMs rewriting user\nqueries, accounting for conversational context. However, engaging LLMs at\ninference time harms efficiency. Current methods address this by distilling\nembeddings from human-rewritten queries to learn the context modeling task.\nYet, these approaches predominantly focus on context modeling, and only treat\nthe contrastive component of the retrieval task within a\ndistillation-independent loss term. To address these limitations, we propose a\nnew distillation method, as a relaxation of the previous objective, unifying\nretrieval and context modeling. We relax the existing training objectives by\ndistilling similarity scores between conversations and documents, rather than\nrelying solely on representation learning. Our proposed distillation objective\nallows for more freedom in the representation space and leverages the\ncontrastive nature of document relevance. Through experiments on Learned Sparse\nRetrieval (LSR) across 5 CS datasets, our approach demonstrates substantial\nimprovements in both in-domain and out-of-domain retrieval performance,\noutperforming state-of-the-art with gains of up to 6 points in recall for\nout-of-domain datasets. Additionally, through the relaxation of the objective,\nwe propose a multi-teacher distillation, using multiple LLMs as teachers,\nyielding additional gains, and outperforming the teachers themselves in\nin-domain experiments. Finally, analysis of the sparsity of the models reveals\nthat our distillation allows for better control over the sparsity of the\ntrained models.\n","authors":["Simon Lupart","Mohammad Aliannejadi","Evangelos Kanoulas"],"pdf_url":"https://arxiv.org/pdf/2410.14609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14596v1","updated":"2024-10-18T16:49:36Z","published":"2024-10-18T16:49:36Z","title":"Teaching Models to Balance Resisting and Accepting Persuasion","summary":"  Large language models (LLMs) are susceptible to persuasion, which can pose\nrisks when models are faced with an adversarial interlocutor. We take a first\nstep towards defending models against persuasion while also arguing that\ndefense against adversarial (i.e. negative) persuasion is only half of the\nequation: models should also be able to accept beneficial (i.e. positive)\npersuasion to improve their answers. We show that optimizing models for only\none side results in poor performance on the other. In order to balance positive\nand negative persuasion, we introduce Persuasion-Balanced Training (or PBT),\nwhich leverages multi-agent recursive dialogue trees to create data and trains\nmodels via preference optimization to accept persuasion when appropriate. PBT\nconsistently improves resistance to misinformation and resilience to being\nchallenged while also resulting in the best overall performance on holistic\ndata containing both positive and negative persuasion. Crucially, we show that\nPBT models are better teammates in multi-agent debates. We find that without\nPBT, pairs of stronger and weaker models have unstable performance, with the\norder in which the models present their answers determining whether the team\nobtains the stronger or weaker model's performance. PBT leads to better and\nmore stable results and less order dependence, with the stronger model\nconsistently pulling the weaker one up.\n","authors":["Elias Stengel-Eskin","Peter Hase","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2410.14596v1.pdf","comment":"Code: https://github.com/esteng/persuasion_balanced_training"},{"id":"http://arxiv.org/abs/2410.14594v1","updated":"2024-10-18T16:44:22Z","published":"2024-10-18T16:44:22Z","title":"Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and\n  Tool Knowledge Bases","summary":"  Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks\nlike secure database interactions and multi-agent code development. However,\nscaling tool capacity beyond agent reasoning or model limits remains a\nchallenge. In this paper, we address these challenges by introducing Toolshed\nKnowledge Bases, a tool knowledge base (vector database) designed to store\nenhanced tool representations and optimize tool selection for large-scale\ntool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a\nnovel ensemble of tool-applied advanced retrieval-augmented generation (RAG)\ntechniques across the pre-retrieval, intra-retrieval, and post-retrieval\nphases, without requiring model fine-tuning. During pre-retrieval, tool\ndocuments are enhanced with key information and stored in the Toolshed\nKnowledge Base. Intra-retrieval focuses on query planning and transformation to\nincrease retrieval accuracy. Post-retrieval refines the retrieved tool\ndocuments and enables self-reflection. Furthermore, by varying both the total\nnumber of tools (tool-M) an Agent has access to and the tool selection\nthreshold (top-k), we address trade-offs between retrieval accuracy, agent\nperformance, and token cost. Our approach achieves 46%, 56%, and 47% absolute\nimprovements on the ToolE single-tool, ToolE multi-tool and Seal-Tools\nbenchmark datasets, respectively (Recall@5).\n","authors":["Elias Lumer"],"pdf_url":"https://arxiv.org/pdf/2410.14594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.13370v2","updated":"2024-10-18T16:44:05Z","published":"2024-04-20T13:15:27Z","title":"Movie101v2: Improved Movie Narration Benchmark","summary":"  Automatic movie narration aims to generate video-aligned plot descriptions to\nassist visually impaired audiences. Unlike standard video captioning, it\ninvolves not only describing key visual details but also inferring plots that\nunfold across multiple movie shots, presenting distinct and complex challenges.\nTo advance this field, we introduce Movie101v2, a large-scale, bilingual\ndataset with enhanced data quality specifically designed for movie narration.\nRevisiting the task, we propose breaking down the ultimate goal of automatic\nmovie narration into three progressive stages, offering a clear roadmap with\ncorresponding evaluation metrics. Based on our new benchmark, we baseline a\nrange of large vision-language models, including GPT-4V, and conduct an\nin-depth analysis of the challenges in narration generation. Our findings\nhighlight that achieving applicable movie narration generation is a fascinating\ngoal that requires significant research.\n","authors":["Zihao Yue","Yepeng Zhang","Ziheng Wang","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2404.13370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13191v2","updated":"2024-10-18T16:42:01Z","published":"2024-10-17T03:38:29Z","title":"MCQG-SRefine: Multiple Choice Question Generation and Evaluation with\n  Iterative Self-Critique, Correction, and Comparison Feedback","summary":"  Automatic question generation (QG) is essential for AI and NLP, particularly\nin intelligent tutoring, dialogue systems, and fact verification. Generating\nmultiple-choice questions (MCQG) for professional exams, like the United States\nMedical Licensing Examination (USMLE), is particularly challenging, requiring\ndomain expertise and complex multi-hop reasoning for high-quality questions.\nHowever, current large language models (LLMs) like GPT-4 struggle with\nprofessional MCQG due to outdated knowledge, hallucination issues, and prompt\nsensitivity, resulting in unsatisfactory quality and difficulty. To address\nthese challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique\nand Correction) framework for converting medical cases into high-quality\nUSMLE-style questions. By integrating expert-driven prompt engineering with\niterative self-critique and self-correction feedback, MCQG-SRefine\nsignificantly enhances human expert satisfaction regarding both the quality and\ndifficulty of the questions. Furthermore, we introduce an LLM-as-Judge-based\nautomatic metric to replace the complex and costly expert evaluation process,\nensuring reliable and expert-aligned assessments.\n","authors":["Zonghai Yao","Aditya Parashar","Huixue Zhou","Won Seok Jang","Feiyun Ouyang","Zhichao Yang","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.13191v2.pdf","comment":"Equal contribution for the first two authors"},{"id":"http://arxiv.org/abs/2410.14589v1","updated":"2024-10-18T16:39:42Z","published":"2024-10-18T16:39:42Z","title":"Dialetto, ma Quanto Dialetto? Transcribing and Evaluating Dialects on a\n  Continuum","summary":"  There is increasing interest in looking at dialects in NLP. However, most\nwork to date still treats dialects as discrete categories. For instance,\nevaluative work in variation-oriented NLP for English often works with Indian\nEnglish or African-American Venacular English as homogeneous categories (Faisal\net al., 2024; Ziems et al., 2023), yet even within one variety there is\nsubstantial variation. We examine within-dialect variation and show that\nperformance critically varies within categories. We measure speech-to-text\nperformance on Italian dialects, and empirically observe a geographical\nperformance disparity. This disparity correlates substantially (-0.5) with\nlinguistic similarity to the highest performing dialect variety. We\ncross-examine our results against dialectometry methods, and interpret the\nperformance disparity to be due to a bias towards dialects that are more\nsimilar to the standard variety in the speech-to-text model examined. We\nadditionally leverage geostatistical methods to predict zero-shot performance\nat unseen sites, and find the incorporation of geographical information to\nsubstantially improve prediction performance, indicating there to be\ngeographical structure in the performance distribution.\n","authors":["Ryan Soh-Eun Shim","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2410.14589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14582v1","updated":"2024-10-18T16:32:10Z","published":"2024-10-18T16:32:10Z","title":"Do LLMs estimate uncertainty well in instruction-following?","summary":"  Large language models (LLMs) could be valuable personal AI agents across\nvarious domains, provided they can precisely follow user instructions. However,\nrecent studies have shown significant limitations in LLMs'\ninstruction-following capabilities, raising concerns about their reliability in\nhigh-stakes applications. Accurately estimating LLMs' uncertainty in adhering\nto instructions is critical to mitigating deployment risks. We present, to our\nknowledge, the first systematic evaluation of the uncertainty estimation\nabilities of LLMs in the context of instruction-following. Our study identifies\nkey challenges with existing instruction-following benchmarks, where multiple\nfactors are entangled with uncertainty stems from instruction-following,\ncomplicating the isolation and comparison across methods and models. To address\nthese issues, we introduce a controlled evaluation setup with two benchmark\nversions of data, enabling a comprehensive comparison of uncertainty estimation\nmethods under various conditions. Our findings show that existing uncertainty\nmethods struggle, particularly when models make subtle errors in instruction\nfollowing. While internal model states provide some improvement, they remain\ninadequate in more complex scenarios. The insights from our controlled\nevaluation setups provide a crucial understanding of LLMs' limitations and\npotential for uncertainty estimation in instruction-following tasks, paving the\nway for more trustworthy AI agents.\n","authors":["Juyeon Heo","Miao Xiong","Christina Heinze-Deml","Jaya Narain"],"pdf_url":"https://arxiv.org/pdf/2410.14582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14581v1","updated":"2024-10-18T16:32:06Z","published":"2024-10-18T16:32:06Z","title":"Optimizing Attention with Mirror Descent: Generalized Max-Margin Token\n  Selection","summary":"  Attention mechanisms have revolutionized several domains of artificial\nintelligence, such as natural language processing and computer vision, by\nenabling models to selectively focus on relevant parts of the input data. While\nrecent work has characterized the optimization dynamics of gradient descent\n(GD) in attention-based models and the structural properties of its preferred\nsolutions, less is known about more general optimization algorithms such as\nmirror descent (MD). In this paper, we investigate the convergence properties\nand implicit biases of a family of MD algorithms tailored for softmax attention\nmechanisms, with the potential function chosen as the $p$-th power of the\n$\\ell_p$-norm. Specifically, we show that these algorithms converge in\ndirection to a generalized hard-margin SVM with an $\\ell_p$-norm objective when\napplied to a classification problem using a softmax attention model. Notably,\nour theoretical results reveal that the convergence rate is comparable to that\nof traditional GD in simpler models, despite the highly nonlinear and nonconvex\nnature of the present problem. Additionally, we delve into the joint\noptimization dynamics of the key-query matrix and the decoder, establishing\nconditions under which this complex joint optimization converges to their\nrespective hard-margin SVM solutions. Lastly, our numerical experiments on real\ndata demonstrate that MD algorithms improve generalization over standard GD and\nexcel in optimal token selection.\n","authors":["Aaron Alvarado Kristanto Julistiono","Davoud Ataee Tarzanagh","Navid Azizan"],"pdf_url":"https://arxiv.org/pdf/2410.14581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14578v1","updated":"2024-10-18T16:26:45Z","published":"2024-10-18T16:26:45Z","title":"Large Language Models Are Overparameterized Text Encoders","summary":"  Large language models (LLMs) demonstrate strong performance as text embedding\nmodels when finetuned with supervised contrastive training. However, their\nlarge size balloons inference time and memory requirements. In this paper, we\nshow that by pruning the last $p\\%$ layers of an LLM before supervised training\nfor only 1000 steps, we can achieve a proportional reduction in memory and\ninference time. We evaluate four different state-of-the-art LLMs on text\nembedding tasks and find that our method can prune up to 30\\% of layers with\nnegligible impact on performance and up to 80\\% with only a modest drop. With\nonly three lines of code, our method is easily implemented in any pipeline for\ntransforming LLMs to text encoders. We also propose $\\text{L}^3 \\text{Prune}$,\na novel layer-pruning strategy based on the model's initial loss that provides\ntwo optimal pruning configurations: a large variant with negligible performance\nloss and a small variant for resource-constrained settings. On average, the\nlarge variant prunes 21\\% of the parameters with a $-0.3$ performance drop, and\nthe small variant only suffers from a $-5.1$ decrease while pruning 74\\% of the\nmodel. We consider these results strong evidence that LLMs are\noverparameterized for text embedding tasks, and can be easily pruned.\n","authors":["Thennal D K","Tim Fischer","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2410.14578v1.pdf","comment":"8 pages of content + 1 for limitations and ethical considerations, 14\n  pages in total including references and appendix, 5+1 figures"},{"id":"http://arxiv.org/abs/2410.14574v1","updated":"2024-10-18T16:20:22Z","published":"2024-10-18T16:20:22Z","title":"MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts","summary":"  Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled\nscalability in deep learning. SMoE has the potential to exponentially increase\nparameter count while maintaining the efficiency of the model by only\nactivating a small subset of these parameters for a given sample. However, it\nhas been observed that SMoE suffers from unstable training and has difficulty\nadapting to new distributions, leading to the model's lack of robustness to\ndata contamination. To overcome these limitations, we first establish a\nconnection between the dynamics of the expert representations in SMoEs and\ngradient descent on a multi-objective optimization problem. Leveraging our\nframework, we then integrate momentum into SMoE and propose a new family of\nSMoEs named MomentumSMoE. We theoretically prove and numerically demonstrate\nthat MomentumSMoE is more stable and robust than SMoE. In particular, we verify\nthe advantages of MomentumSMoE over SMoE on a variety of practical tasks\nincluding ImageNet-1K object recognition and WikiText-103 language modeling. We\ndemonstrate the applicability of MomentumSMoE to many types of SMoE models,\nincluding those in the Sparse MoE model for vision (V-MoE) and the Generalist\nLanguage Model (GLaM). We also show that other advanced momentum-based\noptimization methods, such as Adam, can be easily incorporated into the\nMomentumSMoE framework for designing new SMoE models with even better\nperformance, almost negligible additional computation cost, and simple\nimplementations.\n","authors":["Rachel S. Y. Teo","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.14574v1.pdf","comment":"10 pages in the main text. Published at NeurIPS 2024. The code is\n  available at https://github.com/rachtsy/MomentumSMoE"},{"id":"http://arxiv.org/abs/2410.14567v1","updated":"2024-10-18T16:11:29Z","published":"2024-10-18T16:11:29Z","title":"RAG-ConfusionQA: A Benchmark for Evaluating LLMs on Confusing Questions","summary":"  Conversational AI agents use Retrieval Augmented Generation (RAG) to provide\nverifiable document-grounded responses to user inquiries. However, many natural\nquestions do not have good answers: about 25\\% contain false\nassumptions~\\cite{Yu2023:CREPE}, and over 50\\% are\nambiguous~\\cite{Min2020:AmbigQA}. RAG agents need high-quality data to improve\ntheir responses to confusing questions. This paper presents a novel synthetic\ndata generation method to efficiently create a diverse set of context-grounded\nconfusing questions from a given document corpus. We conduct an empirical\ncomparative evaluation of several large language models as RAG agents to\nmeasure the accuracy of confusion detection and appropriate response\ngeneration. We contribute a benchmark dataset to the public domain.\n","authors":["Zhiyuan Peng","Jinming Nian","Alexandre Evfimievski","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2410.14567v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2410.07400v2","updated":"2024-10-18T15:54:56Z","published":"2024-10-09T19:57:07Z","title":"Advocating Character Error Rate for Multilingual ASR Evaluation","summary":"  Automatic speech recognition (ASR) systems have traditionally been evaluated\nusing English datasets, with the word error rate (WER) serving as the\npredominant metric. WER's simplicity and ease of interpretation have\ncontributed to its widespread adoption, particularly for English. However, as\nASR systems expand to multilingual contexts, WER fails in various ways,\nparticularly with morphologically complex languages or those without clear word\nboundaries. Our work documents the limitations of WER as an evaluation metric\nand advocates for the character error rate (CER) as the primary metric in\nmultilingual ASR evaluation. We show that CER avoids many of the challenges WER\nfaces and exhibits greater consistency across writing systems. We support our\nproposition by conducting human evaluations of ASR transcriptions in three\nlanguages: Malayalam, English, and Arabic, which exhibit distinct morphological\ncharacteristics. We show that CER correlates more closely with human judgments\nthan WER, even for English. To facilitate further research, we release our\nhuman evaluation dataset for future benchmarking of ASR metrics. Our findings\nsuggest that CER should be prioritized, or at least supplemented, in\nmultilingual ASR evaluations to account for the varying linguistic\ncharacteristics of different languages.\n","authors":["Thennal D K","Jesin James","Deepa P Gopinath","Muhammed Ashraf K"],"pdf_url":"https://arxiv.org/pdf/2410.07400v2.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2409.15652v3","updated":"2024-10-18T15:45:39Z","published":"2024-09-24T01:29:24Z","title":"English offensive text detection using CNN based Bi-GRU model","summary":"  Over the years, the number of users of social media has increased\ndrastically. People frequently share their thoughts through social platforms,\nand this leads to an increase in hate content. In this virtual community,\nindividuals share their views, express their feelings, and post photos, videos,\nblogs, and more. Social networking sites like Facebook and Twitter provide\nplatforms to share vast amounts of content with a single click. However, these\nplatforms do not impose restrictions on the uploaded content, which may include\nabusive language and explicit images unsuitable for social media. To resolve\nthis issue, a new idea must be implemented to divide the inappropriate content.\nNumerous studies have been done to automate the process. In this paper, we\npropose a new Bi-GRU-CNN model to classify whether the text is offensive or\nnot. The combination of the Bi-GRU and CNN models outperforms the existing\nmodel.\n","authors":["Tonmoy Roy","Md Robiul Islam","Asif Ahammad Miazee","Anika Antara","Al Amin","Sunjim Hossain"],"pdf_url":"https://arxiv.org/pdf/2409.15652v3.pdf","comment":"5 pages and 6 figures"},{"id":"http://arxiv.org/abs/2405.20850v2","updated":"2024-10-18T15:43:02Z","published":"2024-05-31T14:33:07Z","title":"Improving Reward Models with Synthetic Critiques","summary":"  Reward models (RMs) play a critical role in aligning language models through\nthe process of reinforcement learning from human feedback. RMs are trained to\npredict a score reflecting human preference, which requires significant time\nand cost for human annotation. Additionally, RMs tend to quickly overfit on\nsuperficial features in the training set, hindering their generalization\nperformance on unseen distributions. We propose a novel approach using\nsynthetic natural language critiques generated by large language models to\nprovide additional feedback, evaluating aspects such as instruction following,\ncorrectness, and style. This offers richer signals and more robust features for\nRMs to assess and score on. We demonstrate that high-quality critiques improve\nthe performance and data efficiency of RMs initialized from different\npretrained models, reducing the reliance on costly human annotations.\nFurthermore, incorporating critiques improves both the interpretability and\nrobustness of RM training.\n","authors":["Zihuiwen Ye","Fraser Greenlee-Scott","Max Bartolo","Phil Blunsom","Jon Ander Campos","Matthias Gallé"],"pdf_url":"https://arxiv.org/pdf/2405.20850v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14917v2","updated":"2024-10-18T15:42:06Z","published":"2024-09-23T11:13:25Z","title":"With Ears to See and Eyes to Hear: Sound Symbolism Experiments with\n  Multimodal Large Language Models","summary":"  Recently, Large Language Models (LLMs) and Vision Language Models (VLMs) have\ndemonstrated aptitude as potential substitutes for human participants in\nexperiments testing psycholinguistic phenomena. However, an understudied\nquestion is to what extent models that only have access to vision and text\nmodalities are able to implicitly understand sound-based phenomena via abstract\nreasoning from orthography and imagery alone. To investigate this, we analyse\nthe ability of VLMs and LLMs to demonstrate sound symbolism (i.e., to recognise\na non-arbitrary link between sounds and concepts) as well as their ability to\n\"hear\" via the interplay of the language and vision modules of open and\nclosed-source multimodal models. We perform multiple experiments, including\nreplicating the classic Kiki-Bouba and Mil-Mal shape and magnitude symbolism\ntasks, and comparing human judgements of linguistic iconicity with that of\nLLMs. Our results show that VLMs demonstrate varying levels of agreement with\nhuman labels, and more task information may be required for VLMs versus their\nhuman counterparts for in silico experimentation. We additionally see through\nhigher maximum agreement levels that Magnitude Symbolism is an easier pattern\nfor VLMs to identify than Shape Symbolism, and that an understanding of\nlinguistic iconicity is highly dependent on model size.\n","authors":["Tyler Loakman","Yucheng Li","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2409.14917v2.pdf","comment":"Accepted to EMNLP 2024 (Camera Ready)"},{"id":"http://arxiv.org/abs/2410.14545v1","updated":"2024-10-18T15:40:48Z","published":"2024-10-18T15:40:48Z","title":"Tell me what I need to know: Exploring LLM-based (Personalized)\n  Abstractive Multi-Source Meeting Summarization","summary":"  Meeting summarization is crucial in digital communication, but existing\nsolutions struggle with salience identification to generate personalized,\nworkable summaries, and context understanding to fully comprehend the meetings'\ncontent. Previous attempts to address these issues by considering related\nsupplementary resources (e.g., presentation slides) alongside transcripts are\nhindered by models' limited context sizes and handling the additional\ncomplexities of the multi-source tasks, such as identifying relevant\ninformation in additional files and seamlessly aligning it with the meeting\ncontent. This work explores multi-source meeting summarization considering\nsupplementary materials through a three-stage large language model approach:\nidentifying transcript passages needing additional context, inferring relevant\ndetails from supplementary materials and inserting them into the transcript,\nand generating a summary from this enriched transcript. Our multi-source\napproach enhances model understanding, increasing summary relevance by ~9% and\nproducing more content-rich outputs. We introduce a personalization protocol\nthat extracts participant characteristics and tailors summaries accordingly,\nimproving informativeness by ~10%. This work further provides insights on\nperformance-cost trade-offs across four leading model families, including\nedge-device capable options. Our approach can be extended to similar complex\ngenerative tasks benefitting from additional resources and personalization,\nsuch as dialogue systems and action planning.\n","authors":["Frederic Kirstein","Terry Ruas","Robert Kratel","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2410.14545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02067v2","updated":"2024-10-18T15:39:15Z","published":"2024-07-02T08:55:41Z","title":"Crossroads of Continents: Automated Artifact Extraction for Cultural\n  Adaptation with Large Multimodal Models","summary":"  We present a comprehensive three-phase study to examine (1) the cultural\nunderstanding of Large Multimodal Models (LMMs) by introducing DalleStreet, a\nlarge-scale dataset generated by DALL-E 3 and validated by humans, containing\n9,935 images of 67 countries and 10 concept classes; (2) the underlying\nimplicit and potentially stereotypical cultural associations with a cultural\nartifact extraction task; and (3) an approach to adapt cultural representation\nin an image based on extracted associations using a modular pipeline,\nCultureAdapt. We find disparities in cultural understanding at geographic\nsub-region levels with both open-source (LLaVA) and closed-source (GPT-4V)\nmodels on DalleStreet and other existing benchmarks, which we try to understand\nusing over 18,000 artifacts that we identify in association to different\ncountries. Our findings reveal a nuanced picture of the cultural competence of\nLMMs, highlighting the need to develop culture-aware systems. Dataset and code\nare available at https://github.com/iamshnoo/crossroads\n","authors":["Anjishnu Mukherjee","Ziwei Zhu","Antonios Anastasopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.02067v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2404.11124v2","updated":"2024-10-18T15:34:41Z","published":"2024-04-17T07:15:07Z","title":"What's under the hood: Investigating Automatic Metrics on Meeting\n  Summarization","summary":"  Meeting summarization has become a critical task considering the increase in\nonline interactions. While new techniques are introduced regularly, their\nevaluation uses metrics not designed to capture meeting-specific errors,\nundermining effective evaluation. This paper investigates what the frequently\nused automatic metrics capture and which errors they mask by correlating\nautomatic metric scores with human evaluations across a broad error taxonomy.\nWe commence with a comprehensive literature review on English meeting\nsummarization to define key challenges like speaker dynamics and contextual\nturn-taking and error types such as missing information and linguistic\ninaccuracy, concepts previously loosely defined in the field. We examine the\nrelationship between characteristic challenges and errors by using annotated\ntranscripts and summaries from Transformer-based sequence-to-sequence and\nautoregressive models from the general summary QMSum dataset. Through\nexperimental validation, we find that different model architectures respond\nvariably to challenges in meeting transcripts, resulting in different\npronounced links between challenges and errors. Current default-used metrics\nstruggle to capture observable errors, showing weak to mid-correlations, while\na third of the correlations show trends of error masking. Only a subset reacts\naccurately to specific errors, while most correlations show either\nunresponsiveness or failure to reflect the error's impact on summary quality.\n","authors":["Frederic Kirstein","Jan Philip Wahle","Terry Ruas","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2404.11124v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12874v2","updated":"2024-10-18T15:26:55Z","published":"2024-10-14T18:11:53Z","title":"On Debiasing Text Embeddings Through Context Injection","summary":"  Current advances in Natural Language Processing (NLP) have made it\nincreasingly feasible to build applications leveraging textual data. Generally,\nthe core of these applications rely on having a good semantic representation of\ntext into vectors, via embedding models. However, it has been shown that these\nembeddings capture and perpetuate biases already present in text. While a few\ntechniques have been proposed to debias embeddings, they do not take advantage\nof the recent advances in context understanding of modern embedding models. In\nthis paper, we fill this gap by conducting a review of 19 embedding models by\nquantifying their biases and how well they respond to context injection as a\nmean of debiasing. We show that higher performing models are more prone to\ncapturing biases, but are also better at incorporating context. Surprisingly,\nwe find that while models can easily embed affirmative semantics, they fail at\nembedding neutral semantics. Finally, in a retrieval task, we show that biases\nin embeddings can lead to non-desirable outcomes. We use our new-found insights\nto design a simple algorithm for top $k$ retrieval, where $k$ is dynamically\nselected. We show that our algorithm is able to retrieve all relevant gendered\nand neutral chunks.\n","authors":["Thomas Uriot"],"pdf_url":"https://arxiv.org/pdf/2410.12874v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13901v3","updated":"2024-10-18T15:25:44Z","published":"2024-03-20T18:13:17Z","title":"Train & Constrain: Phonologically Informed Tongue-Twister Generation\n  from Topics and Paraphrases","summary":"  Previous work in phonologically and phonetically grounded language generation\nhas mainly focused on domains such as puns and poetry. In this article, we\npresent new work on the generation of English tongue twisters - a form of\nlanguage that is required to be conditioned on a phoneme level to maximize\nsound overlap, while maintaining semantic consistency with an input topic or\nphrase and still being grammatically correct. We present TwisterLister, a\npipeline for generating phonologically informed tongue twisters from large\nlanguage models (LLMs) that we use to generate TwistList 2.0, the largest\nannotated dataset of tongue twisters to date, consisting of 17K+ examples from\na combination of human and LLM authors. Our generation pipeline involves the\nuse of a phonologically constrained vocabulary alongside LLM prompting to\ngenerate novel, non-derivative tongue twister examples. We additionally present\nthe results of automatic and human evaluation of smaller models trained on our\ngenerated dataset to demonstrate the extent to which phonologically motivated\nlanguage types can be generated without explicit injection of phonological\nknowledge. Additionally, we introduce a phoneme-aware constrained decoding\nmodule (PACD) that can be integrated into an autoregressive language model and\ndemonstrate that this method generates good quality tongue twisters both with\nand without fine-tuning the underlying language model. We also design and\nimplement a range of automatic metrics for the task of tongue twister\ngeneration that is phonologically motivated and captures the unique essence of\ntongue twisters, primarily based on phonemic edit distance (PED)\n","authors":["Tyler Loakman","Chen Tang","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2403.13901v3.pdf","comment":"Accepted Final Version to Computational Linguistics"},{"id":"http://arxiv.org/abs/2406.11580v2","updated":"2024-10-18T15:20:43Z","published":"2024-06-17T14:20:47Z","title":"Error Span Annotation: A Balanced Approach for Human Evaluation of\n  Machine Translation","summary":"  High-quality Machine Translation (MT) evaluation relies heavily on human\njudgments. Comprehensive error classification methods, such as Multidimensional\nQuality Metrics (MQM), are expensive as they are time-consuming and can only be\ndone by experts, whose availability may be limited especially for low-resource\nlanguages. On the other hand, just assigning overall scores, like Direct\nAssessment (DA), is simpler and faster and can be done by translators of any\nlevel, but is less reliable. In this paper, we introduce Error Span Annotation\n(ESA), a human evaluation protocol which combines the continuous rating of DA\nwith the high-level error severity span marking of MQM. We validate ESA by\ncomparing it to MQM and DA for 12 MT systems and one human reference\ntranslation (English to German) from WMT23. The results show that ESA offers\nfaster and cheaper annotations than MQM at the same quality level, without the\nrequirement of expensive MQM experts.\n","authors":["Tom Kocmi","Vilém Zouhar","Eleftherios Avramidis","Roman Grundkiewicz","Marzena Karpinska","Maja Popović","Mrinmaya Sachan","Mariya Shmatova"],"pdf_url":"https://arxiv.org/pdf/2406.11580v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14516v1","updated":"2024-10-18T14:55:14Z","published":"2024-10-18T14:55:14Z","title":"Do LLMs \"know\" internally when they follow instructions?","summary":"  Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. Our analysis of LLM internal states reveal a\ndimension in the input embedding space linked to successful\ninstruction-following. We demonstrate that modifying representations along this\ndimension improves instruction-following success rates compared to random\nchanges, without compromising response quality. Further investigation reveals\nthat this dimension is more closely related to the phrasing of prompts rather\nthan the inherent difficulty of the task or instructions. This discovery also\nsuggests explanations for why LLMs sometimes fail to follow clear instructions\nand why prompt engineering is often effective, even when the content remains\nlargely unchanged. This work provides insight into the internal workings of\nLLMs' instruction-following, paving the way for reliable LLM agents.\n","authors":["Juyeon Heo","Christina Heinze-Deml","Oussama Elachqar","Shirley Ren","Udhay Nallasamy","Andy Miller","Kwan Ho Ryan Chan","Jaya Narain"],"pdf_url":"https://arxiv.org/pdf/2410.14516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14506v1","updated":"2024-10-18T14:38:37Z","published":"2024-10-18T14:38:37Z","title":"SignAttention: On the Interpretability of Transformer Models for Sign\n  Language Translation","summary":"  This paper presents the first comprehensive interpretability analysis of a\nTransformer-based Sign Language Translation (SLT) model, focusing on the\ntranslation from video-based Greek Sign Language to glosses and text.\nLeveraging the Greek Sign Language Dataset, we examine the attention mechanisms\nwithin the model to understand how it processes and aligns visual input with\nsequential glosses. Our analysis reveals that the model pays attention to\nclusters of frames rather than individual ones, with a diagonal alignment\npattern emerging between poses and glosses, which becomes less distinct as the\nnumber of glosses increases. We also explore the relative contributions of\ncross-attention and self-attention at each decoding step, finding that the\nmodel initially relies on video frames but shifts its focus to previously\npredicted tokens as the translation progresses. This work contributes to a\ndeeper understanding of SLT models, paving the way for the development of more\ntransparent and reliable translation systems essential for real-world\napplications.\n","authors":["Pedro Alejandro Dal Bianco","Oscar Agustín Stanchi","Facundo Manuel Quiroga","Franco Ronchetti","Enzo Ferrante"],"pdf_url":"https://arxiv.org/pdf/2410.14506v1.pdf","comment":"Accepted at IAI Workshop @ NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.14480v1","updated":"2024-10-18T14:03:52Z","published":"2024-10-18T14:03:52Z","title":"Combining Entropy and Matrix Nuclear Norm for Enhanced Evaluation of\n  Language Models","summary":"  As large language models (LLMs) continue to advance, the need for precise and\nefficient evaluation metrics becomes more pressing. Traditional approaches,\nwhile informative, often face limitations in computational demands and\ninterpretability. In this paper, we introduce a novel hybrid evaluation method\nthat integrates two established techniques: entropy derived from covariance\nmatrices and the Matrix Nuclear Norm (MNN). Our method begins by normalizing\nhidden states from LLMs, then computes the covariance matrix and MNN from these\nrepresentations. We further calculate the entropy of the covariance matrix to\ncapture uncertainty and redundancy in the model's outputs. By combining these\nmetrics into a composite score, we offer a comprehensive evaluation framework\nthat balances accuracy with computational efficiency. Additionally, our\napproach allows for flexibility in adjusting the weightings between entropy and\nMNN, tailoring the evaluation for different objectives. Through a series of\nexperiments on various LLMs, we demonstrate the robustness and efficacy of our\nmethod, offering deeper insights into model performance. This work contributes\nto the ongoing development of LLM evaluation and opens avenues for future\ninnovations in model assessment techniques.\n","authors":["James Vo"],"pdf_url":"https://arxiv.org/pdf/2410.14480v1.pdf","comment":"The method is currently under experimentation"},{"id":"http://arxiv.org/abs/2410.09804v2","updated":"2024-10-18T14:03:05Z","published":"2024-10-13T11:15:38Z","title":"BlackDAN: A Black-Box Multi-Objective Approach for Effective and\n  Contextual Jailbreaking of Large Language Models","summary":"  While large language models (LLMs) exhibit remarkable capabilities across\nvarious tasks, they encounter potential security risks such as jailbreak\nattacks, which exploit vulnerabilities to bypass security measures and generate\nharmful outputs. Existing jailbreak strategies mainly focus on maximizing\nattack success rate (ASR), frequently neglecting other critical factors,\nincluding the relevance of the jailbreak response to the query and the level of\nstealthiness. This narrow focus on single objectives can result in ineffective\nattacks that either lack contextual relevance or are easily recognizable. In\nthis work, we introduce BlackDAN, an innovative black-box attack framework with\nmulti-objective optimization, aiming to generate high-quality prompts that\neffectively facilitate jailbreaking while maintaining contextual relevance and\nminimizing detectability. BlackDAN leverages Multiobjective Evolutionary\nAlgorithms (MOEAs), specifically the NSGA-II algorithm, to optimize jailbreaks\nacross multiple objectives including ASR, stealthiness, and semantic relevance.\nBy integrating mechanisms like mutation, crossover, and Pareto-dominance,\nBlackDAN provides a transparent and interpretable process for generating\njailbreaks. Furthermore, the framework allows customization based on user\npreferences, enabling the selection of prompts that balance harmfulness,\nrelevance, and other factors. Experimental results demonstrate that BlackDAN\noutperforms traditional single-objective methods, yielding higher success rates\nand improved robustness across various LLMs and multimodal LLMs, while ensuring\njailbreak responses are both relevant and less detectable.\n","authors":["Xinyuan Wang","Victor Shea-Jay Huang","Renmiao Chen","Hao Wang","Chengwei Pan","Lei Sha","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2410.09804v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14346v2","updated":"2024-10-18T13:59:54Z","published":"2024-07-19T14:28:53Z","title":"Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals","summary":"  Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.\n","authors":["Akash Kumar Mohankumar","Gururaj K","Gagan Madan","Amit Singh"],"pdf_url":"https://arxiv.org/pdf/2407.14346v2.pdf","comment":"Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure"},{"id":"http://arxiv.org/abs/2406.13663v4","updated":"2024-10-18T13:16:57Z","published":"2024-06-19T16:10:26Z","title":"Model Internals-based Answer Attribution for Trustworthy\n  Retrieval-Augmented Generation","summary":"  Ensuring the verifiability of model answers is a fundamental challenge for\nretrieval-augmented generation (RAG) in the question answering (QA) domain.\nRecently, self-citation prompting was proposed to make large language models\n(LLMs) generate citations to supporting documents along with their answers.\nHowever, self-citing LLMs often struggle to match the required format, refer to\nnon-existent sources, and fail to faithfully reflect LLMs' context usage\nthroughout the generation. In this work, we present MIRAGE --Model\nInternals-based RAG Explanations -- a plug-and-play approach using model\ninternals for faithful answer attribution in RAG applications. MIRAGE detects\ncontext-sensitive answer tokens and pairs them with retrieved documents\ncontributing to their prediction via saliency methods. We evaluate our proposed\napproach on a multilingual extractive QA dataset, finding high agreement with\nhuman answer attribution. On open-ended QA, MIRAGE achieves citation quality\nand efficiency comparable to self-citation while also allowing for a\nfiner-grained control of attribution parameters. Our qualitative evaluation\nhighlights the faithfulness of MIRAGE's attributions and underscores the\npromising application of model internals for RAG answer attribution.\n","authors":["Jirui Qi","Gabriele Sarti","Raquel Fernández","Arianna Bisazza"],"pdf_url":"https://arxiv.org/pdf/2406.13663v4.pdf","comment":"Accepted by EMNLP 2024 Main Conference. Code and data released at\n  https://github.com/Betswish/MIRAGE"},{"id":"http://arxiv.org/abs/2405.11877v5","updated":"2024-10-18T13:03:05Z","published":"2024-05-20T08:41:15Z","title":"A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI:\n  The First Romanian Natural Language Inference Corpus","summary":"  Natural language inference (NLI), the task of recognizing the entailment\nrelationship in sentence pairs, is an actively studied topic serving as a proxy\nfor natural language understanding. Despite the relevance of the task in\nbuilding conversational agents and improving text classification, machine\ntranslation and other NLP tasks, to the best of our knowledge, there is no\npublicly available NLI corpus for the Romanian language. To this end, we\nintroduce the first Romanian NLI corpus (RoNLI) comprising 58K training\nsentence pairs, which are obtained via distant supervision, and 6K validation\nand test sentence pairs, which are manually annotated with the correct labels.\nWe conduct experiments with multiple machine learning methods based on distant\nlearning, ranging from shallow models based on word embeddings to\ntransformer-based neural networks, to establish a set of competitive baselines.\nFurthermore, we improve on the best model by employing a new curriculum\nlearning strategy based on data cartography. Our dataset and code to reproduce\nthe baselines are available at https://github.com/Eduard6421/RONLI.\n","authors":["Eduard Poesina","Cornelia Caragea","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2405.11877v5.pdf","comment":"Accepted at ACL 2024 (Main)"},{"id":"http://arxiv.org/abs/2410.14442v1","updated":"2024-10-18T13:01:14Z","published":"2024-10-18T13:01:14Z","title":"A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference","summary":"  Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference.\n","authors":["You Wu","Haoyi Wu","Kewei Tu"],"pdf_url":"https://arxiv.org/pdf/2410.14442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10996v2","updated":"2024-10-18T12:54:21Z","published":"2024-06-16T16:17:46Z","title":"Towards Lifelong Dialogue Agents via Relation-aware Memory Construction\n  and Timeline-augmented Response Generation","summary":"  To achieve lifelong human-agent interaction, dialogue agents need to\nconstantly memorize perceived information and properly retrieve it for response\ngeneration (RG). While prior work focuses on getting rid of outdated memories\nto improve retrieval quality, we argue that such memories provide rich,\nimportant contextual cues for RG (e.g., changes in user behaviors) in long-term\nconversations. We present Theanine, a framework for LLM-based lifelong dialogue\nagents. Theanine discards memory removal and manages large-scale memories by\nlinking them based on their temporal and cause-effect relation. Enabled by this\nlinking structure, Theanine augments RG with memory timelines - series of\nmemories representing the evolution or causality of relevant past events. Along\nwith Theanine, we introduce TeaFarm, a counterfactual-driven evaluation scheme,\naddressing the limitation of G-Eval and human efforts in measuring\nmemory-augmented dialogue agents. A supplementary video for Theanine and data\nfor TeaFarm are at https://huggingface.co/spaces/ResearcherScholar/Theanine.\n","authors":["Kai Tzu-iunn Ong","Namyoung Kim","Minju Gwak","Hyungjoo Chae","Taeyoon Kwon","Yohan Jo","Seung-won Hwang","Dongha Lee","Jinyoung Yeo"],"pdf_url":"https://arxiv.org/pdf/2406.10996v2.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2305.17819v3","updated":"2024-10-18T12:49:35Z","published":"2023-05-28T22:46:21Z","title":"Large Language Models, scientific knowledge and factuality: A framework\n  to streamline human expert evaluation","summary":"  The paper introduces a framework for the evaluation of the encoding of\nfactual scientific knowledge, designed to streamline the manual evaluation\nprocess typically conducted by domain experts. Inferring over and extracting\ninformation from Large Language Models (LLMs) trained on a large corpus of\nscientific literature can potentially define a step change in biomedical\ndiscovery, reducing the barriers for accessing and integrating existing medical\nevidence. This work explores the potential of LLMs for dialoguing with\nbiomedical background knowledge, using the context of antibiotic discovery. The\nframework involves of three evaluation steps, each assessing different aspects\nsequentially: fluency, prompt alignment, semantic coherence, factual knowledge,\nand specificity of the generated responses. By splitting these tasks between\nnon-experts and experts, the framework reduces the effort required from the\nlatter. The work provides a systematic assessment on the ability of eleven\nstate-of-the-art models LLMs, including ChatGPT, GPT-4 and Llama 2, in two\nprompting-based tasks: chemical compound definition generation and chemical\ncompound-fungus relation determination. Although recent models have improved in\nfluency, factual accuracy is still low and models are biased towards\nover-represented entities. The ability of LLMs to serve as biomedical knowledge\nbases is questioned, and the need for additional systematic evaluation\nframeworks is highlighted. While LLMs are currently not fit for purpose to be\nused as biomedical factual knowledge bases in a zero-shot setting, there is a\npromising emerging property in the direction of factuality as the models become\ndomain specialised, scale-up in size and level of human feedback.\n","authors":["Magdalena Wysocka","Oskar Wysocki","Maxime Delmas","Vincent Mutel","Andre Freitas"],"pdf_url":"https://arxiv.org/pdf/2305.17819v3.pdf","comment":"Accepted at the Journal of Biomedical Informatics, Volume 158,\n  October 2024, 104724"},{"id":"http://arxiv.org/abs/2410.14425v1","updated":"2024-10-18T12:39:32Z","published":"2024-10-18T12:39:32Z","title":"Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge\n  Distillation","summary":"  Parameter-efficient fine-tuning (PEFT) can bridge the gap between large\nlanguage models (LLMs) and downstream tasks. However, PEFT has been proven\nvulnerable to malicious attacks. Research indicates that poisoned LLMs, even\nafter PEFT, retain the capability to activate internalized backdoors when input\nsamples contain predefined triggers. In this paper, we introduce a novel\nweak-to-strong unlearning algorithm to defend against backdoor attacks based on\nfeature alignment knowledge distillation, named W2SDefense. Specifically, we\nfirst train a small-scale language model through full-parameter fine-tuning to\nserve as the clean teacher model. Then, this teacher model guides the\nlarge-scale poisoned student model in unlearning the backdoor, leveraging PEFT.\nTheoretical analysis suggests that W2SDefense has the potential to enhance the\nstudent model's ability to unlearn backdoor features, preventing the activation\nof the backdoor. We conduct experiments on text classification tasks involving\nthree state-of-the-art language models and three different backdoor attack\nalgorithms. Our empirical results demonstrate the outstanding performance of\nW2SDefense in defending against backdoor attacks without compromising model\nperformance.\n","authors":["Shuai Zhao","Xiaobao Wu","Cong-Duy Nguyen","Meihuizi Jia","Yichao Feng","Luu Anh Tuan"],"pdf_url":"https://arxiv.org/pdf/2410.14425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10918v5","updated":"2024-10-18T12:27:07Z","published":"2024-06-16T12:46:40Z","title":"Multi-LLM QA with Embodied Exploration","summary":"  Large language models (LLMs) have grown in popularity due to their natural\nlanguage interface and pre trained knowledge, leading to rapidly increasing\nsuccess in question-answering (QA) tasks. More recently, multi-agent systems\nwith LLM-based agents (Multi-LLM) have been utilized increasingly more for QA.\nIn these scenarios, the models may each answer the question and reach a\nconsensus or each model is specialized to answer different domain questions.\nHowever, most prior work dealing with Multi-LLM QA has focused on scenarios\nwhere the models are asked in a zero-shot manner or are given information\nsources to extract the answer. For question answering of an unknown\nenvironment, embodied exploration of the environment is first needed to answer\nthe question. This skill is necessary for personalizing embodied AI to\nenvironments such as households. There is a lack of insight into whether a\nMulti-LLM system can handle question-answering based on observations from\nembodied exploration. In this work, we address this gap by investigating the\nuse of Multi-Embodied LLM Explorers (MELE) for QA in an unknown environment.\nMultiple LLM-based agents independently explore and then answer queries about a\nhousehold environment. We analyze different aggregation methods to generate a\nsingle, final answer for each query: debating, majority voting, and training a\ncentral answer module (CAM). Using CAM, we observe a $46\\%$ higher accuracy\ncompared against the other non-learning-based aggregation methods. We provide\ncode and the query dataset for further research.\n","authors":["Bhrij Patel","Vishnu Sashank Dorbala","Amrit Singh Bedi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2406.10918v5.pdf","comment":"16 pages, 9 Figures, 5 Tables"},{"id":"http://arxiv.org/abs/2406.12950v2","updated":"2024-10-18T12:19:41Z","published":"2024-06-18T12:54:47Z","title":"MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular\n  Property Prediction","summary":"  Molecular property prediction (MPP) is a fundamental and crucial task in drug\ndiscovery. However, prior methods are limited by the requirement for a large\nnumber of labeled molecules and their restricted ability to generalize for\nunseen and new tasks, both of which are essential for real-world applications.\nTo address these challenges, we present MolecularGPT for few-shot MPP. From a\nperspective on instruction tuning, we fine-tune large language models (LLMs)\nbased on curated molecular instructions spanning over 1000 property prediction\ntasks. This enables building a versatile and specialized LLM that can be\nadapted to novel MPP tasks without any fine-tuning through zero- and few-shot\nin-context learning (ICL). MolecularGPT exhibits competitive in-context\nreasoning capabilities across 10 downstream evaluation datasets, setting new\nbenchmarks for few-shot molecular prediction tasks. More importantly, with just\ntwo-shot examples, MolecularGPT can outperform standard supervised graph neural\nnetwork methods on 4 out of 7 datasets. It also excels state-of-the-art LLM\nbaselines by up to 15.7% increase on classification accuracy and decrease of\n17.9 on regression metrics (e.g., RMSE) under zero-shot. This study\ndemonstrates the potential of LLMs as effective few-shot molecular property\npredictors. The code is available at https://github.com/NYUSHCS/MolecularGPT.\n","authors":["Yuyan Liu","Sirui Ding","Sheng Zhou","Wenqi Fan","Qiaoyu Tan"],"pdf_url":"https://arxiv.org/pdf/2406.12950v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14405v1","updated":"2024-10-18T12:08:07Z","published":"2024-10-18T12:08:07Z","title":"Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of\n  Language Models for Fact Completion","summary":"  Previous interpretations of language models (LMs) miss important distinctions\nin how these models process factual information. For example, given the query\n\"Astrid Lindgren was born in\" with the corresponding completion \"Sweden\", no\ndifference is made between whether the prediction was based on having the exact\nknowledge of the birthplace of the Swedish author or assuming that a person\nwith a Swedish-sounding name was born in Sweden. In this paper, we investigate\nfour different prediction scenarios for which the LM can be expected to show\ndistinct behaviors. These scenarios correspond to different levels of model\nreliability and types of information being processed - some being less\ndesirable for factual predictions. To facilitate precise interpretations of LMs\nfor fact completion, we propose a model-specific recipe called PrISM for\nconstructing datasets with examples of each scenario based on a set of\ndiagnostic criteria. We apply a popular interpretability method, causal tracing\n(CT), to the four prediction scenarios and find that while CT produces\ndifferent results for each scenario, aggregations over a set of mixed examples\nmay only represent the results from the scenario with the strongest measured\nsignal. In summary, we contribute tools for a more granular study of fact\ncompletion in language models and analyses that provide a more nuanced\nunderstanding of how LMs process fact-related queries.\n","authors":["Denitsa Saynova","Lovisa Hagström","Moa Johansson","Richard Johansson","Marco Kuhlmann"],"pdf_url":"https://arxiv.org/pdf/2410.14405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14399v1","updated":"2024-10-18T12:02:41Z","published":"2024-10-18T12:02:41Z","title":"SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic\n  Reasoning","summary":"  Syllogistic reasoning is crucial for Natural Language Inference (NLI). This\ncapability is particularly significant in specialized domains such as\nbiomedicine, where it can support automatic evidence interpretation and\nscientific discovery. This paper presents SylloBio-NLI, a novel framework that\nleverages external ontologies to systematically instantiate diverse syllogistic\narguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language\nModels (LLMs) on identifying valid conclusions and extracting supporting\nevidence across 28 syllogistic schemes instantiated with human genome pathways.\nExtensive experiments reveal that biomedical syllogistic reasoning is\nparticularly challenging for zero-shot LLMs, which achieve an average accuracy\nbetween 70% on generalized modus ponens and 23% on disjunctive syllogism. At\nthe same time, we found that few-shot prompting can boost the performance of\ndifferent LLMs, including Gemma (+14%) and LLama-3 (+43%). However, a deeper\nanalysis shows that both techniques exhibit high sensitivity to superficial\nlexical variations, highlighting a dependency between reliability, models'\narchitecture, and pre-training regime. Overall, our results indicate that,\nwhile in-context examples have the potential to elicit syllogistic reasoning in\nLLMs, existing models are still far from achieving the robustness and\nconsistency required for safe biomedical NLI applications.\n","authors":["Magdalena Wysocka","Danilo S. Carvalho","Oskar Wysocki","Marco Valentino","Andre Freitas"],"pdf_url":"https://arxiv.org/pdf/2410.14399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14395v1","updated":"2024-10-18T11:58:03Z","published":"2024-10-18T11:58:03Z","title":"Generative AI, Pragmatics, and Authenticity in Second Language Learning","summary":"  There are obvious benefits to integrating generative AI (artificial\nintelligence) into language learning and teaching. Those include using AI as a\nlanguage tutor, creating learning materials, or assessing learner output.\nHowever, due to how AI systems under-stand human language, based on a\nmathematical model using statistical probability, they lack the lived\nexperience to be able to use language with the same social aware-ness as\nhumans. Additionally, there are built-in linguistic and cultural biases based\non their training data which is mostly in English and predominantly from\nWestern sources. Those facts limit AI suitability for some language learning\ninteractions. Stud-ies have clearly shown that systems such as ChatGPT often do\nnot produce language that is pragmatically appropriate. The lack of linguistic\nand cultural authenticity has important implications for how AI is integrated\ninto second language acquisition as well as in instruction targeting\ndevelopment of intercultural communication compe-tence.\n","authors":["Robert Godwin-Jones`"],"pdf_url":"https://arxiv.org/pdf/2410.14395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14391v1","updated":"2024-10-18T11:52:10Z","published":"2024-10-18T11:52:10Z","title":"Analyzing Context Utilization of LLMs in Document-Level Translation","summary":"  Large language models (LLM) are increasingly strong contenders in machine\ntranslation. We study document-level translation, where some words cannot be\ntranslated without context from outside the sentence. We investigate the\nability of prominent LLMs to utilize context by analyzing models' robustness to\nperturbed and randomized document context. We find that LLMs' improved\ndocument-translation performance is not always reflected in pronoun translation\nperformance. We highlight the need for context-aware finetuning of LLMs with a\nfocus on relevant parts of the context to improve their reliability for\ndocument-level translation.\n","authors":["Wafaa Mohammed","Vlad Niculae"],"pdf_url":"https://arxiv.org/pdf/2410.14391v1.pdf","comment":"4 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.14387v1","updated":"2024-10-18T11:39:34Z","published":"2024-10-18T11:39:34Z","title":"How Do Multilingual Models Remember? Investigating Multilingual Factual\n  Recall Mechanisms","summary":"  Large Language Models (LLMs) store and retrieve vast amounts of factual\nknowledge acquired during pre-training. Prior research has localized and\nidentified mechanisms behind knowledge recall; however, it has primarily\nfocused on English monolingual models. The question of how these processes\ngeneralize to other languages and multilingual LLMs remains unexplored. In this\npaper, we address this gap by conducting a comprehensive analysis of two highly\nmultilingual LLMs. We assess the extent to which previously identified\ncomponents and mechanisms of factual recall in English apply to a multilingual\ncontext. Then, we examine when language plays a role in the recall process,\nuncovering evidence of language-independent and language-dependent mechanisms.\n","authors":["Constanza Fierro","Negar Foroutan","Desmond Elliott","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/2410.14387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14375v1","updated":"2024-10-18T11:06:23Z","published":"2024-10-18T11:06:23Z","title":"Fine-Tuning Pre-trained Language Models for Robust Causal Representation\n  Learning","summary":"  The fine-tuning of pre-trained language models (PLMs) has been shown to be\neffective across various domains. By using domain-specific supervised data, the\ngeneral-purpose representation derived from PLMs can be transformed into a\ndomain-specific representation. However, these methods often fail to generalize\nto out-of-domain (OOD) data due to their reliance on non-causal\nrepresentations, often described as spurious features. Existing methods either\nmake use of adjustments with strong assumptions about lack of hidden common\ncauses, or mitigate the effect of spurious features using multi-domain data. In\nthis work, we investigate how fine-tuned pre-trained language models aid\ngeneralizability from single-domain scenarios under mild assumptions, targeting\nmore general and practical real-world scenarios. We show that a robust\nrepresentation can be derived through a so-called causal front-door adjustment,\nbased on a decomposition assumption, using fine-tuned representations as a\nsource of data augmentation. Comprehensive experiments in both synthetic and\nreal-world settings demonstrate the superior generalizability of the proposed\nmethod compared to existing approaches. Our work thus sheds light on the domain\ngeneralization problem by introducing links between fine-tuning and causal\nmechanisms into representation learning.\n","authors":["Jialin Yu","Yuxiang Zhou","Yulan He","Nevin L. Zhang","Ricardo Silva"],"pdf_url":"https://arxiv.org/pdf/2410.14375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10203v3","updated":"2024-10-18T10:43:40Z","published":"2024-06-14T17:38:21Z","title":"A Fundamental Trade-off in Aligned Language Models and its Relation to\n  Sampling Adaptors","summary":"  The relationship between the quality of a string, as judged by a human\nreader, and its probability, $p(\\boldsymbol{y})$ under a language model\nundergirds the development of better language models. For example, many popular\nalgorithms for sampling from a language model have been conceived with the goal\nof manipulating $p(\\boldsymbol{y})$ to place higher probability on strings that\nhumans deem of high quality. In this article, we examine the\nprobability--quality relationship in language models explicitly aligned to\nhuman preferences, e.g., through reinforcement learning through human feedback.\nWe show that, when sampling corpora from an aligned language model, there\nexists a trade-off between the strings' average reward and average\nlog-likelihood under the prior language model, i.e., the same model before\nalignment with human preferences. We provide a formal treatment of this\nphenomenon and demonstrate how a choice of sampling adaptor allows for a\nselection of how much likelihood we exchange for the reward.\n","authors":["Naaman Tan","Josef Valvoda","Tianyu Liu","Anej Svete","Yanxia Qin","Kan Min-Yen","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2406.10203v3.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.14361v1","updated":"2024-10-18T10:40:47Z","published":"2024-10-18T10:40:47Z","title":"Efficiently Computing Susceptibility to Context in Language Models","summary":"  One strength of modern language models is their ability to incorporate\ninformation from a user-input context when answering queries. However, they are\nnot equally sensitive to the subtle changes to that context. To quantify this,\nDu et al. (2024) gives an information-theoretic metric to measure such\nsensitivity. Their metric, susceptibility, is defined as the degree to which\ncontexts can influence a model's response to a query at a distributional level.\nHowever, exactly computing susceptibility is difficult and, thus, Du et al.\n(2024) falls back on a Monte Carlo approximation. Due to the large number of\nsamples required, the Monte Carlo approximation is inefficient in practice. As\na faster alternative, we propose Fisher susceptibility, an efficient method to\nestimate the susceptibility based on Fisher information. Empirically, we\nvalidate that Fisher susceptibility is comparable to Monte Carlo estimated\nsusceptibility across a diverse set of query domains despite its being\n$70\\times$ faster. Exploiting the improved efficiency, we apply Fisher\nsusceptibility to analyze factors affecting the susceptibility of language\nmodels. We observe that larger models are as susceptible as smaller ones.\n","authors":["Tianyu Liu","Kevin Du","Mrinmaya Sachan","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2410.14361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11244v4","updated":"2024-10-18T10:21:31Z","published":"2023-10-17T13:12:32Z","title":"Entity Matching using Large Language Models","summary":"  Entity matching is the task of deciding whether two entity descriptions refer\nto the same real-world entity. Entity matching is a central step in most data\nintegration pipelines. Many state-of-the-art entity matching methods rely on\npre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks\nof these models for entity matching are that (i) the models require significant\namounts of task-specific training data and (ii) the fine-tuned models are not\nrobust concerning out-of-distribution entities. This paper investigates using\ngenerative large language models (LLMs) as a less task-specific training\ndata-dependent and more robust alternative to PLM-based matchers. The study\ncovers hosted and open-source LLMs which can be run locally. We evaluate these\nmodels in a zero-shot scenario and a scenario where task-specific training data\nis available. We compare different prompt designs and the prompt sensitivity of\nthe models. We show that there is no single best prompt but that the prompt\nneeds to be tuned for each model/dataset combination. We further investigate\n(i) the selection of in-context demonstrations, (ii) the generation of matching\nrules, as well as (iii) fine-tuning LLMs using the same pool of training data.\nOur experiments show that the best LLMs require no or only a few training\nexamples to perform comparably to PLMs that were fine-tuned using thousands of\nexamples. LLM-based matchers further exhibit higher robustness to unseen\nentities. We show that GPT4 can generate structured explanations for matching\ndecisions and can automatically identify potential causes of matching errors by\nanalyzing explanations of wrong decisions. We demonstrate that the model can\ngenerate meaningful textual descriptions of the identified error classes, which\ncan help data engineers to improve entity matching pipelines.\n","authors":["Ralph Peeters","Aaron Steiner","Christian Bizer"],"pdf_url":"https://arxiv.org/pdf/2310.11244v4.pdf","comment":"Published in Proceedings of the 28th International Conference on\n  Extending Database Technology (EDBT), 25th March-28th March, 2025, ISBN\n  978-3-89318-098-1 on OpenProceedings.org"},{"id":"http://arxiv.org/abs/2409.19700v3","updated":"2024-10-18T10:15:29Z","published":"2024-09-29T13:16:37Z","title":"2D-TPE: Two-Dimensional Positional Encoding Enhances Table Understanding\n  for Large Language Models","summary":"  Tables are ubiquitous across various domains for concisely representing\nstructured information. Empowering large language models (LLMs) to reason over\ntabular data represents an actively explored direction. However, since typical\nLLMs only support one-dimensional~(1D) inputs, existing methods often flatten\nthe two-dimensional~(2D) table structure into a sequence of tokens, which can\nseverely disrupt the spatial relationships and result in an inevitable loss of\nvital contextual information. In this paper, we first empirically demonstrate\nthe detrimental impact of such flattening operations on the performance of LLMs\nin capturing the spatial information of tables through two elaborate proxy\ntasks. Subsequently, we introduce a simple yet effective positional encoding\nmethod, termed ``2D-TPE'' (Two-Dimensional Table Positional Encoding), to\naddress this challenge. 2D-TPE enables each attention head to dynamically\nselect a permutation order of tokens within the context for attending to them,\nwhere each permutation represents a distinct traversal mode for the table, such\nas column-wise or row-wise traversal. 2D-TPE effectively mitigates the risk of\nlosing essential spatial information while preserving computational efficiency,\nthus better preserving the table structure. Extensive experiments across five\nbenchmarks demonstrate that 2D-TPE outperforms strong baselines, underscoring\nthe importance of preserving the table structure for accurate table\ncomprehension. Comprehensive analysis further reveals the substantially better\nscalability of 2D-TPE to large tables than baselines.\n","authors":["Jia-Nan Li","Jian Guan","Wei Wu","Zhengtao Yu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2409.19700v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10859v2","updated":"2024-10-18T10:02:03Z","published":"2024-10-07T13:46:06Z","title":"FAME: Towards Factual Multi-Task Model Editing","summary":"  Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.\n","authors":["Li Zeng","Yingyu Shan","Zeming Liu","Jiashu Yao","Yuhang Guo"],"pdf_url":"https://arxiv.org/pdf/2410.10859v2.pdf","comment":"9 pages, 3 figures. This paper has been accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2302.08102v2","updated":"2024-10-18T09:58:45Z","published":"2023-02-16T06:01:31Z","title":"Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech\n  Recognition","summary":"  Visual Speech Recognition (VSR) aims to infer speech into text depending on\nlip movements alone. As it focuses on visual information to model the speech,\nits performance is inherently sensitive to personal lip appearances and\nmovements, and this makes the VSR models show degraded performance when they\nare applied to unseen speakers. In this paper, to remedy the performance\ndegradation of the VSR model on unseen speakers, we propose prompt tuning\nmethods of Deep Neural Networks (DNNs) for speaker-adaptive VSR. Specifically,\nmotivated by recent advances in Natural Language Processing (NLP), we finetune\nprompts on adaptation data of target speakers instead of modifying the\npre-trained model parameters. Different from the previous prompt tuning methods\nmainly limited to Transformer variant architecture, we explore different types\nof prompts, the addition, the padding, and the concatenation form prompts that\ncan be applied to the VSR model which is composed of CNN and Transformer in\ngeneral. With the proposed prompt tuning, we show that the performance of the\npre-trained VSR model on unseen speakers can be largely improved by using a\nsmall amount of adaptation data (e.g., less than 5 minutes), even if the\npre-trained model is already developed with large speaker variations. Moreover,\nby analyzing the performance and parameters of different types of prompts, we\ninvestigate when the prompt tuning is preferred over the finetuning methods.\nThe effectiveness of the proposed method is evaluated on both word- and\nsentence-level VSR databases, LRW-ID and GRID.\n","authors":["Minsu Kim","Hyung-Il Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2302.08102v2.pdf","comment":"IEEE TPAMI"},{"id":"http://arxiv.org/abs/2410.13281v2","updated":"2024-10-18T09:50:41Z","published":"2024-10-17T07:15:15Z","title":"BANTH: A Multi-label Hate Speech Detection Dataset for Transliterated\n  Bangla","summary":"  The proliferation of transliterated texts in digital spaces has emphasized\nthe need for detecting and classifying hate speech in languages beyond English,\nparticularly in low-resource languages. As online discourse can perpetuate\ndiscrimination based on target groups, e.g. gender, religion, and origin,\nmulti-label classification of hateful content can help in comprehending hate\nmotivation and enhance content moderation. While previous efforts have focused\non monolingual or binary hate classification tasks, no work has yet addressed\nthe challenge of multi-label hate speech classification in transliterated\nBangla. We introduce BanTH, the first multi-label transliterated Bangla hate\nspeech dataset comprising 37.3k samples. The samples are sourced from YouTube\ncomments, where each instance is labeled with one or more target groups,\nreflecting the regional demographic. We establish novel transformer\nencoder-based baselines by further pre-training on transliterated Bangla\ncorpus. We also propose a novel translation-based LLM prompting strategy for\ntransliterated text. Experiments reveal that our further pre-trained encoders\nare achieving state-of-the-art performance on the BanTH dataset, while our\ntranslation-based prompting outperforms other strategies in the zero-shot\nsetting. The introduction of BanTH not only fills a critical gap in hate speech\nresearch for Bangla but also sets the stage for future exploration into\ncode-mixed and multi-label classification challenges in underrepresented\nlanguages.\n","authors":["Fabiha Haider","Fariha Tanjim Shifat","Md Farhan Ishmam","Deeparghya Dutta Barua","Md Sakib Ul Rahman Sourove","Md Fahim","Md Farhad Alam"],"pdf_url":"https://arxiv.org/pdf/2410.13281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14335v1","updated":"2024-10-18T09:46:38Z","published":"2024-10-18T09:46:38Z","title":"Critical Questions Generation: Motivation and Challenges","summary":"  The development of Large Language Models (LLMs) has brought impressive\nperformances on mitigation strategies against misinformation, such as\ncounterargument generation. However, LLMs are still seriously hindered by\noutdated knowledge and by their tendency to generate hallucinated content. In\norder to circumvent these issues, we propose a new task, namely, Critical\nQuestions Generation, consisting of processing an argumentative text to\ngenerate the critical questions (CQs) raised by it. In argumentation theory CQs\nare tools designed to lay bare the blind spots of an argument by pointing at\nthe information it could be missing. Thus, instead of trying to deploy LLMs to\nproduce knowledgeable and relevant counterarguments, we use them to question\narguments, without requiring any external knowledge. Research on CQs Generation\nusing LLMs requires a reference dataset for large scale experimentation. Thus,\nin this work we investigate two complementary methods to create such a\nresource: (i) instantiating CQs templates as defined by Walton's argumentation\ntheory and (ii), using LLMs as CQs generators. By doing so, we contribute with\na procedure to establish what is a valid CQ and conclude that, while LLMs are\nreasonable CQ generators, they still have a wide margin for improvement in this\ntask.\n","authors":["Blanca Calvo Figueras","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2410.14335v1.pdf","comment":"14 pages, 3 figures, 7 tables, to be published in the 28th Conference\n  on Computational Natural Language Learning (CoNLL 2024)"},{"id":"http://arxiv.org/abs/2410.11677v2","updated":"2024-10-18T09:41:53Z","published":"2024-10-15T15:14:22Z","title":"Understanding Likelihood Over-optimisation in Direct Alignment\n  Algorithms","summary":"  Direct Alignment Algorithms (DAAs), such as Direct Preference Optimisation\n(DPO) and Identity Preference Optimisation (IPO), have emerged as alternatives\nto online Reinforcement Learning from Human Feedback (RLHF) algorithms such as\nProximal Policy Optimisation (PPO) for aligning language models to human\npreferences, without the need for explicit reward modelling. These methods\ngenerally aim to increase the likelihood of generating better (preferred)\ncompletions while discouraging worse (non-preferred) ones, while staying close\nto the original model's behaviour. In this work, we explore the relationship\nbetween completion likelihood and model performance in state-of-the-art DAAs,\nand identify a critical issue of likelihood over-optimisation. Contrary to\nexpectations, we find that higher likelihood of better completions and larger\nmargins between better and worse completion likelihoods do not necessarily lead\nto better performance, and may even degrade it. Our analysis reveals that while\nhigher likelihood correlates with better memorisation of factual knowledge\npatterns, a slightly lower completion likelihood tends to improve output\ndiversity, thus leading to better generalisation to unseen scenarios. Moreover,\nwe identify two key indicators that signal when over-optimised output diversity\nbegins to harm performance: Decreasing Entropy over Top-k Tokens and\nDiminishing Top-k Probability Mass. Our experimental results validate that\nthese indicators are reliable signs of declining performance under different\nregularisations, helping prevent over-optimisation and improve alignment with\nhuman preferences.\n","authors":["Zhengyan Shi","Sander Land","Acyr Locatelli","Matthieu Geist","Max Bartolo"],"pdf_url":"https://arxiv.org/pdf/2410.11677v2.pdf","comment":"Preprint Version"},{"id":"http://arxiv.org/abs/2403.05902v2","updated":"2024-10-18T09:39:14Z","published":"2024-03-09T12:46:53Z","title":"MaiBaam Annotation Guidelines","summary":"  This document provides the annotation guidelines for MaiBaam, a Bavarian\ncorpus manually annotated with part-of-speech (POS) tags, syntactic\ndependencies, and German lemmas. MaiBaam belongs to the Universal Dependencies\n(UD) project, and our annotations elaborate on the general and German UD\nversion 2 guidelines. In this document, we detail how to preprocess and\ntokenize Bavarian data, provide an overview of the POS tags and dependencies we\nuse, explain annotation decisions that would also apply to closely related\nlanguages like German, and lastly we introduce and motivate decisions that are\nspecific to Bavarian grammar.\n","authors":["Verena Blaschke","Barbara Kovačić","Siyao Peng","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2403.05902v2.pdf","comment":"Updated for UD v2.15 (German lemmas added)"},{"id":"http://arxiv.org/abs/2410.10291v2","updated":"2024-10-18T09:26:46Z","published":"2024-10-14T08:45:35Z","title":"Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal\n  Perspective","summary":"  Accurate interpretation and visualization of human instructions are crucial\nfor text-to-image (T2I) synthesis. However, current models struggle to capture\nsemantic variations from word order changes, and existing evaluations, relying\non indirect metrics like text-image similarity, fail to reliably assess these\nchallenges. This often obscures poor performance on complex or uncommon\nlinguistic patterns by the focus on frequent word combinations. To address\nthese deficiencies, we propose a novel metric called SemVarEffect and a\nbenchmark named SemVarBench, designed to evaluate the causality between\nsemantic variations in inputs and outputs in T2I synthesis. Semantic variations\nare achieved through two types of linguistic permutations, while avoiding\neasily predictable literal variations. Experiments reveal that the\nCogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.\nSemantic variations in object relations are less understood than attributes,\nscoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in\nUNet or Transformers plays a crucial role in handling semantic variations, a\nfactor previously overlooked by a focus on textual encoders. Our work\nestablishes an effective evaluation framework that advances the T2I synthesis\ncommunity's exploration of human instruction understanding. Our benchmark and\ncode are available at https://github.com/zhuxiangru/SemVarBench .\n","authors":["Xiangru Zhu","Penglei Sun","Yaoxian Song","Yanghua Xiao","Zhixu Li","Chengyu Wang","Jun Huang","Bei Yang","Xiaoxiao Xu"],"pdf_url":"https://arxiv.org/pdf/2410.10291v2.pdf","comment":"The only change in the current version update is the replacement of\n  the template with a more precise one"},{"id":"http://arxiv.org/abs/2410.14309v1","updated":"2024-10-18T09:15:35Z","published":"2024-10-18T09:15:35Z","title":"LoGU: Long-form Generation with Uncertainty Expressions","summary":"  While Large Language Models (LLMs) demonstrate impressive capabilities, they\nstill struggle with generating factually incorrect content (i.e.,\nhallucinations). A promising approach to mitigate this issue is enabling models\nto express uncertainty when unsure. Previous research on uncertainty modeling\nhas primarily focused on short-form QA, but realworld applications often\nrequire much longer responses. In this work, we introduce the task of Long-form\nGeneration with Uncertainty(LoGU). We identify two key challenges: Uncertainty\nSuppression, where models hesitate to express uncertainty, and Uncertainty\nMisalignment, where models convey uncertainty inaccurately. To tackle these\nchallenges, we propose a refinement-based data collection framework and a\ntwo-stage training pipeline. Our framework adopts a divide-and-conquer\nstrategy, refining uncertainty based on atomic claims. The collected data are\nthen used in training through supervised fine-tuning (SFT) and direct\npreference optimization (DPO) to enhance uncertainty expression. Extensive\nexperiments on three long-form instruction following datasets show that our\nmethod significantly improves accuracy, reduces hallucinations, and maintains\nthe comprehensiveness of responses.\n","authors":["Ruihan Yang","Caiqi Zhang","Zhisong Zhang","Xinting Huang","Sen Yang","Nigel Collier","Dong Yu","Deqing Yang"],"pdf_url":"https://arxiv.org/pdf/2410.14309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05672v2","updated":"2024-10-18T09:12:45Z","published":"2023-06-09T05:04:13Z","title":"I run as fast as a rabbit, can you? A Multilingual Simile Dialogue\n  Dataset","summary":"  A simile is a figure of speech that compares two different things (called the\ntenor and the vehicle) via shared properties. The tenor and the vehicle are\nusually connected with comparator words such as \"like\" or \"as\". The simile\nphenomena are unique and complex in a real-life dialogue scene where the tenor\nand the vehicle can be verbal phrases or sentences, mentioned by different\nspeakers, exist in different sentences, or occur in reversed order. However,\nthe current simile research usually focuses on similes in a triplet tuple\n(tenor, property, vehicle) or a single sentence where the tenor and vehicle are\nusually entities or noun phrases, which could not reflect complex simile\nphenomena in real scenarios. In this paper, we propose a novel and high-quality\nmultilingual simile dialogue (MSD) dataset to facilitate the study of complex\nsimile phenomena. The MSD is the largest manually annotated simile data\n($\\sim$20K) and it contains both English and Chinese data. Meanwhile, the MSD\ndata can also be used on dialogue tasks to test the ability of dialogue systems\nwhen using similes. We design 3 simile tasks (recognition, interpretation, and\ngeneration) and 2 dialogue tasks (retrieval and generation) with MSD. For each\ntask, we provide experimental results from strong pre-trained or\nstate-of-the-art models. The experiments demonstrate the challenge of MSD and\nwe have released the data/code on GitHub.\n","authors":["Longxuan Ma","Weinan Zhang","Shuhan Zhou","Churui Sun","Changxin Ke","Ting Liu"],"pdf_url":"https://arxiv.org/pdf/2306.05672v2.pdf","comment":"13 Pages, 1 Figure, 12 Tables, ACL 2023 findings"},{"id":"http://arxiv.org/abs/2406.13632v3","updated":"2024-10-18T09:07:53Z","published":"2024-06-19T15:28:29Z","title":"Can Few-shot Work in Long-Context? Recycling the Context to Generate\n  Demonstrations","summary":"  Despite recent advancements in Large Language Models (LLMs), their\nperformance on tasks involving long contexts remains sub-optimal. In-Context\nLearning (ICL) with few-shot examples may be an appealing solution to enhance\nLLM performance in this scenario; However, na\\\"ively adding ICL examples with\nlong context introduces challenges, including substantial token overhead added\nfor each few-shot example and context mismatch between the demonstrations and\nthe target query. In this work, we propose to automatically generate few-shot\nexamples for long context QA tasks by recycling contexts. Specifically, given a\nlong input context (1-3k tokens) and a query, we generate additional\nquery-output pairs from the given context as few-shot examples, while\nintroducing the context only once. This ensures that the demonstrations are\nleveraging the same context as the target query while only adding a small\nnumber of tokens to the prompt. We further enhance each demonstration by\ninstructing the model to explicitly identify the relevant paragraphs before the\nanswer, which improves performance while providing fine-grained attribution to\nthe answer source. We apply our method on multiple LLMs and obtain substantial\nimprovements (+16 absolute points on average across models) on various QA\ndatasets with long context, especially when the answer lies within the middle\nof the context. Surprisingly, despite introducing only single-hop ICL examples,\nLLMs also successfully generalize to multi-hop long-context QA using our\napproach.\n","authors":["Arie Cattan","Alon Jacovi","Alex Fabrikant","Jonathan Herzig","Roee Aharoni","Hannah Rashkin","Dror Marcus","Avinatan Hassidim","Yossi Matias","Idan Szpektor","Avi Caciularu"],"pdf_url":"https://arxiv.org/pdf/2406.13632v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09075v3","updated":"2024-10-18T09:02:46Z","published":"2023-12-14T16:10:56Z","title":"Towards Verifiable Text Generation with Evolving Memory and\n  Self-Reflection","summary":"  Despite the remarkable ability of large language models (LLMs) in language\ncomprehension and generation, they often suffer from producing factually\nincorrect information, also known as hallucination. A promising solution to\nthis issue is verifiable text generation, which prompts LLMs to generate\ncontent with citations for accuracy verification. However, verifiable text\ngeneration is non-trivial due to the focus-shifting phenomenon, the intricate\nreasoning needed to align the claim with correct citations, and the dilemma\nbetween the precision and breadth of retrieved documents. In this paper, we\npresent VTG, an innovative framework for Verifiable Text Generation with\nevolving memory and self-reflection. VTG introduces evolving long short-term\nmemory to retain both valuable documents and recent documents. A two-tier\nverifier equipped with an evidence finder is proposed to rethink and reflect on\nthe relationship between the claim and citations. Furthermore, active retrieval\nand diverse query generation are utilized to enhance both the precision and\nbreadth of the retrieved documents. We conduct extensive experiments on five\ndatasets across three knowledge-intensive tasks and the results reveal that VTG\nsignificantly outperforms baselines.\n","authors":["Hao Sun","Hengyi Cai","Bo Wang","Yingyan Hou","Xiaochi Wei","Shuaiqiang Wang","Yan Zhang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2312.09075v3.pdf","comment":"EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.13824v2","updated":"2024-10-18T09:01:01Z","published":"2024-10-17T17:48:54Z","title":"Harnessing Webpage UIs for Text-Rich Visual Understanding","summary":"  Text-rich visual understanding-the ability to process environments where\ndense textual content is integrated with visuals-is crucial for multimodal\nlarge language models (MLLMs) to interact effectively with structured\nenvironments. To enhance this capability, we propose synthesizing general\nmultimodal instructions from webpage UIs using text-based large language models\n(LLMs). Despite lacking direct visual input, text-based LLMs are able to\nprocess structured text representations from webpage accessibility trees. These\ninstructions are then paired with UI screenshots to train multimodal models. We\nintroduce MultiUI, a dataset containing 7.3 million samples from 1 million\nwebsites, covering diverse multimodal tasks and UI layouts. Models trained on\nMultiUI not only excel in web UI tasks-achieving up to a 48% improvement on\nVisualWebBench and a 19.1% boost in element accuracy on a web agent dataset\nMind2Web-but also generalize surprisingly well to non-web UI tasks and even to\nnon-UI domains, such as document understanding, OCR, and chart interpretation.\nThese results highlight the broad applicability of web UI data for advancing\ntext-rich visual understanding across various scenarios.\n","authors":["Junpeng Liu","Tianyue Ou","Yifan Song","Yuxiao Qu","Wai Lam","Chenyan Xiong","Wenhu Chen","Graham Neubig","Xiang Yue"],"pdf_url":"https://arxiv.org/pdf/2410.13824v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12013v2","updated":"2024-10-18T08:57:30Z","published":"2024-06-26T12:33:34Z","title":"Dating ancient manuscripts using radiocarbon and AI-based writing style\n  analysis","summary":"  Determining the chronology of ancient handwritten manuscripts is essential\nfor reconstructing the evolution of ideas. For the Dead Sea Scrolls, this is\nparticularly important. However, there is an almost complete lack of\ndate-bearing manuscripts evenly distributed across the timeline and written in\nsimilar scripts available for palaeographic comparison. Here, we present Enoch,\na state-of-the-art AI-based date-prediction model, trained on the basis of new\nradiocarbon-dated samples of the scrolls. Enoch uses established\nhandwriting-style descriptors and applies Bayesian ridge regression. The\nchallenge of this study is that the number of radiocarbon-dated manuscripts is\nsmall, while current machine learning requires an abundance of training data.\nWe show that by using combined angular and allographic writing style feature\nvectors and applying Bayesian ridge regression, Enoch could predict the\nradiocarbon-based dates from style, supported by leave-one-out validation, with\nvaried MAEs of 27.9 to 30.7 years relative to the radiocarbon dating. Enoch was\nthen used to estimate the dates of 135 unseen manuscripts, revealing that 79\nper cent of the samples were considered 'realistic' upon palaeographic post-hoc\nevaluation. We present a new chronology of the scrolls. The radiocarbon ranges\nand Enoch's style-based predictions are often older than the traditionally\nassumed palaeographic estimates. In the range of 300-50 BCE, Enoch's date\nprediction provides an improved granularity. The study is in line with current\ndevelopments in multimodal machine-learning techniques, and the methods can be\nused for date prediction in other partially-dated manuscript collections. This\nresearch shows how Enoch's quantitative, probability-based approach can be a\ntool for palaeographers and historians, re-dating ancient Jewish key texts and\ncontributing to current debates on Jewish and Christian origins.\n","authors":["Mladen Popović","Maruf A. Dhali","Lambert Schomaker","Johannes van der Plicht","Kaare Lund Rasmussen","Jacopo La Nasa","Ilaria Degano","Maria Perla Colombini","Eibert Tigchelaar"],"pdf_url":"https://arxiv.org/pdf/2407.12013v2.pdf","comment":"16 pages of main article, 103 pages of supplementary materials; the\n  first version of this article is originally prepared in July 2023 after the\n  completion of all the experiments"},{"id":"http://arxiv.org/abs/2310.14626v2","updated":"2024-10-18T08:56:18Z","published":"2023-10-23T07:00:51Z","title":"Conversational Recommender System and Large Language Model Are Made for\n  Each Other in E-commerce Pre-sales Dialogue","summary":"  E-commerce pre-sales dialogue aims to understand and elicit user needs and\npreferences for the items they are seeking so as to provide appropriate\nrecommendations. Conversational recommender systems (CRSs) learn user\nrepresentation and provide accurate recommendations based on dialogue context,\nbut rely on external knowledge. Large language models (LLMs) generate responses\nthat mimic pre-sales dialogues after fine-tuning, but lack domain-specific\nknowledge for accurate recommendations. Intuitively, the strengths of LLM and\nCRS in E-commerce pre-sales dialogues are complementary, yet no previous work\nhas explored this. This paper investigates the effectiveness of combining LLM\nand CRS in E-commerce pre-sales dialogues, proposing two collaboration methods:\nCRS assisting LLM and LLM assisting CRS. We conduct extensive experiments on a\nreal-world dataset of Ecommerce pre-sales dialogues. We analyze the impact of\ntwo collaborative approaches with two CRSs and two LLMs on four tasks of\nEcommerce pre-sales dialogue. We find that collaborations between CRS and LLM\ncan be very effective in some cases.\n","authors":["Yuanxing Liu","Wei-Nan Zhang","Yifan Chen","Yuchi Zhang","Haopeng Bai","Fan Feng","Hengbin Cui","Yongbin Li","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2310.14626v2.pdf","comment":"EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2405.20680v4","updated":"2024-10-18T08:54:37Z","published":"2024-05-31T08:22:49Z","title":"Unraveling and Mitigating Retriever Inconsistencies in\n  Retrieval-Augmented Large Language Models","summary":"  Although Retrieval-Augmented Large Language Models (RALMs) demonstrate their\nsuperiority in terms of factuality, they do not consistently outperform the\noriginal retrieval-free Language Models (LMs). Our experiments reveal that this\nexample-level performance inconsistency exists not only between\nretrieval-augmented and retrieval-free LM but also among different retrievers.\nTo understand this phenomenon, we investigate the degeneration behavior of\nRALMs and theoretically decompose it into four categories. Further analysis\nbased on our decomposition reveals that the innate difference in knowledge\nsources and the unpredictable degeneration of the reader model contribute most\nto the inconsistency. Drawing from our analysis, we introduce Ensemble of\nRetrievers (EoR), a trainable framework that can adaptively retrieve from\ndifferent knowledge sources and effectively decrease unpredictable reader\nerrors. Our experiments on Open Domain Question Answering show that EoR\nsubstantially improves performance over the RALM with a single retriever by\nconsiderably reducing inconsistent behaviors.\n","authors":["Mingda Li","Xinyu Li","Yifan Chen","Wenfeng Xuan","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.20680v4.pdf","comment":"ACL 2024 (findings)"},{"id":"http://arxiv.org/abs/2406.15053v2","updated":"2024-10-18T08:51:55Z","published":"2024-06-21T11:00:38Z","title":"PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement\n  on Multilingual and Multi-Cultural Data","summary":"  Evaluation of multilingual Large Language Models (LLMs) is challenging due to\na variety of factors -- the lack of benchmarks with sufficient linguistic\ndiversity, contamination of popular benchmarks into LLM pre-training data and\nthe lack of local, cultural nuances in translated benchmarks. In this work, we\nstudy human and LLM-based evaluation in a multilingual, multi-cultural setting.\nWe evaluate 30 models across 10 Indic languages by conducting 90K human\nevaluations and 30K LLM-based evaluations and find that models such as GPT-4o\nand Llama-3 70B consistently perform best for most Indic languages. We build\nleaderboards for two evaluation settings - pairwise comparison and direct\nassessment and analyze the agreement between humans and LLMs. We find that\nhumans and LLMs agree fairly well in the pairwise setting but the agreement\ndrops for direct assessment evaluation especially for languages such as Bengali\nand Odia. We also check for various biases in human and LLM-based evaluation\nand find evidence of self-bias in the GPT-based evaluator. Our work presents a\nsignificant step towards scaling up multilingual evaluation of LLMs.\n","authors":["Ishaan Watts","Varun Gumma","Aditya Yadavalli","Vivek Seshadri","Manohar Swaminathan","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2406.15053v2.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.14289v1","updated":"2024-10-18T08:49:24Z","published":"2024-10-18T08:49:24Z","title":"SwaQuAD-24: QA Benchmark Dataset in Swahili","summary":"  This paper proposes the creation of a Swahili Question Answering (QA)\nbenchmark dataset, aimed at addressing the underrepresentation of Swahili in\nnatural language processing (NLP). Drawing from established benchmarks like\nSQuAD, GLUE, KenSwQuAD, and KLUE, the dataset will focus on providing\nhigh-quality, annotated question-answer pairs that capture the linguistic\ndiversity and complexity of Swahili. The dataset is designed to support a\nvariety of applications, including machine translation, information retrieval,\nand social services like healthcare chatbots. Ethical considerations, such as\ndata privacy, bias mitigation, and inclusivity, are central to the dataset\ndevelopment. Additionally, the paper outlines future expansion plans to include\ndomain-specific content, multimodal integration, and broader crowdsourcing\nefforts. The Swahili QA dataset aims to foster technological innovation in East\nAfrica and provide an essential resource for NLP research and applications in\nlow-resource languages.\n","authors":["Alfred Malengo Kondoro"],"pdf_url":"https://arxiv.org/pdf/2410.14289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14276v1","updated":"2024-10-18T08:31:22Z","published":"2024-10-18T08:31:22Z","title":"EcomEdit: An Automated E-commerce Knowledge Editing Framework for\n  Enhanced Product and Purchase Intention Understanding","summary":"  Knowledge Editing (KE) aims to correct and update factual information in\nLarge Language Models (LLMs) to ensure accuracy and relevance without\ncomputationally expensive fine-tuning. Though it has been proven effective in\nseveral domains, limited work has focused on its application within the\ne-commerce sector. However, there are naturally occurring scenarios that make\nKE necessary in this domain, such as the timely updating of product features\nand trending purchase intentions by customers, which necessitate further\nexploration. In this paper, we pioneer the application of KE in the e-commerce\ndomain by presenting ECOMEDIT, an automated e-commerce knowledge editing\nframework tailored for e-commerce-related knowledge and tasks. Our framework\nleverages more powerful LLMs as judges to enable automatic knowledge conflict\ndetection and incorporates conceptualization to enhance the semantic coverage\nof the knowledge to be edited. Through extensive experiments, we demonstrate\nthe effectiveness of ECOMEDIT in improving LLMs' understanding of product\ndescriptions and purchase intentions. We also show that LLMs, after our\nediting, can achieve stronger performance on downstream e-commerce tasks.\n","authors":["Ching Ming Samuel Lau","Weiqi Wang","Haochen Shi","Baixuan Xu","Jiaxin Bai","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2410.14276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14273v1","updated":"2024-10-18T08:27:02Z","published":"2024-10-18T08:27:02Z","title":"REEF: Representation Encoding Fingerprints for Large Language Models","summary":"  Protecting the intellectual property of open-source Large Language Models\n(LLMs) is very important, because training LLMs costs extensive computational\nresources and data. Therefore, model owners and third parties need to identify\nwhether a suspect model is a subsequent development of the victim model. To\nthis end, we propose a training-free REEF to identify the relationship between\nthe suspect and victim models from the perspective of LLMs' feature\nrepresentations. Specifically, REEF computes and compares the centered kernel\nalignment similarity between the representations of a suspect model and a\nvictim model on the same samples. This training-free REEF does not impair the\nmodel's general capabilities and is robust to sequential fine-tuning, pruning,\nmodel merging, and permutations. In this way, REEF provides a simple and\neffective way for third parties and models' owners to protect LLMs'\nintellectual property together. The code is available at\nhttps://github.com/tmylla/REEF.\n","authors":["Jie Zhang","Dongrui Liu","Chen Qian","Linfeng Zhang","Yong Liu","Yu Qiao","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2410.14273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14268v1","updated":"2024-10-18T08:22:07Z","published":"2024-10-18T08:22:07Z","title":"MoDification: Mixture of Depths Made Easy","summary":"  Long-context efficiency has recently become a trending topic in serving large\nlanguage models (LLMs). And mixture of depths (MoD) is proposed as a perfect\nfit to bring down both latency and memory. In this paper, however, we discover\nthat MoD can barely transform existing LLMs without costly training over an\nextensive number of tokens. To enable the transformations from any LLMs to MoD\nones, we showcase top-k operator in MoD should be promoted to threshold-p\noperator, and refinement to architecture and data should also be crafted along.\nAll these designs form our method termed MoDification. Through a comprehensive\nset of experiments covering model scales from 3B to 70B, we exhibit\nMoDification strikes an excellent balance between efficiency and effectiveness.\nMoDification can achieve up to ~1.2x speedup in latency and ~1.8x reduction in\nmemory compared to original LLMs especially in long-context applications.\n","authors":["Chen Zhang","Meizhi Zhong","Qimeng Wang","Xuantao Lu","Zheyu Ye","Chengqiang Lu","Yan Gao","Yao Hu","Kehai Chen","Min Zhang","Dawei Song"],"pdf_url":"https://arxiv.org/pdf/2410.14268v1.pdf","comment":"12 pages, 9 figures, 5 tables, work in progress"},{"id":"http://arxiv.org/abs/2406.06572v2","updated":"2024-10-18T08:20:38Z","published":"2024-06-03T17:07:46Z","title":"Graph Neural Network Enhanced Retrieval for Question Answering of LLMs","summary":"  Retrieval augmented generation has revolutionized large language model (LLM)\noutputs by providing factual supports. Nevertheless, it struggles to capture\nall the necessary knowledge for complex reasoning questions. Existing retrieval\nmethods typically divide reference documents into passages, treating them in\nisolation. These passages, however, are often interrelated, such as passages\nthat are contiguous or share the same keywords. Therefore, it is crucial to\nrecognize such relatedness for enhancing the retrieval process. In this paper,\nwe propose a novel retrieval method, called GNN-Ret, which leverages graph\nneural networks (GNNs) to enhance retrieval by exploiting the relatedness\nbetween passages. Specifically, we first construct a graph of passages by\nconnecting passages that are structure-related or keyword-related. A graph\nneural network (GNN) is then leveraged to exploit the relationships between\npassages and improve the retrieval of supporting passages. Furthermore, we\nextend our method to handle multi-hop reasoning questions using a recurrent\ngraph neural network (RGNN), named RGNN-Ret. At each step, RGNN-Ret integrates\nthe graphs of passages from previous steps, thereby enhancing the retrieval of\nsupporting passages. Extensive experiments on benchmark datasets demonstrate\nthat GNN-Ret achieves higher accuracy for question answering with a single\nquery of LLMs than strong baselines that require multiple queries, and RGNN-Ret\nfurther improves accuracy and achieves state-of-the-art performance, with up to\n10.4% accuracy improvement on the 2WikiMQA dataset.\n","authors":["Zijian Li","Qingyan Guo","Jiawei Shao","Lei Song","Jiang Bian","Jun Zhang","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2406.06572v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.14262v1","updated":"2024-10-18T08:18:18Z","published":"2024-10-18T08:18:18Z","title":"Good Parenting is all you need -- Multi-agentic LLM Hallucination\n  Mitigation","summary":"  This study explores the ability of Large Language Model (LLM) agents to\ndetect and correct hallucinations in AI-generated content. A primary agent was\ntasked with creating a blog about a fictional Danish artist named Flipfloppidy,\nwhich was then reviewed by another agent for factual inaccuracies. Most LLMs\nhallucinated the existence of this artist. Across 4,900 test runs involving\nvarious combinations of primary and reviewing agents, advanced AI models such\nas Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in\nidentifying hallucinations and successfully revised outputs in 85% to 100% of\ncases following feedback. These findings underscore the potential of advanced\nAI models to significantly enhance the accuracy and reliability of generated\ncontent, providing a promising approach to improving AI workflow orchestration.\n","authors":[" Edward"," Kwartler","Matthew Berman","Alan Aqrawi"],"pdf_url":"https://arxiv.org/pdf/2410.14262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14259v1","updated":"2024-10-18T08:14:10Z","published":"2024-10-18T08:14:10Z","title":"Beyond Binary: Towards Fine-Grained LLM-Generated Text Detection via\n  Role Recognition and Involvement Measurement","summary":"  The rapid development of large language models (LLMs), like ChatGPT, has\nresulted in the widespread presence of LLM-generated content on social media\nplatforms, raising concerns about misinformation, data biases, and privacy\nviolations, which can undermine trust in online discourse. While detecting\nLLM-generated content is crucial for mitigating these risks, current methods\noften focus on binary classification, failing to address the complexities of\nreal-world scenarios like human-AI collaboration. To move beyond binary\nclassification and address these challenges, we propose a new paradigm for\ndetecting LLM-generated content. This approach introduces two novel tasks: LLM\nRole Recognition (LLM-RR), a multi-class classification task that identifies\nspecific roles of LLM in content generation, and LLM Influence Measurement\n(LLM-IM), a regression task that quantifies the extent of LLM involvement in\ncontent creation. To support these tasks, we propose LLMDetect, a benchmark\ndesigned to evaluate detectors' performance on these new tasks. LLMDetect\nincludes the Hybrid News Detection Corpus (HNDC) for training detectors, as\nwell as DetectEval, a comprehensive evaluation suite that considers five\ndistinct cross-context variations and multi-intensity variations within the\nsame LLM role. This allows for a thorough assessment of detectors'\ngeneralization and robustness across diverse contexts. Our empirical validation\nof 10 baseline detection methods demonstrates that fine-tuned PLM-based models\nconsistently outperform others on both tasks, while advanced LLMs face\nchallenges in accurately detecting their own generated content. Our\nexperimental results and analysis offer insights for developing more effective\ndetection models for LLM-generated content. This research enhances the\nunderstanding of LLM-generated content and establishes a foundation for more\nnuanced detection methodologies.\n","authors":["Zihao Cheng","Li Zhou","Feng Jiang","Benyou Wang","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2410.14259v1.pdf","comment":"Social Media, Large Language Models, LLM-generated Text Detection,\n  AI-assisted News Detection"},{"id":"http://arxiv.org/abs/2410.14255v1","updated":"2024-10-18T08:04:36Z","published":"2024-10-18T08:04:36Z","title":"Nova: An Iterative Planning and Search Approach to Enhance Novelty and\n  Diversity of LLM Generated Ideas","summary":"  Scientific innovation is pivotal for humanity, and harnessing large language\nmodels (LLMs) to generate research ideas could transform discovery. However,\nexisting LLMs often produce simplistic and repetitive suggestions due to their\nlimited ability in acquiring external knowledge for innovation. To address this\nproblem, we introduce an enhanced planning and search methodology designed to\nboost the creative potential of LLM-based systems. Our approach involves an\niterative process to purposely plan the retrieval of external knowledge,\nprogressively enriching the idea generation with broader and deeper insights.\nValidation through automated and human assessments indicates that our framework\nsubstantially elevates the quality of generated ideas, particularly in novelty\nand diversity. The number of unique novel ideas produced by our framework is\n3.4 times higher than without it. Moreover, our method outperforms the current\nstate-of-the-art, generating at least 2.5 times more top-rated ideas based on\n170 seed papers in a Swiss Tournament evaluation.\n","authors":["Xiang Hu","Hongyu Fu","Jinge Wang","Yifeng Wang","Zhikun Li","Renjun Xu","Yu Lu","Yaochu Jin","Lili Pan","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2410.14255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17524v4","updated":"2024-10-18T08:03:02Z","published":"2024-04-26T16:41:00Z","title":"On the Use of Large Language Models to Generate Capability Ontologies","summary":"  Capability ontologies are increasingly used to model functionalities of\nsystems or machines. The creation of such ontological models with all\nproperties and constraints of capabilities is very complex and can only be done\nby ontology experts. However, Large Language Models (LLMs) have shown that they\ncan generate machine-interpretable models from natural language text input and\nthus support engineers / ontology experts. Therefore, this paper investigates\nhow LLMs can be used to create capability ontologies. We present a study with a\nseries of experiments in which capabilities with varying complexities are\ngenerated using different prompting techniques and with different LLMs. Errors\nin the generated ontologies are recorded and compared. To analyze the quality\nof the generated ontologies, a semi-automated approach based on RDF syntax\nchecking, OWL reasoning, and SHACL constraints is used. The results of this\nstudy are very promising because even for complex capabilities, the generated\nontologies are almost free of errors.\n","authors":["Luis Miguel Vieira da Silva","Aljosha Köcher","Felix Gehlhoff","Alexander Fay"],"pdf_url":"https://arxiv.org/pdf/2404.17524v4.pdf","comment":"\\c{opyright} 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2410.14251v1","updated":"2024-10-18T08:01:39Z","published":"2024-10-18T08:01:39Z","title":"Synthesizing Post-Training Data for LLMs through Multi-Agent Simulation","summary":"  Post-training is essential for enabling large language models (LLMs) to\nfollow human instructions. Inspired by the recent success of using LLMs to\nsimulate human society, we leverage multi-agent simulation to automatically\ngenerate diverse text-based scenarios, capturing a wide range of real-world\nhuman needs. We propose MATRIX, a multi-agent simulator that creates realistic\nand scalable scenarios. Leveraging these outputs, we introduce a novel\nscenario-driven instruction generator MATRIX-Gen for controllable and highly\nrealistic data synthesis. Extensive experiments demonstrate that our framework\neffectively generates both general and domain-specific data. Notably, on\nAlpacaEval 2 and Arena-Hard benchmarks, Llama-3-8B-Base, post-trained on\ndatasets synthesized by MATRIX-Gen with just 20K instruction-response pairs,\noutperforms Meta's Llama-3-8B-Instruct model, which was trained on over 10M\npairs; see our project at https://github.com/ShuoTang123/MATRIX-Gen.\n","authors":["Shuo Tang","Xianghe Pang","Zexi Liu","Bohan Tang","Rui Ye","Xiaowen Dong","Yanfeng Wang","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14248v1","updated":"2024-10-18T07:52:22Z","published":"2024-10-18T07:52:22Z","title":"Addressing Blind Guessing: Calibration of Selection Bias in\n  Multiple-Choice Question Answering by Video Language Models","summary":"  Evaluating Video Language Models (VLMs) is a challenging task. Due to its\ntransparency, Multiple-Choice Question Answering (MCQA) is widely used to\nmeasure the performance of these models through accuracy. However, existing\nMCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to\nselection bias, when models disproportionately favor certain answer options\nbased on positional patterns observed during training. In this work, we conduct\na comprehensive empirical analysis of several VLM architectures across major\ndatasets designed to assess complex video-focused reasoning. We identify where\nthe bias is most pronounced and demonstrate to what extent model responses\nreflect genuine understanding of video content and related questions, as\nopposed to reliance on arbitrary patterns or superficial cues, such as answer\nposition. By decomposing the MCQA task and adapting fairness bias metrics to\nVLMs, we introduce a post-processing calibration technique BOLD to balance this\nbias. Our results show that reducing selection bias improves not only debiasing\nmetrics but also overall model performance, including Accuracy and F1 Mean\nscore. Our method, by suppressing \"blind guessing\", offers a more cost- and\ntime-effective approach to mitigating selection bias compared to existing\ntechniques. This study represents the first focused investigation of selection\nbias in video-to-text LLM-powered models.\n","authors":["Olga Loginova","Oleksandr Bezrukov","Alexey Kravets"],"pdf_url":"https://arxiv.org/pdf/2410.14248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14236v1","updated":"2024-10-18T07:36:57Z","published":"2024-10-18T07:36:57Z","title":"A Novel Method to Metigate Demographic and Expert Bias in ICD Coding\n  with Causal Inference","summary":"  ICD(International Classification of Diseases) coding involves assigning ICD\ncodes to patients visit based on their medical notes. Considering ICD coding as\na multi-label text classification task, researchers have developed\nsophisticated methods. Despite progress, these models often suffer from label\nimbalance and may develop spurious correlations with demographic factors.\nAdditionally, while human coders assign ICD codes, the inclusion of irrelevant\ninformation from unrelated experts introduces biases. To combat these issues,\nwe propose a novel method to mitigate Demographic and Expert biases in ICD\ncoding through Causal Inference (DECI). We provide a novel causality-based\ninterpretation in ICD Coding that models make predictions by three distinct\npathways. And based counterfactual reasoning, DECI mitigate demographic and\nexpert biases. Experimental results show that DECI outperforms state-of-the-art\nmodels, offering a significant advancement in accurate and unbiased ICD coding.\n","authors":["Bin Zhang","Junli Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07962v2","updated":"2024-10-18T07:34:39Z","published":"2024-06-12T07:41:44Z","title":"Toward a Method to Generate Capability Ontologies from Natural Language\n  Descriptions","summary":"  To achieve a flexible and adaptable system, capability ontologies are\nincreasingly leveraged to describe functions in a machine-interpretable way.\nHowever, modeling such complex ontological descriptions is still a manual and\nerror-prone task that requires a significant amount of effort and ontology\nexpertise. This contribution presents an innovative method to automate\ncapability ontology modeling using Large Language Models (LLMs), which have\nproven to be well suited for such tasks. Our approach requires only a natural\nlanguage description of a capability, which is then automatically inserted into\na predefined prompt using a few-shot prompting technique. After prompting an\nLLM, the resulting capability ontology is automatically verified through\nvarious steps in a loop with the LLM to check the overall correctness of the\ncapability ontology. First, a syntax check is performed, then a check for\ncontradictions, and finally a check for hallucinations and missing ontology\nelements. Our method greatly reduces manual effort, as only the initial natural\nlanguage description and a final human review and possible correction are\nnecessary, thereby streamlining the capability ontology generation process.\n","authors":["Luis Miguel Vieira da Silva","Aljosha Köcher","Felix Gehlhoff","Alexander Fay"],"pdf_url":"https://arxiv.org/pdf/2406.07962v2.pdf","comment":"\\c{opyright} 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2410.14235v1","updated":"2024-10-18T07:34:21Z","published":"2024-10-18T07:34:21Z","title":"Towards Robust Knowledge Representations in Multilingual LLMs for\n  Equivalence and Inheritance based Consistent Reasoning","summary":"  Reasoning and linguistic skills form the cornerstone of human intelligence,\nfacilitating problem-solving and decision-making. Recent advances in Large\nLanguage Models (LLMs) have led to impressive linguistic capabilities and\nemergent reasoning behaviors, fueling widespread adoption across application\ndomains. However, LLMs still struggle with complex reasoning tasks,\nhighlighting their systemic limitations. In this work, we focus on evaluating\nwhether LLMs have the requisite representations to reason using two\nfoundational relationships: \"equivalence\" and \"inheritance\". We introduce novel\ntasks and benchmarks spanning six languages and observe that current SOTA LLMs\noften produce conflicting answers to the same questions across languages in\n17.3-57.5% of cases and violate inheritance constraints in up to 37.2% cases.\nTo enhance consistency across languages, we propose novel \"Compositional\nRepresentations\" where tokens are represented as composition of equivalent\ntokens across languages, with resulting conflict reduction (up to -4.7%)\nindicating benefits of shared LLM representations.\n","authors":["Gaurav Arora","Srujana Merugu","Shreya Jain","Vaibhav Saxena"],"pdf_url":"https://arxiv.org/pdf/2410.14235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14231v1","updated":"2024-10-18T07:25:00Z","published":"2024-10-18T07:25:00Z","title":"Unveiling Large Language Models Generated Texts: A Multi-Level\n  Fine-Grained Detection Framework","summary":"  Large language models (LLMs) have transformed human writing by enhancing\ngrammar correction, content expansion, and stylistic refinement. However, their\nwidespread use raises concerns about authorship, originality, and ethics, even\npotentially threatening scholarly integrity. Existing detection methods, which\nmainly rely on single-feature analysis and binary classification, often fail to\neffectively identify LLM-generated text in academic contexts. To address these\nchallenges, we propose a novel Multi-level Fine-grained Detection (MFD)\nframework that detects LLM-generated text by integrating low-level structural,\nhigh-level semantic, and deep-level linguistic features, while conducting\nsentence-level evaluations of lexicon, grammar, and syntax for comprehensive\nanalysis. To improve detection of subtle differences in LLM-generated text and\nenhance robustness against paraphrasing, we apply two mainstream evasion\ntechniques to rewrite the text. These variations, along with original texts,\nare used to train a text encoder via contrastive learning, extracting\nhigh-level semantic features of sentence to boost detection generalization.\nFurthermore, we leverage advanced LLM to analyze the entire text and extract\ndeep-level linguistic features, enhancing the model's ability to capture\ncomplex patterns and nuances while effectively incorporating contextual\ninformation. Extensive experiments on public datasets show that the MFD model\noutperforms existing methods, achieving an MAE of 0.1346 and an accuracy of\n88.56%. Our research provides institutions and publishers with an effective\nmechanism to detect LLM-generated text, mitigating risks of compromised\nauthorship. Educators and editors can use the model's predictions to refine\nverification and plagiarism prevention protocols, ensuring adherence to\nstandards.\n","authors":["Zhen Tao","Zhiyu Li","Runyu Chen","Dinghao Xi","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2410.14231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14225v1","updated":"2024-10-18T07:14:54Z","published":"2024-10-18T07:14:54Z","title":"Few-Shot Joint Multimodal Entity-Relation Extraction via\n  Knowledge-Enhanced Cross-modal Prompt Model","summary":"  Joint Multimodal Entity-Relation Extraction (JMERE) is a challenging task\nthat aims to extract entities and their relations from text-image pairs in\nsocial media posts. Existing methods for JMERE require large amounts of labeled\ndata. However, gathering and annotating fine-grained multimodal data for JMERE\nposes significant challenges. Initially, we construct diverse and comprehensive\nmultimodal few-shot datasets fitted to the original data distribution. To\naddress the insufficient information in the few-shot setting, we introduce the\n\\textbf{K}nowledge-\\textbf{E}nhanced \\textbf{C}ross-modal \\textbf{P}rompt\n\\textbf{M}odel (KECPM) for JMERE. This method can effectively address the\nproblem of insufficient information in the few-shot setting by guiding a large\nlanguage model to generate supplementary background knowledge. Our proposed\nmethod comprises two stages: (1) a knowledge ingestion stage that dynamically\nformulates prompts based on semantic similarity guide ChatGPT generating\nrelevant knowledge and employs self-reflection to refine the knowledge; (2) a\nknowledge-enhanced language model stage that merges the auxiliary knowledge\nwith the original input and utilizes a transformer-based model to align with\nJMERE's required output format. We extensively evaluate our approach on a\nfew-shot dataset derived from the JMERE dataset, demonstrating its superiority\nover strong baselines in terms of both micro and macro F$_1$ scores.\nAdditionally, we present qualitative analyses and case studies to elucidate the\neffectiveness of our model.\n","authors":["Li Yuan","Yi Cai","Junsheng Huang"],"pdf_url":"https://arxiv.org/pdf/2410.14225v1.pdf","comment":"accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2409.09785v3","updated":"2024-10-18T07:11:35Z","published":"2024-09-15T16:32:49Z","title":"Large Language Model Based Generative Error Correction: A Challenge and\n  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition","summary":"  Given recent advances in generative AI technology, a key question is how\nlarge language models (LLMs) can enhance acoustic modeling tasks using text\ndecoding results from a frozen, pretrained automatic speech recognition (ASR)\nmodel. To explore new capabilities in language modeling for speech processing,\nwe introduce the generative speech transcription error correction (GenSEC)\nchallenge. This challenge comprises three post-ASR language modeling tasks: (i)\npost-ASR transcription correction, (ii) speaker tagging, and (iii) emotion\nrecognition. These tasks aim to emulate future LLM-based agents handling\nvoice-based interfaces while remaining accessible to a broad audience by\nutilizing open pretrained language models or agent-based APIs. We also discuss\ninsights from baseline evaluations, as well as lessons learned for designing\nfuture evaluations.\n","authors":["Chao-Han Huck Yang","Taejin Park","Yuan Gong","Yuanchao Li","Zhehuai Chen","Yen-Ting Lin","Chen Chen","Yuchen Hu","Kunal Dhawan","Piotr Żelasko","Chao Zhang","Yun-Nung Chen","Yu Tsao","Jagadeesh Balam","Boris Ginsburg","Sabato Marco Siniscalchi","Eng Siong Chng","Peter Bell","Catherine Lai","Shinji Watanabe","Andreas Stolcke"],"pdf_url":"https://arxiv.org/pdf/2409.09785v3.pdf","comment":"IEEE SLT 2024. The initial draft version has been done in December\n  2023. Post-ASR Text Processing and Understanding Community and LlaMA-7B\n  pre-training correction model:\n  https://huggingface.co/GenSEC-LLM/SLT-Task1-Llama2-7b-HyPo-baseline"},{"id":"http://arxiv.org/abs/2410.09421v2","updated":"2024-10-18T07:10:38Z","published":"2024-10-12T07:56:47Z","title":"VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language\n  Models Alignment","summary":"  As large vision-language models (LVLMs) evolve rapidly, the demand for\nhigh-quality and diverse data to align these models becomes increasingly\ncrucial. However, the creation of such data with human supervision proves\ncostly and time-intensive. In this paper, we investigate the efficacy of AI\nfeedback to scale supervision for aligning LVLMs. We introduce VLFeedback, the\nfirst large-scale vision-language feedback dataset, comprising over 82K\nmulti-modal instructions and comprehensive rationales generated by\noff-the-shelf models without human annotations. To evaluate the effectiveness\nof AI feedback for vision-language alignment, we train Silkie, an LVLM\nfine-tuned via direct preference optimization on VLFeedback. Silkie showcases\nexceptional performance regarding helpfulness, visual faithfulness, and safety\nmetrics. It outperforms its base model by 6.9\\% and 9.5\\% in perception and\ncognition tasks, reduces hallucination issues on MMHal-Bench, and exhibits\nenhanced resilience against red-teaming attacks. Furthermore, our analysis\nunderscores the advantage of AI feedback, particularly in fostering preference\ndiversity to deliver more comprehensive improvements. Our dataset, training\ncode and models are available at https://vlf-silkie.github.io.\n","authors":["Lei Li","Zhihui Xie","Mukai Li","Shunian Chen","Peiyi Wang","Liang Chen","Yazheng Yang","Benyou Wang","Lingpeng Kong","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2410.09421v2.pdf","comment":"EMNLP 2024 Main Conference camera-ready version (fixed small typos).\n  This article supersedes arXiv:2312.10665"},{"id":"http://arxiv.org/abs/2403.04808v3","updated":"2024-10-18T07:05:57Z","published":"2024-03-06T10:55:30Z","title":"WaterMax: breaking the LLM watermark detectability-robustness-quality\n  trade-off","summary":"  Watermarking is a technical means to dissuade malfeasant usage of Large\nLanguage Models. This paper proposes a novel watermarking scheme, so-called\nWaterMax, that enjoys high detectability while sustaining the quality of the\ngenerated text of the original LLM. Its new design leaves the LLM untouched (no\nmodification of the weights, logits, temperature, or sampling technique).\nWaterMax balances robustness and complexity contrary to the watermarking\ntechniques of the literature inherently provoking a trade-off between quality\nand robustness. Its performance is both theoretically proven and experimentally\nvalidated. It outperforms all the SotA techniques under the most complete\nbenchmark suite. Code available at https://github.com/eva-giboulot/WaterMax.\n","authors":["Eva Giboulot","Teddy Furon"],"pdf_url":"https://arxiv.org/pdf/2403.04808v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14024v4","updated":"2024-10-18T06:59:24Z","published":"2024-06-20T06:42:27Z","title":"LLM Critics Help Catch Bugs in Mathematics: Towards a Better\n  Mathematical Verifier with Natural Language Feedback","summary":"  In recent progress, mathematical verifiers have achieved success in\nmathematical reasoning tasks by validating the correctness of solutions\ngenerated by policy models. However, existing verifiers are trained with binary\nclassification labels, which are not informative enough for the model to\naccurately assess the solutions. To mitigate the aforementioned insufficiency\nof binary labels, we introduce step-wise natural language feedback as rationale\nlabels, that is, the correctness of each step and the detailed explanations. In\nthis paper, we propose Math-Minos, a natural language feedback-enhanced\nverifier by constructing automatically generated training data and a two-stage\ntraining paradigm for effective training and efficient inference. Our\nexperiments reveal that a small set of natural language feedback can\nsignificantly boost the performance of the verifier in both verification and\nreinforcement learning. We have released the code and data for further\nexploration.\n","authors":["Bofei Gao","Zefan Cai","Runxin Xu","Peiyi Wang","Ce Zheng","Runji Lin","Keming Lu","Dayiheng Liu","Chang Zhou","Wen Xiao","Junjie Hu","Tianyu Liu","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2406.14024v4.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2410.14211v1","updated":"2024-10-18T06:57:19Z","published":"2024-10-18T06:57:19Z","title":"Paths-over-Graph: Knowledge Graph Enpowered Large Language Model\n  Reasoning","summary":"  Large Language Models (LLMs) have achieved impressive results in various\ntasks but struggle with hallucination problems and lack of relevant knowledge,\nespecially in deep complex reasoning and knowledge-intensive tasks. Knowledge\nGraphs (KGs), which capture vast amounts of facts in a structured format, offer\na reliable source of knowledge for reasoning. However, existing KG-based LLM\nreasoning methods face challenges like handling multi-hop reasoning,\nmulti-entity questions, and effectively utilizing graph structures. To address\nthese issues, we propose Paths-over-Graph (PoG), a novel method that enhances\nLLM reasoning by integrating knowledge reasoning paths from KGs, improving the\ninterpretability and faithfulness of LLM outputs. PoG tackles multi-hop and\nmulti-entity questions through a three-phase dynamic multi-hop path\nexploration, which combines the inherent knowledge of LLMs with factual\nknowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant\ninformation from the graph exploration first and introduces efficient\nthree-step pruning techniques that incorporate graph structures, LLM prompting,\nand a pre-trained language model (e.g., SBERT) to effectively narrow down the\nexplored candidate paths. This ensures all reasoning paths contain highly\nrelevant information captured from KGs, making the reasoning faithful and\ninterpretable in problem-solving. PoG innovatively utilizes graph structure to\nprune the irrelevant noise and represents the first method to implement\nmulti-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive\nexperiments on five benchmark KGQA datasets demonstrate PoG outperforms the\nstate-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an\naverage accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo\nsurpasses ToG with GPT-4 by up to 23.9%.\n","authors":["Xingyu Tan","Xiaoyang Wang","Qing Liu","Xiwei Xu","Xin Yuan","Wenjie Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.14211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19740v2","updated":"2024-10-18T06:57:08Z","published":"2024-05-30T06:38:32Z","title":"PertEval: Unveiling Real Knowledge Capacity of LLMs with\n  Knowledge-Invariant Perturbations","summary":"  Expert-designed close-ended benchmarks are indispensable in assessing the\nknowledge capacity of large language models (LLMs). Despite their widespread\nuse, concerns have mounted regarding their reliability due to limited test\nscenarios and an unavoidable risk of data contamination. To rectify this, we\npresent PertEval, a toolkit devised for in-depth probing of LLMs' knowledge\ncapacity through \\textbf{knowledge-invariant perturbations}. These\nperturbations employ human-like restatement techniques to generate on-the-fly\ntest samples from static benchmarks, meticulously retaining knowledge-critical\ncontent while altering irrelevant details. Our toolkit further includes a suite\nof \\textbf{response consistency analyses} that compare performance on raw vs.\nperturbed test sets to precisely assess LLMs' genuine knowledge capacity. Six\nrepresentative LLMs are re-evaluated using PertEval. Results reveal\nsignificantly inflated performance of the LLMs on raw benchmarks, including an\nabsolute 25.8% overestimation for GPT-4. Additionally, through a nuanced\nresponse pattern analysis, we discover that PertEval retains LLMs' uncertainty\nto specious knowledge, and reveals their potential rote memorization to correct\noptions which leads to overestimated performance. We also find that the\ndetailed response consistency analyses by PertEval could illuminate various\nweaknesses in existing LLMs' knowledge mastery and guide the development of\nrefinement. Our findings provide insights for advancing more robust and\ngenuinely knowledgeable LLMs. Our code is available at\n\\url{https://github.com/aigc-apps/PertEval}.\n","authors":["Jiatong Li","Renjun Hu","Kunzhe Huang","Yan Zhuang","Qi Liu","Mengxiao Zhu","Xing Shi","Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2405.19740v2.pdf","comment":"Accepted by NeurIPS '24 D&B Spotlight; 28 pages, 15 figures, 14\n  tables"},{"id":"http://arxiv.org/abs/2403.01976v5","updated":"2024-10-18T06:52:17Z","published":"2024-03-04T12:19:28Z","title":"SciAssess: Benchmarking LLM Proficiency in Scientific Literature\n  Analysis","summary":"  Recent breakthroughs in Large Language Models (LLMs) have revolutionized\nscientific literature analysis. However, existing benchmarks fail to adequately\nevaluate the proficiency of LLMs in this domain, particularly in scenarios\nrequiring higher-level abilities beyond mere memorization and the handling of\nmultimodal data. In response to this gap, we introduce SciAssess, a benchmark\nspecifically designed for the comprehensive evaluation of LLMs in scientific\nliterature analysis. It aims to thoroughly assess the efficacy of LLMs by\nevaluating their capabilities in Memorization (L1), Comprehension (L2), and\nAnalysis \\& Reasoning (L3). It encompasses a variety of tasks drawn from\ndiverse scientific fields, including biology, chemistry, material, and\nmedicine. To ensure the reliability of SciAssess, rigorous quality control\nmeasures have been implemented, ensuring accuracy, anonymization, and\ncompliance with copyright standards. SciAssess evaluates 11 LLMs, highlighting\ntheir strengths and areas for improvement. We hope this evaluation supports the\nongoing development of LLM applications in scientific literature analysis.\nSciAssess and its resources are available at\n\\url{https://github.com/sci-assess/SciAssess}.\n","authors":["Hengxing Cai","Xiaochen Cai","Junhan Chang","Sihang Li","Lin Yao","Changxin Wang","Zhifeng Gao","Hongshuai Wang","Yongge Li","Mujie Lin","Shuwen Yang","Jiankun Wang","Mingjun Xu","Jin Huang","Xi Fang","Jiaxi Zhuang","Yuqi Yin","Yaqi Li","Changhong Chen","Zheng Cheng","Zifeng Zhao","Linfeng Zhang","Guolin Ke"],"pdf_url":"https://arxiv.org/pdf/2403.01976v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14208v1","updated":"2024-10-18T06:50:15Z","published":"2024-10-18T06:50:15Z","title":"Montessori-Instruct: Generate Influential Training Data Tailored for\n  Student Learning","summary":"  Synthetic data has been widely used to train large language models, but their\ngenerative nature inevitably introduces noisy, non-informative, and misleading\nlearning signals. In this paper, we propose Montessori-Instruct, a novel data\nsynthesis framework that tailors the data synthesis ability of the teacher\nlanguage model toward the student language model's learning process.\nSpecifically, we utilize local data influence of synthetic training data points\non students to characterize students' learning preferences. Then, we train the\nteacher model with Direct Preference Optimization (DPO) to generate synthetic\ndata tailored toward student learning preferences. Experiments with\nLlama3-8B-Instruct (teacher) and Llama3-8B (student) on Alpaca Eval and\nMT-Bench demonstrate that Montessori-Instruct significantly outperforms\nstandard synthesis methods by 18.35\\% and 46.24\\% relatively. Our method also\nbeats data synthesized by a stronger teacher model, GPT-4o. Further analysis\nconfirms the benefits of teacher's learning to generate more influential\ntraining data in the student's improved learning, the advantages of local data\ninfluence in accurately measuring student preferences, and the robustness of\nMontessori-Instruct across different student models. Our code and data are\nopen-sourced at https://github.com/cxcscmu/Montessori-Instruct.\n","authors":["Xiaochuan Li","Zichun Yu","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.14208v1.pdf","comment":"Codes and data are open-sourced at\n  https://github.com/cxcscmu/Montessori-Instruct"},{"id":"http://arxiv.org/abs/2410.14204v1","updated":"2024-10-18T06:38:22Z","published":"2024-10-18T06:38:22Z","title":"MediTOD: An English Dialogue Dataset for Medical History Taking with\n  Comprehensive Annotations","summary":"  Medical task-oriented dialogue systems can assist doctors by collecting\npatient medical history, aiding in diagnosis, or guiding treatment selection,\nthereby reducing doctor burnout and expanding access to medical services.\nHowever, doctor-patient dialogue datasets are not readily available, primarily\ndue to privacy regulations. Moreover, existing datasets lack comprehensive\nannotations involving medical slots and their different attributes, such as\nsymptoms and their onset, progression, and severity. These comprehensive\nannotations are crucial for accurate diagnosis. Finally, most existing datasets\nare non-English, limiting their utility for the larger research community.\n  In response, we introduce MediTOD, a new dataset of doctor-patient dialogues\nin English for the medical history-taking task. Collaborating with doctors, we\ndevise a questionnaire-based labeling scheme tailored to the medical domain.\nThen, medical professionals create the dataset with high-quality comprehensive\nannotations, capturing medical slots and their attributes. We establish\nbenchmarks in supervised and few-shot settings on MediTOD for natural language\nunderstanding, policy learning, and natural language generation subtasks,\nevaluating models from both TOD and biomedical domains. We make MediTOD\npublicly available for future research.\n","authors":["Vishal Vivek Saley","Goonjan Saha","Rocktim Jyoti Das","Dinesh Raghu"," Mausam"],"pdf_url":"https://arxiv.org/pdf/2410.14204v1.pdf","comment":"EMNLP2024 Camera Ready Version"},{"id":"http://arxiv.org/abs/2410.14202v1","updated":"2024-10-18T06:35:17Z","published":"2024-10-18T06:35:17Z","title":"Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay\n  Scoring with Rationale Generated by LLMs","summary":"  Existing automated essay scoring (AES) has solely relied on essay text\nwithout using explanatory rationales for the scores, thereby forgoing an\nopportunity to capture the specific aspects evaluated by rubric indicators in a\nfine-grained manner. This paper introduces Rationale-based Multiple Trait\nScoring (RMTS), a novel approach for multi-trait essay scoring that integrates\nprompt-engineering-based large language models (LLMs) with a fine-tuning-based\nessay scoring model using a smaller large language model (S-LLM). RMTS uses an\nLLM-based trait-wise rationale generation system where a separate LLM agent\ngenerates trait-specific rationales based on rubric guidelines, which the\nscoring model uses to accurately predict multi-trait scores. Extensive\nexperiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize,\nshow that RMTS significantly outperforms state-of-the-art models and vanilla\nS-LLMs in trait-specific scoring. By assisting quantitative assessment with\nfine-grained qualitative rationales, RMTS enhances the trait-wise reliability,\nproviding partial explanations about essays.\n","authors":["SeongYeub Chu","JongWoo Kim","Bryan Wong","MunYong Yi"],"pdf_url":"https://arxiv.org/pdf/2410.14202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14200v1","updated":"2024-10-18T06:31:40Z","published":"2024-10-18T06:31:40Z","title":"E3D-GPT: Enhanced 3D Visual Foundation for Medical Vision-Language Model","summary":"  The development of 3D medical vision-language models holds significant\npotential for disease diagnosis and patient treatment. However, compared to 2D\nmedical images, 3D medical images, such as CT scans, face challenges related to\nlimited training data and high dimension, which severely restrict the progress\nof 3D medical vision-language models. To address these issues, we collect a\nlarge amount of unlabeled 3D CT data and utilize self-supervised learning to\nconstruct a 3D visual foundation model for extracting 3D visual features. Then,\nwe apply 3D spatial convolutions to aggregate and project high-level image\nfeatures, reducing computational complexity while preserving spatial\ninformation. We also construct two instruction-tuning datasets based on BIMCV-R\nand CT-RATE to fine-tune the 3D vision-language model. Our model demonstrates\nsuperior performance compared to existing methods in report generation, visual\nquestion answering, and disease diagnosis. Code and data will be made publicly\navailable soon.\n","authors":["Haoran Lai","Zihang Jiang","Qingsong Yao","Rongsheng Wang","Zhiyang He","Xiaodong Tao","Wei Wei","Weifu Lv","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.14200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14198v1","updated":"2024-10-18T06:25:27Z","published":"2024-10-18T06:25:27Z","title":"Supervised Chain of Thought","summary":"  Large Language Models (LLMs) have revolutionized natural language processing\nand hold immense potential for advancing Artificial Intelligence. However, the\ncore architecture of most mainstream LLMs -- the Transformer -- has inherent\nlimitations in computational depth, rendering them theoretically incapable of\nsolving many reasoning tasks that demand increasingly deep computations. Chain\nof Thought (CoT) prompting has emerged as a technique to address these\narchitectural limitations, as evidenced by several theoretical studies. It\noffers a promising approach to solving complex reasoning tasks that were\npreviously beyond the capabilities of these models. Despite its successes, CoT\nand its variants (such as Tree of Thought, Graph of Thought, etc.) rely on a\n\"one-prompt-for-all\" approach, using a single prompt structure (e.g., \"think\nstep by step\") for a wide range of tasks -- from counting and sorting to\nsolving mathematical and algorithmic problems. This approach poses significant\nchallenges for models to generate the correct reasoning steps, as the model\nmust navigate through a vast prompt template space to find the appropriate\ntemplate for each task. In this work, we build upon previous theoretical\nanalyses of CoT to demonstrate how the one-prompt-for-all approach can\nnegatively affect the computability of LLMs. We partition the solution search\nspace into two: the prompt space and the answer space. Our findings show that\ntask-specific supervision is essential for navigating the prompt space\naccurately and achieving optimal performance. Through experiments with\nstate-of-the-art LLMs, we reveal a gap in reasoning performance when\nsupervision is applied versus when it is not.\n","authors":["Xiang Zhang","Dujian Ding"],"pdf_url":"https://arxiv.org/pdf/2410.14198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06833v2","updated":"2024-10-18T06:19:22Z","published":"2024-04-10T08:49:27Z","title":"Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural\n  Knowledge","summary":"  Recent studies have highlighted the presence of cultural biases in Large\nLanguage Models (LLMs), yet often lack a robust methodology to dissect these\nphenomena comprehensively. Our work aims to bridge this gap by delving into the\nFood domain, a universally relevant yet culturally diverse aspect of human\nlife. We introduce FmLAMA, a multilingual dataset centered on food-related\ncultural facts and variations in food practices. We analyze LLMs across various\narchitectures and configurations, evaluating their performance in both\nmonolingual and multilingual settings. By leveraging templates in six different\nlanguages, we investigate how LLMs interact with language-specific and cultural\nknowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias\ntowards food knowledge prevalent in the United States; (2) Incorporating\nrelevant cultural context significantly improves LLMs' ability to access\ncultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is\nhighly dependent on the interplay between the probing language, the specific\nmodel architecture, and the cultural context in question. This research\nunderscores the complexity of integrating cultural understanding into LLMs and\nemphasizes the importance of culturally diverse datasets to mitigate biases and\nenhance model performance across different cultural domains.\n","authors":["Li Zhou","Taelin Karidi","Wanlong Liu","Nicolas Garneau","Yong Cao","Wenyu Chen","Haizhou Li","Daniel Hershcovich"],"pdf_url":"https://arxiv.org/pdf/2404.06833v2.pdf","comment":"cultural bias analysis, cultural knowledge probing, large language\n  models, cultural NLP"},{"id":"http://arxiv.org/abs/2405.15585v3","updated":"2024-10-18T06:14:50Z","published":"2024-05-24T14:13:54Z","title":"Synergizing In-context Learning with Hints for End-to-end Task-oriented\n  Dialog Systems","summary":"  End-to-end Task-Oriented Dialog (TOD) systems typically require extensive\ntraining datasets to perform well. In contrast, large language model (LLM)\nbased TOD systems can excel even with limited data due to their ability to\nlearn tasks through in-context exemplars. However, these models lack alignment\nwith the style of responses in training data and often generate comprehensive\nresponses, making it difficult for users to grasp the information quickly. In\nresponse, we propose SyncTOD that synergizes LLMs with task-specific hints to\nimprove alignment in low-data settings. SyncTOD employs small auxiliary models\nto provide hints and select exemplars for in-context prompts. With ChatGPT,\nSyncTOD achieves superior performance compared to LLM-based baselines and SoTA\nmodels in low-data settings, while retaining competitive performance in\nfull-data settings.\n","authors":["Vishal Vivek Saley","Rocktim Jyoti Das","Dinesh Raghu"," Mausam"],"pdf_url":"https://arxiv.org/pdf/2405.15585v3.pdf","comment":"EMNLP2024 Camera-Ready Version"},{"id":"http://arxiv.org/abs/2410.14194v1","updated":"2024-10-18T06:09:41Z","published":"2024-10-18T06:09:41Z","title":"Speciesism in Natural Language Processing Research","summary":"  Natural Language Processing (NLP) research on AI Safety and social bias in AI\nhas focused on safety for humans and social bias against human minorities.\nHowever, some AI ethicists have argued that the moral significance of nonhuman\nanimals has been ignored in AI research. Therefore, the purpose of this study\nis to investigate whether there is speciesism, i.e., discrimination against\nnonhuman animals, in NLP research. First, we explain why nonhuman animals are\nrelevant in NLP research. Next, we survey the findings of existing research on\nspeciesism in NLP researchers, data, and models and further investigate this\nproblem in this study. The findings of this study suggest that speciesism\nexists within researchers, data, and models, respectively. Specifically, our\nsurvey and experiments show that (a) among NLP researchers, even those who\nstudy social bias in AI, do not recognize speciesism or speciesist bias; (b)\namong NLP data, speciesist bias is inherent in the data annotated in the\ndatasets used to evaluate NLP models; (c) OpenAI GPTs, recent NLP models,\nexhibit speciesist bias by default. Finally, we discuss how we can reduce\nspeciesism in NLP research.\n","authors":["Masashi Takeshita","Rafal Rzepka"],"pdf_url":"https://arxiv.org/pdf/2410.14194v1.pdf","comment":"This article is a preprint and has not been peer-reviewed. The\n  postprint has been accepted for publication in AI and Ethics. Please cite the\n  final version of the article once it is published"},{"id":"http://arxiv.org/abs/2410.04422v4","updated":"2024-10-18T06:09:31Z","published":"2024-10-06T09:29:19Z","title":"Hyper-multi-step: The Truth Behind Difficult Long-context Tasks","summary":"  Long-context language models (LCLM), characterized by their extensive context\nwindow, is becoming increasingly popular. Meanwhile, many long-context\nbenchmarks present challenging tasks that even the most advanced LCLMs struggle\nto complete. However, the underlying sources of various challenging\nlong-context tasks have seldom been studied. To bridge this gap, we conduct\nexperiments to indicate their difficulty stems primarily from two basic issues:\n\"multi-matching retrieval,\" which requires the simultaneous retrieval of\nmultiple items, and \"logic-based retrieval,\" which necessitates logical\njudgment within retrieval criteria. These two problems, while seemingly\nstraightforward, actually exceed the capabilities of LCLMs because they are\nproven to be hyper-multi-step (demanding numerous steps to solve) in nature.\nThis finding could explain why LLMs struggle with more advanced long-context\ntasks, providing a more accurate perspective for rethinking solutions for them.\n","authors":["Yijiong Yu","Ma Xiufa","Fang Jianwei","Zhi Xu","Su Guangyao","Wang Jiancheng","Yongfeng Huang","Zhixiao Qi","Wei Wang","Weifeng Liu","Ran Chen","Ji Pei"],"pdf_url":"https://arxiv.org/pdf/2410.04422v4.pdf","comment":"Our code is publicly available at\n  https://github.com/yuyijiong/hard_retrieval_for_llm and the datasets is at\n  https://huggingface.co/datasets/yuyijiong/difficult_retrieval"},{"id":"http://arxiv.org/abs/2410.14184v1","updated":"2024-10-18T05:31:13Z","published":"2024-10-18T05:31:13Z","title":"MetaAlign: Align Large Language Models with Diverse Preferences during\n  Inference Time","summary":"  Large Language Models (LLMs) acquire extensive knowledge and remarkable\nabilities from extensive text corpora, making them powerful tools for various\napplications. To make LLMs more usable, aligning them with human preferences is\nessential. Existing alignment techniques, such as Reinforcement Learning from\nHuman Feedback (RLHF) and Direct Preference Optimization (DPO), typically embed\npredefined preferences directly within the model's parameters. These methods,\nhowever, often result in a static alignment that can not account for the\ndiversity of human preferences in practical applications. In response to this\nchallenge, we propose an effective method, \\textbf{MetaAlign}, which aims to\nhelp LLMs dynamically align with various explicit or implicit preferences\nspecified at inference time. Experimental results show that LLMs optimized on\nour meticulously constructed MetaAlign Dataset can effectively align with any\npreferences specified at the inference stage, validating the feasibility of\nMetaAlign. We hope that our work can provide some insights into the alignment\nof language models.\n","authors":["Mozhi Zhang","Pengyu Wang","Chenkun Tan","Mianqiu Huang","Dong Zhang","Yaqian Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.14184v1.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.00131v2","updated":"2024-10-18T05:22:02Z","published":"2024-09-30T18:12:18Z","title":"Fisher Information-based Efficient Curriculum Federated Learning with\n  Large Language Models","summary":"  As a promising paradigm to collaboratively train models with decentralized\ndata, Federated Learning (FL) can be exploited to fine-tune Large Language\nModels (LLMs). While LLMs correspond to huge size, the scale of the training\ndata significantly increases, which leads to tremendous amounts of computation\nand communication costs. The training data is generally non-Independent and\nIdentically Distributed (non-IID), which requires adaptive data processing\nwithin each device. Although Low Rank Adaptation (LoRA) can significantly\nreduce the scale of parameters to update in the fine-tuning process, it still\ntakes unaffordable time to transfer the low-rank parameters of all the layers\nin LLMs. In this paper, we propose a Fisher Information-based Efficient\nCurriculum Federated Learning framework (FibecFed) with two novel methods,\ni.e., adaptive federated curriculum learning and efficient sparse parameter\nupdate. First, we propose a fisher information-based method to adaptively\nsample data within each device to improve the effectiveness of the FL\nfine-tuning process. Second, we dynamically select the proper layers for global\naggregation and sparse parameters for local update with LoRA so as to improve\nthe efficiency of the FL fine-tuning process. Extensive experimental results\nbased on 10 datasets demonstrate that FibecFed yields excellent performance (up\nto 45.35% in terms of accuracy) and superb fine-tuning speed (up to 98.61%\nfaster) compared with 17 baseline approaches).\n","authors":["Ji Liu","Jiaxiang Ren","Ruoming Jin","Zijie Zhang","Yang Zhou","Patrick Valduriez","Dejing Dou"],"pdf_url":"https://arxiv.org/pdf/2410.00131v2.pdf","comment":"27 pages, 8 figures, 14 tables, to appear in EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.14182v1","updated":"2024-10-18T05:21:05Z","published":"2024-10-18T05:21:05Z","title":"LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs","summary":"  Laboratory accidents pose significant risks to human life and property,\nunderscoring the importance of robust safety protocols. Despite advancements in\nsafety training, laboratory personnel may still unknowingly engage in unsafe\npractices. With the increasing reliance on large language models (LLMs) for\nguidance in various fields, including laboratory settings, there is a growing\nconcern about their reliability in critical safety-related decision-making.\nUnlike trained human researchers, LLMs lack formal lab safety education,\nraising questions about their ability to provide safe and accurate guidance.\nExisting research on LLM trustworthiness primarily focuses on issues such as\nethical compliance, truthfulness, and fairness but fails to fully cover\nsafety-critical real-world applications, like lab safety. To address this gap,\nwe propose the Laboratory Safety Benchmark (LabSafety Bench), a comprehensive\nevaluation framework based on a new taxonomy aligned with Occupational Safety\nand Health Administration (OSHA) protocols. This benchmark includes 765\nmultiple-choice questions verified by human experts, assessing LLMs and vision\nlanguage models (VLMs) performance in lab safety contexts. Our evaluations\ndemonstrate that while GPT-4o outperforms human participants, it is still prone\nto critical errors, highlighting the risks of relying on LLMs in\nsafety-critical environments. Our findings emphasize the need for specialized\nbenchmarks to accurately assess the trustworthiness of LLMs in real-world\nsafety applications.\n","authors":["Yujun Zhou","Jingdong Yang","Kehan Guo","Pin-Yu Chen","Tian Gao","Werner Geyer","Nuno Moniz","Nitesh V Chawla","Xiangliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.14182v1.pdf","comment":"50 pages, 19 figures"},{"id":"http://arxiv.org/abs/2410.14180v1","updated":"2024-10-18T05:16:39Z","published":"2024-10-18T05:16:39Z","title":"XForecast: Evaluating Natural Language Explanations for Time Series\n  Forecasting","summary":"  Time series forecasting aids decision-making, especially for stakeholders who\nrely on accurate predictions, making it very important to understand and\nexplain these models to ensure informed decisions. Traditional explainable AI\n(XAI) methods, which underline feature or temporal importance, often require\nexpert knowledge. In contrast, natural language explanations (NLEs) are more\naccessible to laypeople. However, evaluating forecast NLEs is difficult due to\nthe complex causal relationships in time series data. To address this, we\nintroduce two new performance metrics based on simulatability, assessing how\nwell a human surrogate can predict model forecasts using the explanations.\nExperiments show these metrics differentiate good from poor explanations and\nalign with human judgments. Utilizing these metrics, we further evaluate the\nability of state-of-the-art large language models (LLMs) to generate\nexplanations for time series data, finding that numerical reasoning, rather\nthan model size, is the main factor influencing explanation quality.\n","authors":["Taha Aksu","Chenghao Liu","Amrita Saha","Sarah Tan","Caiming Xiong","Doyen Sahoo"],"pdf_url":"https://arxiv.org/pdf/2410.14180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14179v1","updated":"2024-10-18T05:15:50Z","published":"2024-10-18T05:15:50Z","title":"MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart\n  Problems","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated impressive\nabilities across various tasks, including visual question answering and chart\ncomprehension, yet existing benchmarks for chart-related tasks fall short in\ncapturing the complexity of real-world multi-chart scenarios. Current\nbenchmarks primarily focus on single-chart tasks, neglecting the multi-hop\nreasoning required to extract and integrate information from multiple charts,\nwhich is essential in practical applications. To fill this gap, we introduce\nMultiChartQA, a benchmark that evaluates MLLMs' capabilities in four key areas:\ndirect question answering, parallel question answering, comparative reasoning,\nand sequential reasoning. Our evaluation of a wide range of MLLMs reveals\nsignificant performance gaps compared to humans. These results highlight the\nchallenges in multi-chart comprehension and the potential of MultiChartQA to\ndrive advancements in this field. Our code and data are available at\nhttps://github.com/Zivenzhu/Multi-chart-QA\n","authors":["Zifeng Zhu","Mengzhao Jia","Zhihan Zhang","Lang Li","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.14179v1.pdf","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.10270v2","updated":"2024-10-18T05:07:38Z","published":"2024-10-14T08:21:25Z","title":"QUIS: Question-guided Insights Generation for Automated Exploratory Data\n  Analysis","summary":"  Discovering meaningful insights from a large dataset, known as Exploratory\nData Analysis (EDA), is a challenging task that requires thorough exploration\nand analysis of the data. Automated Data Exploration (ADE) systems use\ngoal-oriented methods with Large Language Models and Reinforcement Learning\ntowards full automation. However, these methods require human involvement to\nanticipate goals that may limit insight extraction, while fully automated\nsystems demand significant computational resources and retraining for new\ndatasets. We introduce QUIS, a fully automated EDA system that operates in two\nstages: insight generation (ISGen) driven by question generation (QUGen). The\nQUGen module generates questions in iterations, refining them from previous\niterations to enhance coverage without human intervention or manually curated\nexamples. The ISGen module analyzes data to produce multiple relevant insights\nin response to each question, requiring no prior training and enabling QUIS to\nadapt to new datasets.\n","authors":["Abhijit Manatkar","Ashlesha Akella","Parthivi Gupta","Krishnasuri Narayanam"],"pdf_url":"https://arxiv.org/pdf/2410.10270v2.pdf","comment":"Accepted for ENLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2408.15545v3","updated":"2024-10-18T05:04:53Z","published":"2024-08-28T05:41:52Z","title":"SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding","summary":"  Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks.\n","authors":["Sihang Li","Jin Huang","Jiaxi Zhuang","Yaorui Shi","Xiaochen Cai","Mingjun Xu","Xiang Wang","Linfeng Zhang","Guolin Ke","Hengxing Cai"],"pdf_url":"https://arxiv.org/pdf/2408.15545v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13276v2","updated":"2024-10-18T05:01:11Z","published":"2024-10-17T07:07:09Z","title":"SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs","summary":"  Attention is the cornerstone of modern Large Language Models (LLMs). Yet its\nquadratic complexity limits the efficiency and scalability of LLMs, especially\nfor those with a long-context window. A promising approach addressing this\nlimitation is to leverage the sparsity in attention. However, existing\nsparsity-based solutions predominantly rely on predefined patterns or\nheuristics to approximate sparsity. This practice falls short to fully capture\nthe dynamic nature of attention sparsity in language-based tasks. This paper\nargues that attention sparsity should be learned rather than predefined. To\nthis end, we design SeerAttention, a new Attention mechanism that augments the\nconventional attention with a learnable gate that adaptively selects\nsignificant blocks in an attention map and deems the rest blocks sparse. Such\nblock-level sparsity effectively balances accuracy and speedup. To enable\nefficient learning of the gating network, we develop a customized\nFlashAttention implementation that extracts the block-level ground truth of\nattention map with minimum overhead. SeerAttention not only applies to\npost-training, but also excels in long-context fine-tuning. Our results show\nthat at post-training stages, SeerAttention significantly outperforms\nstate-of-the-art static or heuristic-based sparse attention methods, while also\nbeing more versatile and flexible to adapt to varying context lengths and\nsparsity ratios. When applied to long-context fine-tuning with YaRN,\nSeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context\nlength with minimal perplexity loss, offering a 5.67x speedup over\nFlashAttention-2.\n","authors":["Yizhao Gao","Zhichen Zeng","Dayou Du","Shijie Cao","Hayden Kwok-Hay So","Ting Cao","Fan Yang","Mao Yang"],"pdf_url":"https://arxiv.org/pdf/2410.13276v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05541v2","updated":"2024-10-18T04:52:38Z","published":"2024-08-10T12:44:49Z","title":"P3: A Policy-Driven, Pace-Adaptive, and Diversity-Promoted Framework for\n  data pruning in LLM Training","summary":"  In the rapidly advancing field of Large Language Models (LLMs), effectively\nleveraging existing datasets during fine-tuning to maximize the model's\npotential is of paramount importance. This paper introduces P3, an adaptive\nframework aimed at optimizing the task-specific fine-tuning process through\niterative data pruning. P3 consists of three key components: (1) Policy-driven\nDifficulty Measurement, which dynamically assesses data difficulty based on the\nmodel's real-time performance, replacing static metrics with adaptable\nevaluations; (2) Pace-Adaptive Selection, leveraging self-paced learning to\nprogressively introduce more challenging data, thereby enhancing model\ncapability; (3) Diversity Promotion, incorporating Determinantal Point Process\n(DPP) to ensure data diversity across epochs, enriching the learning process.\nWe validate P3 on the reasoning scenarios, APPS and MATH, demonstrating\nsignificant improvements over traditional data pruning methods. By advancing\ndynamic data selection and utilization strategies, P3 contributes both a\ntheoretical framework and concrete approach to fully exploit existing data for\nLLMs' performance improvement, offering utility across diverse tasks.\n","authors":["Yingxuan Yang","Huayi Wang","Muning Wen","Xiaoyun Mo","Qiuying Peng","Jun Wang","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.05541v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14862v4","updated":"2024-10-18T04:39:35Z","published":"2024-06-21T04:39:03Z","title":"LatentExplainer: Explaining Latent Representations in Deep Generative\n  Models with Multi-modal Foundation Models","summary":"  Deep generative models like VAEs and diffusion models have advanced various\ngeneration tasks by leveraging latent variables to learn data distributions and\ngenerate high-quality samples. Despite the field of explainable AI making\nstrides in interpreting machine learning models, understanding latent variables\nin generative models remains challenging. This paper introduces\n\\textit{LatentExplainer}, a framework for automatically generating semantically\nmeaningful explanations of latent variables in deep generative models.\n\\textit{LatentExplainer} tackles three main challenges: inferring the meaning\nof latent variables, aligning explanations with inductive biases, and handling\nvarying degrees of explainability. Our approach perturbs latent variables,\ninterpreting changes in generated data, and uses multi-modal large language\nmodels (MLLMs) to produce human-understandable explanations. We evaluate our\nproposed method on several real-world and synthetic datasets, and the results\ndemonstrate superior performance in generating high-quality explanations for\nlatent variables. The results highlight the effectiveness of incorporating\ninductive biases and uncertainty quantification, significantly enhancing model\ninterpretability.\n","authors":["Mengdan Zhu","Raasikh Kanjiani","Jiahui Lu","Andrew Choi","Qirui Ye","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.14862v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15820v2","updated":"2024-10-18T04:38:47Z","published":"2024-09-24T07:34:50Z","title":"Supervised Fine-Tuning Achieve Rapid Task Adaption Via Alternating\n  Attention Head Activation Patterns","summary":"  LLMs' performance on complex tasks is still unsatisfactory. A key issue is\nthat presently LLMs learn in a data-driven schema, while the instructions about\nthese complex tasks are both scarce and hard to collect or construct. On the\ncontrary, a prominent phenomenon is that LLMs can learn rather fast on simpler\ntasks with adequate prior knowledge captured during pretraining stage. Thus, if\nthe prerequisite and mechanism of such rapid generalization could be\nelucidated, it could enhance the efficiency and effectiveness of the LLM's\nability to learn complex tasks. Thus, in this paper, we employ a gradient-based\nmethod, to dissect the process that the SFT process adapts LLMs to downstream\ntasks via the perspective of attention patterns. We find that: (1) LLMs\nselectively activate task-specific attention heads during SFT; (2) activation\npatterns for complex tasks are combinations of basic task patterns; and (3)\nchanges in a few parameters can significantly impact activation patterns after\nSFT on a small number of samples.Based on these insights, experiments are\nconducted to actually enhance the efficiency and effectiveness of SFT.\n","authors":["Yang Zhao","Li Du","Xiao Ding","Kai Xiong","Ting Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2409.15820v2.pdf","comment":"in review"},{"id":"http://arxiv.org/abs/2407.00902v2","updated":"2024-10-18T04:37:33Z","published":"2024-07-01T01:57:21Z","title":"From Introspection to Best Practices: Principled Analysis of\n  Demonstrations in Multimodal In-Context Learning","summary":"  Motivated by in-context learning (ICL) capabilities of Large Language models\n(LLMs), multimodal LLMs with additional visual modality are also exhibited with\nsimilar ICL abilities when multiple image-text pairs are provided as\ndemonstrations. However, relatively less work has been done to investigate the\nprinciples behind how and why multimodal ICL works. We conduct a systematic and\nprincipled evaluation of multimodal ICL for models of different scales on a\nbroad spectrum of new yet critical tasks. Through perturbations over different\nmodality information, we show that modalities matter differently across tasks\nin multimodal ICL. Guided by task-specific modality impact, we recommend\nmodality-driven demonstration strategies to boost ICL performance. We also find\nthat models may follow inductive biases from multimodal ICL even if they are\nrarely seen in or contradict semantic priors from pretraining data. Our\nprincipled analysis provides a comprehensive way of understanding the role of\ndemonstrations in multimodal in-context learning, and sheds light on\neffectively improving multimodal ICL on a wide range of tasks.\n","authors":["Nan Xu","Fei Wang","Sheng Zhang","Hoifung Poon","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2407.00902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15349v2","updated":"2024-10-18T04:32:49Z","published":"2024-05-24T08:42:40Z","title":"Everything is Editable: Extend Knowledge Editing to Unstructured Data in\n  Large Language Models","summary":"  Recent knowledge editing methods have primarily focused on modifying\nstructured knowledge in large language models. However, this task setting\noverlooks the fact that a significant portion of real-world knowledge is stored\nin an unstructured format, characterized by long-form content, noise, and a\ncomplex yet comprehensive nature. Techniques like local layer key-value storage\nand term-driven optimization, as used in previous methods like MEMIT, are not\neffective for handling unstructured knowledge. To address these challenges, we\npropose a novel Unstructured Knowledge Editing method, namely UnKE, which\nextends previous assumptions in the layer dimension and token dimension.\nFirstly, in the layer dimension, we propose non-local block key-value storage\nto replace local layer key-value storage, increasing the representation ability\nof key-value pairs and incorporating attention layer knowledge. Secondly, in\nthe token dimension, we replace term-driven optimization with cause-driven\noptimization, which edits the last token directly while preserving context,\navoiding the need to locate terms and preventing the loss of context\ninformation. Results on newly proposed unstructured knowledge editing dataset\n(UnKEBench) and traditional structured datasets demonstrate that UnKE achieves\nremarkable performance, surpassing strong baselines. In addition, UnKE has\nrobust batch editing and sequential editing capabilities.\n","authors":["Jingcheng Deng","Zihao Wei","Liang Pang","Hanxing Ding","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2405.15349v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14166v1","updated":"2024-10-18T04:17:16Z","published":"2024-10-18T04:17:16Z","title":"LLM The Genius Paradox: A Linguistic and Math Expert's Struggle with\n  Simple Word-based Counting Problems","summary":"  Interestingly, LLMs yet struggle with some basic tasks that humans find\ntrivial to handle, e.g., counting the number of character r's in the word\n\"strawberry\". There are several popular conjectures (e.g., tokenization,\narchitecture and training data) regarding the reason for deficiency of LLMs in\nsimple word-based counting problems, sharing the similar belief that such\nfailure stems from model pretraining hence probably inevitable during\ndeployment. In this paper, we carefully design multiple evaluation settings to\ninvestigate validity of prevalent conjectures. Meanwhile, we measure\ntransferability of advanced mathematical and coding reasoning capabilities from\nspecialized LLMs to simple counting tasks. Although specialized LLMs suffer\nfrom counting problems as well, we find conjectures about inherent deficiency\nof LLMs invalid and further seek opportunities to elicit knowledge and\ncapabilities from LLMs that are beneficial to counting tasks. Compared with\nstrategies such as finetuning and in-context learning that are commonly adopted\nto enhance performance on new or challenging tasks, we show that engaging\nreasoning is the most robust and efficient way to help LLMs better perceive\ntasks with more accurate responses.\n  We hope our conjecture validation design could provide insights into the\nstudy of future critical failure modes of LLMs. Based on challenges in\ntransferring advanced capabilities to much simpler tasks, we call for more\nattention to model capability acquisition and evaluation. We also highlight the\nimportance of cultivating consciousness of \"reasoning before responding\" during\nmodel pretraining.\n","authors":["Nan Xu","Xuezhe Ma"],"pdf_url":"https://arxiv.org/pdf/2410.14166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14165v1","updated":"2024-10-18T04:13:51Z","published":"2024-10-18T04:13:51Z","title":"Automated Genre-Aware Article Scoring and Feedback Using Large Language\n  Models","summary":"  This paper focuses on the development of an advanced intelligent article\nscoring system that not only assesses the overall quality of written work but\nalso offers detailed feature-based scoring tailored to various article genres.\nBy integrating the pre-trained BERT model with the large language model\nChat-GPT, the system gains a deep understanding of both the content and\nstructure of the text, enabling it to provide a thorough evaluation along with\ntargeted suggestions for improvement. Experimental results demonstrate that\nthis system outperforms traditional scoring methods across multiple public\ndatasets, particularly in feature-based assessments, offering a more accurate\nreflection of the quality of different article types. Moreover, the system\ngenerates personalized feedback to assist users in enhancing their writing\nskills, underscoring the potential and practical value of automated scoring\ntechnologies in educational contexts.\n","authors":["Chihang Wang","Yuxin Dong","Zhenhong Zhang","Ruotong Wang","Shuo Wang","Jiajing Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13170v2","updated":"2024-10-18T04:13:05Z","published":"2024-06-19T02:53:39Z","title":"Amphista: Bi-directional Multi-head Decoding for Accelerating LLM\n  Inference","summary":"  Large Language Models (LLMs) inherently use autoregressive decoding, which\nlacks parallelism in inference and results in significantly slow inference\nspeed. While methods such as Medusa constructs parallelized heads, they lack\nadequate information interaction across different prediction positions. To\novercome this limitation, we introduce Amphista, an enhanced speculative\ndecoding framework that builds upon Medusa. Specifically, Amphista models an\nAuto-embedding Block capable of parallel inference, incorporating\nbi-directional attention to enable interaction between different drafting\nheads. Additionally, Amphista integrates Staged Adaptation Layers, which ensure\na seamless transition of semantic information from the target model's\nautoregressive inference to the drafting heads' non-autoregressive inference,\neffectively achieving paradigm shift and feature fusion. Experimental results\non Vicuna models using MT-Bench and Spec-Bench demonstrate that Amphista\nachieves substantial acceleration while maintaining generation quality. On\nMT-Bench, Amphista delivers up to 2.75$\\times$ speedup over vanilla\nautoregressive decoding and 1.40$\\times$ over Medusa on Vicuna 33B in\nwall-clock time.\n","authors":["Zeping Li","Xinlong Yang","Ziheng Gao","Ji Liu","Guanchen Li","Zhuang Liu","Dong Li","Jinzhang Peng","Lu Tian","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2406.13170v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16710v4","updated":"2024-10-18T04:02:31Z","published":"2024-04-25T16:20:23Z","title":"LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding","summary":"  We present LayerSkip, an end-to-end solution to speed-up inference of large\nlanguage models (LLMs). First, during training we apply layer dropout, with low\ndropout rates for earlier layers and higher dropout rates for later layers, and\nan early exit loss where all transformer layers share the same exit. Second,\nduring inference, we show that this training recipe increases the accuracy of\nearly exit at earlier layers, without adding any auxiliary layers or modules to\nthe model. Third, we present a novel self-speculative decoding solution where\nwe exit at early layers and verify and correct with remaining layers of the\nmodel. Our proposed self-speculative decoding approach has less memory\nfootprint than other speculative decoding approaches and benefits from shared\ncompute and activations of the draft and verification stages. We run\nexperiments on different Llama model sizes on different types of training:\npretraining from scratch, continual pretraining, finetuning on specific data\ndomain, and finetuning on specific task. We implement our inference solution\nand show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x\non coding, and 2.0x on TOPv2 semantic parsing task. We open source our code and\ncheckpoints at https://github.com/facebookresearch/LayerSkip.\n","authors":["Mostafa Elhoushi","Akshat Shrivastava","Diana Liskovich","Basil Hosmer","Bram Wasti","Liangzhen Lai","Anas Mahmoud","Bilge Acun","Saurabh Agarwal","Ahmed Roman","Ahmed A Aly","Beidi Chen","Carole-Jean Wu"],"pdf_url":"https://arxiv.org/pdf/2404.16710v4.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2410.14157v1","updated":"2024-10-18T03:48:53Z","published":"2024-10-18T03:48:53Z","title":"Beyond Autoregression: Discrete Diffusion for Complex Reasoning and\n  Planning","summary":"  Autoregressive language models, despite their impressive capabilities,\nstruggle with complex reasoning and long-term planning tasks. We introduce\ndiscrete diffusion models as a novel solution to these challenges. Through the\nlens of subgoal imbalance, we demonstrate how diffusion models effectively\nlearn difficult subgoals that elude autoregressive approaches. We propose\nMulti-granularity Diffusion Modeling (MDM), which prioritizes subgoals based on\ndifficulty during learning. On complex tasks like Countdown, Sudoku, and\nBoolean Satisfiability Problems, MDM significantly outperforms autoregressive\nmodels without using search techniques. For instance, MDM achieves 91.5\\% and\n100\\% accuracy on Countdown and Sudoku, respectively, compared to 45.8\\% and\n20.7\\% for autoregressive models. Our work highlights the potential of\ndiffusion-based approaches in advancing AI capabilities for sophisticated\nlanguage understanding and problem-solving tasks.\n","authors":["Jiacheng Ye","Jiahui Gao","Shansan Gong","Lin Zheng","Xin Jiang","Zhenguo Li","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2410.14157v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14155v1","updated":"2024-10-18T03:45:42Z","published":"2024-10-18T03:45:42Z","title":"Towards Faithful Natural Language Explanations: A Study Using Activation\n  Patching in Large Language Models","summary":"  Large Language Models (LLMs) are capable of generating persuasive Natural\nLanguage Explanations (NLEs) to justify their answers. However, the\nfaithfulness of these explanations should not be readily trusted at face value.\nRecent studies have proposed various methods to measure the faithfulness of\nNLEs, typically by inserting perturbations at the explanation or feature level.\nWe argue that these approaches are neither comprehensive nor correctly designed\naccording to the established definition of faithfulness. Moreover, we highlight\nthe risks of grounding faithfulness findings on out-of-distribution samples. In\nthis work, we leverage a causal mediation technique called activation patching,\nto measure the faithfulness of an explanation towards supporting the explained\nanswer. Our proposed metric, Causal Faithfulness quantifies the consistency of\ncausal attributions between explanations and the corresponding model outputs as\nthe indicator of faithfulness. We experimented across models varying from 2B to\n27B parameters and found that models that underwent alignment tuning tend to\nproduce more faithful and plausible explanations. We find that Causal\nFaithfulness is a promising improvement over existing faithfulness tests by\ntaking into account the model's internal computations and avoiding out of\ndistribution concerns that could otherwise undermine the validity of\nfaithfulness assessments. We release the code in\n\\url{https://github.com/wj210/Causal-Faithfulness}\n","authors":["Wei Jie Yeo","Ranjan Satapthy","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2410.14155v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.14152v1","updated":"2024-10-18T03:43:42Z","published":"2024-10-18T03:43:42Z","title":"SRAP-Agent: Simulating and Optimizing Scarce Resource Allocation Policy\n  with LLM-based Agent","summary":"  Public scarce resource allocation plays a crucial role in economics as it\ndirectly influences the efficiency and equity in society. Traditional studies\nincluding theoretical model-based, empirical study-based and simulation-based\nmethods encounter limitations due to the idealized assumption of complete\ninformation and individual rationality, as well as constraints posed by limited\navailable data. In this work, we propose an innovative framework, SRAP-Agent\n(Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based\nAgent), which integrates Large Language Models (LLMs) into economic\nsimulations, aiming to bridge the gap between theoretical models and real-world\ndynamics. Using public housing allocation scenarios as a case study, we conduct\nextensive policy simulation experiments to verify the feasibility and\neffectiveness of the SRAP-Agent and employ the Policy Optimization Algorithm\nwith certain optimization objectives. The source code can be found in\nhttps://github.com/jijiarui-cather/SRAPAgent_Framework\n","authors":["Jiarui Ji","Yang Li","Hongtao Liu","Zhicheng Du","Zhewei Wei","Weiran Shen","Qi Qi","Yankai Lin"],"pdf_url":"https://arxiv.org/pdf/2410.14152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14150v1","updated":"2024-10-18T03:40:45Z","published":"2024-10-18T03:40:45Z","title":"Utilizing Large Language Models for Event Deconstruction to Enhance\n  Multimodal Aspect-Based Sentiment Analysis","summary":"  With the rapid development of the internet, the richness of User-Generated\nContentcontinues to increase, making Multimodal Aspect-Based Sentiment Analysis\n(MABSA) a research hotspot. Existing studies have achieved certain results in\nMABSA, but they have not effectively addressed the analytical challenges in\nscenarios where multiple entities and sentiments coexist. This paper\ninnovatively introduces Large Language Models (LLMs) for event decomposition\nand proposes a reinforcement learning framework for Multimodal Aspect-based\nSentiment Analysis (MABSA-RL) framework. This framework decomposes the original\ntext into a set of events using LLMs, reducing the complexity of analysis,\nintroducing reinforcement learning to optimize model parameters. Experimental\nresults show that MABSA-RL outperforms existing advanced methods on two\nbenchmark datasets. This paper provides a new research perspective and method\nfor multimodal aspect-level sentiment analysis.\n","authors":["Xiaoyong Huang","Heli Sun","Qunshu Gao","Wenjie Huang","Ruichen Cao"],"pdf_url":"https://arxiv.org/pdf/2410.14150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12151v2","updated":"2024-10-18T03:36:03Z","published":"2024-08-22T06:40:32Z","title":"A Tighter Complexity Analysis of SparseGPT","summary":"  In this work, we improved the analysis of the running time of SparseGPT\n[Frantar, Alistarh ICML 2023] from $O(d^{3})$ to $O(d^{\\omega} + d^{2+a+o(1)} +\nd^{1+\\omega(1,1,a)-a})$ for any $a \\in [0, 1]$, where $\\omega$ is the exponent\nof matrix multiplication. In particular, for the current $\\omega \\approx 2.371$\n[Alman, Duan, Williams, Xu, Xu, Zhou 2024], our running time boils down to\n$O(d^{2.53})$. This running time is due to the analysis of the lazy update\nbehavior in iterative maintenance problems such as [Deng, Song, Weinstein 2022;\nBrand, Song, Zhou ICML 2024].\n","authors":["Xiaoyu Li","Yingyu Liang","Zhenmei Shi","Zhao Song"],"pdf_url":"https://arxiv.org/pdf/2408.12151v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14148v1","updated":"2024-10-18T03:34:32Z","published":"2024-10-18T03:34:32Z","title":"Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in\n  Vision-Language Alignment","summary":"  The recent advancements in large language models (LLMs) and pre-trained\nvision models have accelerated the development of vision-language large models\n(VLLMs), enhancing the interaction between visual and linguistic modalities.\nDespite their notable success across various domains, VLLMs face challenges in\nmodality alignment, which can lead to issues like hallucinations and unsafe\ncontent generation. Current alignment techniques often rely on coarse feedback\nand external datasets, limiting scalability and performance. In this paper, we\npropose FiSAO (Fine-Grained Self-Alignment Optimization), a novel\nself-alignment method that utilizes the model's own visual encoder as a\nfine-grained verifier to improve vision-language alignment without the need for\nadditional data. By leveraging token-level feedback from the vision encoder,\nFiSAO significantly improves vision-language alignment, even surpassing\ntraditional preference tuning methods that require additional data. Through\nboth theoretical analysis and experimental validation, we demonstrate that\nFiSAO effectively addresses the misalignment problem in VLLMs, marking the\nfirst instance of token-level rewards being applied to such models.\n","authors":["Chenhang Cui","An Zhang","Yiyang Zhou","Zhaorun Chen","Gelei Deng","Huaxiu Yao","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.14148v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2402.13606v3","updated":"2024-10-18T03:34:12Z","published":"2024-02-21T08:20:06Z","title":"A Comprehensive Study of Multilingual Confidence Estimation on Large\n  Language Models","summary":"  The tendency of Large Language Models (LLMs) to generate hallucinations\nraises concerns regarding their reliability. Therefore, confidence estimations\nindicating the extent of trustworthiness of the generations become essential.\nHowever, current LLM confidence estimations in languages other than English\nremain underexplored. This paper addresses this gap by introducing a\ncomprehensive investigation of Multilingual Confidence estimation (MlingConf)\non LLMs, focusing on both language-agnostic (LA) and language-specific (LS)\ntasks to explore the performance and language dominance effects of multilingual\nconfidence estimations on different tasks. The benchmark comprises four\nmeticulously checked and human-evaluate high-quality multilingual datasets for\nLA tasks and one for the LS task tailored to specific social, cultural, and\ngeographical contexts of a language. Our experiments reveal that on LA tasks\nEnglish exhibits notable linguistic dominance in confidence estimations than\nother languages, while on LS tasks, using question-related language to prompt\nLLMs demonstrates better linguistic dominance in multilingual confidence\nestimations. The phenomena inspire a simple yet effective native-tone prompting\nstrategy by employing language-specific prompts for LS tasks, effectively\nimproving LLMs' reliability and accuracy on LS tasks.\n","authors":["Boyang Xue","Hongru Wang","Rui Wang","Sheng Wang","Zezhong Wang","Yiming Du","Bin Liang","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2402.13606v3.pdf","comment":"Comments: n pages; Previously this version appeared as\n  arXiv:2410.12478 which was submitted as a new work by accident"},{"id":"http://arxiv.org/abs/2410.14145v1","updated":"2024-10-18T03:33:18Z","published":"2024-10-18T03:33:18Z","title":"CAPE: A Chinese Dataset for Appraisal-based Emotional Generation using\n  Large Language Models","summary":"  Generating emotionally appropriate responses in conversations with large\nlanguage models presents a significant challenge due to the complexities of\nhuman emotions and cognitive processes, which remain largely underexplored in\ntheir critical role in social interactions. In this study, we introduce a\ntwo-stage automatic data generation framework to create CAPE, a Chinese dataset\nnamed Cognitive Appraisal theory-based Emotional corpus. This corpus\nfacilitates the generation of dialogues with contextually appropriate emotional\nresponses by accounting for diverse personal and situational factors. We\npropose two tasks utilizing this dataset: emotion prediction and next utterance\nprediction. Both automated and human evaluations demonstrate that agents\ntrained on our dataset can deliver responses that are more aligned with human\nemotional expressions. Our study shows the potential for advancing emotional\nexpression in conversational agents, paving the way for more nuanced and\nmeaningful human-computer interactions.\n","authors":["June M. Liu","He Cao","Renliang Sun","Rui Wang","Yu Li","Jiaxing Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.14145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14144v1","updated":"2024-10-18T03:32:00Z","published":"2024-10-18T03:32:00Z","title":"A Lightweight Multi Aspect Controlled Text Generation Solution For Large\n  Language Models","summary":"  Large language models (LLMs) show remarkable abilities with instruction\ntuning. However, they fail to achieve ideal tasks when lacking high-quality\ninstruction tuning data on target tasks. Multi-Aspect Controllable Text\nGeneration (MCTG) is a representative task for this dilemma, where aspect\ndatasets are usually biased and correlated. Existing work exploits additional\nmodel structures and strategies for solutions, limiting adaptability to LLMs.\nTo activate MCTG ability of LLMs, we propose a lightweight MCTG pipeline based\non data augmentation. We analyze bias and correlations in traditional datasets,\nand address these concerns with augmented control attributes and sentences.\nAugmented datasets are feasible for instruction tuning. In our experiments,\nLLMs perform better in MCTG after data augmentation, with a 20% accuracy rise\nand less aspect correlations.\n","authors":["Chenyang Zhang","Jiayi Lin","Haibo Tong","Bingxuan Hou","Dongyu Zhang","Jialin Li","Junli Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02052v3","updated":"2024-10-18T03:27:37Z","published":"2024-10-02T21:42:35Z","title":"ExACT: Teaching AI Agents to Explore with Reflective-MCTS and\n  Exploratory Learning","summary":"  Autonomous agents have demonstrated significant potential in automating\ncomplex multistep decision-making tasks. However, even state-of-the-art\nvision-language models (VLMs), such as GPT-4o, still fall short of human-level\nperformance, particularly in intricate web environments and long-horizon tasks.\nTo address these limitations, we present ExACT, an approach to combine\ntest-time search and self-learning to build o1-like models for agentic\napplications. We first introduce Reflective Monte Carlo Tree Search (R-MCTS), a\nnovel test time algorithm designed to enhance AI agents' ability to explore\ndecision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating\ncontrastive reflection, allowing agents to learn from past interactions and\ndynamically improve their search efficiency; and 2) using multi-agent debate\nfor reliable state evaluation. Next, we introduce Exploratory Learning, a novel\nlearning strategy to teach agents to search at inference time without relying\non any external search algorithms. On the challenging VisualWebArena benchmark,\nour GPT-4o based R-MCTS agent achieves a 6% to 30% relative improvement across\nvarious tasks compared to the previous state-of-the-art. Additionally, we show\nthat the knowledge and experience gained from test-time search can be\neffectively transferred back to GPT-4o via fine-tuning. After Exploratory\nLearning, GPT-4o 1) demonstrates the ability to explore the environment,\nevaluate a state, and backtrack to viable ones when it detects that the current\nstate cannot lead to success, and 2) matches 87% of R-MCTS's performance while\nusing significantly less compute. Notably, our work demonstrates the compute\nscaling properties in both training - data collection with R-MCTS - and testing\ntime. These results suggest a promising research direction to enhance VLMs'\ncapabilities for agentic applications via test-time search and self-learning.\n","authors":["Xiao Yu","Baolin Peng","Vineeth Vajipey","Hao Cheng","Michel Galley","Jianfeng Gao","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2410.02052v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14141v1","updated":"2024-10-18T03:26:06Z","published":"2024-10-18T03:26:06Z","title":"Coherence-Driven Multimodal Safety Dialogue with Active Learning for\n  Embodied Agents","summary":"  When assisting people in daily tasks, robots need to accurately interpret\nvisual cues and respond effectively in diverse safety-critical situations, such\nas sharp objects on the floor. In this context, we present M-CoDAL, a\nmultimodal-dialogue system specifically designed for embodied agents to better\nunderstand and communicate in safety-critical situations. The system leverages\ndiscourse coherence relations to enhance its contextual understanding and\ncommunication abilities. To train this system, we introduce a novel\nclustering-based active learning mechanism that utilizes an external Large\nLanguage Model (LLM) to identify informative instances. Our approach is\nevaluated using a newly created multimodal dataset comprising 1K safety\nviolations extracted from 2K Reddit images. These violations are annotated\nusing a Large Multimodal Model (LMM) and verified by human annotators. Results\nwith this dataset demonstrate that our approach improves resolution of safety\nsituations, user sentiment, as well as safety of the conversation. Next, we\ndeploy our dialogue system on a Hello Robot Stretch robot and conduct a\nwithin-subject user study with real-world participants. In the study,\nparticipants role-play two safety scenarios with different levels of severity\nwith the robot and receive interventions from our model and a baseline system\npowered by OpenAI's ChatGPT. The study results corroborate and extend the\nfindings from automated evaluation, showing that our proposed system is more\npersuasive and competent in a real-world embodied agent setting.\n","authors":["Sabit Hassan","Hye-Young Chung","Xiang Zhi Tan","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2410.14141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12478v2","updated":"2024-10-18T03:19:51Z","published":"2024-10-16T11:46:55Z","title":"MlingConf: A Comprehensive Study of Multilingual Confidence Estimation\n  on Large Language Models","summary":"  The tendency of Large Language Models (LLMs) to generate hallucinations\nraises concerns regarding their reliability. Therefore, confidence estimations\nindicating the extent of trustworthiness of the generations become essential.\nHowever, current LLM confidence estimations in languages other than English\nremain underexplored. This paper addresses this gap by introducing a\ncomprehensive investigation of Multilingual Confidence estimation (MlingConf)\non LLMs, focusing on both language-agnostic (LA) and language-specific (LS)\ntasks to explore the performance and language dominance effects of multilingual\nconfidence estimations on different tasks. The benchmark comprises four\nmeticulously checked and human-evaluate high-quality multilingual datasets for\nLA tasks and one for the LS task tailored to specific social, cultural, and\ngeographical contexts of a language. Our experiments reveal that on LA tasks\nEnglish exhibits notable linguistic dominance in confidence estimations than\nother languages, while on LS tasks, using question-related language to prompt\nLLMs demonstrates better linguistic dominance in multilingual confidence\nestimations. The phenomena inspire a simple yet effective native-tone prompting\nstrategy by employing language-specific prompts for LS tasks, effectively\nimproving LLMs' reliability and accuracy on LS tasks.\n","authors":["Boyang Xue","Hongru Wang","Rui Wang","Sheng Wang","Zezhong Wang","Yiming Du","Bin Liang","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2410.12478v2.pdf","comment":"Comments: This work was intended as a replacement of arXiv:2402.13606\n  and any subsequent updates will appear there"},{"id":"http://arxiv.org/abs/2410.04717v3","updated":"2024-10-18T03:18:50Z","published":"2024-10-07T03:15:11Z","title":"$\\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction\n  Diversity on Generalization","summary":"  Understanding and accurately following instructions is critical for large\nlanguage models (LLMs) to be effective across diverse tasks. In this work, we\nrigorously examine the key factors that enable models to generalize to unseen\ninstructions, providing insights to guide the collection of data for\ninstruction-tuning. Through controlled experiments, inspired by the\nTuring-complete Markov algorithm, we demonstrate that such generalization\n$\\textbf{only emerges}$ when training data is diversified enough across\nsemantic domains. Our findings also reveal that merely diversifying within\nlimited domains fails to ensure robust generalization. In contrast,\ncross-domain data diversification, even under constrained data budgets,\nsignificantly enhances a model's adaptability. We further extend our analysis\nto real-world scenarios, including fine-tuning of\n$\\textit{$\\textbf{specialist}$}$ and $\\textit{$\\textbf{generalist}$}$ models.\nIn both cases, we demonstrate that 1) better performance can be achieved by\nincreasing the diversity of an established dataset while keeping the data size\nconstant, and 2) when scaling up the data, diversifying the semantics of\ninstructions is more effective than simply increasing the quantity of similar\ndata. Our research provides important insights for dataset collation,\nparticularly when optimizing model performance by expanding training data for\nboth specialist and generalist scenarios. We show that careful consideration of\ndata diversification is key: training specialist models with data extending\nbeyond their core domain leads to significant performance improvements, while\ngeneralist models benefit from diverse data mixtures that enhance their overall\ninstruction-following capabilities across a wide range of applications. Our\nresults highlight the critical role of strategic diversification and offer\nclear guidelines for improving data quality.\n","authors":["Dylan Zhang","Justin Wang","Francois Charton"],"pdf_url":"https://arxiv.org/pdf/2410.04717v3.pdf","comment":"Fix formatting issues"},{"id":"http://arxiv.org/abs/2410.13804v2","updated":"2024-10-18T03:15:21Z","published":"2024-10-17T17:41:15Z","title":"BenTo: Benchmark Task Reduction with In-Context Transferability","summary":"  Evaluating large language models (LLMs) is costly: it requires the generation\nand examination of LLM outputs on a large-scale benchmark of various tasks.\nThis paper investigates how to efficiently reduce the tasks used to benchmark\nLLMs without affecting the evaluation quality. Our study reveals that task\ntransferability and relevance provide critical information to identify the most\nrepresentative subset of tasks via optimizing a facility location function. We\npropose a practically efficient metric for estimating the transferability\nbetween two tasks via in-context learning (ICL). By analyzing the pairwise\ntransferability, we can reduce tasks in a modern LLM benchmark (e.g., MMLU or\nFLAN) to 5% while inducing only a <4% difference to the evaluation on the\noriginal benchmark. Compared to prior works, our method is training-free,\ngradient-free, and highly efficient requiring ICL only.\n","authors":["Hongyu Zhao","Ming Li","Lichao Sun","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.13804v2.pdf","comment":"https://github.com/tianyi-lab/bento"},{"id":"http://arxiv.org/abs/2409.03258v2","updated":"2024-10-18T03:11:28Z","published":"2024-09-05T05:34:16Z","title":"GraphInsight: Unlocking Insights in Large Language Models for Graph\n  Structure Understanding","summary":"  Although Large Language Models (LLMs) have demonstrated potential in\nprocessing graphs, they struggle with comprehending graphical structure\ninformation through prompts of graph description sequences, especially as the\ngraph size increases. We attribute this challenge to the uneven memory\nperformance of LLMs across different positions in graph description sequences,\nknown as ''positional biases''. To address this, we propose GraphInsight, a\nnovel framework aimed at improving LLMs' comprehension of both macro- and\nmicro-level graphical information. GraphInsight is grounded in two key\nstrategies: 1) placing critical graphical information in positions where LLMs\nexhibit stronger memory performance, and 2) investigating a lightweight\nexternal knowledge base for regions with weaker memory performance, inspired by\nretrieval-augmented generation (RAG). Moreover, GraphInsight explores\nintegrating these two strategies into LLM agent processes for composite graph\ntasks that require multi-step reasoning. Extensive empirical studies on\nbenchmarks with a wide range of evaluation tasks show that GraphInsight\nsignificantly outperforms all other graph description methods (e.g., prompting\ntechniques and reordering strategies) in understanding graph structures of\nvarying sizes.\n","authors":["Yukun Cao","Shuo Han","Zengyi Gao","Zezhong Ding","Xike Xie","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.03258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13960v3","updated":"2024-10-18T03:10:13Z","published":"2024-06-20T03:02:38Z","title":"AutoPal: Autonomous Adaptation to Users for Personal AI Companionship","summary":"  Previous research has demonstrated the potential of AI agents to act as\ncompanions that can provide constant emotional support for humans. In this\npaper, we emphasize the necessity of autonomous adaptation in personal AI\ncompanionship, an underexplored yet promising direction. Such adaptability is\ncrucial as it can facilitate more tailored interactions with users and allow\nthe agent to evolve in response to users' changing needs. However, imbuing\nagents with autonomous adaptability presents unique challenges, including\nidentifying optimal adaptations to meet users' expectations and ensuring a\nsmooth transition during the adaptation process. To address them, we devise a\nhierarchical framework, AutoPal, that enables controllable and authentic\nadjustments to the agent's persona based on user interactions. A\npersonamatching dataset is constructed to facilitate the learning of optimal\npersona adaptations. Extensive experiments demonstrate the effectiveness of\nAutoPal and highlight the importance of autonomous adaptability in AI\ncompanionship.\n","authors":["Yi Cheng","Wenge Liu","Kaishuai Xu","Wenjun Hou","Yi Ouyang","Chak Tou Leong","Xian Wu","Yefeng Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.13960v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07825v3","updated":"2024-10-18T03:06:39Z","published":"2024-03-12T17:04:28Z","title":"Efficiently Quantifying and Mitigating Ripple Effects in Model Editing","summary":"  Large Language Models have revolutionized numerous tasks with their\nremarkable efficacy. However, editing these models, crucial for rectifying\noutdated or erroneous information, often leads to a complex issue known as the\nripple effect in the hidden space. While difficult to detect, this effect can\nsignificantly impede the efficacy of model editing tasks and deteriorate model\nperformance. This paper addresses this scientific challenge by proposing a\nnovel evaluation methodology, Graphical Impact Evaluation(GIE), which\nquantitatively evaluates the adaptations of the model and the subsequent impact\nof editing. Furthermore, we introduce the Selective Impact Revision(SIR), a\nmodel editing method designed to mitigate this ripple effect. Our comprehensive\nevaluations reveal that the ripple effect in the hidden space is a significant\nissue in all current model editing methods. However, our proposed methods, GIE\nand SIR, effectively identify and alleviate this issue, contributing to the\nadvancement of LLM editing techniques.\n","authors":["Jianchen Wang","Zhouhong Gu","Xiaoxuan Zhu","Lin Zhang","Haoning Ye","Zhuozhi Xiong","Hongwei Feng","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.07825v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13408v2","updated":"2024-10-18T03:05:01Z","published":"2024-10-17T10:14:52Z","title":"MoR: Mixture of Ranks for Low-Rank Adaptation Tuning","summary":"  Low-Rank Adaptation (LoRA) drives research to align its performance with full\nfine-tuning. However, significant challenges remain: (1) Simply increasing the\nrank size of LoRA does not effectively capture high-rank information, which\nleads to a performance bottleneck.(2) MoE-style LoRA methods substantially\nincrease parameters and inference latency, contradicting the goals of efficient\nfine-tuning and ease of application. To address these challenges, we introduce\nMixture of Ranks (MoR), which learns rank-specific information for different\ntasks based on input and efficiently integrates multi-rank information. We\nfirstly propose a new framework that equates the integration of multiple LoRAs\nto expanding the rank of LoRA. Moreover, we hypothesize that low-rank LoRA\nalready captures sufficient intrinsic information, and MoR can derive high-rank\ninformation through mathematical transformations of the low-rank components.\nThus, MoR can reduces the learning difficulty of LoRA and enhances its\nmulti-task capabilities. MoR achieves impressive results, with MoR delivering a\n1.31\\% performance improvement while using only 93.93\\% of the parameters\ncompared to baseline methods.\n","authors":["Chuanyu Tang","Yilong Chen","Zhenyu Zhang","Junyuan Shang","Wenyuan Zhang","Yong Huang","Tingwen Liu"],"pdf_url":"https://arxiv.org/pdf/2410.13408v2.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.12841v2","updated":"2024-10-18T03:03:01Z","published":"2024-10-09T17:33:15Z","title":"UniAutoML: A Human-Centered Framework for Unified Discriminative and\n  Generative AutoML with Large Language Models","summary":"  Automated Machine Learning (AutoML) has simplified complex ML processes such\nas data pre-processing, model selection, and hyper-parameter searching.\nHowever, traditional AutoML frameworks focus solely on discriminative tasks,\noften falling short in tackling AutoML for generative models. Additionally,\nthese frameworks lack interpretability and user engagement during the training\nprocess, primarily due to the absence of human-centered design. It leads to a\nlack of transparency in final decision-making and limited user control,\npotentially reducing trust and adoption of AutoML methods. To address these\nlimitations, we introduce UniAutoML, a human-centered AutoML framework that\nleverages Large Language Models (LLMs) to unify AutoML for both discriminative\n(e.g., Transformers and CNNs for classification or regression tasks) and\ngenerative tasks (e.g., fine-tuning diffusion models or LLMs). The\nhuman-centered design of UniAutoML innovatively features a conversational user\ninterface (CUI) that facilitates natural language interactions, providing users\nwith real-time guidance, feedback, and progress updates for better\ninterpretability. This design enhances transparency and user control throughout\nthe AutoML training process, allowing users to seamlessly break down or modify\nthe model being trained. To mitigate potential risks associated with LLM\ngenerated content, UniAutoML incorporates a safety guardline that filters\ninputs and censors outputs. We evaluated UniAutoML's performance and usability\nthrough experiments on eight diverse datasets and user studies involving 25\nparticipants, demonstrating that UniAutoML not only enhances performance but\nalso improves user control and trust. Our human-centered design bridges the gap\nbetween AutoML capabilities and user understanding, making ML more accessible\nto a broader audience.\n","authors":["Jiayi Guo","Zan Chen","Yingrui Ji","Liyun Zhang","Daqin Luo","Zhigang Li","Yiqin Shen"],"pdf_url":"https://arxiv.org/pdf/2410.12841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14132v1","updated":"2024-10-18T03:00:03Z","published":"2024-10-18T03:00:03Z","title":"ViConsFormer: Constituting Meaningful Phrases of Scene Texts using\n  Transformer-based Method in Vietnamese Text-based Visual Question Answering","summary":"  Text-based VQA is a challenging task that requires machines to use scene\ntexts in given images to yield the most appropriate answer for the given\nquestion. The main challenge of text-based VQA is exploiting the meaning and\ninformation from scene texts. Recent studies tackled this challenge by\nconsidering the spatial information of scene texts in images via embedding 2D\ncoordinates of their bounding boxes. In this study, we follow the definition of\nmeaning from linguistics to introduce a novel method that effectively exploits\nthe information from scene texts written in Vietnamese. Experimental results\nshow that our proposed method obtains state-of-the-art results on two\nlarge-scale Vietnamese Text-based VQA datasets. The implementation can be found\nat this link.\n","authors":["Nghia Hieu Nguyen","Tho Thanh Quan","Ngan Luu-Thuy Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.14132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12847v2","updated":"2024-10-18T02:56:32Z","published":"2024-10-10T07:48:53Z","title":"ACCEPT: Adaptive Codebook for Composite and Efficient Prompt Tuning","summary":"  Prompt Tuning has been a popular Parameter-Efficient Fine-Tuning method\nattributed to its remarkable performance with few updated parameters on various\nlarge-scale pretrained Language Models (PLMs). Traditionally, each prompt has\nbeen considered indivisible and updated independently, leading the parameters\nincrease proportionally as prompt length grows. To address this issue, we\npropose Adaptive Codebook for Composite and Efficient Prompt Tuning (ACCEPT).\nIn our method, we refer to the concept of product quantization (PQ), allowing\nall soft prompts to share a set of learnable codebook vectors in each subspace,\nwith each prompt differentiated by a set of adaptive weights. We achieve the\nsuperior performance on 17 diverse natural language tasks including natural\nlanguage understanding (NLU) and question answering (QA) tasks by tuning only\n0.3% of parameters of the PLMs. Our approach also excels in few-shot and large\nmodel settings, highlighting its significant potential.\n","authors":["Yu-Chen Lin","Wei-Hua Li","Jun-Cheng Chen","Chu-Song Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12847v2.pdf","comment":"EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2406.05213v2","updated":"2024-10-18T02:55:27Z","published":"2024-06-07T18:54:40Z","title":"On Subjective Uncertainty Quantification and Calibration in Natural\n  Language Generation","summary":"  Applications of large language models often involve the generation of\nfree-form responses, in which case uncertainty quantification becomes\nchallenging. This is due to the need to identify task-specific uncertainties\n(e.g., about the semantics) which appears difficult to define in general cases.\nThis work addresses these challenges from a perspective of Bayesian decision\ntheory, starting from the assumption that our utility is characterized by a\nsimilarity measure that compares a generated response with a hypothetical true\nresponse. We discuss how this assumption enables principled quantification of\nthe model's subjective uncertainty and its calibration. We further derive a\nmeasure for epistemic uncertainty, based on a missing data perspective and its\ncharacterization as an excess risk. The proposed methods can be applied to\nblack-box language models. We illustrate the methods on question answering and\nmachine translation tasks. Our experiments provide a principled evaluation of\ntask-specific calibration, and demonstrate that epistemic uncertainty offers a\npromising deferral strategy for efficient data acquisition in in-context\nlearning.\n","authors":["Ziyu Wang","Chris Holmes"],"pdf_url":"https://arxiv.org/pdf/2406.05213v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13166v2","updated":"2024-10-18T02:53:14Z","published":"2024-10-17T02:47:10Z","title":"An Evolved Universal Transformer Memory","summary":"  Prior methods propose to offset the escalating costs of modern foundation\nmodels by dropping specific parts of their contexts with hand-designed rules,\nwhile attempting to preserve their original performance. We overcome this\ntrade-off with Neural Attention Memory Models (NAMMs), introducing a learned\nnetwork for memory management that improves both the performance and efficiency\nof transformers. We evolve NAMMs atop pre-trained transformers to provide\ndifferent latent contexts focusing on the most relevant information for\nindividual layers and attention heads. NAMMs are universally applicable to any\nmodel using self-attention as they condition exclusively on the values in the\nproduced attention matrices. Learning NAMMs on a small set of problems, we\nachieve substantial performance improvements across multiple long-context\nbenchmarks while cutting the model's input contexts up to a fraction of the\noriginal sizes. We show the generality of our conditioning enables zero-shot\ntransfer of NAMMs trained only on language to entirely new transformer\narchitectures even across input modalities, with their benefits carrying over\nto vision and reinforcement learning.\n","authors":["Edoardo Cetin","Qi Sun","Tianyu Zhao","Yujin Tang"],"pdf_url":"https://arxiv.org/pdf/2410.13166v2.pdf","comment":"29 pages, 14 figures. Preprint, under submission. Source code is\n  available at https://github.com/SakanaAI/evo-memory"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2410.14672v1","updated":"2024-10-18T17:59:04Z","published":"2024-10-18T17:59:04Z","title":"BiGR: Harnessing Binary Latent Codes for Image Generation and Improved\n  Visual Representation Capabilities","summary":"  We introduce BiGR, a novel conditional image generation model using compact\nbinary latent codes for generative training, focusing on enhancing both\ngeneration and representation capabilities. BiGR is the first conditional\ngenerative model that unifies generation and discrimination within the same\nframework. BiGR features a binary tokenizer, a masked modeling mechanism, and a\nbinary transcoder for binary code prediction. Additionally, we introduce a\nnovel entropy-ordered sampling method to enable efficient image generation.\nExtensive experiments validate BiGR's superior performance in generation\nquality, as measured by FID-50k, and representation capabilities, as evidenced\nby linear-probe accuracy. Moreover, BiGR showcases zero-shot generalization\nacross various vision tasks, enabling applications such as image inpainting,\noutpainting, editing, interpolation, and enrichment, without the need for\nstructural modifications. Our findings suggest that BiGR unifies generative and\ndiscriminative tasks effectively, paving the way for further advancements in\nthe field.\n","authors":["Shaozhe Hao","Xuantong Liu","Xianbiao Qi","Shihao Zhao","Bojia Zi","Rong Xiao","Kai Han","Kwan-Yee K. Wong"],"pdf_url":"https://arxiv.org/pdf/2410.14672v1.pdf","comment":"Project page: https://haoosz.github.io/BiGR"},{"id":"http://arxiv.org/abs/2410.14669v1","updated":"2024-10-18T17:58:21Z","published":"2024-10-18T17:58:21Z","title":"NaturalBench: Evaluating Vision-Language Models on Natural Adversarial\n  Samples","summary":"  Vision-language models (VLMs) have made significant progress in recent\nvisual-question-answering (VQA) benchmarks that evaluate complex\nvisio-linguistic reasoning. However, are these models truly effective? In this\nwork, we show that VLMs still struggle with natural images and questions that\nhumans can easily answer, which we term natural adversarial samples. We also\nfind it surprisingly easy to generate these VQA samples from natural image-text\ncorpora using off-the-shelf models like CLIP and ChatGPT. We propose a\nsemi-automated approach to collect a new benchmark, NaturalBench, for reliably\nevaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a\n$\\textbf{vision-centric}$ design by pairing each question with two images that\nyield different answers, preventing blind solutions from answering without\nusing the images. This makes NaturalBench more challenging than previous\nbenchmarks that can be solved with commonsense priors. We evaluate 53\nstate-of-the-art VLMs on NaturalBench, showing that models like\nLLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o\nlag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is\nhard from two angles: (1) Compositionality: Solving NaturalBench requires\ndiverse visio-linguistic skills, including understanding attribute bindings,\nobject relationships, and advanced reasoning like logic and counting. To this\nend, unlike prior work that uses a single tag per sample, we tag each\nNaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2)\nBiases: NaturalBench exposes severe biases in VLMs, as models often choose the\nsame answer regardless of the image. Lastly, we apply our benchmark curation\nmethod to diverse data sources, including long captions (over 100 words) and\nnon-English languages like Chinese and Hindi, highlighting its potential for\ndynamic evaluations of VLMs.\n","authors":["Baiqi Li","Zhiqiu Lin","Wenxuan Peng","Jean de Dieu Nyandwi","Daniel Jiang","Zixian Ma","Simran Khanuja","Ranjay Krishna","Graham Neubig","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2410.14669v1.pdf","comment":"Accepted to NeurIPS 24; We open-source our dataset at:\n  https://huggingface.co/datasets/BaiqiL/NaturalBench; Project page at:\n  https://linzhiqiu.github.io/papers/naturalbench/"},{"id":"http://arxiv.org/abs/2409.06445v2","updated":"2024-10-18T17:37:51Z","published":"2024-09-10T12:00:40Z","title":"Learning Generative Interactive Environments By Trained Agent\n  Exploration","summary":"  World models are increasingly pivotal in interpreting and simulating the\nrules and actions of complex environments. Genie, a recent model, excels at\nlearning from visually diverse environments but relies on costly\nhuman-collected data. We observe that their alternative method of using random\nagents is too limited to explore the environment. We propose to improve the\nmodel by employing reinforcement learning based agents for data generation.\nThis approach produces diverse datasets that enhance the model's ability to\nadapt and perform well across various scenarios and realistic actions within\nthe environment. In this paper, we first release the model GenieRedux - an\nimplementation based on Genie. Additionally, we introduce GenieRedux-G, a\nvariant that uses the agent's readily available actions to factor out action\nprediction uncertainty during validation. Our evaluation, including a\nreplication of the Coinrun case study, shows that GenieRedux-G achieves\nsuperior visual fidelity and controllability using the trained agent\nexploration. The proposed approach is reproducable, scalable and adaptable to\nnew types of environments. Our codebase is available at\nhttps://github.com/insait-institute/GenieRedux .\n","authors":["Naser Kazemi","Nedko Savov","Danda Paudel","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2409.06445v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14634v1","updated":"2024-10-18T17:35:33Z","published":"2024-10-18T17:35:33Z","title":"Parallel Backpropagation for Inverse of a Convolution with Application\n  to Normalizing Flows","summary":"  Inverse of an invertible convolution is an important operation that comes up\nin Normalizing Flows, Image Deblurring, etc. The naive algorithm for\nbackpropagation of this operation using Gaussian elimination has running time\n$O(n^3)$ where $n$ is the number of pixels in the image. We give a fast\nparallel backpropagation algorithm with running time $O(\\sqrt{n})$ for a square\nimage and provide a GPU implementation of the same. Inverse Convolutions are\nusually used in Normalizing Flows in the sampling pass, making them slow. We\npropose to use Inverse Convolutions in the forward (image to latent vector)\npass of the Normalizing flow. Since the sampling pass is the inverse of the\nforward pass, it will use convolutions only, resulting in efficient sampling\ntimes. We use our parallel backpropagation algorithm for optimizing the inverse\nconvolution layer resulting in fast training times also. We implement this\napproach in various Normalizing Flow backbones, resulting in our Inverse-Flow\nmodels. We benchmark Inverse-Flow on standard datasets and show significantly\nimproved sampling times with similar bits per dimension compared to previous\nmodels.\n","authors":["Sandeep Nagar","Girish Varma"],"pdf_url":"https://arxiv.org/pdf/2410.14634v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.14633v1","updated":"2024-10-18T17:32:39Z","published":"2024-10-18T17:32:39Z","title":"Swiss Army Knife: Synergizing Biases in Knowledge from Vision Foundation\n  Models for Multi-Task Learning","summary":"  Vision Foundation Models (VFMs) have demonstrated outstanding performance on\nnumerous downstream tasks. However, due to their inherent representation biases\noriginating from different training paradigms, VFMs exhibit advantages and\ndisadvantages across distinct vision tasks. Although amalgamating the strengths\nof multiple VFMs for downstream tasks is an intuitive strategy, effectively\nexploiting these biases remains a significant challenge. In this paper, we\npropose a novel and versatile \"Swiss Army Knife\" (SAK) solution, which\nadaptively distills knowledge from a committee of VFMs to enhance multi-task\nlearning. Unlike existing methods that use a single backbone for knowledge\ntransfer, our approach preserves the unique representation bias of each teacher\nby collaborating the lightweight Teacher-Specific Adapter Path modules with the\nTeacher-Agnostic Stem. Through dynamic selection and combination of\nrepresentations with Mixture-of-Representations Routers, our SAK is capable of\nsynergizing the complementary strengths of multiple VFMs. Extensive experiments\nshow that our SAK remarkably outperforms prior state of the arts in multi-task\nlearning by 10% on the NYUD-v2 benchmark, while also providing a flexible and\nrobust framework that can readily accommodate more advanced model designs.\n","authors":["Yuxiang Lu","Shengcao Cao","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01804v4","updated":"2024-10-18T17:20:20Z","published":"2024-10-02T17:59:09Z","title":"EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis","summary":"  We present Exact Volumetric Ellipsoid Rendering (EVER), a method for\nreal-time differentiable emission-only volume rendering. Unlike recent\nrasterization based approach by 3D Gaussian Splatting (3DGS), our primitive\nbased representation allows for exact volume rendering, rather than alpha\ncompositing 3D Gaussian billboards. As such, unlike 3DGS our formulation does\nnot suffer from popping artifacts and view dependent density, but still\nachieves frame rates of $\\sim\\!30$ FPS at 720p on an NVIDIA RTX4090. Since our\napproach is built upon ray tracing it enables effects such as defocus blur and\ncamera distortion (e.g. such as from fisheye cameras), which are difficult to\nachieve by rasterization. We show that our method is more accurate with fewer\nblending issues than 3DGS and follow-up work on view-consistent rendering,\nespecially on the challenging large-scale scenes from the Zip-NeRF dataset\nwhere it achieves sharpest results among real-time techniques.\n","authors":["Alexander Mai","Peter Hedman","George Kopanas","Dor Verbin","David Futschik","Qiangeng Xu","Falko Kuester","Jonathan T. Barron","Yinda Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01804v4.pdf","comment":"Project page: https://half-potato.gitlab.io/posts/ever"},{"id":"http://arxiv.org/abs/2410.14612v1","updated":"2024-10-18T17:05:03Z","published":"2024-10-18T17:05:03Z","title":"MultiOrg: A Multi-rater Organoid-detection Dataset","summary":"  High-throughput image analysis in the biomedical domain has gained\nsignificant attention in recent years, driving advancements in drug discovery,\ndisease prediction, and personalized medicine. Organoids, specifically, are an\nactive area of research, providing excellent models for human organs and their\nfunctions. Automating the quantification of organoids in microscopy images\nwould provide an effective solution to overcome substantial manual\nquantification bottlenecks, particularly in high-throughput image analysis.\nHowever, there is a notable lack of open biomedical datasets, in contrast to\nother domains, such as autonomous driving, and, notably, only few of them have\nattempted to quantify annotation uncertainty. In this work, we present MultiOrg\na comprehensive organoid dataset tailored for object detection tasks with\nuncertainty quantification. This dataset comprises over 400 high-resolution 2d\nmicroscopy images and curated annotations of more than 60,000 organoids. Most\nimportantly, it includes three label sets for the test data, independently\nannotated by two experts at distinct time points. We additionally provide a\nbenchmark for organoid detection, and make the best model available through an\neasily installable, interactive plugin for the popular image visualization tool\nNapari, to perform organoid quantification.\n","authors":["Christina Bukas","Harshavardhan Subramanian","Fenja See","Carina Steinchen","Ivan Ezhov","Gowtham Boosarpu","Sara Asgharpour","Gerald Burgstaller","Mareike Lehmann","Florian Kofler","Marie Piraud"],"pdf_url":"https://arxiv.org/pdf/2410.14612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14595v1","updated":"2024-10-18T16:48:31Z","published":"2024-10-18T16:48:31Z","title":"DRACO-DehazeNet: An Efficient Image Dehazing Network Combining Detail\n  Recovery and a Novel Contrastive Learning Paradigm","summary":"  Image dehazing is crucial for clarifying images obscured by haze or fog, but\ncurrent learning-based approaches is dependent on large volumes of training\ndata and hence consumed significant computational power. Additionally, their\nperformance is often inadequate under non-uniform or heavy haze. To address\nthese challenges, we developed the Detail Recovery And Contrastive DehazeNet,\nwhich facilitates efficient and effective dehazing via a dense dilated inverted\nresidual block and an attention-based detail recovery network that tailors\nenhancements to specific dehazed scene contexts. A major innovation is its\nability to train effectively with limited data, achieved through a novel\nquadruplet loss-based contrastive dehazing paradigm. This approach distinctly\nseparates hazy and clear image features while also distinguish lower-quality\nand higher-quality dehazed images obtained from each sub-modules of our\nnetwork, thereby refining the dehazing process to a larger extent. Extensive\ntests on a variety of benchmarked haze datasets demonstrated the superiority of\nour approach. The code repository for this work will be available soon.\n","authors":["Gao Yu Lee","Tanmoy Dam","Md Meftahul Ferdaus","Daniel Puiu Poenar","Vu Duong"],"pdf_url":"https://arxiv.org/pdf/2410.14595v1.pdf","comment":"Submitted to a journal and currently under review. Once the paper is\n  accepted and published, the copyright will be transferred to the\n  corresponding journal"},{"id":"http://arxiv.org/abs/2404.13370v2","updated":"2024-10-18T16:44:05Z","published":"2024-04-20T13:15:27Z","title":"Movie101v2: Improved Movie Narration Benchmark","summary":"  Automatic movie narration aims to generate video-aligned plot descriptions to\nassist visually impaired audiences. Unlike standard video captioning, it\ninvolves not only describing key visual details but also inferring plots that\nunfold across multiple movie shots, presenting distinct and complex challenges.\nTo advance this field, we introduce Movie101v2, a large-scale, bilingual\ndataset with enhanced data quality specifically designed for movie narration.\nRevisiting the task, we propose breaking down the ultimate goal of automatic\nmovie narration into three progressive stages, offering a clear roadmap with\ncorresponding evaluation metrics. Based on our new benchmark, we baseline a\nrange of large vision-language models, including GPT-4V, and conduct an\nin-depth analysis of the challenges in narration generation. Our findings\nhighlight that achieving applicable movie narration generation is a fascinating\ngoal that requires significant research.\n","authors":["Zihao Yue","Yepeng Zhang","Ziheng Wang","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2404.13370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17777v2","updated":"2024-10-18T16:31:49Z","published":"2024-09-26T12:15:13Z","title":"Harnessing Shared Relations via Multimodal Mixup Contrastive Learning\n  for Multimodal Classification","summary":"  Deep multimodal learning has shown remarkable success by leveraging\ncontrastive learning to capture explicit one-to-one relations across\nmodalities. However, real-world data often exhibits shared relations beyond\nsimple pairwise associations. We propose M3CoL, a Multimodal Mixup Contrastive\nLearning approach to capture nuanced shared relations inherent in multimodal\ndata. Our key contribution is a Mixup-based contrastive loss that learns robust\nrepresentations by aligning mixed samples from one modality with their\ncorresponding samples from other modalities thereby capturing shared relations\nbetween them. For multimodal classification tasks, we introduce a framework\nthat integrates a fusion module with unimodal prediction modules for auxiliary\nsupervision during training, complemented by our proposed Mixup-based\ncontrastive loss. Through extensive experiments on diverse datasets (N24News,\nROSMAP, BRCA, and Food-101), we demonstrate that M3CoL effectively captures\nshared multimodal relations and generalizes across domains. It outperforms\nstate-of-the-art methods on N24News, ROSMAP, and BRCA, while achieving\ncomparable performance on Food-101. Our work highlights the significance of\nlearning shared relations for robust multimodal learning, opening up promising\navenues for future research.\n","authors":["Raja Kumar","Raghav Singhal","Pranamya Kulkarni","Deval Mehta","Kshitij Jadhav"],"pdf_url":"https://arxiv.org/pdf/2409.17777v2.pdf","comment":"RK and RS contributed equally to this work, 20 Pages, 8 Figures, 9\n  Tables. Another version of the paper accepted at NeurIPS 2024 Workshop on\n  Unifying Representations in Neural Models (UniReps)"},{"id":"http://arxiv.org/abs/2410.08107v2","updated":"2024-10-18T16:26:30Z","published":"2024-10-10T16:54:23Z","title":"IncEventGS: Pose-Free Gaussian Splatting from a Single Event Camera","summary":"  Implicit neural representation and explicit 3D Gaussian Splatting (3D-GS) for\nnovel view synthesis have achieved remarkable progress with frame-based camera\n(e.g. RGB and RGB-D cameras) recently. Compared to frame-based camera, a novel\ntype of bio-inspired visual sensor, i.e. event camera, has demonstrated\nadvantages in high temporal resolution, high dynamic range, low power\nconsumption and low latency. Due to its unique asynchronous and irregular data\ncapturing process, limited work has been proposed to apply neural\nrepresentation or 3D Gaussian splatting for an event camera. In this work, we\npresent IncEventGS, an incremental 3D Gaussian Splatting reconstruction\nalgorithm with a single event camera. To recover the 3D scene representation\nincrementally, we exploit the tracking and mapping paradigm of conventional\nSLAM pipelines for IncEventGS. Given the incoming event stream, the tracker\nfirstly estimates an initial camera motion based on prior reconstructed 3D-GS\nscene representation. The mapper then jointly refines both the 3D scene\nrepresentation and camera motion based on the previously estimated motion\ntrajectory from the tracker. The experimental results demonstrate that\nIncEventGS delivers superior performance compared to prior NeRF-based methods\nand other related baselines, even we do not have the ground-truth camera poses.\nFurthermore, our method can also deliver better performance compared to\nstate-of-the-art event visual odometry methods in terms of camera motion\nestimation. Code is publicly available at:\nhttps://github.com/wu-cvgl/IncEventGS.\n","authors":["Jian Huang","Chengrui Dong","Peidong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.08107v2.pdf","comment":"Code Page: https://github.com/wu-cvgl/IncEventGS"},{"id":"http://arxiv.org/abs/2410.13174v2","updated":"2024-10-18T16:26:30Z","published":"2024-10-17T02:57:35Z","title":"Scalable Drift Monitoring in Medical Imaging AI","summary":"  The integration of artificial intelligence (AI) into medical imaging has\nadvanced clinical diagnostics but poses challenges in managing model drift and\nensuring long-term reliability. To address these challenges, we develop MMC+,\nan enhanced framework for scalable drift monitoring, building upon the\nCheXstray framework that introduced real-time drift detection for medical\nimaging AI models using multi-modal data concordance. This work extends the\noriginal framework's methodologies, providing a more scalable and adaptable\nsolution for real-world healthcare settings and offers a reliable and\ncost-effective alternative to continuous performance monitoring addressing\nlimitations of both continuous and periodic monitoring methods. MMC+ introduces\ncritical improvements to the original framework, including more robust handling\nof diverse data streams, improved scalability with the integration of\nfoundation models like MedImageInsight for high-dimensional image embeddings\nwithout site-specific training, and the introduction of uncertainty bounds to\nbetter capture drift in dynamic clinical environments. Validated with\nreal-world data from Massachusetts General Hospital during the COVID-19\npandemic, MMC+ effectively detects significant data shifts and correlates them\nwith model performance changes. While not directly predicting performance\ndegradation, MMC+ serves as an early warning system, indicating when AI systems\nmay deviate from acceptable performance bounds and enabling timely\ninterventions. By emphasizing the importance of monitoring diverse data streams\nand evaluating data shifts alongside model performance, this work contributes\nto the broader adoption and integration of AI solutions in clinical settings.\n","authors":["Jameson Merkow","Felix J. Dorfner","Xiyu Yang","Alexander Ersoy","Giridhar Dasegowda","Mannudeep Kalra","Matthew P. Lungren","Christopher P. Bridge","Ivan Tarapov"],"pdf_url":"https://arxiv.org/pdf/2410.13174v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14574v1","updated":"2024-10-18T16:20:22Z","published":"2024-10-18T16:20:22Z","title":"MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts","summary":"  Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled\nscalability in deep learning. SMoE has the potential to exponentially increase\nparameter count while maintaining the efficiency of the model by only\nactivating a small subset of these parameters for a given sample. However, it\nhas been observed that SMoE suffers from unstable training and has difficulty\nadapting to new distributions, leading to the model's lack of robustness to\ndata contamination. To overcome these limitations, we first establish a\nconnection between the dynamics of the expert representations in SMoEs and\ngradient descent on a multi-objective optimization problem. Leveraging our\nframework, we then integrate momentum into SMoE and propose a new family of\nSMoEs named MomentumSMoE. We theoretically prove and numerically demonstrate\nthat MomentumSMoE is more stable and robust than SMoE. In particular, we verify\nthe advantages of MomentumSMoE over SMoE on a variety of practical tasks\nincluding ImageNet-1K object recognition and WikiText-103 language modeling. We\ndemonstrate the applicability of MomentumSMoE to many types of SMoE models,\nincluding those in the Sparse MoE model for vision (V-MoE) and the Generalist\nLanguage Model (GLaM). We also show that other advanced momentum-based\noptimization methods, such as Adam, can be easily incorporated into the\nMomentumSMoE framework for designing new SMoE models with even better\nperformance, almost negligible additional computation cost, and simple\nimplementations.\n","authors":["Rachel S. Y. Teo","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.14574v1.pdf","comment":"10 pages in the main text. Published at NeurIPS 2024. The code is\n  available at https://github.com/rachtsy/MomentumSMoE"},{"id":"http://arxiv.org/abs/2410.13242v2","updated":"2024-10-18T15:41:44Z","published":"2024-10-17T05:53:13Z","title":"Fundus to Fluorescein Angiography Video Generation as a Retinal\n  Generative Foundation Model","summary":"  Fundus fluorescein angiography (FFA) is crucial for diagnosing and monitoring\nretinal vascular issues but is limited by its invasive nature and restricted\naccessibility compared to color fundus (CF) imaging. Existing methods that\nconvert CF images to FFA are confined to static image generation, missing the\ndynamic lesional changes. We introduce Fundus2Video, an autoregressive\ngenerative adversarial network (GAN) model that generates dynamic FFA videos\nfrom single CF images. Fundus2Video excels in video generation, achieving an\nFVD of 1497.12 and a PSNR of 11.77. Clinical experts have validated the\nfidelity of the generated videos. Additionally, the model's generator\ndemonstrates remarkable downstream transferability across ten external public\ndatasets, including blood vessel segmentation, retinal disease diagnosis,\nsystemic disease prediction, and multimodal retrieval, showcasing impressive\nzero-shot and few-shot capabilities. These findings position Fundus2Video as a\npowerful, non-invasive alternative to FFA exams and a versatile retinal\ngenerative foundation model that captures both static and temporal retinal\nfeatures, enabling the representation of complex inter-modality relationships.\n","authors":["Weiyi Zhang","Jiancheng Yang","Ruoyu Chen","Siyu Huang","Pusheng Xu","Xiaolan Chen","Shanfu Lu","Hongyu Cao","Mingguang He","Danli Shi"],"pdf_url":"https://arxiv.org/pdf/2410.13242v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14540v1","updated":"2024-10-18T15:29:19Z","published":"2024-10-18T15:29:19Z","title":"Multi-modal Pose Diffuser: A Multimodal Generative Conditional Pose\n  Prior","summary":"  The Skinned Multi-Person Linear (SMPL) model plays a crucial role in 3D human\npose estimation, providing a streamlined yet effective representation of the\nhuman body. However, ensuring the validity of SMPL configurations during tasks\nsuch as human mesh regression remains a significant challenge , highlighting\nthe necessity for a robust human pose prior capable of discerning realistic\nhuman poses. To address this, we introduce MOPED:\n\\underline{M}ulti-m\\underline{O}dal \\underline{P}os\\underline{E}\n\\underline{D}iffuser. MOPED is the first method to leverage a novel multi-modal\nconditional diffusion model as a prior for SMPL pose parameters. Our method\noffers powerful unconditional pose generation with the ability to condition on\nmulti-modal inputs such as images and text. This capability enhances the\napplicability of our approach by incorporating additional context often\noverlooked in traditional pose priors. Extensive experiments across three\ndistinct tasks-pose estimation, pose denoising, and pose completion-demonstrate\nthat our multi-modal diffusion model-based prior significantly outperforms\nexisting methods. These results indicate that our model captures a broader\nspectrum of plausible human poses.\n","authors":["Calvin-Khang Ta","Arindam Dutta","Rohit Kundu","Rohit Lal","Hannah Dela Cruz","Dripta S. Raychaudhuri","Amit Roy-Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2410.14540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14536v1","updated":"2024-10-18T15:23:34Z","published":"2024-10-18T15:23:34Z","title":"A Hybrid Feature Fusion Deep Learning Framework for Leukemia Cancer\n  Detection in Microscopic Blood Sample Using Gated Recurrent Unit and\n  Uncertainty Quantification","summary":"  Acute lymphoblastic leukemia (ALL) is the most malignant form of leukemia and\nthe most common cancer in adults and children. Traditionally, leukemia is\ndiagnosed by analyzing blood and bone marrow smears under a microscope, with\nadditional cytochemical tests for confirmation. However, these methods are\nexpensive, time consuming, and highly dependent on expert knowledge. In recent\nyears, deep learning, particularly Convolutional Neural Networks (CNNs), has\nprovided advanced methods for classifying microscopic smear images, aiding in\nthe detection of leukemic cells. These approaches are quick, cost effective,\nand not subject to human bias. However, most methods lack the ability to\nquantify uncertainty, which could lead to critical misdiagnoses. In this\nresearch, hybrid deep learning models (InceptionV3-GRU, EfficientNetB3-GRU,\nMobileNetV2-GRU) were implemented to classify ALL. Bayesian optimization was\nused to fine tune the model's hyperparameters and improve its performance.\nAdditionally, Deep Ensemble uncertainty quantification was applied to address\nuncertainty during leukemia image classification. The proposed models were\ntrained on the publicly available datasets ALL-IDB1 and ALL-IDB2. Their results\nwere then aggregated at the score level using the sum rule. The parallel\narchitecture used in these models offers a high level of confidence in\ndifferentiating between ALL and non-ALL cases. The proposed method achieved a\nremarkable detection accuracy rate of 100% on the ALL-IDB1 dataset, 98.07% on\nthe ALL-IDB2 dataset, and 98.64% on the combined dataset, demonstrating its\npotential for accurate and reliable leukemia diagnosis.\n","authors":["Maksuda Akter","Rabea Khatun","Md Manowarul Islam"],"pdf_url":"https://arxiv.org/pdf/2410.14536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14524v1","updated":"2024-10-18T15:08:05Z","published":"2024-10-18T15:08:05Z","title":"Less is More: Selective Reduction of CT Data for Self-Supervised\n  Pre-Training of Deep Learning Models with Contrastive Learning Improves\n  Downstream Classification Performance","summary":"  Self-supervised pre-training of deep learning models with contrastive\nlearning is a widely used technique in image analysis. Current findings\nindicate a strong potential for contrastive pre-training on medical images.\nHowever, further research is necessary to incorporate the particular\ncharacteristics of these images. We hypothesize that the similarity of medical\nimages hinders the success of contrastive learning in the medical imaging\ndomain. To this end, we investigate different strategies based on deep\nembedding, information theory, and hashing in order to identify and reduce\nredundancy in medical pre-training datasets. The effect of these different\nreduction strategies on contrastive learning is evaluated on two pre-training\ndatasets and several downstream classification tasks. In all of our\nexperiments, dataset reduction leads to a considerable performance gain in\ndownstream tasks, e.g., an AUC score improvement from 0.78 to 0.83 for the\nCOVID CT Classification Grand Challenge, 0.97 to 0.98 for the OrganSMNIST\nClassification Challenge and 0.73 to 0.83 for a brain hemorrhage classification\ntask. Furthermore, pre-training is up to nine times faster due to the dataset\nreduction. In conclusion, the proposed approach highlights the importance of\ndataset quality and provides a transferable approach to improve contrastive\npre-training for classification downstream tasks on medical images.\n","authors":["Daniel Wolf","Tristan Payer","Catharina Silvia Lisson","Christoph Gerhard Lisson","Meinrad Beer","Michael Götz","Timo Ropinski"],"pdf_url":"https://arxiv.org/pdf/2410.14524v1.pdf","comment":"Published in Computers in Biology and Medicine"},{"id":"http://arxiv.org/abs/2409.14485v3","updated":"2024-10-18T15:03:08Z","published":"2024-09-22T15:13:31Z","title":"Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding","summary":"  Although current Multi-modal Large Language Models (MLLMs) demonstrate\npromising results in video understanding, processing extremely long videos\nremains an ongoing challenge. Typically, MLLMs struggle with handling thousands\nof visual tokens that exceed the maximum context length, and they suffer from\nthe information decay due to token aggregation. Another challenge is the high\ncomputational cost stemming from the large number of video tokens. To tackle\nthese issues, we propose Video-XL, an extra-long vision language model designed\nfor efficient hour-scale video understanding. Specifically, we argue that LLMs\ncan be adapted as effective visual condensers and propose Visual Context Latent\nSummarization which condenses visual contexts into highly compact forms.\nExtensive experiments demonstrate that our model achieves promising results on\npopular long video understanding benchmarks. For example, Video-XL outperforms\nthe current state-of-the-art method on VNBench by nearly 10\\% in accuracy.\nMoreover, Video-XL presents an impressive balance between efficiency and\neffectiveness, processing 2048 frames on a single 80GB GPU while achieving\nnearly 95% accuracy in the Needle-in-a-Haystack evaluation.\n","authors":["Yan Shu","Peitian Zhang","Zheng Liu","Minghao Qin","Junjie Zhou","Tiejun Huang","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.14485v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14509v1","updated":"2024-10-18T14:43:34Z","published":"2024-10-18T14:43:34Z","title":"CLIP-VAD: Exploiting Vision-Language Models for Voice Activity Detection","summary":"  Voice Activity Detection (VAD) is the process of automatically determining\nwhether a person is speaking and identifying the timing of their speech in an\naudiovisual data. Traditionally, this task has been tackled by processing\neither audio signals or visual data, or by combining both modalities through\nfusion or joint learning. In our study, drawing inspiration from recent\nadvancements in visual-language models, we introduce a novel approach\nleveraging Contrastive Language-Image Pretraining (CLIP) models. The CLIP\nvisual encoder analyzes video segments composed of the upper body of an\nindividual, while the text encoder handles textual descriptions automatically\ngenerated through prompt engineering. Subsequently, embeddings from these\nencoders are fused through a deep neural network to perform VAD. Our\nexperimental analysis across three VAD benchmarks showcases the superior\nperformance of our method compared to existing visual VAD approaches. Notably,\nour approach outperforms several audio-visual methods despite its simplicity,\nand without requiring pre-training on extensive audio-visual datasets.\n","authors":["Andrea Appiani","Cigdem Beyan"],"pdf_url":"https://arxiv.org/pdf/2410.14509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14508v1","updated":"2024-10-18T14:43:05Z","published":"2024-10-18T14:43:05Z","title":"LEAD: Latent Realignment for Human Motion Diffusion","summary":"  Our goal is to generate realistic human motion from natural language. Modern\nmethods often face a trade-off between model expressiveness and text-to-motion\nalignment. Some align text and motion latent spaces but sacrifice\nexpressiveness; others rely on diffusion models producing impressive motions,\nbut lacking semantic meaning in their latent space. This may compromise\nrealism, diversity, and applicability. Here, we address this by combining\nlatent diffusion with a realignment mechanism, producing a novel, semantically\nstructured space that encodes the semantics of language. Leveraging this\ncapability, we introduce the task of textual motion inversion to capture novel\nmotion concepts from a few examples. For motion synthesis, we evaluate LEAD on\nHumanML3D and KIT-ML and show comparable performance to the state-of-the-art in\nterms of realism, diversity, and text-motion consistency. Our qualitative\nanalysis and user study reveal that our synthesized motions are sharper, more\nhuman-like and comply better with the text compared to modern methods. For\nmotion textual inversion, our method demonstrates improved capacity in\ncapturing out-of-distribution characteristics in comparison to traditional\nVAEs.\n","authors":["Nefeli Andreou","Xi Wang","Victoria Fernández Abrevaya","Marie-Paule Cani","Yiorgos Chrysanthou","Vicky Kalogeiton"],"pdf_url":"https://arxiv.org/pdf/2410.14508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04960v2","updated":"2024-10-18T14:42:50Z","published":"2024-10-07T11:59:54Z","title":"On Efficient Variants of Segment Anything Model: A Survey","summary":"  The Segment Anything Model (SAM) is a foundational model for image\nsegmentation tasks, known for its strong generalization across diverse\napplications. However, its impressive performance comes with significant\ncomputational and resource demands, making it challenging to deploy in\nresource-limited environments such as edge devices. To address this, a variety\nof SAM variants have been proposed to enhance efficiency while keeping\naccuracy. This survey provides the first comprehensive review of these\nefficient SAM variants. We begin by exploring the motivations driving this\nresearch. We then present core techniques used in SAM and model acceleration.\nThis is followed by a detailed exploration of SAM acceleration strategies,\ncategorized by approach, and a discussion of several future research\ndirections. Finally, we offer a unified and extensive evaluation of these\nmethods across various hardware, assessing their efficiency and accuracy on\nrepresentative benchmarks, and providing a clear comparison of their overall\nperformance.\n","authors":["Xiaorui Sun","Jun Liu","Heng Tao Shen","Xiaofeng Zhu","Ping Hu"],"pdf_url":"https://arxiv.org/pdf/2410.04960v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07361v2","updated":"2024-10-18T14:38:03Z","published":"2024-06-11T15:28:48Z","title":"Deep Implicit Optimization for Robust and Flexible Image Registration","summary":"  Deep Learning in Image Registration (DLIR) methods have been tremendously\nsuccessful in image registration due to their speed and ability to incorporate\nweak label supervision at training time. However, DLIR methods forego many of\nthe benefits of classical optimization-based methods. The functional nature of\ndeep networks do not guarantee that the predicted transformation is a local\nminima of the registration objective, the representation of the transformation\n(displacement/velocity field/affine) is fixed, and the networks are not robust\nto domain shift. Our method aims to bridge this gap between classical and\nlearning methods by incorporating optimization as a layer in a deep network. A\ndeep network is trained to predict multi-scale dense feature images that are\nregistered using a black box iterative optimization solver. This optimal warp\nis then used to minimize image and label alignment errors. By implicitly\ndifferentiating end-to-end through an iterative optimization solver, our\nlearned features are registration and label-aware, and the warp functions are\nguaranteed to be local minima of the registration objective in the feature\nspace. Our framework shows excellent performance on in-domain datasets, and is\nagnostic to domain shift such as anisotropy and varying intensity profiles. For\nthe first time, our method allows switching between arbitrary transformation\nrepresentations (free-form to diffeomorphic) at test time with zero retraining.\nEnd-to-end feature learning also facilitates interpretability of features, and\nout-of-the-box promptability using additional label-fidelity terms at\ninference.\n","authors":["Rohit Jena","Pratik Chaudhari","James C. Gee"],"pdf_url":"https://arxiv.org/pdf/2406.07361v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14505v1","updated":"2024-10-18T14:37:37Z","published":"2024-10-18T14:37:37Z","title":"Neural Real-Time Recalibration for Infrared Multi-Camera Systems","summary":"  Currently, there are no learning-free or neural techniques for real-time\nrecalibration of infrared multi-camera systems. In this paper, we address the\nchallenge of real-time, highly-accurate calibration of multi-camera infrared\nsystems, a critical task for time-sensitive applications. Unlike traditional\ncalibration techniques that lack adaptability and struggle with on-the-fly\nrecalibrations, we propose a neural network-based method capable of dynamic\nreal-time calibration. The proposed method integrates a differentiable\nprojection model that directly correlates 3D geometries with their 2D image\nprojections and facilitates the direct optimization of both intrinsic and\nextrinsic camera parameters. Key to our approach is the dynamic camera pose\nsynthesis with perturbations in camera parameters, emulating realistic\noperational challenges to enhance model robustness. We introduce two model\nvariants: one designed for multi-camera systems with onboard processing of 2D\npoints, utilizing the direct 2D projections of 3D fiducials, and another for\nimage-based systems, employing color-coded projected points for implicitly\nestablishing correspondence. Through rigorous experimentation, we demonstrate\nour method is more accurate than traditional calibration techniques with or\nwithout perturbations while also being real-time, marking a significant leap in\nthe field of real-time multi-camera system calibration. The source code can be\nfound at https://github.com/theICTlab/neural-recalibration\n","authors":["Benyamin Mehmandar","Reza Talakoob","Charalambos Poullis"],"pdf_url":"https://arxiv.org/pdf/2410.14505v1.pdf","comment":"real-time camera calibration, infrared camera, neural calibration"},{"id":"http://arxiv.org/abs/2410.14489v1","updated":"2024-10-18T14:19:13Z","published":"2024-10-18T14:19:13Z","title":"An Integrated Deep Learning Model for Skin Cancer Detection Using Hybrid\n  Feature Fusion Technique","summary":"  Skin cancer is a serious and potentially fatal disease caused by DNA damage.\nEarly detection significantly increases survival rates, making accurate\ndiagnosis crucial. In this groundbreaking study, we present a hybrid framework\nbased on Deep Learning (DL) that achieves precise classification of benign and\nmalignant skin lesions. Our approach begins with dataset preprocessing to\nenhance classification accuracy, followed by training two separate pre-trained\nDL models, InceptionV3 and DenseNet121. By fusing the results of each model\nusing the weighted sum rule, our system achieves exceptional accuracy rates.\nSpecifically, we achieve a 92.27% detection accuracy rate, 92.33% sensitivity,\n92.22% specificity, 90.81% precision, and 91.57% F1-score, outperforming\nexisting models and demonstrating the robustness and trustworthiness of our\nhybrid approach. Our study represents a significant advance in skin cancer\ndiagnosis and provides a promising foundation for further research in the\nfield. With the potential to save countless lives through earlier detection,\nour hybrid deep-learning approach is a game-changer in the fight against skin\ncancer.\n","authors":["Maksuda Akter","Rabea Khatun","Md. Alamin Talukder","Md. Manowarul Islam","Md. Ashraf Uddin"],"pdf_url":"https://arxiv.org/pdf/2410.14489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14470v1","updated":"2024-10-18T13:54:46Z","published":"2024-10-18T13:54:46Z","title":"How Do Training Methods Influence the Utilization of Vision Models?","summary":"  Not all learnable parameters (e.g., weights) contribute equally to a neural\nnetwork's decision function. In fact, entire layers' parameters can sometimes\nbe reset to random values with little to no impact on the model's decisions. We\nrevisit earlier studies that examined how architecture and task complexity\ninfluence this phenomenon and ask: is this phenomenon also affected by how we\ntrain the model? We conducted experimental evaluations on a diverse set of\nImageNet-1k classification models to explore this, keeping the architecture and\ntraining data constant but varying the training pipeline. Our findings reveal\nthat the training method strongly influences which layers become critical to\nthe decision function for a given task. For example, improved training regimes\nand self-supervised training increase the importance of early layers while\nsignificantly under-utilizing deeper layers. In contrast, methods such as\nadversarial training display an opposite trend. Our preliminary results extend\nprevious findings, offering a more nuanced understanding of the inner mechanics\nof neural networks.\n  Code: https://github.com/paulgavrikov/layer_criticality\n","authors":["Paul Gavrikov","Shashank Agnihotri","Margret Keuper","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2410.14470v1.pdf","comment":"Accepted at the Interpretable AI: Past, Present and Future Workshop\n  at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2403.09939v2","updated":"2024-10-18T13:51:50Z","published":"2024-03-15T00:43:03Z","title":"Quantization Effects on Neural Networks Perception: How would\n  quantization change the perceptual field of vision models?","summary":"  Neural network quantization is a critical technique for deploying models on\nresource-limited devices. Despite its widespread use, the impact of\nquantization on model perceptual fields, particularly in relation to class\nactivation maps (CAMs), remains underexplored. This study investigates how\nquantization influences the spatial recognition abilities of vision models by\nexamining the alignment between CAMs and visual salient objects maps across\nvarious architectures. Utilizing a dataset of 10,000 images from ImageNet, we\nconduct a comprehensive evaluation of six diverse CNN architectures: VGG16,\nResNet50, EfficientNet, MobileNet, SqueezeNet, and DenseNet. Through the\nsystematic application of quantization techniques, we identify subtle changes\nin CAMs and their alignment with Salient object maps. Our results demonstrate\nthe differing sensitivities of these architectures to quantization and\nhighlight its implications for model performance and interpretability in\nreal-world applications. This work primarily contributes to a deeper\nunderstanding of neural network quantization, offering insights essential for\ndeploying efficient and interpretable models in practical settings.\n","authors":["Mohamed Amine Kerkouri","Marouane Tliba","Aladine Chetouani","Alessandro Bruno"],"pdf_url":"https://arxiv.org/pdf/2403.09939v2.pdf","comment":"Accepted & presented at IPTA 2024"},{"id":"http://arxiv.org/abs/2410.14462v1","updated":"2024-10-18T13:44:29Z","published":"2024-10-18T13:44:29Z","title":"LUDVIG: Learning-free Uplifting of 2D Visual features to Gaussian\n  Splatting scenes","summary":"  We address the task of uplifting visual features or semantic masks from 2D\nvision models to 3D scenes represented by Gaussian Splatting. Whereas common\napproaches rely on iterative optimization-based procedures, we show that a\nsimple yet effective aggregation technique yields excellent results. Applied to\nsemantic masks from Segment Anything (SAM), our uplifting approach leads to\nsegmentation quality comparable to the state of the art. We then extend this\nmethod to generic DINOv2 features, integrating 3D scene geometry through graph\ndiffusion, and achieve competitive segmentation results despite DINOv2 not\nbeing trained on millions of annotated masks like SAM.\n","authors":["Juliette Marrie","Romain Ménégaux","Michael Arbel","Diane Larlus","Julien Mairal"],"pdf_url":"https://arxiv.org/pdf/2410.14462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19525v3","updated":"2024-10-18T13:13:44Z","published":"2024-04-30T12:56:14Z","title":"MicroDreamer: Efficient 3D Generation in $\\sim$20 Seconds by Score-based\n  Iterative Reconstruction","summary":"  Optimization-based approaches, such as score distillation sampling (SDS),\nshow promise in zero-shot 3D generation but suffer from low efficiency,\nprimarily due to the high number of function evaluations (NFEs) required for\neach sample and the limitation of optimization confined to latent space. This\npaper introduces score-based iterative reconstruction (SIR), an efficient and\ngeneral algorithm mimicking a differentiable 3D reconstruction process to\nreduce the NFEs and enable optimization in pixel space. Given a single set of\nimages sampled from a multi-view score-based diffusion model, SIR repeatedly\noptimizes 3D parameters, unlike the single-step optimization in SDS. With other\nimprovements in training, we present an efficient approach called MicroDreamer\nthat generally applies to various 3D representations and 3D generation tasks.\nIn particular, MicroDreamer is 5-20 times faster than SDS in generating neural\nradiance field while retaining a comparable performance and takes about 20\nseconds to create meshes from 3D Gaussian splatting on a single A100 GPU,\nhalving the time of the fastest optimization-based baseline DreamGaussian with\nsignificantly superior performance compared to the measurement standard\ndeviation. Our code is available at https://github.com/ML-GSAI/MicroDreamer.\n","authors":["Luxi Chen","Zhengyi Wang","Zihan Zhou","Tingting Gao","Hang Su","Jun Zhu","Chongxuan Li"],"pdf_url":"https://arxiv.org/pdf/2404.19525v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14445v1","updated":"2024-10-18T13:04:35Z","published":"2024-10-18T13:04:35Z","title":"Toward Generalizing Visual Brain Decoding to Unseen Subjects","summary":"  Visual brain decoding aims to decode visual information from human brain\nactivities. Despite the great progress, one critical limitation of current\nbrain decoding research lies in the lack of generalization capability to unseen\nsubjects. Prior works typically focus on decoding brain activity of individuals\nbased on the observation that different subjects exhibit different brain\nactivities, while it remains unclear whether brain decoding can be generalized\nto unseen subjects. This study aims to answer this question. We first\nconsolidate an image-fMRI dataset consisting of stimulus-image and\nfMRI-response pairs, involving 177 subjects in the movie-viewing task of the\nHuman Connectome Project (HCP). This dataset allows us to investigate the brain\ndecoding performance with the increase of participants. We then present a\nlearning paradigm that applies uniform processing across all subjects, instead\nof employing different network heads or tokenizers for individuals as in\nprevious methods, which can accommodate a large number of subjects to explore\nthe generalization capability across different subjects. A series of\nexperiments are conducted and we have the following findings. First, the\nnetwork exhibits clear generalization capabilities with the increase of\ntraining subjects. Second, the generalization capability is common to popular\nnetwork architectures (MLP, CNN and Transformer). Third, the generalization\nperformance is affected by the similarity between subjects. Our findings reveal\nthe inherent similarities in brain activities across individuals. With the\nemerging of larger and more comprehensive datasets, it is possible to train a\nbrain decoding foundation model in the future.Codes and models can be found at\nhttps://github.com/Xiangtaokong/TGBD.\n","authors":["Xiangtao Kong","Kexin Huang","Ping Li","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.14445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14429v1","updated":"2024-10-18T12:48:22Z","published":"2024-10-18T12:48:22Z","title":"FashionR2R: Texture-preserving Rendered-to-Real Image Translation with\n  Diffusion Models","summary":"  Modeling and producing lifelike clothed human images has attracted\nresearchers' attention from different areas for decades, with the complexity\nfrom highly articulated and structured content. Rendering algorithms decompose\nand simulate the imaging process of a camera, while are limited by the accuracy\nof modeled variables and the efficiency of computation. Generative models can\nproduce impressively vivid human images, however still lacking in\ncontrollability and editability. This paper studies photorealism enhancement of\nrendered images, leveraging generative power from diffusion models on the\ncontrolled basis of rendering. We introduce a novel framework to translate\nrendered images into their realistic counterparts, which consists of two\nstages: Domain Knowledge Injection (DKI) and Realistic Image Generation (RIG).\nIn DKI, we adopt positive (real) domain finetuning and negative (rendered)\ndomain embedding to inject knowledge into a pretrained Text-to-image (T2I)\ndiffusion model. In RIG, we generate the realistic image corresponding to the\ninput rendered image, with a Texture-preserving Attention Control (TAC) to\npreserve fine-grained clothing textures, exploiting the decoupled features\nencoded in the UNet structure. Additionally, we introduce SynFashion dataset,\nfeaturing high-quality digital clothing images with diverse textures. Extensive\nexperimental results demonstrate the superiority and effectiveness of our\nmethod in rendered-to-real image translation.\n","authors":["Rui Hu","Qian He","Gaofeng He","Jiedong Zhuang","Huang Chen","Huafeng Liu","Huamin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14429v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.14423v1","updated":"2024-10-18T12:37:51Z","published":"2024-10-18T12:37:51Z","title":"Integrating Deep Learning with Fundus and Optical Coherence Tomography\n  for Cardiovascular Disease Prediction","summary":"  Early identification of patients at risk of cardiovascular diseases (CVD) is\ncrucial for effective preventive care, reducing healthcare burden, and\nimproving patients' quality of life. This study demonstrates the potential of\nretinal optical coherence tomography (OCT) imaging combined with fundus\nphotographs for identifying future adverse cardiac events. We used data from\n977 patients who experienced CVD within a 5-year interval post-image\nacquisition, alongside 1,877 control participants without CVD, totaling 2,854\nsubjects. We propose a novel binary classification network based on a\nMulti-channel Variational Autoencoder (MCVAE), which learns a latent embedding\nof patients' fundus and OCT images to classify individuals into two groups:\nthose likely to develop CVD in the future and those who are not. Our model,\ntrained on both imaging modalities, achieved promising results (AUROC 0.78 +/-\n0.02, accuracy 0.68 +/- 0.002, precision 0.74 +/- 0.02, sensitivity 0.73 +/-\n0.02, and specificity 0.68 +/- 0.01), demonstrating its efficacy in identifying\npatients at risk of future CVD events based on their retinal images. This study\nhighlights the potential of retinal OCT imaging and fundus photographs as\ncost-effective, non-invasive alternatives for predicting cardiovascular disease\nrisk. The widespread availability of these imaging techniques in optometry\npractices and hospitals further enhances their potential for large-scale CVD\nrisk screening. Our findings contribute to the development of standardized,\naccessible methods for early CVD risk identification, potentially improving\npreventive care strategies and patient outcomes.\n","authors":["Cynthia Maldonado-Garcia","Arezoo Zakeri","Alejandro F Frangi","Nishant Ravikumar"],"pdf_url":"https://arxiv.org/pdf/2410.14423v1.pdf","comment":"Part of the book series: Lecture Notes in Computer Science\n  ((LNCS,volume 15155))"},{"id":"http://arxiv.org/abs/2405.08431v4","updated":"2024-10-18T12:25:31Z","published":"2024-05-14T08:51:16Z","title":"Similarity and Quality Metrics for MR Image-To-Image Translation","summary":"  Image-to-image translation can create large impact in medical imaging, as\nimages can be synthetically transformed to other modalities, sequence types,\nhigher resolutions or lower noise levels. To ensure patient safety, these\nmethods should be validated by human readers, which requires a considerable\namount of time and costs. Quantitative metrics can effectively complement such\nstudies and provide reproducible and objective assessment of synthetic images.\nIf a reference is available, the similarity of MR images is frequently\nevaluated by SSIM and PSNR metrics, even though these metrics are not or too\nsensitive regarding specific distortions. When reference images to compare with\nare not available, non-reference quality metrics can reliably detect specific\ndistortions, such as blurriness. To provide an overview on distortion\nsensitivity, we quantitatively analyze 11 similarity (reference) and 12 quality\n(non-reference) metrics for assessing synthetic images. We additionally include\na metric on a downstream segmentation task. We investigate the sensitivity\nregarding 11 kinds of distortions and typical MR artifacts, and analyze the\ninfluence of different normalization methods on each metric and distortion.\nFinally, we derive recommendations for effective usage of the analyzed\nsimilarity and quality metrics for evaluation of image-to-image translation\nmodels.\n","authors":["Melanie Dohmen","Mark Klemens","Ivo Baltruschat","Tuan Truong","Matthias Lenga"],"pdf_url":"https://arxiv.org/pdf/2405.08431v4.pdf","comment":"21 pages, 8 figures, supplement with 16 pages, 10 figures, submitted\n  to Nature Scientific Reports"},{"id":"http://arxiv.org/abs/2409.08031v2","updated":"2024-10-18T12:22:11Z","published":"2024-09-12T13:23:24Z","title":"LED: Light Enhanced Depth Estimation at Night","summary":"  Nighttime camera-based depth estimation is a highly challenging task,\nespecially for autonomous driving applications, where accurate depth perception\nis essential for ensuring safe navigation. We aim to improve the reliability of\nperception systems at night time, where models trained on daytime data often\nfail in the absence of precise but costly LiDAR sensors. In this work, we\nintroduce Light Enhanced Depth (LED), a novel cost-effective approach that\nsignificantly improves depth estimation in low-light environments by harnessing\na pattern projected by high definition headlights available in modern vehicles.\nLED leads to significant performance boosts across multiple depth-estimation\narchitectures (encoder-decoder, Adabins, DepthFormer) both on synthetic and\nreal datasets. Furthermore, increased performances beyond illuminated areas\nreveal a holistic enhancement in scene understanding. Finally, we release the\nNighttime Synthetic Drive Dataset, a new synthetic and photo-realistic\nnighttime dataset, which comprises 49,990 comprehensively annotated images.\n","authors":["Simon de Moreau","Yasser Almehio","Andrei Bursuc","Hafid El-Idrissi","Bogdan Stanciulescu","Fabien Moutarde"],"pdf_url":"https://arxiv.org/pdf/2409.08031v2.pdf","comment":"Preprint. Code and dataset available on the project page :\n  https://simondemoreau.github.io/LED/"},{"id":"http://arxiv.org/abs/2406.08552v2","updated":"2024-10-18T12:05:21Z","published":"2024-06-12T18:00:08Z","title":"DiTFastAttn: Attention Compression for Diffusion Transformer Models","summary":"  Diffusion Transformers (DiT) excel at image and video generation but face\ncomputational challenges due to the quadratic complexity of self-attention\noperators. We propose DiTFastAttn, a post-training compression method to\nalleviate the computational bottleneck of DiT. We identify three key\nredundancies in the attention computation during DiT inference: (1) spatial\nredundancy, where many attention heads focus on local information; (2) temporal\nredundancy, with high similarity between the attention outputs of neighboring\nsteps; (3) conditional redundancy, where conditional and unconditional\ninferences exhibit significant similarity. We propose three techniques to\nreduce these redundancies: (1) Window Attention with Residual Sharing to reduce\nspatial redundancy; (2) Attention Sharing across Timesteps to exploit the\nsimilarity between steps; (3) Attention Sharing across CFG to skip redundant\ncomputations during conditional generation. We apply DiTFastAttn to DiT,\nPixArt-Sigma for image generation tasks, and OpenSora for video generation\ntasks. Our results show that for image generation, our method reduces up to 76%\nof the attention FLOPs and achieves up to 1.8x end-to-end speedup at\nhigh-resolution (2k x 2k) generation.\n","authors":["Zhihang Yuan","Hanling Zhang","Pu Lu","Xuefei Ning","Linfeng Zhang","Tianchen Zhao","Shengen Yan","Guohao Dai","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2406.08552v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14400v1","updated":"2024-10-18T12:04:23Z","published":"2024-10-18T12:04:23Z","title":"Variable Aperture Bokeh Rendering via Customized Focal Plane Guidance","summary":"  Bokeh rendering is one of the most popular techniques in photography. It can\nmake photographs visually appealing, forcing users to focus their attentions on\nparticular area of image. However, achieving satisfactory bokeh effect usually\npresents significant challenge, since mobile cameras with restricted optical\nsystems are constrained, while expensive high-end DSLR lens with large aperture\nshould be needed. Therefore, many deep learning-based computational photography\nmethods have been developed to mimic the bokeh effect in recent years.\nNevertheless, most of these methods were limited to rendering bokeh effect in\ncertain single aperture. There lacks user-friendly bokeh rendering method that\ncan provide precise focal plane control and customised bokeh generation. There\nas well lacks authentic realistic bokeh dataset that can potentially promote\nbokeh learning on variable apertures. To address these two issues, in this\npaper, we have proposed an effective controllable bokeh rendering method, and\ncontributed a Variable Aperture Bokeh Dataset (VABD). In the proposed method,\nuser can customize focal plane to accurately locate concerned subjects and\nselect target aperture information for bokeh rendering. Experimental results on\npublic EBB! benchmark dataset and our constructed dataset VABD have\ndemonstrated that the customized focal plane together aperture prompt can\nbootstrap model to simulate realistic bokeh effect. The proposed method has\nachieved competitive state-of-the-art performance with only 4.4M parameters,\nwhich is much lighter than mainstream computational bokeh models. The\ncontributed dataset and source codes will be released on github\nhttps://github.com/MoTong-AI-studio/VABM.\n","authors":["Kang Chen","Shijun Yan","Aiwen Jiang","Han Li","Zhifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14398v1","updated":"2024-10-18T12:02:21Z","published":"2024-10-18T12:02:21Z","title":"Dynamic Negative Guidance of Diffusion Models","summary":"  Negative Prompting (NP) is widely utilized in diffusion models, particularly\nin text-to-image applications, to prevent the generation of undesired features.\nIn this paper, we show that conventional NP is limited by the assumption of a\nconstant guidance scale, which may lead to highly suboptimal results, or even\ncomplete failure, due to the non-stationarity and state-dependence of the\nreverse process. Based on this analysis, we derive a principled technique\ncalled Dynamic Negative Guidance, which relies on a near-optimal time and state\ndependent modulation of the guidance without requiring additional training.\nUnlike NP, negative guidance requires estimating the posterior class\nprobability during the denoising process, which is achieved with limited\nadditional computational overhead by tracking the discrete Markov Chain during\nthe generative process. We evaluate the performance of DNG class-removal on\nMNIST and CIFAR10, where we show that DNG leads to higher safety, preservation\nof class balance and image quality when compared with baseline methods.\nFurthermore, we show that it is possible to use DNG with Stable Diffusion to\nobtain more accurate and less invasive guidance than NP.\n","authors":["Felix Koulischer","Johannes Deleu","Gabriel Raya","Thomas Demeester","Luca Ambrogioni"],"pdf_url":"https://arxiv.org/pdf/2410.14398v1.pdf","comment":"Paper currently under review. Submitted to ICLR 2025"},{"id":"http://arxiv.org/abs/2410.14389v1","updated":"2024-10-18T11:49:40Z","published":"2024-10-18T11:49:40Z","title":"SurgeryV2: Bridging the Gap Between Model Merging and Multi-Task\n  Learning with Deep Representation Surgery","summary":"  Model merging-based multitask learning (MTL) offers a promising approach for\nperforming MTL by merging multiple expert models without requiring access to\nraw training data. However, in this paper, we examine the merged model's\nrepresentation distribution and uncover a critical issue of \"representation\nbias\". This bias arises from a significant distribution gap between the\nrepresentations of the merged and expert models, leading to the suboptimal\nperformance of the merged MTL model. To address this challenge, we first\npropose a representation surgery solution called Surgery. Surgery is a\nlightweight, task-specific module that aligns the final layer representations\nof the merged model with those of the expert models, effectively alleviating\nbias and improving the merged model's performance. Despite these improvements,\na performance gap remains compared to the traditional MTL method. Further\nanalysis reveals that representation bias phenomena exist at each layer of the\nmerged model, and aligning representations only in the last layer is\ninsufficient for fully reducing systemic bias because biases introduced at each\nlayer can accumulate and interact in complex ways. To tackle this, we then\npropose a more comprehensive solution, deep representation surgery (also called\nSurgeryV2), which mitigates representation bias across all layers, and thus\nbridges the performance gap between model merging-based MTL and traditional\nMTL. Finally, we design an unsupervised optimization objective to optimize both\nthe Surgery and SurgeryV2 modules. Our experimental results show that\nincorporating these modules into state-of-the-art (SOTA) model merging schemes\nleads to significant performance gains. Notably, our SurgeryV2 scheme reaches\nalmost the same level as individual expert models or the traditional MTL model.\nThe code is available at \\url{https://github.com/EnnengYang/SurgeryV2}.\n","authors":["Enneng Yang","Li Shen","Zhenyi Wang","Guibing Guo","Xingwei Wang","Xiaocun Cao","Jie Zhang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2410.14389v1.pdf","comment":"This paper is an extended version of our previous work\n  [arXiv:2402.02705] presented at ICML 2024"},{"id":"http://arxiv.org/abs/2410.14379v1","updated":"2024-10-18T11:07:12Z","published":"2024-10-18T11:07:12Z","title":"AnomalyNCD: Towards Novel Anomaly Class Discovery in Industrial\n  Scenarios","summary":"  In the industrial scenario, anomaly detection could locate but cannot\nclassify anomalies. To complete their capability, we study to automatically\ndiscover and recognize visual classes of industrial anomalies. In terms of\nmulti-class anomaly classification, previous methods cluster anomalies\nrepresented by frozen pre-trained models but often fail due to poor\ndiscrimination. Novel class discovery (NCD) has the potential to tackle this.\nHowever, it struggles with non-prominent and semantically weak anomalies that\nchallenge network learning focus. To address these, we introduce AnomalyNCD, a\nmulti-class anomaly classification framework compatible with existing anomaly\ndetection methods. This framework learns anomaly-specific features and\nclassifies anomalies in a self-supervised manner. Initially, a technique called\nMain Element Binarization (MEBin) is first designed, which segments primary\nanomaly regions into masks to alleviate the impact of incorrect detections on\nlearning. Subsequently, we employ mask-guided contrastive representation\nlearning to improve feature discrimination, which focuses network attention on\nisolated anomalous regions and reduces the confusion of erroneous inputs\nthrough re-corrected pseudo labels. Finally, to enable flexible classification\nat both region and image levels during inference, we develop a region merging\nstrategy that determines the overall image category based on the classified\nanomaly regions. Our method outperforms the state-of-the-art works on the MVTec\nAD and MTD datasets. Compared with the current methods, AnomalyNCD combined\nwith zero-shot anomaly detection method achieves a 10.8% $F_1$ gain, 8.8% NMI\ngain, and 9.5% ARI gain on MVTec AD, 12.8% $F_1$ gain, 5.7% NMI gain, and 10.8%\nARI gain on MTD. The source code is available at\nhttps://github.com/HUST-SLOW/AnomalyNCD.\n","authors":["Ziming Huang","Xurui Li","Haotian Liu","Feng Xue","Yuzhe Wang","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.14379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14365v1","updated":"2024-10-18T10:51:10Z","published":"2024-10-18T10:51:10Z","title":"Impact of imperfect annotations on CNN training and performance for\n  instance segmentation and classification in digital pathology","summary":"  Segmentation and classification of large numbers of instances, such as cell\nnuclei, are crucial tasks in digital pathology for accurate diagnosis. However,\nthe availability of high-quality datasets for deep learning methods is often\nlimited due to the complexity of the annotation process. In this work, we\ninvestigate the impact of noisy annotations on the training and performance of\na state-of-the-art CNN model for the combined task of detecting, segmenting and\nclassifying nuclei in histopathology images. In this context, we investigate\nthe conditions for determining an appropriate number of training epochs to\nprevent overfitting to annotation noise during training. Our results indicate\nthat the utilisation of a small, correctly annotated validation set is\ninstrumental in avoiding overfitting and maintaining model performance to a\nlarge extent. Additionally, our findings underscore the beneficial role of\npre-training.\n","authors":["Laura Gálvez Jiménez","Christine Decaestecker"],"pdf_url":"https://arxiv.org/pdf/2410.14365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.08102v2","updated":"2024-10-18T09:58:45Z","published":"2023-02-16T06:01:31Z","title":"Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech\n  Recognition","summary":"  Visual Speech Recognition (VSR) aims to infer speech into text depending on\nlip movements alone. As it focuses on visual information to model the speech,\nits performance is inherently sensitive to personal lip appearances and\nmovements, and this makes the VSR models show degraded performance when they\nare applied to unseen speakers. In this paper, to remedy the performance\ndegradation of the VSR model on unseen speakers, we propose prompt tuning\nmethods of Deep Neural Networks (DNNs) for speaker-adaptive VSR. Specifically,\nmotivated by recent advances in Natural Language Processing (NLP), we finetune\nprompts on adaptation data of target speakers instead of modifying the\npre-trained model parameters. Different from the previous prompt tuning methods\nmainly limited to Transformer variant architecture, we explore different types\nof prompts, the addition, the padding, and the concatenation form prompts that\ncan be applied to the VSR model which is composed of CNN and Transformer in\ngeneral. With the proposed prompt tuning, we show that the performance of the\npre-trained VSR model on unseen speakers can be largely improved by using a\nsmall amount of adaptation data (e.g., less than 5 minutes), even if the\npre-trained model is already developed with large speaker variations. Moreover,\nby analyzing the performance and parameters of different types of prompts, we\ninvestigate when the prompt tuning is preferred over the finetuning methods.\nThe effectiveness of the proposed method is evaluated on both word- and\nsentence-level VSR databases, LRW-ID and GRID.\n","authors":["Minsu Kim","Hyung-Il Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2302.08102v2.pdf","comment":"IEEE TPAMI"},{"id":"http://arxiv.org/abs/2410.14343v1","updated":"2024-10-18T09:51:43Z","published":"2024-10-18T09:51:43Z","title":"2D-3D Deformable Image Registration of Histology Slide and Micro-CT with\n  ML-based Initialization","summary":"  Recent developments in the registration of histology and micro-computed\ntomography ({\\mu}CT) have broadened the perspective of pathological\napplications such as virtual histology based on {\\mu}CT. This topic remains\nchallenging because of the low image quality of soft tissue CT. Additionally,\nsoft tissue samples usually deform during the histology slide preparation,\nmaking it difficult to correlate the structures between histology slide and\n{\\mu}CT. In this work, we propose a novel 2D-3D multi-modal deformable image\nregistration method. The method uses a machine learning (ML) based\ninitialization followed by the registration. The registration is finalized by\nan analytical out-of-plane deformation refinement. The method is evaluated on\ndatasets acquired from tonsil and tumor tissues. {\\mu}CTs of both\nphase-contrast and conventional absorption modalities are investigated. The\nregistration results from the proposed method are compared with those from\nintensity- and keypoint-based methods. The comparison is conducted using both\nvisual and fiducial-based evaluations. The proposed method demonstrates\nsuperior performance compared to the other two methods.\n","authors":["Junan Chen","Matteo Ronchetti","Verena Stehl","Van Nguyen","Muhannad Al Kallaa","Mahesh Thalwaththe Gedara","Claudia Lölkes","Stefan Moser","Maximilian Seidl","Matthias Wieczorek"],"pdf_url":"https://arxiv.org/pdf/2410.14343v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.14340v1","updated":"2024-10-18T09:51:14Z","published":"2024-10-18T09:51:14Z","title":"Zero-shot Action Localization via the Confidence of Large\n  Vision-Language Models","summary":"  Precise action localization in untrimmed video is vital for fields such as\nprofessional sports and minimally invasive surgery, where the delineation of\nparticular motions in recordings can dramatically enhance analysis. But in many\ncases, large scale datasets with video-label pairs for localization are\nunavailable, limiting the opportunity to fine-tune video-understanding models.\nRecent developments in large vision-language models (LVLM) address this need\nwith impressive zero-shot capabilities in a variety of video understanding\ntasks. However, the adaptation of image-based LVLMs, with their powerful visual\nquestion answering capabilities, to action localization in long-form video is\nstill relatively unexplored. To this end, we introduce a true ZEro-shot Action\nLocalization method (ZEAL). Specifically, we leverage the built-in action\nknowledge of a large language model (LLM) to inflate actions into\nhighly-detailed descriptions of the archetypal start and end of the action.\nThese descriptions serve as queries to LVLM for generating frame-level\nconfidence scores which can be aggregated to produce localization outputs. The\nsimplicity and flexibility of our method lends it amenable to more capable\nLVLMs as they are developed, and we demonstrate remarkable results in zero-shot\naction localization on a challenging benchmark, without any training.\n","authors":["Josiah Aklilu","Xiaohan Wang","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2410.14340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14334v1","updated":"2024-10-18T09:44:35Z","published":"2024-10-18T09:44:35Z","title":"Evaluating the evaluators: Towards human-aligned metrics for missing\n  markers reconstruction","summary":"  Animation data is often obtained through optical motion capture systems,\nwhich utilize a multitude of cameras to establish the position of optical\nmarkers. However, system errors or occlusions can result in missing markers,\nthe manual cleaning of which can be time-consuming. This has sparked interest\nin machine learning-based solutions for missing marker reconstruction in the\nacademic community. Most academic papers utilize a simplistic mean square error\nas the main metric. In this paper, we show that this metric does not correlate\nwith subjective perception of the fill quality. We introduce and evaluate a set\nof better-correlated metrics that can drive progress in the field.\n","authors":["Taras Kucherenko","Derek Peristy","Judith Bütepage"],"pdf_url":"https://arxiv.org/pdf/2410.14334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14332v1","updated":"2024-10-18T09:44:25Z","published":"2024-10-18T09:44:25Z","title":"Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension","summary":"  Recent advances in Large Language Models (LLMs) have catalyzed the\ndevelopment of Large Multimodal Models (LMMs). However, existing research\nprimarily focuses on tuning language and image instructions, ignoring the\ncritical pretraining phase where models learn to process textual and visual\nmodalities jointly. In this paper, we propose a new pretraining paradigm for\nLMMs to enhance the visual comprehension capabilities of LLMs by introducing a\nnovel cross-modal comprehension stage. Specifically, we design a dynamically\nlearnable prompt token pool and employ the Hungarian algorithm to replace part\nof the original visual tokens with the most relevant prompt tokens. Then, we\nconceptualize visual tokens as analogous to a \"foreign language\" for the LLMs\nand propose a mixed attention mechanism with bidirectional visual attention and\nunidirectional textual attention to comprehensively enhance the understanding\nof visual tokens. Meanwhile, we integrate a detailed caption generation task,\nleveraging rich descriptions to further facilitate LLMs in understanding visual\nsemantic information. After pretraining on 1.5 million publicly accessible\ndata, we present a new foundation model called Croc. Experimental results\ndemonstrate that Croc achieves new state-of-the-art performance on massive\nvision-language benchmarks. To support reproducibility and facilitate further\nresearch, we release the training code and pre-trained model weights at\nhttps://github.com/deepglint/Croc.\n","authors":["Yin Xie","Kaicheng Yang","Ninghua Yang","Weimo Deng","Xiangzi Dai","Tiancheng Gu","Yumeng Wang","Xiang An","Yongle Zhao","Ziyong Feng","Jiankang Deng"],"pdf_url":"https://arxiv.org/pdf/2410.14332v1.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2410.14326v1","updated":"2024-10-18T09:37:38Z","published":"2024-10-18T09:37:38Z","title":"Fast proxy centers for Jeffreys centroids: The Jeffreys-Fisher-Rao and\n  the inductive Gauss-Bregman centers","summary":"  The symmetric Kullback-Leibler centroid also called the Jeffreys centroid of\na set of mutually absolutely continuous probability distributions on a measure\nspace provides a notion of centrality which has proven useful in many tasks\nincluding information retrieval, information fusion, and clustering in image,\nvideo and sound processing. However, the Jeffreys centroid is not available in\nclosed-form for sets of categorical or normal distributions, two widely used\nstatistical models, and thus need to be approximated numerically in practice.\nIn this paper, we first propose the new Jeffreys-Fisher-Rao center defined as\nthe Fisher-Rao midpoint of the sided Kullback-Leibler centroids as a plug-in\nreplacement of the Jeffreys centroid. This Jeffreys-Fisher-Rao center admits a\ngeneric formula for uni-parameter exponential family distributions, and\nclosed-form formula for categorical and normal distributions, matches exactly\nthe Jeffreys centroid for same-mean normal distributions, and is experimentally\nobserved in practice to be close to the Jeffreys centroid. Second, we define a\nnew type of inductive centers generalizing the principle of Gauss\narithmetic-geometric double sequence mean for pairs of densities of any given\nexponential family. This center is shown experimentally to approximate very\nwell the Jeffreys centroid and is suggested to use when the Jeffreys-Fisher-Rao\ncenter is not available in closed form. Moreover, this Gauss-Bregman inductive\ncenter always converges and matches the Jeffreys centroid for sets of same-mean\nnormal distributions. We report on our experiments demonstrating the use of the\nJeffreys-Fisher-Rao and Gauss-Bregman centers instead of the Jeffreys centroid.\nFinally, we conclude this work by reinterpreting these fast proxy centers of\nJeffreys centroids under the lens of dually flat spaces in information\ngeometry.\n","authors":["Frank Nielsen"],"pdf_url":"https://arxiv.org/pdf/2410.14326v1.pdf","comment":"35 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.14324v1","updated":"2024-10-18T09:36:10Z","published":"2024-10-18T09:36:10Z","title":"HiCo: Hierarchical Controllable Diffusion Model for Layout-to-image\n  Generation","summary":"  The task of layout-to-image generation involves synthesizing images based on\nthe captions of objects and their spatial positions. Existing methods still\nstruggle in complex layout generation, where common bad cases include object\nmissing, inconsistent lighting, conflicting view angles, etc. To effectively\naddress these issues, we propose a \\textbf{Hi}erarchical \\textbf{Co}ntrollable\n(HiCo) diffusion model for layout-to-image generation, featuring object\nseperable conditioning branch structure. Our key insight is to achieve spatial\ndisentanglement through hierarchical modeling of layouts. We use a multi branch\nstructure to represent hierarchy and aggregate them in fusion module. To\nevaluate the performance of multi-objective controllable layout generation in\nnatural scenes, we introduce the HiCo-7K benchmark, derived from the GRIT-20M\ndataset and manually cleaned. https://github.com/360CVGroup/HiCo_T2I.\n","authors":["Bo Cheng","Yuhang Ma","Liebucha Wu","Shanyuan Liu","Ao Ma","Xiaoyu Wu","Dawei Leng","Yuhui Yin"],"pdf_url":"https://arxiv.org/pdf/2410.14324v1.pdf","comment":"NeurIPS2024"},{"id":"http://arxiv.org/abs/2410.13824v2","updated":"2024-10-18T09:01:01Z","published":"2024-10-17T17:48:54Z","title":"Harnessing Webpage UIs for Text-Rich Visual Understanding","summary":"  Text-rich visual understanding-the ability to process environments where\ndense textual content is integrated with visuals-is crucial for multimodal\nlarge language models (MLLMs) to interact effectively with structured\nenvironments. To enhance this capability, we propose synthesizing general\nmultimodal instructions from webpage UIs using text-based large language models\n(LLMs). Despite lacking direct visual input, text-based LLMs are able to\nprocess structured text representations from webpage accessibility trees. These\ninstructions are then paired with UI screenshots to train multimodal models. We\nintroduce MultiUI, a dataset containing 7.3 million samples from 1 million\nwebsites, covering diverse multimodal tasks and UI layouts. Models trained on\nMultiUI not only excel in web UI tasks-achieving up to a 48% improvement on\nVisualWebBench and a 19.1% boost in element accuracy on a web agent dataset\nMind2Web-but also generalize surprisingly well to non-web UI tasks and even to\nnon-UI domains, such as document understanding, OCR, and chart interpretation.\nThese results highlight the broad applicability of web UI data for advancing\ntext-rich visual understanding across various scenarios.\n","authors":["Junpeng Liu","Tianyue Ou","Yifan Song","Yuxiao Qu","Wai Lam","Chenyan Xiong","Wenhu Chen","Graham Neubig","Xiang Yue"],"pdf_url":"https://arxiv.org/pdf/2410.13824v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12013v2","updated":"2024-10-18T08:57:30Z","published":"2024-06-26T12:33:34Z","title":"Dating ancient manuscripts using radiocarbon and AI-based writing style\n  analysis","summary":"  Determining the chronology of ancient handwritten manuscripts is essential\nfor reconstructing the evolution of ideas. For the Dead Sea Scrolls, this is\nparticularly important. However, there is an almost complete lack of\ndate-bearing manuscripts evenly distributed across the timeline and written in\nsimilar scripts available for palaeographic comparison. Here, we present Enoch,\na state-of-the-art AI-based date-prediction model, trained on the basis of new\nradiocarbon-dated samples of the scrolls. Enoch uses established\nhandwriting-style descriptors and applies Bayesian ridge regression. The\nchallenge of this study is that the number of radiocarbon-dated manuscripts is\nsmall, while current machine learning requires an abundance of training data.\nWe show that by using combined angular and allographic writing style feature\nvectors and applying Bayesian ridge regression, Enoch could predict the\nradiocarbon-based dates from style, supported by leave-one-out validation, with\nvaried MAEs of 27.9 to 30.7 years relative to the radiocarbon dating. Enoch was\nthen used to estimate the dates of 135 unseen manuscripts, revealing that 79\nper cent of the samples were considered 'realistic' upon palaeographic post-hoc\nevaluation. We present a new chronology of the scrolls. The radiocarbon ranges\nand Enoch's style-based predictions are often older than the traditionally\nassumed palaeographic estimates. In the range of 300-50 BCE, Enoch's date\nprediction provides an improved granularity. The study is in line with current\ndevelopments in multimodal machine-learning techniques, and the methods can be\nused for date prediction in other partially-dated manuscript collections. This\nresearch shows how Enoch's quantitative, probability-based approach can be a\ntool for palaeographers and historians, re-dating ancient Jewish key texts and\ncontributing to current debates on Jewish and Christian origins.\n","authors":["Mladen Popović","Maruf A. Dhali","Lambert Schomaker","Johannes van der Plicht","Kaare Lund Rasmussen","Jacopo La Nasa","Ilaria Degano","Maria Perla Colombini","Eibert Tigchelaar"],"pdf_url":"https://arxiv.org/pdf/2407.12013v2.pdf","comment":"16 pages of main article, 103 pages of supplementary materials; the\n  first version of this article is originally prepared in July 2023 after the\n  completion of all the experiments"},{"id":"http://arxiv.org/abs/2410.14285v1","updated":"2024-10-18T08:40:26Z","published":"2024-10-18T08:40:26Z","title":"Advanced Underwater Image Quality Enhancement via Hybrid\n  Super-Resolution Convolutional Neural Networks and Multi-Scale Retinex-Based\n  Defogging Techniques","summary":"  The difficulties of underwater image degradation due to light scattering,\nabsorption, and fog-like particles which lead to low resolution and poor\nvisibility are discussed in this study report. We suggest a sophisticated\nhybrid strategy that combines Multi-Scale Retinex (MSR) defogging methods with\nSuper-Resolution Convolutional Neural Networks (SRCNN) to address these\nproblems. The Retinex algorithm mimics human visual perception to reduce uneven\nlighting and fogging, while the SRCNN component improves the spatial resolution\nof underwater photos.Through the combination of these methods, we are able to\nenhance the clarity, contrast, and colour restoration of underwater images,\noffering a reliable way to improve image quality in difficult underwater\nconditions. The research conducts extensive experiments on real-world\nunderwater datasets to further illustrate the efficacy of the suggested\napproach. In terms of sharpness, visibility, and feature retention,\nquantitative evaluation which use metrics like the Structural Similarity Index\nMeasure (SSIM) and Peak Signal-to-Noise Ratio (PSNR) demonstrates notable\nadvances over conventional techniques.In real-time underwater applications like\nmarine exploration, underwater robotics, and autonomous underwater vehicles,\nwhere clear and high-resolution imaging is crucial for operational success, the\ncombination of deep learning and conventional image processing techniques\noffers a computationally efficient framework with superior results.\n","authors":["Yugandhar Reddy Gogireddy","Jithendra Reddy Gogireddy"],"pdf_url":"https://arxiv.org/pdf/2410.14285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14283v1","updated":"2024-10-18T08:39:56Z","published":"2024-10-18T08:39:56Z","title":"Takin-ADA: Emotion Controllable Audio-Driven Animation with Canonical\n  and Landmark Loss Optimization","summary":"  Existing audio-driven facial animation methods face critical challenges,\nincluding expression leakage, ineffective subtle expression transfer, and\nimprecise audio-driven synchronization. We discovered that these issues stem\nfrom limitations in motion representation and the lack of fine-grained control\nover facial expressions. To address these problems, we present Takin-ADA, a\nnovel two-stage approach for real-time audio-driven portrait animation. In the\nfirst stage, we introduce a specialized loss function that enhances subtle\nexpression transfer while reducing unwanted expression leakage. The second\nstage utilizes an advanced audio processing technique to improve lip-sync\naccuracy. Our method not only generates precise lip movements but also allows\nflexible control over facial expressions and head motions. Takin-ADA achieves\nhigh-resolution (512x512) facial animations at up to 42 FPS on an RTX 4090 GPU,\noutperforming existing commercial solutions. Extensive experiments demonstrate\nthat our model significantly surpasses previous methods in video quality,\nfacial dynamics realism, and natural head movements, setting a new benchmark in\nthe field of audio-driven facial animation.\n","authors":["Bin Lin","Yanzhen Yu","Jianhao Ye","Ruitao Lv","Yuguang Yang","Ruoye Xie","Pan Yu","Hongbin Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.14283v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2410.14282v1","updated":"2024-10-18T08:39:49Z","published":"2024-10-18T08:39:49Z","title":"You Only Look Twice! for Failure Causes Identification of Drill Bits","summary":"  Efficient identification of the root causes of drill bit failure is crucial\ndue to potential impacts such as operational losses, safety threats, and\ndelays. Early recognition of these failures enables proactive maintenance,\nreducing risks and financial losses associated with unforeseen breakdowns and\nprolonged downtime. Thus, our study investigates various causes of drill bit\nfailure using images of different blades. The process involves annotating\ncutters with their respective locations and damage types, followed by the\ndevelopment of two YOLO Location and Damage Cutter Detection models, as well as\nmulti-class multi-label Decision Tree and Random Forests models to identify the\ncauses of failure by assessing the cutters' location and damage type.\nAdditionally, RRFCI is proposed for the classification of failure causes.\nNotably, the cutter location detection model achieved a high score of 0.97 mPA,\nand the cutter damage detection model yielded a 0.49 mPA. The rule-based\napproach over-performed both DT and RF in failure cause identification,\nachieving a macro-average F1-score of 0.94 across all damage causes. The\nintegration of the complete automated pipeline successfully identified 100\\% of\nthe 24 failure causes when tested on independent sets of ten drill bits,\nshowcasing its potential to efficiently assist experts in identifying the root\ncauses of drill bit damages.\n","authors":["Asma Yamani","Nehal Al-Otaiby","Haifa Al-Shemmeri","Imane Boudellioua"],"pdf_url":"https://arxiv.org/pdf/2410.14282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14279v1","updated":"2024-10-18T08:35:57Z","published":"2024-10-18T08:35:57Z","title":"ClearSR: Latent Low-Resolution Image Embeddings Help Diffusion-Based\n  Real-World Super Resolution Models See Clearer","summary":"  We present ClearSR, a new method that can better take advantage of latent\nlow-resolution image (LR) embeddings for diffusion-based real-world image\nsuper-resolution (Real-ISR). Previous Real-ISR models mostly focus on how to\nactivate more generative priors of text-to-image diffusion models to make the\noutput high-resolution (HR) images look better. However, since these methods\nrely too much on the generative priors, the content of the output images is\noften inconsistent with the input LR ones. To mitigate the above issue, in this\nwork, we explore using latent LR embeddings to constrain the control signals\nfrom ControlNet, and extract LR information at both detail and structure\nlevels. We show that the proper use of latent LR embeddings can produce\nhigher-quality control signals, which enables the super-resolution results to\nbe more consistent with the LR image and leads to clearer visual results. In\naddition, we also show that latent LR embeddings can be used to control the\ninference stage, allowing for the improvement of fidelity and generation\nability simultaneously. Experiments demonstrate that our model can achieve\nbetter performance across multiple metrics on several test sets and generate\nmore consistent SR results with LR images than existing methods. Our code will\nbe made publicly available.\n","authors":["Yuhao Wan","Peng-Tao Jiang","Qibin Hou","Hao Zhang","Jinwei Chen","Ming-Ming Cheng","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2410.14279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08091v2","updated":"2024-10-18T08:27:05Z","published":"2024-10-10T16:33:27Z","title":"Distribution Guidance Network for Weakly Supervised Point Cloud Semantic\n  Segmentation","summary":"  Despite alleviating the dependence on dense annotations inherent to fully\nsupervised methods, weakly supervised point cloud semantic segmentation suffers\nfrom inadequate supervision signals. In response to this challenge, we\nintroduce a novel perspective that imparts auxiliary constraints by regulating\nthe feature space under weak supervision. Our initial investigation identifies\nwhich distributions accurately characterize the feature space, subsequently\nleveraging this priori to guide the alignment of the weakly supervised\nembeddings. Specifically, we analyze the superiority of the mixture of von\nMises-Fisher distributions (moVMF) among several common distribution\ncandidates. Accordingly, we develop a Distribution Guidance Network (DGNet),\nwhich comprises a weakly supervised learning branch and a distribution\nalignment branch. Leveraging reliable clustering initialization derived from\nthe weakly supervised learning branch, the distribution alignment branch\nalternately updates the parameters of the moVMF and the network, ensuring\nalignment with the moVMF-defined latent space. Extensive experiments validate\nthe rationality and effectiveness of our distribution choice and network\ndesign. Consequently, DGNet achieves state-of-the-art performance under\nmultiple datasets and various weakly supervised settings.\n","authors":["Zhiyi Pan","Wei Gao","Shan Liu","Ge Li"],"pdf_url":"https://arxiv.org/pdf/2410.08091v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02512v2","updated":"2024-10-18T08:25:52Z","published":"2024-05-03T22:55:56Z","title":"SatSwinMAE: Efficient Autoencoding for Multiscale Time-series Satellite\n  Imagery","summary":"  Recent advancements in foundation models have significantly impacted various\nfields, including natural language processing, computer vision, and multi-modal\ntasks. One area that stands to benefit greatly is Earth observation, where\nthese models can efficiently process large-scale, unlabeled geospatial data. In\nthis work we extend the SwinMAE model to integrate temporal information for\nsatellite time-series data. The architecture employs a hierarchical 3D Masked\nAutoencoder (MAE) with Video Swin Transformer blocks to effectively capture\nmulti-scale spatio-temporal dependencies in satellite imagery. To enhance\ntransfer learning, we incorporate both encoder and decoder pretrained weights,\nalong with skip connections to preserve scale-specific information. This forms\nan architecture similar to SwinUNet with an additional temporal component. Our\napproach shows significant performance improvements over existing\nstate-of-the-art foundation models for all the evaluated downstream tasks: land\ncover segmentation, building density prediction, flood mapping, wildfire scar\nmapping and multi-temporal crop segmentation. Particularly, in the land cover\nsegmentation task of the PhilEO Bench dataset, it outperforms other geospatial\nfoundation models with a 10.4% higher accuracy.\n","authors":["Yohei Nakayama","Jiawei Su","Luis M. Pazos-Outón"],"pdf_url":"https://arxiv.org/pdf/2405.02512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14265v1","updated":"2024-10-18T08:20:37Z","published":"2024-10-18T08:20:37Z","title":"HYPNOS : Highly Precise Foreground-focused Diffusion Finetuning for\n  Inanimate Objects","summary":"  In recent years, personalized diffusion-based text-to-image generative tasks\nhave been a hot topic in computer vision studies. A robust diffusion model is\ndetermined by its ability to perform near-perfect reconstruction of certain\nproduct outcomes given few related input samples. Unfortunately, the current\nprominent diffusion-based finetuning technique falls short in maintaining the\nforeground object consistency while being constrained to produce diverse\nbackgrounds in the image outcome. In the worst scenario, the overfitting issue\nmay occur, meaning that the foreground object is less controllable due to the\ncondition above, for example, the input prompt information is transferred\nambiguously to both foreground and background regions, instead of the supposed\nbackground region only. To tackle the issues above, we proposed Hypnos, a\nhighly precise foreground-focused diffusion finetuning technique. On the image\nlevel, this strategy works best for inanimate object generation tasks, and to\ndo so, Hypnos implements two main approaches, namely: (i) a content-centric\nprompting strategy and (ii) the utilization of our additional\nforeground-focused discriminative module. The utilized module is connected with\nthe diffusion model and finetuned with our proposed set of supervision\nmechanism. Combining the strategies above yielded to the foreground-background\ndisentanglement capability of the diffusion model. Our experimental results\nshowed that the proposed strategy gave a more robust performance and visually\npleasing results compared to the former technique. For better elaborations, we\nalso provided extensive studies to assess the fruitful outcomes above, which\nreveal how personalization behaves in regard to several training conditions.\n","authors":["Oliverio Theophilus Nathanael","Jonathan Samuel Lumentut","Nicholas Hans Muliawan","Edbert Valencio Angky","Felix Indra Kurniadi","Alfi Yusrotis Zakiyyah","Jeklin Harefa"],"pdf_url":"https://arxiv.org/pdf/2410.14265v1.pdf","comment":"26 pages, 12 figures, to appear on the Rich Media with Generative AI\n  workshop in conjunction with Asian Conference on Computer Vision (ACCV) 2024"},{"id":"http://arxiv.org/abs/2406.00125v3","updated":"2024-10-18T08:18:36Z","published":"2024-05-31T18:32:46Z","title":"TotalVibeSegmentator: Full Body MRI Segmentation for the NAKO and UK\n  Biobank","summary":"  Objectives: To present a publicly available torso segmentation network for\nlarge epidemiology datasets on volumetric interpolated breath-hold examination\n(VIBE) images. Materials & Methods: We extracted preliminary segmentations from\nTotalSegmentator, spine, and body composition networks for VIBE images, then\nimproved them iteratively and retrained a nnUNet network. Using subsets of NAKO\n(85 subjects) and UK Biobank (16 subjects), we evaluated with Dice-score on a\nholdout set (12 subjects) and existing organ segmentation approach (1000\nsubjects), generating 71 semantic segmentation types for VIBE images. We\nprovide an additional network for the vertebra segments 22 individual vertebra\ntypes. Results: We achieved an average Dice score of 0.89 +- 0.07 overall 71\nsegmentation labels. We scored > 0.90 Dice-score on the abdominal organs except\nfor the pancreas with a Dice of 0.70. Conclusion: Our work offers a detailed\nand refined publicly available full torso segmentation on VIBE images.\n","authors":["Robert Graf","Paul-Sören Platzek","Evamaria Olga Riedel","Constanze Ramschütz","Sophie Starck","Hendrik Kristian Möller","Matan Atad","Henry Völzke","Robin Bülow","Carsten Oliver Schmidt","Julia Rüdebusch","Matthias Jung","Marco Reisert","Jakob Weiss","Maximilian Löffler","Fabian Bamberg","Bene Wiestler","Johannes C. Paetzold","Daniel Rueckert","Jan Stefan Kirschke"],"pdf_url":"https://arxiv.org/pdf/2406.00125v3.pdf","comment":"https://github.com/robert-graf/TotalVibeSegmentator"},{"id":"http://arxiv.org/abs/2410.14250v1","updated":"2024-10-18T08:01:36Z","published":"2024-10-18T08:01:36Z","title":"Vision-Language Navigation with Energy-Based Policy","summary":"  Vision-language navigation (VLN) requires an agent to execute actions\nfollowing human instructions. Existing VLN models are optimized through expert\ndemonstrations by supervised behavioural cloning or incorporating manual reward\nengineering. While straightforward, these efforts overlook the accumulation of\nerrors in the Markov decision process, and struggle to match the distribution\nof the expert policy. Going beyond this, we propose an Energy-based Navigation\nPolicy (ENP) to model the joint state-action distribution using an energy-based\nmodel. At each step, low energy values correspond to the state-action pairs\nthat the expert is most likely to perform, and vice versa. Theoretically, the\noptimization objective is equivalent to minimizing the forward divergence\nbetween the occupancy measure of the expert and ours. Consequently, ENP learns\nto globally align with the expert policy by maximizing the likelihood of the\nactions and modeling the dynamics of the navigation states in a collaborative\nmanner. With a variety of VLN architectures, ENP achieves promising\nperformances on R2R, REVERIE, RxR, and R2R-CE, unleashing the power of existing\nVLN models.\n","authors":["Rui Liu","Wenguan Wang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2410.14250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13621v2","updated":"2024-10-18T08:01:27Z","published":"2024-10-17T14:55:09Z","title":"Enhanced Prompt-leveraged Weakly Supervised Cancer Segmentation based on\n  Segment Anything","summary":"  This work proposes a novel approach beyond supervised learning for effective\npathological image analysis, addressing the challenge of limited robust labeled\ndata. Pathological diagnosis of diseases like cancer has conventionally relied\non the evaluation of morphological features by physicians and pathologists.\nHowever, recent advancements in compute-aided diagnosis (CAD) systems are\ngaining significant attention as diagnostic support tools. Although the\nadvancement of deep learning has improved CAD significantly, segmentation\nmodels typically require large pixel-level annotated dataset, and such labeling\nis expensive. Existing studies not based on supervised approaches still\nstruggle with limited generalization, and no practical approach has emerged\nyet. To address this issue, we present a weakly supervised semantic\nsegmentation (WSSS) model by combining class activation map and Segment\nAnything Model (SAM)-based pseudo-labeling. For effective pretraining, we adopt\nthe SAM-a foundation model that is pretrained on large datasets and operates in\nzero-shot configurations using only coarse prompts. The proposed approach\ntransfer enhanced Attention Dropout Layer's knowledge to SAM, thereby\ngenerating pseudo-labels. To demonstrate the superiority of the proposed\nmethod, experimental studies are conducted on histopathological breast cancer\ndatasets. The proposed method outperformed other WSSS methods across three\ndatasets, demonstrating its efficiency by achieving this with only 12GB of GPU\nmemory during training. Our code is available at :\nhttps://github.com/QI-NemoSong/EPLC-SAM\n","authors":["Joonhyeon Song","Seohwan Yun","Seongho Yoon","Joohyeok Kim","Sangmin Lee"],"pdf_url":"https://arxiv.org/pdf/2410.13621v2.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.14247v1","updated":"2024-10-18T07:52:03Z","published":"2024-10-18T07:52:03Z","title":"ERDDCI: Exact Reversible Diffusion via Dual-Chain Inversion for\n  High-Quality Image Editing","summary":"  Diffusion models (DMs) have been successfully applied to real image editing.\nThese models typically invert images into latent noise vectors used to\nreconstruct the original images (known as inversion), and then edit them during\nthe inference process. However, recent popular DMs often rely on the assumption\nof local linearization, where the noise injected during the inversion process\nis expected to approximate the noise removed during the inference process.\nWhile DM efficiently generates images under this assumption, it can also\naccumulate errors during the diffusion process due to the assumption,\nultimately negatively impacting the quality of real image reconstruction and\nediting. To address this issue, we propose a novel method, referred to as\nERDDCI (Exact Reversible Diffusion via Dual-Chain Inversion). ERDDCI uses the\nnew Dual-Chain Inversion (DCI) for joint inference to derive an exact\nreversible diffusion process. By using DCI, our method effectively avoids the\ncumbersome optimization process in existing inversion approaches and achieves\nhigh-quality image editing. Additionally, to accommodate image operations under\nhigh guidance scales, we introduce a dynamic control strategy that enables more\nrefined image reconstruction and editing. Our experiments demonstrate that\nERDDCI significantly outperforms state-of-the-art methods in a 50-step\ndiffusion process. It achieves rapid and precise image reconstruction with an\nSSIM of 0.999 and an LPIPS of 0.001, and also delivers competitive results in\nimage editing.\n","authors":["Jimin Dai","Yingzhen Zhang","Shuo Chen","Jian Yang","Lei Luo"],"pdf_url":"https://arxiv.org/pdf/2410.14247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14245v1","updated":"2024-10-18T07:51:31Z","published":"2024-10-18T07:51:31Z","title":"PReP: Efficient context-based shape retrieval for missing parts","summary":"  In this paper we study the problem of shape part retrieval in the point cloud\ndomain. Shape retrieval methods in the literature rely on the presence of an\nexisting query object, but what if the part we are looking for is not\navailable? We present Part Retrieval Pipeline (PReP), a pipeline that\ncreatively utilizes metric learning techniques along with a trained\nclassification model to measure the suitability of potential replacement parts\nfrom a database, as part of an application scenario targeting circular economy.\nThrough an innovative training procedure with increasing difficulty, it is able\nto learn to recognize suitable parts relying only on shape context. Thanks to\nits low parameter size and computational requirements, it can be used to sort\nthrough a warehouse of potentially tens of thousand of spare parts in just a\nfew seconds. We also establish an alternative baseline approach to compare\nagainst, and extensively document the unique challenges associated with this\ntask, as well as identify the design choices to solve them.\n","authors":["Vlassis Fotis","Ioannis Romanelis","Georgios Mylonas","Athanasios Kalogeras","Konstantinos Moustakas"],"pdf_url":"https://arxiv.org/pdf/2410.14245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14242v1","updated":"2024-10-18T07:47:59Z","published":"2024-10-18T07:47:59Z","title":"Pseudo-label Refinement for Improving Self-Supervised Learning Systems","summary":"  Self-supervised learning systems have gained significant attention in recent\nyears by leveraging clustering-based pseudo-labels to provide supervision\nwithout the need for human annotations. However, the noise in these\npseudo-labels caused by the clustering methods poses a challenge to the\nlearning process leading to degraded performance. In this work, we propose a\npseudo-label refinement (SLR) algorithm to address this issue. The cluster\nlabels from the previous epoch are projected to the current epoch\ncluster-labels space and a linear combination of the new label and the\nprojected label is computed as a soft refined label containing the information\nfrom the previous epoch clusters as well as from the current epoch. In contrast\nto the common practice of using the maximum value as a cluster/class indicator,\nwe employ hierarchical clustering on these soft pseudo-labels to generate\nrefined hard-labels. This approach better utilizes the information embedded in\nthe soft labels, outperforming the simple maximum value approach for hard label\ngeneration. The effectiveness of the proposed SLR algorithm is evaluated in the\ncontext of person re-identification (Re-ID) using unsupervised domain\nadaptation (UDA). Experimental results demonstrate that the modified Re-ID\nbaseline, incorporating the SLR algorithm, achieves significantly improved mean\nAverage Precision (mAP) performance in various UDA tasks, including\nreal-to-synthetic, synthetic-to-real, and different real-to-real scenarios.\nThese findings highlight the efficacy of the SLR algorithm in enhancing the\nperformance of self-supervised learning systems.\n","authors":[" Zia-ur-Rehman","Arif Mahmood","Wenxiong Kang"],"pdf_url":"https://arxiv.org/pdf/2410.14242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14238v1","updated":"2024-10-18T07:40:41Z","published":"2024-10-18T07:40:41Z","title":"Storyboard guided Alignment for Fine-grained Video Action Recognition","summary":"  Fine-grained video action recognition can be conceptualized as a video-text\nmatching problem. Previous approaches often rely on global video semantics to\nconsolidate video embeddings, which can lead to misalignment in video-text\npairs due to a lack of understanding of action semantics at an atomic\ngranularity level. To tackle this challenge, we propose a multi-granularity\nframework based on two observations: (i) videos with different global semantics\nmay share similar atomic actions or appearances, and (ii) atomic actions within\na video can be momentary, slow, or even non-directly related to the global\nvideo semantics. Inspired by the concept of storyboarding, which disassembles a\nscript into individual shots, we enhance global video semantics by generating\nfine-grained descriptions using a pre-trained large language model. These\ndetailed descriptions capture common atomic actions depicted in videos. A\nfiltering metric is proposed to select the descriptions that correspond to the\natomic actions present in both the videos and the descriptions. By employing\nglobal semantics and fine-grained descriptions, we can identify key frames in\nvideos and utilize them to aggregate embeddings, thereby making the embedding\nmore accurate. Extensive experiments on various video action recognition\ndatasets demonstrate superior performance of our proposed method in supervised,\nfew-shot, and zero-shot settings.\n","authors":["Enqi Liu","Liyuan Pan","Yan Yang","Yiran Zhong","Zhijing Wu","Xinxiao Wu","Liu Liu"],"pdf_url":"https://arxiv.org/pdf/2410.14238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.05822v3","updated":"2024-10-18T07:24:54Z","published":"2023-08-10T18:43:44Z","title":"Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded\n  Egocentric Perception","summary":"  We depend on our own memory to encode, store, and retrieve our experiences.\nHowever, memory lapses can occur. One promising avenue for achieving memory\naugmentation is through the use of augmented reality head-mounted displays to\ncapture and preserve egocentric videos, a practice commonly referred to as\nlifelogging. However, a significant challenge arises from the sheer volume of\nvideo data generated through lifelogging, as the current technology lacks the\ncapability to encode and store such large amounts of data efficiently. Further,\nretrieving specific information from extensive video archives requires\nsubstantial computational power, further complicating the task of quickly\naccessing desired content. To address these challenges, we propose a memory\naugmentation agent that involves leveraging natural language encoding for video\ndata and storing them in a vector database. This approach harnesses the power\nof large vision language models to perform the language encoding process.\nAdditionally, we propose using large language models to facilitate natural\nlanguage querying. Our agent underwent extensive evaluation using the QA-Ego4D\ndataset and achieved state-of-the-art results with a BLEU score of 8.3,\noutperforming conventional machine learning models that scored between 3.4 and\n5.8. Additionally, we conducted a user study in which participants interacted\nwith the human memory augmentation agent through episodic memory and open-ended\nquestions. The results of this study show that the agent results in\nsignificantly better recall performance on episodic memory tasks compared to\nhuman participants. The results also highlight the agent's practical\napplicability and user acceptance.\n","authors":["Junxiao Shen","John Dudley","Per Ola Kristensson"],"pdf_url":"https://arxiv.org/pdf/2308.05822v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18791v3","updated":"2024-10-18T07:21:56Z","published":"2024-03-27T17:35:24Z","title":"Object Pose Estimation via the Aggregation of Diffusion Features","summary":"  Estimating the pose of objects from images is a crucial task of 3D scene\nunderstanding, and recent approaches have shown promising results on very large\nbenchmarks. However, these methods experience a significant performance drop\nwhen dealing with unseen objects. We believe that it results from the limited\ngeneralizability of image features. To address this problem, we have an\nin-depth analysis on the features of diffusion models, e.g. Stable Diffusion,\nwhich hold substantial potential for modeling unseen objects. Based on this\nanalysis, we then innovatively introduce these diffusion features for object\npose estimation. To achieve this, we propose three distinct architectures that\ncan effectively capture and aggregate diffusion features of different\ngranularity, greatly improving the generalizability of object pose estimation.\nOur approach outperforms the state-of-the-art methods by a considerable margin\non three popular benchmark datasets, LM, O-LM, and T-LESS. In particular, our\nmethod achieves higher accuracy than the previous best arts on unseen objects:\n97.9% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the\nstrong generalizability of our method. Our code is released at\nhttps://github.com/Tianfu18/diff-feats-pose.\n","authors":["Tianfu Wang","Guosheng Hu","Hongguang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18791v3.pdf","comment":"Accepted to CVPR2024, fix typo"},{"id":"http://arxiv.org/abs/2410.09421v2","updated":"2024-10-18T07:10:38Z","published":"2024-10-12T07:56:47Z","title":"VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language\n  Models Alignment","summary":"  As large vision-language models (LVLMs) evolve rapidly, the demand for\nhigh-quality and diverse data to align these models becomes increasingly\ncrucial. However, the creation of such data with human supervision proves\ncostly and time-intensive. In this paper, we investigate the efficacy of AI\nfeedback to scale supervision for aligning LVLMs. We introduce VLFeedback, the\nfirst large-scale vision-language feedback dataset, comprising over 82K\nmulti-modal instructions and comprehensive rationales generated by\noff-the-shelf models without human annotations. To evaluate the effectiveness\nof AI feedback for vision-language alignment, we train Silkie, an LVLM\nfine-tuned via direct preference optimization on VLFeedback. Silkie showcases\nexceptional performance regarding helpfulness, visual faithfulness, and safety\nmetrics. It outperforms its base model by 6.9\\% and 9.5\\% in perception and\ncognition tasks, reduces hallucination issues on MMHal-Bench, and exhibits\nenhanced resilience against red-teaming attacks. Furthermore, our analysis\nunderscores the advantage of AI feedback, particularly in fostering preference\ndiversity to deliver more comprehensive improvements. Our dataset, training\ncode and models are available at https://vlf-silkie.github.io.\n","authors":["Lei Li","Zhihui Xie","Mukai Li","Shunian Chen","Peiyi Wang","Liang Chen","Yazheng Yang","Benyou Wang","Lingpeng Kong","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2410.09421v2.pdf","comment":"EMNLP 2024 Main Conference camera-ready version (fixed small typos).\n  This article supersedes arXiv:2312.10665"},{"id":"http://arxiv.org/abs/2410.14214v1","updated":"2024-10-18T07:02:57Z","published":"2024-10-18T07:02:57Z","title":"MambaSCI: Efficient Mamba-UNet for Quad-Bayer Patterned Video Snapshot\n  Compressive Imaging","summary":"  Color video snapshot compressive imaging (SCI) employs computational imaging\ntechniques to capture multiple sequential video frames in a single\nBayer-patterned measurement. With the increasing popularity of quad-Bayer\npattern in mainstream smartphone cameras for capturing high-resolution videos,\nmobile photography has become more accessible to a wider audience. However,\nexisting color video SCI reconstruction algorithms are designed based on the\ntraditional Bayer pattern. When applied to videos captured by quad-Bayer\ncameras, these algorithms often result in color distortion and ineffective\ndemosaicing, rendering them impractical for primary equipment. To address this\nchallenge, we propose the MambaSCI method, which leverages the Mamba and UNet\narchitectures for efficient reconstruction of quad-Bayer patterned color video\nSCI. To the best of our knowledge, our work presents the first algorithm for\nquad-Bayer patterned SCI reconstruction, and also the initial application of\nthe Mamba model to this task. Specifically, we customize Residual-Mamba-Blocks,\nwhich residually connect the Spatial-Temporal Mamba (STMamba),\nEdge-Detail-Reconstruction (EDR) module, and Channel Attention (CA) module.\nRespectively, STMamba is used to model long-range spatial-temporal dependencies\nwith linear complexity, EDR is for better edge-detail reconstruction, and CA is\nused to compensate for the missing channel information interaction in Mamba\nmodel. Experiments demonstrate that MambaSCI surpasses state-of-the-art methods\nwith lower computational and memory costs. PyTorch style pseudo-code for the\ncore modules is provided in the supplementary materials.\n","authors":["Zhenghao Pan","Haijin Zeng","Jiezhang Cao","Yongyong Chen","Kai Zhang","Yong Xu"],"pdf_url":"https://arxiv.org/pdf/2410.14214v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.10167v2","updated":"2024-10-18T06:57:51Z","published":"2024-10-14T05:23:12Z","title":"X-Fi: A Modality-Invariant Foundation Model for Multimodal Human Sensing","summary":"  Human sensing, which employs various sensors and advanced deep learning\ntechnologies to accurately capture and interpret human body information, has\nsignificantly impacted fields like public security and robotics. However,\ncurrent human sensing primarily depends on modalities such as cameras and\nLiDAR, each of which has its own strengths and limitations. Furthermore,\nexisting multi-modal fusion solutions are typically designed for fixed modality\ncombinations, requiring extensive retraining when modalities are added or\nremoved for diverse scenarios. In this paper, we propose a modality-invariant\nfoundation model for all modalities, X-Fi, to address this issue. X-Fi enables\nthe independent or combinatory use of sensor modalities without additional\ntraining by utilizing a transformer structure to accommodate variable input\nsizes and incorporating a novel \"X-fusion\" mechanism to preserve\nmodality-specific features during multimodal integration. This approach not\nonly enhances adaptability but also facilitates the learning of complementary\nfeatures across modalities. Extensive experiments conducted on the MM-Fi and\nXRF55 datasets, employing six distinct modalities, demonstrate that X-Fi\nachieves state-of-the-art performance in human pose estimation (HPE) and human\nactivity recognition (HAR) tasks. The findings indicate that our proposed model\ncan efficiently support a wide range of human sensing applications, ultimately\ncontributing to the evolution of scalable, multimodal sensing technologies.\n","authors":["Xinyan Chen","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2410.10167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14210v1","updated":"2024-10-18T06:52:51Z","published":"2024-10-18T06:52:51Z","title":"Shape Transformation Driven by Active Contour for Class-Imbalanced\n  Semi-Supervised Medical Image Segmentation","summary":"  Annotating 3D medical images demands expert knowledge and is time-consuming.\nAs a result, semi-supervised learning (SSL) approaches have gained significant\ninterest in 3D medical image segmentation. The significant size differences\namong various organs in the human body lead to imbalanced class distribution,\nwhich is a major challenge in the real-world application of these SSL\napproaches. To address this issue, we develop a novel Shape Transformation\ndriven by Active Contour (STAC), that enlarges smaller organs to alleviate\nimbalanced class distribution across different organs. Inspired by curve\nevolution theory in active contour methods, STAC employs a signed distance\nfunction (SDF) as the level set function, to implicitly represent the shape of\norgans, and deforms voxels in the direction of the steepest descent of SDF\n(i.e., the normal vector). To ensure that the voxels far from expansion organs\nremain unchanged, we design an SDF-based weight function to control the degree\nof deformation for each voxel. We then use STAC as a data-augmentation process\nduring the training stage. Experimental results on two benchmark datasets\ndemonstrate that the proposed method significantly outperforms some\nstate-of-the-art methods. Source code is publicly available at\nhttps://github.com/GuGuLL123/STAC.\n","authors":["Yuliang Gu","Yepeng Liu","Zhichao Sun","Jinchi Zhu","Yongchao Xu","Laurent Najman"],"pdf_url":"https://arxiv.org/pdf/2410.14210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06719v3","updated":"2024-10-18T06:39:27Z","published":"2024-10-09T09:43:36Z","title":"Suppress Content Shift: Better Diffusion Features via Off-the-Shelf\n  Generation Techniques","summary":"  Diffusion models are powerful generative models, and this capability can also\nbe applied to discrimination. The inner activations of a pre-trained diffusion\nmodel can serve as features for discriminative tasks, namely, diffusion\nfeature. We discover that diffusion feature has been hindered by a hidden yet\nuniversal phenomenon that we call content shift. To be specific, there are\ncontent differences between features and the input image, such as the exact\nshape of a certain object. We locate the cause of content shift as one inherent\ncharacteristic of diffusion models, which suggests the broad existence of this\nphenomenon in diffusion feature. Further empirical study also indicates that\nits negative impact is not negligible even when content shift is not visually\nperceivable. Hence, we propose to suppress content shift to enhance the overall\nquality of diffusion features. Specifically, content shift is related to the\ninformation drift during the process of recovering an image from the noisy\ninput, pointing out the possibility of turning off-the-shelf generation\ntechniques into tools for content shift suppression. We further propose a\npractical guideline named GATE to efficiently evaluate the potential benefit of\na technique and provide an implementation of our methodology. Despite the\nsimplicity, the proposed approach has achieved superior results on various\ntasks and datasets, validating its potential as a generic booster for diffusion\nfeatures. Our code is available at\nhttps://github.com/Darkbblue/diffusion-content-shift.\n","authors":["Benyuan Meng","Qianqian Xu","Zitai Wang","Zhiyong Yang","Xiaochun Cao","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2410.06719v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2410.03558"},{"id":"http://arxiv.org/abs/2410.14201v1","updated":"2024-10-18T06:31:57Z","published":"2024-10-18T06:31:57Z","title":"Text-to-Image Representativity Fairness Evaluation Framework","summary":"  Text-to-Image generative systems are progressing rapidly to be a source of\nadvertisement and media and could soon serve as image searches or artists.\nHowever, there is a significant concern about the representativity bias these\nmodels embody and how these biases can propagate in the social fabric after\nfine-tuning them. Therefore, continuously monitoring and evaluating these\nmodels for fairness is important. To address this issue, we propose\nText-to-Image (TTI) Representativity Fairness Evaluation Framework. In this\nframework, we evaluate three aspects of a TTI system; diversity, inclusion, and\nquality. For each aspect, human-based and model-based approaches are proposed\nand evaluated for their ability to capture the bias and whether they can\nsubstitute each other. The framework starts by suggesting the prompts for\ngenerating the images for the evaluation based on the context and the sensitive\nattributes under study. Then the three aspects are evaluated using the proposed\napproaches. Based on the evaluation, a decision is made regarding the\nrepresentativity bias within the TTI system. The evaluation of our framework on\nStable Diffusion shows that the framework can effectively capture the bias in\nTTI systems. The results also confirm that our proposed model based-approaches\ncan substitute human-based approaches in three out of four components with high\ncorrelation, which could potentially reduce costs and automate the process. The\nstudy suggests that continual learning of the model on more inclusive data\nacross disadvantaged minorities such as Indians and Middle Easterners is\nessential to mitigate current stereotyping and lack of inclusiveness.\n","authors":["Asma Yamani","Malak Baslyman"],"pdf_url":"https://arxiv.org/pdf/2410.14201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14200v1","updated":"2024-10-18T06:31:40Z","published":"2024-10-18T06:31:40Z","title":"E3D-GPT: Enhanced 3D Visual Foundation for Medical Vision-Language Model","summary":"  The development of 3D medical vision-language models holds significant\npotential for disease diagnosis and patient treatment. However, compared to 2D\nmedical images, 3D medical images, such as CT scans, face challenges related to\nlimited training data and high dimension, which severely restrict the progress\nof 3D medical vision-language models. To address these issues, we collect a\nlarge amount of unlabeled 3D CT data and utilize self-supervised learning to\nconstruct a 3D visual foundation model for extracting 3D visual features. Then,\nwe apply 3D spatial convolutions to aggregate and project high-level image\nfeatures, reducing computational complexity while preserving spatial\ninformation. We also construct two instruction-tuning datasets based on BIMCV-R\nand CT-RATE to fine-tune the 3D vision-language model. Our model demonstrates\nsuperior performance compared to existing methods in report generation, visual\nquestion answering, and disease diagnosis. Code and data will be made publicly\navailable soon.\n","authors":["Haoran Lai","Zihang Jiang","Qingsong Yao","Rongsheng Wang","Zhiyang He","Xiaodong Tao","Wei Wei","Weifu Lv","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.14200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03558v3","updated":"2024-10-18T06:19:45Z","published":"2024-10-04T16:05:14Z","title":"Not All Diffusion Model Activations Have Been Evaluated as\n  Discriminative Features","summary":"  Diffusion models are initially designed for image generation. Recent research\nshows that the internal signals within their backbones, named activations, can\nalso serve as dense features for various discriminative tasks such as semantic\nsegmentation. Given numerous activations, selecting a small yet effective\nsubset poses a fundamental problem. To this end, the early study of this field\nperforms a large-scale quantitative comparison of the discriminative ability of\nthe activations. However, we find that many potential activations have not been\nevaluated, such as the queries and keys used to compute attention scores.\nMoreover, recent advancements in diffusion architectures bring many new\nactivations, such as those within embedded ViT modules. Both combined,\nactivation selection remains unresolved but overlooked. To tackle this issue,\nthis paper takes a further step with a much broader range of activations\nevaluated. Considering the significant increase in activations, a full-scale\nquantitative comparison is no longer operational. Instead, we seek to\nunderstand the properties of these activations, such that the activations that\nare clearly inferior can be filtered out in advance via simple qualitative\nevaluation. After careful analysis, we discover three properties universal\namong diffusion models, enabling this study to go beyond specific models. On\ntop of this, we present effective feature selection solutions for several\npopular diffusion models. Finally, the experiments across multiple\ndiscriminative tasks validate the superiority of our method over the SOTA\ncompetitors. Our code is available at\nhttps://github.com/Darkbblue/generic-diffusion-feature.\n","authors":["Benyuan Meng","Qianqian Xu","Zitai Wang","Xiaochun Cao","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2410.03558v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14195v1","updated":"2024-10-18T06:12:36Z","published":"2024-10-18T06:12:36Z","title":"Rethinking Transformer for Long Contextual Histopathology Whole Slide\n  Image Analysis","summary":"  Histopathology Whole Slide Image (WSI) analysis serves as the gold standard\nfor clinical cancer diagnosis in the daily routines of doctors. To develop\ncomputer-aided diagnosis model for WSIs, previous methods typically employ\nMulti-Instance Learning to enable slide-level prediction given only slide-level\nlabels. Among these models, vanilla attention mechanisms without pairwise\ninteractions have traditionally been employed but are unable to model\ncontextual information. More recently, self-attention models have been utilized\nto address this issue. To alleviate the computational complexity of long\nsequences in large WSIs, methods like HIPT use region-slicing, and TransMIL\nemploys approximation of full self-attention. Both approaches suffer from\nsuboptimal performance due to the loss of key information. Moreover, their use\nof absolute positional embedding struggles to effectively handle long\ncontextual dependencies in shape-varying WSIs. In this paper, we first analyze\nhow the low-rank nature of the long-sequence attention matrix constrains the\nrepresentation ability of WSI modelling. Then, we demonstrate that the rank of\nattention matrix can be improved by focusing on local interactions via a local\nattention mask. Our analysis shows that the local mask aligns with the\nattention patterns in the lower layers of the Transformer. Furthermore, the\nlocal attention mask can be implemented during chunked attention calculation,\nreducing the quadratic computational complexity to linear with a small local\nbandwidth. Building on this, we propose a local-global hybrid Transformer for\nboth computational acceleration and local-global information interactions\nmodelling. Our method, Long-contextual MIL (LongMIL), is evaluated through\nextensive experiments on various WSI tasks to validate its superiority. Our\ncode will be available at github.com/invoker-LL/Long-MIL.\n","authors":["Honglin Li","Yunlong Zhang","Pingyi Chen","Zhongyi Shui","Chenglu Zhu","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2410.14195v1.pdf","comment":"NeurIPS-2024. arXiv admin note: text overlap with arXiv:2311.12885"},{"id":"http://arxiv.org/abs/2410.13195v2","updated":"2024-10-18T06:02:28Z","published":"2024-10-17T03:48:02Z","title":"UniG: Modelling Unitary 3D Gaussians for View-consistent 3D\n  Reconstruction","summary":"  In this work, we present UniG, a view-consistent 3D reconstruction and novel\nview synthesis model that generates a high-fidelity representation of 3D\nGaussians from sparse images. Existing 3D Gaussians-based methods usually\nregress Gaussians per-pixel of each view, create 3D Gaussians per view\nseparately, and merge them through point concatenation. Such a view-independent\nreconstruction approach often results in a view inconsistency issue, where the\npredicted positions of the same 3D point from different views may have\ndiscrepancies. To address this problem, we develop a DETR (DEtection\nTRansformer)-like framework, which treats 3D Gaussians as decoder queries and\nupdates their parameters layer by layer by performing multi-view\ncross-attention (MVDFA) over multiple input images. In this way, multiple views\nnaturally contribute to modeling a unitary representation of 3D Gaussians,\nthereby making 3D reconstruction more view-consistent. Moreover, as the number\nof 3D Gaussians used as decoder queries is irrespective of the number of input\nviews, allow an arbitrary number of input images without causing memory\nexplosion. Extensive experiments validate the advantages of our approach,\nshowcasing superior performance over existing methods quantitatively (improving\nPSNR by 4.2 dB when trained on Objaverse and tested on the GSO benchmark) and\nqualitatively. The code will be released at https://github.com/jwubz123/UNIG.\n","authors":["Jiamin Wu","Kenkun Liu","Yukai Shi","Xiaoke Jiang","Yuan Yao","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.13195v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14189v1","updated":"2024-10-18T05:48:06Z","published":"2024-10-18T05:48:06Z","title":"Neural Signed Distance Function Inference through Splatting 3D Gaussians\n  Pulled on Zero-Level Set","summary":"  It is vital to infer a signed distance function (SDF) in multi-view based\nsurface reconstruction. 3D Gaussian splatting (3DGS) provides a novel\nperspective for volume rendering, and shows advantages in rendering efficiency\nand quality. Although 3DGS provides a promising neural rendering option, it is\nstill hard to infer SDFs for surface reconstruction with 3DGS due to the\ndiscreteness, the sparseness, and the off-surface drift of 3D Gaussians. To\nresolve these issues, we propose a method that seamlessly merge 3DGS with the\nlearning of neural SDFs. Our key idea is to more effectively constrain the SDF\ninference with the multi-view consistency. To this end, we dynamically align 3D\nGaussians on the zero-level set of the neural SDF using neural pulling, and\nthen render the aligned 3D Gaussians through the differentiable rasterization.\nMeanwhile, we update the neural SDF by pulling neighboring space to the pulled\n3D Gaussians, which progressively refine the signed distance field near the\nsurface. With both differentiable pulling and splatting, we jointly optimize 3D\nGaussians and the neural SDF with both RGB and geometry constraints, which\nrecovers more accurate, smooth, and complete surfaces with more geometry\ndetails. Our numerical and visual comparisons show our superiority over the\nstate-of-the-art results on the widely used benchmarks.\n","authors":["Wenyuan Zhang","Yu-Shen Liu","Zhizhong Han"],"pdf_url":"https://arxiv.org/pdf/2410.14189v1.pdf","comment":"Accepted by NeurIPS 2024. Project page:\n  https://wen-yuan-zhang.github.io/GS-Pull/"},{"id":"http://arxiv.org/abs/2406.13123v2","updated":"2024-10-18T05:20:34Z","published":"2024-06-19T00:38:19Z","title":"ViLCo-Bench: VIdeo Language COntinual learning Benchmark","summary":"  Video language continual learning involves continuously adapting to\ninformation from video and text inputs, enhancing a model's ability to handle\nnew tasks while retaining prior knowledge. This field is a relatively\nunder-explored area, and establishing appropriate datasets is crucial for\nfacilitating communication and research in this field. In this study, we\npresent the first dedicated benchmark, ViLCo-Bench, designed to evaluate\ncontinual learning models across a range of video-text tasks. The dataset\ncomprises ten-minute-long videos and corresponding language queries collected\nfrom publicly available datasets. Additionally, we introduce a novel\nmemory-efficient framework that incorporates self-supervised learning and\nmimics long-term and short-term memory effects. This framework addresses\nchallenges including memory complexity from long video clips, natural language\ncomplexity from open queries, and text-video misalignment. We posit that\nViLCo-Bench, with greater complexity compared to existing continual learning\nbenchmarks, would serve as a critical tool for exploring the video-language\ndomain, extending beyond conventional class-incremental tasks, and addressing\ncomplex and limited annotation issues. The curated data, evaluations, and our\nnovel method are available at https://github.com/cruiseresearchgroup/ViLCo.\n","authors":["Tianqi Tang","Shohreh Deldari","Hao Xue","Celso De Melo","Flora D. Salim"],"pdf_url":"https://arxiv.org/pdf/2406.13123v2.pdf","comment":"14 pages, 4 figures, 8 tables, Accepted at NeurIPS Dataset and\n  Benchmark Track 2024"},{"id":"http://arxiv.org/abs/2404.10210v3","updated":"2024-10-18T05:17:29Z","published":"2024-04-16T01:41:22Z","title":"MK-SGN: A Spiking Graph Convolutional Network with Multimodal Fusion and\n  Knowledge Distillation for Skeleton-based Action Recognition","summary":"  In recent years, skeleton-based action recognition, leveraging multimodal\nGraph Convolutional Networks (GCN), has achieved remarkable results. However,\ndue to their deep structure and reliance on continuous floating-point\noperations, GCN-based methods are energy-intensive. We propose an innovative\nSpiking Graph Convolutional Network with Multimodal Fusion and Knowledge\nDistillation (MK-SGN) to address this issue. By merging the energy efficiency\nof Spiking Neural Network (SNN) with the graph representation capability of\nGCN, the proposed MK-SGN reduces energy consumption while maintaining\nrecognition accuracy. Firstly, we convert Graph Convolutional Networks (GCN)\ninto Spiking Graph Convolutional Networks (SGN) establishing a new benchmark\nand paving the way for future research exploration. During this process, we\nintroduce a spiking attention mechanism and design a Spiking-Spatio Graph\nConvolution module with a Spatial Global Spiking Attention mechanism (SA-SGC),\nenhancing feature learning capability. Secondly, we propose a Spiking\nMultimodal Fusion module (SMF), leveraging mutual information to process\nmultimodal data more efficiently. Lastly, we delve into knowledge distillation\nmethods from multimodal GCN to SGN and propose a novel, integrated method that\nsimultaneously focuses on both intermediate layer distillation and soft label\ndistillation to improve the performance of SGN. MK-SGN outperforms the\nstate-of-the-art GCN-like frameworks on three challenging datasets for\nskeleton-based action recognition in reducing energy consumption. It also\noutperforms the state-of-the-art SNN frameworks in accuracy. Specifically, our\nmethod reduces energy consumption by more than 98% compared to typical\nGCN-based methods, while maintaining competitive accuracy on the NTU-RGB+D 60\ncross-subject split using 4-time steps.\n","authors":["Naichuan Zheng","Hailun Xia","Zeyu Liang","Yuanyuan Chai"],"pdf_url":"https://arxiv.org/pdf/2404.10210v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14179v1","updated":"2024-10-18T05:15:50Z","published":"2024-10-18T05:15:50Z","title":"MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart\n  Problems","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated impressive\nabilities across various tasks, including visual question answering and chart\ncomprehension, yet existing benchmarks for chart-related tasks fall short in\ncapturing the complexity of real-world multi-chart scenarios. Current\nbenchmarks primarily focus on single-chart tasks, neglecting the multi-hop\nreasoning required to extract and integrate information from multiple charts,\nwhich is essential in practical applications. To fill this gap, we introduce\nMultiChartQA, a benchmark that evaluates MLLMs' capabilities in four key areas:\ndirect question answering, parallel question answering, comparative reasoning,\nand sequential reasoning. Our evaluation of a wide range of MLLMs reveals\nsignificant performance gaps compared to humans. These results highlight the\nchallenges in multi-chart comprehension and the potential of MultiChartQA to\ndrive advancements in this field. Our code and data are available at\nhttps://github.com/Zivenzhu/Multi-chart-QA\n","authors":["Zifeng Zhu","Mengzhao Jia","Zhihan Zhang","Lang Li","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.14179v1.pdf","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2305.19513v2","updated":"2024-10-18T05:14:57Z","published":"2023-05-31T02:52:38Z","title":"Hard Region Aware Network for Remote Sensing Change Detection","summary":"  Change detection (CD) is essential for various real-world applications, such\nas urban management and disaster assessment. Numerous CD methods have been\nproposed, and considerable results have been achieved recently. However,\ndetecting changes in hard regions, i.e., the change boundary and irrelevant\npseudo changes caused by background clutters, remains difficult for these\nmethods, since they pose equal attention for all regions in bi-temporal images.\nThis paper proposes a novel change detection network, termed as HRANet, which\nprovides accurate change maps via hard region mining. Specifically, an online\nhard region estimation branch is constructed to model the pixel-wise hard\nsamples, supervised by the error between predicted change maps and\ncorresponding ground truth during the training process. A cross-layer knowledge\nreview module is introduced to distill temporal change information from\nlow-level to high-level features, thereby enhancing the feature representation\ncapabilities. Finally, the hard region aware features extracted from the online\nhard region estimation branch and multi-level temporal difference features are\naggregated into a unified feature representation to improve the accuracy of CD.\nExperimental results on two benchmark datasets demonstrate the superior\nperformance of HRANet in the CD task.\n","authors":["Zhenglai Li","Chang Tang","Xinwang Liu","Xingchen Hu","Xianju Li","Ning Li","Changdong Li"],"pdf_url":"https://arxiv.org/pdf/2305.19513v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01701v2","updated":"2024-10-18T05:12:00Z","published":"2024-08-03T07:47:16Z","title":"Signal-SGN: A Spiking Graph Convolutional Network for Skeletal Action\n  Recognition via Learning Temporal-Frequency Dynamics","summary":"  In skeletal-based action recognition, Graph Convolutional Networks (GCNs)\nbased methods face limitations due to their complexity and high energy\nconsumption. Spiking Neural Networks (SNNs) have gained attention in recent\nyears for their low energy consumption, but existing methods combining GCNs and\nSNNs fail to fully utilize the temporal characteristics of skeletal sequences,\nleading to increased storage and computational costs. To address this issue, we\npropose a Signal-SGN(Spiking Graph Convolutional Network), which leverages the\ntemporal dimension of skeletal sequences as the spiking timestep and treats\nfeatures as discrete stochastic signals. The core of the network consists of a\n1D Spiking Graph Convolutional Network (1D-SGN) and a Frequency Spiking\nConvolutional Network (FSN). The SGN performs graph convolution on single\nframes and incorporates spiking network characteristics to capture inter-frame\ntemporal relationships, while the FSN uses Fast Fourier Transform (FFT) and\ncomplex convolution to extract temporal-frequency features. We also introduce a\nmulti-scale wavelet transform feature fusion module(MWTF) to capture spectral\nfeatures of temporal signals, enhancing the model's classification capability.\nWe propose a pluggable temporal-frequency spatial semantic feature extraction\nmodule(TFSM) to enhance the model's ability to distinguish features without\nincreasing inference-phase consumption. Our numerous experiments on the NTU\nRGB+D, NTU RGB+D 120, and NW-UCLA datasets demonstrate that the proposed models\nnot only surpass existing SNN-based methods in accuracy but also reduce\ncomputational and storage costs during training. Furthermore, they achieve\ncompetitive accuracy compared to corresponding GCN-based methods, which is\nquite remarkable.\n","authors":["Naichuan Zheng","Hailun Xia","Dapeng Liu"],"pdf_url":"https://arxiv.org/pdf/2408.01701v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14178v1","updated":"2024-10-18T05:11:07Z","published":"2024-10-18T05:11:07Z","title":"Feature Augmentation based Test-Time Adaptation","summary":"  Test-time adaptation (TTA) allows a model to be adapted to an unseen domain\nwithout accessing the source data. Due to the nature of practical environments,\nTTA has a limited amount of data for adaptation. Recent TTA methods further\nrestrict this by filtering input data for reliability, making the effective\ndata size even smaller and limiting adaptation potential. To address this\nissue, We propose Feature Augmentation based Test-time Adaptation (FATA), a\nsimple method that fully utilizes the limited amount of input data through\nfeature augmentation. FATA employs Normalization Perturbation to augment\nfeatures and adapts the model using the FATA loss, which makes the outputs of\nthe augmented and original features similar. FATA is model-agnostic and can be\nseamlessly integrated into existing models without altering the model\narchitecture. We demonstrate the effectiveness of FATA on various models and\nscenarios on ImageNet-C and Office-Home, validating its superiority in diverse\nreal-world conditions.\n","authors":["Younggeol Cho","Youngrae Kim","Junho Yoon","Seunghoon Hong","Dongman Lee"],"pdf_url":"https://arxiv.org/pdf/2410.14178v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.14177v1","updated":"2024-10-18T05:09:07Z","published":"2024-10-18T05:09:07Z","title":"Learning autonomous driving from aerial imagery","summary":"  In this work, we consider the problem of learning end to end perception to\ncontrol for ground vehicles solely from aerial imagery. Photogrammetric\nsimulators allow the synthesis of novel views through the transformation of\npre-generated assets into novel views.However, they have a large setup cost,\nrequire careful collection of data and often human effort to create usable\nsimulators. We use a Neural Radiance Field (NeRF) as an intermediate\nrepresentation to synthesize novel views from the point of view of a ground\nvehicle. These novel viewpoints can then be used for several downstream\nautonomous navigation applications. In this work, we demonstrate the utility of\nnovel view synthesis though the application of training a policy for end to end\nlearning from images and depth data. In a traditional real to sim to real\nframework, the collected data would be transformed into a visual simulator\nwhich could then be used to generate novel views. In contrast, using a NeRF\nallows a compact representation and the ability to optimize over the parameters\nof the visual simulator as more data is gathered in the environment. We\ndemonstrate the efficacy of our method in a custom built mini-city environment\nthrough the deployment of imitation policies on robotic cars. We additionally\nconsider the task of place localization and demonstrate that our method is able\nto relocalize the car in the real world.\n","authors":["Varun Murali","Guy Rosman","Sertac Karaman","Daniela Rus"],"pdf_url":"https://arxiv.org/pdf/2410.14177v1.pdf","comment":"Presented at IROS 2024"},{"id":"http://arxiv.org/abs/2402.13876v3","updated":"2024-10-18T05:04:40Z","published":"2024-02-21T15:35:59Z","title":"Scene Prior Filtering for Depth Super-Resolution","summary":"  Multi-modal fusion is vital to the success of super-resolution of depth maps.\nHowever, commonly used fusion strategies, such as addition and concatenation,\nfall short of effectively bridging the modal gap. As a result, guided image\nfiltering methods have been introduced to mitigate this issue. Nevertheless, it\nis observed that their filter kernels usually encounter significant texture\ninterference and edge inaccuracy. To tackle these two challenges, we introduce\na Scene Prior Filtering network, SPFNet, which utilizes the priors surface\nnormal and semantic map from large-scale models. Specifically, we design an\nAll-in-one Prior Propagation that computes the similarity between multi-modal\nscene priors, i.e., RGB, normal, semantic, and depth, to reduce the texture\ninterference. In addition, we present a One-to-one Prior Embedding that\ncontinuously embeds each single-modal prior into depth using Mutual Guided\nFiltering, further alleviating the texture interference while enhancing edges.\nOur SPFNet has been extensively evaluated on both real and synthetic datasets,\nachieving state-of-the-art performance.\n","authors":["Zhengxue Wang","Zhiqiang Yan","Ming-Hsuan Yang","Jinshan Pan","Guangwei Gao","Ying Tai","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2402.13876v3.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2406.14862v4","updated":"2024-10-18T04:39:35Z","published":"2024-06-21T04:39:03Z","title":"LatentExplainer: Explaining Latent Representations in Deep Generative\n  Models with Multi-modal Foundation Models","summary":"  Deep generative models like VAEs and diffusion models have advanced various\ngeneration tasks by leveraging latent variables to learn data distributions and\ngenerate high-quality samples. Despite the field of explainable AI making\nstrides in interpreting machine learning models, understanding latent variables\nin generative models remains challenging. This paper introduces\n\\textit{LatentExplainer}, a framework for automatically generating semantically\nmeaningful explanations of latent variables in deep generative models.\n\\textit{LatentExplainer} tackles three main challenges: inferring the meaning\nof latent variables, aligning explanations with inductive biases, and handling\nvarying degrees of explainability. Our approach perturbs latent variables,\ninterpreting changes in generated data, and uses multi-modal large language\nmodels (MLLMs) to produce human-understandable explanations. We evaluate our\nproposed method on several real-world and synthetic datasets, and the results\ndemonstrate superior performance in generating high-quality explanations for\nlatent variables. The results highlight the effectiveness of incorporating\ninductive biases and uncertainty quantification, significantly enhancing model\ninterpretability.\n","authors":["Mengdan Zhu","Raasikh Kanjiani","Jiahui Lu","Andrew Choi","Qirui Ye","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.14862v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00902v2","updated":"2024-10-18T04:37:33Z","published":"2024-07-01T01:57:21Z","title":"From Introspection to Best Practices: Principled Analysis of\n  Demonstrations in Multimodal In-Context Learning","summary":"  Motivated by in-context learning (ICL) capabilities of Large Language models\n(LLMs), multimodal LLMs with additional visual modality are also exhibited with\nsimilar ICL abilities when multiple image-text pairs are provided as\ndemonstrations. However, relatively less work has been done to investigate the\nprinciples behind how and why multimodal ICL works. We conduct a systematic and\nprincipled evaluation of multimodal ICL for models of different scales on a\nbroad spectrum of new yet critical tasks. Through perturbations over different\nmodality information, we show that modalities matter differently across tasks\nin multimodal ICL. Guided by task-specific modality impact, we recommend\nmodality-driven demonstration strategies to boost ICL performance. We also find\nthat models may follow inductive biases from multimodal ICL even if they are\nrarely seen in or contradict semantic priors from pretraining data. Our\nprincipled analysis provides a comprehensive way of understanding the role of\ndemonstrations in multimodal in-context learning, and sheds light on\neffectively improving multimodal ICL on a wide range of tasks.\n","authors":["Nan Xu","Fei Wang","Sheng Zhang","Hoifung Poon","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2407.00902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12324v2","updated":"2024-10-18T04:32:34Z","published":"2024-10-16T07:44:56Z","title":"PAPL-SLAM: Principal Axis-Anchored Monocular Point-Line SLAM","summary":"  In point-line SLAM systems, the utilization of line structural information\nand the optimization of lines are two significant problems. The former is\nusually addressed through structural regularities, while the latter typically\ninvolves using minimal parameter representations of lines in optimization.\nHowever, separating these two steps leads to the loss of constraint information\nto each other. We anchor lines with similar directions to a principal axis and\noptimize them with $n+2$ parameters for $n$ lines, solving both problems\ntogether. Our method considers scene structural information, which can be\neasily extended to different world hypotheses while significantly reducing the\nnumber of line parameters to be optimized, enabling rapid and accurate mapping\nand tracking. To further enhance the system's robustness and avoid mismatch, we\nhave modeled the line-axis probabilistic data association and provided the\nalgorithm for axis creation, updating, and optimization. Additionally,\nconsidering that most real-world scenes conform to the Atlanta World\nhypothesis, we provide a structural line detection strategy based on vertical\npriors and vanishing points. Experimental results and ablation studies on\nvarious indoor and outdoor datasets demonstrate the effectiveness of our\nsystem.\n","authors":["Guanghao Li","Yu Cao","Qi Chen","Yifan Yang","Jian Pu"],"pdf_url":"https://arxiv.org/pdf/2410.12324v2.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.04733v2","updated":"2024-10-18T04:28:28Z","published":"2024-10-07T03:52:06Z","title":"PredFormer: Transformers Are Effective Spatial-Temporal Predictive\n  Learners","summary":"  Spatiotemporal predictive learning methods generally fall into two\ncategories: recurrent-based approaches, which face challenges in\nparallelization and performance, and recurrent-free methods, which employ\nconvolutional neural networks (CNNs) as encoder-decoder architectures. These\nmethods benefit from strong inductive biases but often at the expense of\nscalability and generalization. This paper proposes PredFormer, a pure\ntransformer-based framework for spatiotemporal predictive learning. Motivated\nby the Vision Transformers (ViT) design, PredFormer leverages carefully\ndesigned Gated Transformer blocks, following a comprehensive analysis of 3D\nattention mechanisms, including full-, factorized-, and\ninterleaved-spatial-temporal attention. With its recurrent-free,\ntransformer-based design, PredFormer is both simple and efficient,\nsignificantly outperforming previous methods by large margins. Extensive\nexperiments on synthetic and real-world datasets demonstrate that PredFormer\nachieves state-of-the-art performance. On Moving MNIST, PredFormer achieves a\n51.3% reduction in MSE relative to SimVP. For TaxiBJ, the model decreases MSE\nby 33.1% and boosts FPS from 533 to 2364. Additionally, on WeatherBench, it\nreduces MSE by 11.1% while enhancing FPS from 196 to 404. These performance\ngains in both accuracy and efficiency demonstrate PredFormer's potential for\nreal-world applications. The source code will be released at\nhttps://github.com/yyyujintang/PredFormer .\n","authors":["Yujin Tang","Lu Qi","Fei Xie","Xiangtai Li","Chao Ma","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2410.04733v2.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.04127v2","updated":"2024-10-18T04:23:00Z","published":"2024-07-04T19:00:34Z","title":"Biometric Authentication Based on Enhanced Remote Photoplethysmography\n  Signal Morphology","summary":"  Remote photoplethysmography (rPPG) is a non-contact method for measuring\ncardiac signals from facial videos, offering a convenient alternative to\ncontact photoplethysmography (cPPG) obtained from contact sensors. Recent\nstudies have shown that each individual possesses a unique cPPG signal\nmorphology that can be utilized as a biometric identifier, which has inspired\nus to utilize the morphology of rPPG signals extracted from facial videos for\nperson authentication. Since the facial appearance and rPPG are mixed in the\nfacial videos, we first de-identify facial videos to remove facial appearance\nwhile preserving the rPPG information, which protects facial privacy and\nguarantees that only rPPG is used for authentication. The de-identified videos\nare fed into an rPPG model to get the rPPG signal morphology for\nauthentication. In the first training stage, unsupervised rPPG training is\nperformed to get coarse rPPG signals. In the second training stage, an\nrPPG-cPPG hybrid training is performed by incorporating external cPPG datasets\nto achieve rPPG biometric authentication and enhance rPPG signal morphology.\nOur approach needs only de-identified facial videos with subject IDs to train\nrPPG authentication models. The experimental results demonstrate that rPPG\nsignal morphology hidden in facial videos can be used for biometric\nauthentication. The code is available at\nhttps://github.com/zhaodongsun/rppg_biometrics.\n","authors":["Zhaodong Sun","Xiaobai Li","Jukka Komulainen","Guoying Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.04127v2.pdf","comment":"accepted by IJCB 2024, Best Paper Runner-Up Award"},{"id":"http://arxiv.org/abs/2410.14169v1","updated":"2024-10-18T04:19:10Z","published":"2024-10-18T04:19:10Z","title":"DaRePlane: Direction-aware Representations for Dynamic Scene\n  Reconstruction","summary":"  Numerous recent approaches to modeling and re-rendering dynamic scenes\nleverage plane-based explicit representations, addressing slow training times\nassociated with models like neural radiance fields (NeRF) and Gaussian\nsplatting (GS). However, merely decomposing 4D dynamic scenes into multiple 2D\nplane-based representations is insufficient for high-fidelity re-rendering of\nscenes with complex motions. In response, we present DaRePlane, a novel\ndirection-aware representation approach that captures scene dynamics from six\ndifferent directions. This learned representation undergoes an inverse\ndual-tree complex wavelet transformation (DTCWT) to recover plane-based\ninformation. Within NeRF pipelines, DaRePlane computes features for each\nspace-time point by fusing vectors from these recovered planes, then passed to\na tiny MLP for color regression. When applied to Gaussian splatting, DaRePlane\ncomputes the features of Gaussian points, followed by a tiny multi-head MLP for\nspatial-time deformation prediction. Notably, to address redundancy introduced\nby the six real and six imaginary direction-aware wavelet coefficients, we\nintroduce a trainable masking approach, mitigating storage issues without\nsignificant performance decline. To demonstrate the generality and efficiency\nof DaRePlane, we test it on both regular and surgical dynamic scenes, for both\nNeRF and GS systems. Extensive experiments show that DaRePlane yields\nstate-of-the-art performance in novel view synthesis for various complex\ndynamic scenes.\n","authors":["Ange Lou","Benjamin Planche","Zhongpai Gao","Yamin Li","Tianyu Luan","Hao Ding","Meng Zheng","Terrence Chen","Ziyan Wu","Jack Noble"],"pdf_url":"https://arxiv.org/pdf/2410.14169v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2403.02265"},{"id":"http://arxiv.org/abs/2410.13726v2","updated":"2024-10-18T04:19:02Z","published":"2024-10-17T16:32:36Z","title":"DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework\n  for Talking Head Video Generation","summary":"  Talking head generation intends to produce vivid and realistic talking head\nvideos from a single portrait and speech audio clip. Although significant\nprogress has been made in diffusion-based talking head generation, almost all\nmethods rely on autoregressive strategies, which suffer from limited context\nutilization beyond the current generation step, error accumulation, and slower\ngeneration speed. To address these challenges, we present DAWN (Dynamic frame\nAvatar With Non-autoregressive diffusion), a framework that enables all-at-once\ngeneration of dynamic-length video sequences. Specifically, it consists of two\nmain components: (1) audio-driven holistic facial dynamics generation in the\nlatent motion space, and (2) audio-driven head pose and blink generation.\nExtensive experiments demonstrate that our method generates authentic and vivid\nvideos with precise lip motions, and natural pose/blink movements.\nAdditionally, with a high generation speed, DAWN possesses strong extrapolation\ncapabilities, ensuring the stable production of high-quality long videos. These\nresults highlight the considerable promise and potential impact of DAWN in the\nfield of talking head video generation. Furthermore, we hope that DAWN sparks\nfurther exploration of non-autoregressive approaches in diffusion models. Our\ncode will be publicly available at https://github.com/Hanbo-Cheng/DAWN-pytorch.\n","authors":["Hanbo Cheng","Limin Lin","Chenyu Liu","Pengcheng Xia","Pengfei Hu","Jiefeng Ma","Jun Du","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2410.13726v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14164v1","updated":"2024-10-18T04:04:58Z","published":"2024-10-18T04:04:58Z","title":"Optimal DLT-based Solutions for the Perspective-n-Point","summary":"  We propose a modified normalized direct linear transform (DLT) algorithm for\nsolving the perspective-n-point (PnP) problem with much better behavior than\nthe conventional DLT. The modification consists of analytically weighting the\ndifferent measurements in the linear system with a negligible increase in\ncomputational load. Our approach exhibits clear improvements -- in both\nperformance and runtime -- when compared to popular methods such as EPnP, CPnP,\nRPnP, and OPnP. Our new non-iterative solution approaches that of the true\noptimal found via Gauss-Newton optimization, but at a fraction of the\ncomputational cost. Our optimal DLT (oDLT) implementation, as well as the\nexperiments, are released in open source.\n","authors":["Sébastien Henry","John A. Christian"],"pdf_url":"https://arxiv.org/pdf/2410.14164v1.pdf","comment":"8 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.14161v1","updated":"2024-10-18T04:00:26Z","published":"2024-10-18T04:00:26Z","title":"Unlabeled Action Quality Assessment Based on Multi-dimensional Adaptive\n  Constrained Dynamic Time Warping","summary":"  The growing popularity of online sports and exercise necessitates effective\nmethods for evaluating the quality of online exercise executions. Previous\naction quality assessment methods, which relied on labeled scores from motion\nvideos, exhibited slightly lower accuracy and discriminability. This limitation\nhindered their rapid application to newly added exercises. To address this\nproblem, this paper presents an unlabeled Multi-Dimensional Exercise Distance\nAdaptive Constrained Dynamic Time Warping (MED-ACDTW) method for action quality\nassessment. Our approach uses an athletic version of DTW to compare features\nfrom template and test videos, eliminating the need for score labels during\ntraining. The result shows that utilizing both 2D and 3D spatial dimensions,\nalong with multiple human body features, improves the accuracy by 2-3% compared\nto using either 2D or 3D pose estimation alone. Additionally, employing MED for\nscore calculation enhances the precision of frame distance matching, which\nsignificantly boosts overall discriminability. The adaptive constraint scheme\nenhances the discriminability of action quality assessment by approximately\n30%. Furthermore, to address the absence of a standardized perspective in\nsports class evaluations, we introduce a new dataset called BGym.\n","authors":["Renguang Chen","Guolong Zheng","Xu Yang","Zhide Chen","Jiwu Shu","Wencheng Yang","Kexin Zhu","Chen Feng"],"pdf_url":"https://arxiv.org/pdf/2410.14161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14159v1","updated":"2024-10-18T03:58:29Z","published":"2024-10-18T03:58:29Z","title":"Assessing Open-world Forgetting in Generative Image Model Customization","summary":"  Recent advances in diffusion models have significantly enhanced image\ngeneration capabilities. However, customizing these models with new classes\noften leads to unintended consequences that compromise their reliability. We\nintroduce the concept of open-world forgetting to emphasize the vast scope of\nthese unintended alterations, contrasting it with the well-studied closed-world\nforgetting, which is measurable by evaluating performance on a limited set of\nclasses or skills. Our research presents the first comprehensive investigation\ninto open-world forgetting in diffusion models, focusing on semantic and\nappearance drift of representations. We utilize zero-shot classification to\nanalyze semantic drift, revealing that even minor model adaptations lead to\nunpredictable shifts affecting areas far beyond newly introduced concepts, with\ndramatic drops in zero-shot classification of up to 60%. Additionally, we\nobserve significant changes in texture and color of generated content when\nanalyzing appearance drift. To address these issues, we propose a mitigation\nstrategy based on functional regularization, designed to preserve original\ncapabilities while accommodating new concepts. Our study aims to raise\nawareness of unintended changes due to model customization and advocates for\nthe analysis of open-world forgetting in future research on model customization\nand finetuning methods. Furthermore, we provide insights for developing more\nrobust adaptation methodologies.\n","authors":["Héctor Laria","Alex Gomez-Villa","Imad Eddine Marouf","Kai Wang","Bogdan Raducanu","Joost van de Weijer"],"pdf_url":"https://arxiv.org/pdf/2410.14159v1.pdf","comment":"Project page: https://hecoding.github.io/open-world-forgetting/"},{"id":"http://arxiv.org/abs/2410.14148v1","updated":"2024-10-18T03:34:32Z","published":"2024-10-18T03:34:32Z","title":"Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in\n  Vision-Language Alignment","summary":"  The recent advancements in large language models (LLMs) and pre-trained\nvision models have accelerated the development of vision-language large models\n(VLLMs), enhancing the interaction between visual and linguistic modalities.\nDespite their notable success across various domains, VLLMs face challenges in\nmodality alignment, which can lead to issues like hallucinations and unsafe\ncontent generation. Current alignment techniques often rely on coarse feedback\nand external datasets, limiting scalability and performance. In this paper, we\npropose FiSAO (Fine-Grained Self-Alignment Optimization), a novel\nself-alignment method that utilizes the model's own visual encoder as a\nfine-grained verifier to improve vision-language alignment without the need for\nadditional data. By leveraging token-level feedback from the vision encoder,\nFiSAO significantly improves vision-language alignment, even surpassing\ntraditional preference tuning methods that require additional data. Through\nboth theoretical analysis and experimental validation, we demonstrate that\nFiSAO effectively addresses the misalignment problem in VLLMs, marking the\nfirst instance of token-level rewards being applied to such models.\n","authors":["Chenhang Cui","An Zhang","Yiyang Zhou","Zhaorun Chen","Gelei Deng","Huaxiu Yao","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.14148v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2410.14143v1","updated":"2024-10-18T03:31:00Z","published":"2024-10-18T03:31:00Z","title":"Preview-based Category Contrastive Learning for Knowledge Distillation","summary":"  Knowledge distillation is a mainstream algorithm in model compression by\ntransferring knowledge from the larger model (teacher) to the smaller model\n(student) to improve the performance of student. Despite many efforts, existing\nmethods mainly investigate the consistency between instance-level feature\nrepresentation or prediction, which neglects the category-level information and\nthe difficulty of each sample, leading to undesirable performance. To address\nthese issues, we propose a novel preview-based category contrastive learning\nmethod for knowledge distillation (PCKD). It first distills the structural\nknowledge of both instance-level feature correspondence and the relation\nbetween instance features and category centers in a contrastive learning\nfashion, which can explicitly optimize the category representation and explore\nthe distinct correlation between representations of instances and categories,\ncontributing to discriminative category centers and better classification\nresults. Besides, we introduce a novel preview strategy to dynamically\ndetermine how much the student should learn from each sample according to their\ndifficulty. Different from existing methods that treat all samples equally and\ncurriculum learning that simply filters out hard samples, our method assigns a\nsmall weight for hard instances as a preview to better guide the student\ntraining. Extensive experiments on several challenging datasets, including\nCIFAR-100 and ImageNet, demonstrate the superiority over state-of-the-art\nmethods.\n","authors":["Muhe Ding","Jianlong Wu","Xue Dong","Xiaojie Li","Pengda Qin","Tian Gan","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2410.14143v1.pdf","comment":"14 pages, 8 figures, Journal"},{"id":"http://arxiv.org/abs/2410.13674v2","updated":"2024-10-18T03:28:38Z","published":"2024-10-17T15:33:35Z","title":"Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning\n  via Image-Guided Diffusion","summary":"  Low-quality or scarce data has posed significant challenges for training deep\nneural networks in practice. While classical data augmentation cannot\ncontribute very different new data, diffusion models opens up a new door to\nbuild self-evolving AI by generating high-quality and diverse synthetic data\nthrough text-guided prompts. However, text-only guidance cannot control\nsynthetic images' proximity to the original images, resulting in\nout-of-distribution data detrimental to the model performance. To overcome the\nlimitation, we study image guidance to achieve a spectrum of interpolations\nbetween synthetic and real images. With stronger image guidance, the generated\nimages are similar to the training data but hard to learn. While with weaker\nimage guidance, the synthetic images will be easier for model but contribute to\na larger distribution gap with the original data. The generated full spectrum\nof data enables us to build a novel \"Diffusion Curriculum (DisCL)\". DisCL\nadjusts the image guidance level of image synthesis for each training stage: It\nidentifies and focuses on hard samples for the model and assesses the most\neffective guidance level of synthetic images to improve hard data learning. We\napply DisCL to two challenging tasks: long-tail (LT) classification and\nlearning from low-quality data. It focuses on lower-guidance images of\nhigh-quality to learn prototypical features as a warm-up of learning\nhigher-guidance images that might be weak on diversity or quality. Extensive\nexperiments showcase a gain of 2.7% and 2.1% in OOD and ID macro-accuracy when\napplying DisCL to iWildCam dataset. On ImageNet-LT, DisCL improves the base\nmodel's tail-class accuracy from 4.4% to 23.64% and leads to a 4.02%\nimprovement in all-class accuracy.\n","authors":["Yijun Liang","Shweta Bhardwaj","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.13674v2.pdf","comment":"23 pages, including references and appendix. Code is available at\n  http://github.com/tianyi-lab/DisCL"},{"id":"http://arxiv.org/abs/2410.02052v3","updated":"2024-10-18T03:27:37Z","published":"2024-10-02T21:42:35Z","title":"ExACT: Teaching AI Agents to Explore with Reflective-MCTS and\n  Exploratory Learning","summary":"  Autonomous agents have demonstrated significant potential in automating\ncomplex multistep decision-making tasks. However, even state-of-the-art\nvision-language models (VLMs), such as GPT-4o, still fall short of human-level\nperformance, particularly in intricate web environments and long-horizon tasks.\nTo address these limitations, we present ExACT, an approach to combine\ntest-time search and self-learning to build o1-like models for agentic\napplications. We first introduce Reflective Monte Carlo Tree Search (R-MCTS), a\nnovel test time algorithm designed to enhance AI agents' ability to explore\ndecision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating\ncontrastive reflection, allowing agents to learn from past interactions and\ndynamically improve their search efficiency; and 2) using multi-agent debate\nfor reliable state evaluation. Next, we introduce Exploratory Learning, a novel\nlearning strategy to teach agents to search at inference time without relying\non any external search algorithms. On the challenging VisualWebArena benchmark,\nour GPT-4o based R-MCTS agent achieves a 6% to 30% relative improvement across\nvarious tasks compared to the previous state-of-the-art. Additionally, we show\nthat the knowledge and experience gained from test-time search can be\neffectively transferred back to GPT-4o via fine-tuning. After Exploratory\nLearning, GPT-4o 1) demonstrates the ability to explore the environment,\nevaluate a state, and backtrack to viable ones when it detects that the current\nstate cannot lead to success, and 2) matches 87% of R-MCTS's performance while\nusing significantly less compute. Notably, our work demonstrates the compute\nscaling properties in both training - data collection with R-MCTS - and testing\ntime. These results suggest a promising research direction to enhance VLMs'\ncapabilities for agentic applications via test-time search and self-learning.\n","authors":["Xiao Yu","Baolin Peng","Vineeth Vajipey","Hao Cheng","Michel Galley","Jianfeng Gao","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2410.02052v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14138v1","updated":"2024-10-18T03:22:06Z","published":"2024-10-18T03:22:06Z","title":"ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and\n  Wisdom","summary":"  Large vision-language models (LVLMs) have witnessed significant progress on\nvisual understanding tasks. However, they often prioritize language knowledge\nover image information on visual reasoning tasks, incurring performance\ndegradation. To tackle this issue, we first identify the drawbacks of existing\nsolutions (i.e., insufficient and irrelevant visual descriptions, and limited\nmulti-modal capacities). We then decompose visual reasoning process into two\nstages: visual perception (i.e., eyesight) and textual reasoning (i.e.,\nwisdom), and introduce a novel visual reasoning framework named ProReason. This\nframework features multi-run proactive perception and decoupled\nvision-reasoning capabilities. Briefly, given a multi-modal question, ProReason\niterates proactive information collection and reasoning until the answer can be\nconcluded with necessary and sufficient visual descriptions. Notably, the\ndisassociation of capabilities allows seamless integration of existing large\nlanguage models (LLMs) to compensate for the reasoning deficits of LVLMs. Our\nextensive experiments demonstrate that ProReason outperforms both existing\nmulti-step reasoning frameworks and passive peer methods on a wide range of\nbenchmarks for both open-source and closed-source models. In addition, with the\nassistance of LLMs, ProReason achieves a performance improvement of up to 15%\non MMMU benchmark. Our insights into existing solutions and the decoupled\nperspective for feasible integration of LLMs illuminate future research on\nvisual reasoning techniques, especially LLM-assisted ones.\n","authors":["Jingqi Zhou","Sheng Wang","Jingwei Dong","Lei Li","Jiahui Gao","Lingpeng Kong","Chuan Wu"],"pdf_url":"https://arxiv.org/pdf/2410.14138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14132v1","updated":"2024-10-18T03:00:03Z","published":"2024-10-18T03:00:03Z","title":"ViConsFormer: Constituting Meaningful Phrases of Scene Texts using\n  Transformer-based Method in Vietnamese Text-based Visual Question Answering","summary":"  Text-based VQA is a challenging task that requires machines to use scene\ntexts in given images to yield the most appropriate answer for the given\nquestion. The main challenge of text-based VQA is exploiting the meaning and\ninformation from scene texts. Recent studies tackled this challenge by\nconsidering the spatial information of scene texts in images via embedding 2D\ncoordinates of their bounding boxes. In this study, we follow the definition of\nmeaning from linguistics to introduce a novel method that effectively exploits\nthe information from scene texts written in Vietnamese. Experimental results\nshow that our proposed method obtains state-of-the-art results on two\nlarge-scale Vietnamese Text-based VQA datasets. The implementation can be found\nat this link.\n","authors":["Nghia Hieu Nguyen","Tho Thanh Quan","Ngan Luu-Thuy Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.14132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14131v1","updated":"2024-10-18T02:57:14Z","published":"2024-10-18T02:57:14Z","title":"Deep Learning Applications in Medical Image Analysis: Advancements,\n  Challenges, and Future Directions","summary":"  Medical image analysis has emerged as an essential element of contemporary\nhealthcare, facilitating physicians in achieving expedited and precise\ndiagnosis. Recent breakthroughs in deep learning, a subset of artificial\nintelligence, have markedly revolutionized the analysis of medical pictures,\nimproving the accuracy and efficiency of clinical procedures. Deep learning\nalgorithms, especially convolutional neural networks (CNNs), have demonstrated\nremarkable proficiency in autonomously learning features from multidimensional\nmedical pictures, including MRI, CT, and X-ray scans, without the necessity for\nmanual feature extraction. These models have been utilized across multiple\nmedical disciplines, including pathology, radiology, ophthalmology, and\ncardiology, where they aid in illness detection, classification, and\nsegmentation tasks......\n","authors":["Aimina Ali Eli","Abida Ali"],"pdf_url":"https://arxiv.org/pdf/2410.14131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11656v2","updated":"2024-10-18T02:34:02Z","published":"2023-11-20T10:45:39Z","title":"Double-Condensing Attention Condenser: Leveraging Attention in Deep\n  Learning to Detect Skin Cancer from Skin Lesion Images","summary":"  Skin cancer is the most common type of cancer in the United States and is\nestimated to affect one in five Americans. Recent advances have demonstrated\nstrong performance on skin cancer detection, as exemplified by state of the art\nperformance in the SIIM-ISIC Melanoma Classification Challenge; however these\nsolutions leverage ensembles of complex deep neural architectures requiring\nimmense storage and compute costs, and therefore may not be tractable. A recent\nmovement for TinyML applications is integrating Double-Condensing Attention\nCondensers (DC-AC) into a self-attention neural network backbone architecture\nto allow for faster and more efficient computation. This paper explores\nleveraging an efficient self-attention structure to detect skin cancer in skin\nlesion images and introduces a deep neural network design with DC-AC customized\nfor skin cancer detection from skin lesion images. The final model is publicly\navailable as a part of a global open-source initiative dedicated to\naccelerating advancement in machine learning to aid clinicians in the fight\nagainst cancer. Future work of this research includes iterating on the design\nof the selected network architecture and refining the approach to generalize to\nother forms of cancer.\n","authors":["Chi-en Amy Tai","Elizabeth Janes","Chris Czarnecki","Alexander Wong"],"pdf_url":"https://arxiv.org/pdf/2311.11656v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05617v2","updated":"2024-10-18T02:15:51Z","published":"2024-08-10T19:31:21Z","title":"Residual-INR: Communication Efficient On-Device Learning Using Implicit\n  Neural Representation","summary":"  Edge computing is a distributed computing paradigm that collects and\nprocesses data at or near the source of data generation. The on-device learning\nat edge relies on device-to-device wireless communication to facilitate\nreal-time data sharing and collaborative decision-making among multiple\ndevices. This significantly improves the adaptability of the edge computing\nsystem to the changing environments. However, as the scale of the edge\ncomputing system is getting larger, communication among devices is becoming the\nbottleneck because of the limited bandwidth of wireless communication leads to\nlarge data transfer latency. To reduce the amount of device-to-device data\ntransmission and accelerate on-device learning, in this paper, we propose\nResidual-INR, a fog computing-based communication-efficient on-device learning\nframework by utilizing implicit neural representation (INR) to compress\nimages/videos into neural network weights. Residual-INR enhances data transfer\nefficiency by collecting JPEG images from edge devices, compressing them into\nINR format at the fog node, and redistributing them for on-device learning. By\nusing a smaller INR for full image encoding and a separate object INR for\nhigh-quality object region reconstruction through residual encoding, our\ntechnique can reduce the encoding redundancy while maintaining the object\nquality. Residual-INR is a promising solution for edge on-device learning\nbecause it reduces data transmission by up to 5.16 x across a network of 10\nedge devices. It also facilitates CPU-free accelerated on-device learning,\nachieving up to 2.9 x speedup without sacrificing accuracy. Our code is\navailable at: https://github.com/sharclab/Residual-INR.\n","authors":["Hanqiu Chen","Xuebin Yao","Pradeep Subedi","Cong Hao"],"pdf_url":"https://arxiv.org/pdf/2408.05617v2.pdf","comment":"This paper has been accepted by ICCAD 2024"},{"id":"http://arxiv.org/abs/2407.01003v3","updated":"2024-10-18T01:50:27Z","published":"2024-07-01T06:35:53Z","title":"Embedded Prompt Tuning: Towards Enhanced Calibration of Pretrained\n  Models for Medical Images","summary":"  Foundation models pre-trained on large-scale data have been widely witnessed\nto achieve success in various natural imaging downstream tasks.\nParameter-efficient fine-tuning (PEFT) methods aim to adapt foundation models\nto new domains by updating only a small portion of parameters in order to\nreduce computational overhead. However, the effectiveness of these PEFT\nmethods, especially in cross-domain few-shot scenarios, e.g., medical image\nanalysis, has not been fully explored. In this work, we facilitate the study of\nthe performance of PEFT when adapting foundation models to medical image\nclassification tasks. Furthermore, to alleviate the limitations of prompt\nintroducing ways and approximation capabilities on Transformer architectures of\nmainstream prompt tuning methods, we propose the Embedded Prompt Tuning (EPT)\nmethod by embedding prompt tokens into the expanded channels. We also find that\nthere are anomalies in the feature space distribution of foundation models\nduring pre-training process, and prompt tuning can help mitigate this negative\nimpact. To explain this phenomenon, we also introduce a novel perspective to\nunderstand prompt tuning: Prompt tuning is a distribution calibrator. And we\nsupport it by analyzing patch-wise scaling and feature separation operations\ncontained in EPT. Our experiments show that EPT outperforms several\nstate-of-the-art fine-tuning methods by a significant margin on few-shot\nmedical image classification tasks, and completes the fine-tuning process\nwithin highly competitive time, indicating EPT is an effective PEFT method. The\nsource code is available at github.com/zuwenqiang/EPT.\n","authors":["Wenqiang Zu","Shenghao Xie","Qing Zhao","Guoqi Li","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2407.01003v3.pdf","comment":"16 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:2306.09579, arXiv:2203.12119 by other authors"},{"id":"http://arxiv.org/abs/2402.10403v3","updated":"2024-10-18T01:44:05Z","published":"2024-02-16T02:01:24Z","title":"Polyhedral Complex Derivation from Piecewise Trilinear Networks","summary":"  Recent advancements in visualizing deep neural networks provide insights into\ntheir structures and mesh extraction from Continuous Piecewise Affine (CPWA)\nfunctions. Meanwhile, developments in neural surface representation learning\nincorporate non-linear positional encoding, addressing issues like spectral\nbias; however, this poses challenges in applying mesh extraction techniques\nbased on CPWA functions. Focusing on trilinear interpolating methods as\npositional encoding, we present theoretical insights and an analytical mesh\nextraction, showing the transformation of hypersurfaces to flat planes within\nthe trilinear region under the eikonal constraint. Moreover, we introduce a\nmethod for approximating intersecting points among three hypersurfaces\ncontributing to broader applications. We empirically validate correctness and\nparsimony through chamfer distance and efficiency, and angular distance, while\nexamining the correlation between the eikonal loss and the planarity of the\nhypersurfaces.\n","authors":["Jin-Hwa Kim"],"pdf_url":"https://arxiv.org/pdf/2402.10403v3.pdf","comment":"Accepted at NeurIPS 2024. Updated with the camera-ready version"},{"id":"http://arxiv.org/abs/2406.14878v2","updated":"2024-10-18T01:40:19Z","published":"2024-06-21T05:58:19Z","title":"MOS: Model Synergy for Test-Time Adaptation on LiDAR-Based 3D Object\n  Detection","summary":"  LiDAR-based 3D object detection is crucial for various applications but often\nexperiences performance degradation in real-world deployments due to domain\nshifts. While most studies focus on cross-dataset shifts, such as changes in\nenvironments and object geometries, practical corruptions from sensor\nvariations and weather conditions remain underexplored. In this work, we\npropose a novel online test-time adaptation framework for 3D detectors that\neffectively tackles these shifts, including a challenging cross-corruption\nscenario where cross-dataset shifts and corruptions co-occur. By leveraging\nlong-term knowledge from previous test batches, our approach mitigates\ncatastrophic forgetting and adapts effectively to diverse shifts. Specifically,\nwe propose a Model Synergy (MOS) strategy that dynamically selects historical\ncheckpoints with diverse knowledge and assembles them to best accommodate the\ncurrent test batch. This assembly is directed by our proposed Synergy Weights\n(SW), which perform a weighted averaging of the selected checkpoints,\nminimizing redundancy in the composite model. The SWs are computed by\nevaluating the similarity of predicted bounding boxes on the test data and the\nindependence of features between checkpoint pairs in the model bank. To\nmaintain an efficient and informative model bank, we discard checkpoints with\nthe lowest average SW scores, replacing them with newly updated models. Our\nmethod was rigorously tested against existing test-time adaptation strategies\nacross three datasets and eight types of corruptions, demonstrating superior\nadaptability to dynamic scenes and conditions. Notably, it achieved a 67.3%\nimprovement in a challenging cross-corruption scenario, offering a more\ncomprehensive benchmark for adaptation. The source code will be made publicly\navailable.\n","authors":["Zhuoxiao Chen","Junjie Meng","Mahsa Baktashmotlagh","Yonggang Zhang","Zi Huang","Yadan Luo"],"pdf_url":"https://arxiv.org/pdf/2406.14878v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14103v1","updated":"2024-10-18T00:50:56Z","published":"2024-10-18T00:50:56Z","title":"Extreme Precipitation Nowcasting using Multi-Task Latent Diffusion\n  Models","summary":"  Deep learning models have made remarkable strides in precipitation\nprediction, yet they continue to struggle with capturing the spatial details of\nthe features of radar images, particularly over high precipitation intensity\nareas. This shortcoming is evident in the form of low forecast accuracy in the\nspatial positioning of radar echo images across varying precipitation intensity\nregions. To address this challenge, we introduce the multi-task latent\ndiffusion model(MTLDM), a novel approach for precipitation prediction. The\nbasic concept of the MTLDM is based on the understanding that the radar image\nrepresenting precipitation is the result of multiple factors. Therefore, we\nadopt a divide-and-conquer approach, that is, we decompose the radar image\nusing decomposition technology and then predict the decomposed sub-images\nseparately. We conceptualize the precipitation image as a composition of\nvarious components corresponding to different precipitation intensities. The\nMTLDM decomposes the precipitation image into these distinct components and\nemploys a dedicated task to predict each one. This method enables\nspatiotemporally consistent prediction of real-world precipitation areas up to\n5-80 min in advance, outperforming existing state-of-the-art techniques across\nmultiple evaluation metrics.\n","authors":["Li Chaorong","Ling Xudong","Yang Qiang","Qin Fengqing","Huang Yuanyuan"],"pdf_url":"https://arxiv.org/pdf/2410.14103v1.pdf","comment":"12 pages, 6figures"},{"id":"http://arxiv.org/abs/2410.03302v3","updated":"2024-10-18T00:46:30Z","published":"2024-10-04T10:36:22Z","title":"Action Selection Learning for Multi-label Multi-view Action Recognition","summary":"  Multi-label multi-view action recognition aims to recognize multiple\nconcurrent or sequential actions from untrimmed videos captured by multiple\ncameras. Existing work has focused on multi-view action recognition in a narrow\narea with strong labels available, where the onset and offset of each action\nare labeled at the frame-level. This study focuses on real-world scenarios\nwhere cameras are distributed to capture a wide-range area with only weak\nlabels available at the video-level. We propose the method named Multi-view\nAction Selection Learning (MultiASL), which leverages action selection learning\nto enhance view fusion by selecting the most useful information from different\nviewpoints. The proposed method includes a Multi-view Spatial-Temporal\nTransformer video encoder to extract spatial and temporal features from\nmulti-viewpoint videos. Action Selection Learning is employed at the\nframe-level, using pseudo ground-truth obtained from weak labels at the\nvideo-level, to identify the most relevant frames for action recognition.\nExperiments in a real-world office environment using the MM-Office dataset\ndemonstrate the superior performance of the proposed method compared to\nexisting methods. The source code is available at\nhttps://github.com/thanhhff/MultiASL/.\n","authors":["Trung Thanh Nguyen","Yasutomo Kawanishi","Takahiro Komamizu","Ichiro Ide"],"pdf_url":"https://arxiv.org/pdf/2410.03302v3.pdf","comment":"ACM Multimedia Asia 2024"},{"id":"http://arxiv.org/abs/2410.14093v1","updated":"2024-10-18T00:18:27Z","published":"2024-10-18T00:18:27Z","title":"Enhancing In-vehicle Multiple Object Tracking Systems with Embeddable\n  Ising Machines","summary":"  A cognitive function of tracking multiple objects, needed in autonomous\nmobile vehicles, comprises object detection and their temporal association.\nWhile great progress owing to machine learning has been recently seen for\nelaborating the similarity matrix between the objects that have been recognized\nand the objects detected in a current video frame, less for the assignment\nproblem that finally determines the temporal association, which is a\ncombinatorial optimization problem. Here we show an in-vehicle multiple object\ntracking system with a flexible assignment function for tracking through\nmultiple long-term occlusion events. To solve the flexible assignment problem\nformulated as a nondeterministic polynomial time-hard problem, the system\nrelies on an embeddable Ising machine based on a quantum-inspired algorithm\ncalled simulated bifurcation. Using a vehicle-mountable computing platform, we\ndemonstrate a realtime system-wide throughput (23 frames per second on average)\nwith the enhanced functionality.\n","authors":["Kosuke Tatsumura","Yohei Hamakawa","Masaya Yamasaki","Koji Oya","Hiroshi Fujimoto"],"pdf_url":"https://arxiv.org/pdf/2410.14093v1.pdf","comment":"18 pages, 7 figures, 2 tables"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2408.08821v3","updated":"2024-10-18T17:50:57Z","published":"2024-08-16T16:09:59Z","title":"EasyRec: Simple yet Effective Language Models for Recommendation","summary":"  Deep neural networks have become a powerful technique for learning\nrepresentations from user-item interaction data in collaborative filtering (CF)\nfor recommender systems. However, many existing methods heavily rely on unique\nuser and item IDs, which limits their ability to perform well in practical\nzero-shot learning scenarios where sufficient training data may be unavailable.\nInspired by the success of language models (LMs) and their strong\ngeneralization capabilities, a crucial question arises: How can we harness the\npotential of language models to empower recommender systems and elevate its\ngeneralization capabilities to new heights? In this study, we propose EasyRec -\nan effective and easy-to-use approach that seamlessly integrates text-based\nsemantic understanding with collaborative signals. EasyRec employs a\ntext-behavior alignment framework, which combines contrastive learning with\ncollaborative language model tuning, to ensure a strong alignment between the\ntext-enhanced semantic space and the collaborative behavior information.\nExtensive empirical evaluations across diverse real-world datasets demonstrate\nthe superior performance of EasyRec compared to state-of-the-art alternative\nmodels, particularly in the challenging text-based zero-shot recommendation\nscenarios. Furthermore, the study highlights the potential of seamlessly\nintegrating EasyRec as a plug-and-play component into text-enhanced\ncollaborative filtering frameworks, thereby empowering existing recommender\nsystems to elevate their recommendation performance and adapt to the evolving\nuser preferences in dynamic environments. For better result reproducibility of\nour EasyRec framework, the model implementation details, source code, and\ndatasets are available at the link: https://github.com/HKUDS/EasyRec.\n","authors":["Xubin Ren","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2408.08821v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14629v1","updated":"2024-10-18T17:30:17Z","published":"2024-10-18T17:30:17Z","title":"SIMformer: Single-Layer Vanilla Transformer Can Learn Free-Space\n  Trajectory Similarity","summary":"  Free-space trajectory similarity calculation, e.g., DTW, Hausdorff, and\nFrechet, often incur quadratic time complexity, thus learning-based methods\nhave been proposed to accelerate the computation. The core idea is to train an\nencoder to transform trajectories into representation vectors and then compute\nvector similarity to approximate the ground truth. However, existing methods\nface dual challenges of effectiveness and efficiency: 1) they all utilize\nEuclidean distance to compute representation similarity, which leads to the\nsevere curse of dimensionality issue -- reducing the distinguishability among\nrepresentations and significantly affecting the accuracy of subsequent\nsimilarity search tasks; 2) most of them are trained in triplets manner and\noften necessitate additional information which downgrades the efficiency; 3)\nprevious studies, while emphasizing the scalability in terms of efficiency,\noverlooked the deterioration of effectiveness when the dataset size grows. To\ncope with these issues, we propose a simple, yet accurate, fast, scalable model\nthat only uses a single-layer vanilla transformer encoder as the feature\nextractor and employs tailored representation similarity functions to\napproximate various ground truth similarity measures. Extensive experiments\ndemonstrate our model significantly mitigates the curse of dimensionality issue\nand outperforms the state-of-the-arts in effectiveness, efficiency, and\nscalability.\n","authors":["Chuang Yang","Renhe Jiang","Xiaohang Xu","Chuan Xiao","Kaoru Sezaki"],"pdf_url":"https://arxiv.org/pdf/2410.14629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14625v1","updated":"2024-10-18T17:27:07Z","published":"2024-10-18T17:27:07Z","title":"Enhancing AI Accessibility in Veterinary Medicine: Linking Classifiers\n  and Electronic Health Records","summary":"  In the rapidly evolving landscape of veterinary healthcare, integrating\nmachine learning (ML) clinical decision-making tools with electronic health\nrecords (EHRs) promises to improve diagnostic accuracy and patient care.\nHowever, the seamless integration of ML classifiers into existing EHRs in\nveterinary medicine is frequently hindered by the rigidity of EHR systems or\nthe limited availability of IT resources. To address this shortcoming, we\npresent Anna, a freely-available software solution that provides ML classifier\nresults for EHR laboratory data in real-time.\n","authors":["Chun Yin Kong","Picasso Vasquez","Makan Farhoodimoghadam","Chris Brandt","Titus C. Brown","Krystle L. Reagan","Allison Zwingenberger","Stefan M. Keller"],"pdf_url":"https://arxiv.org/pdf/2410.14625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14609v1","updated":"2024-10-18T17:03:17Z","published":"2024-10-18T17:03:17Z","title":"DiSCo Meets LLMs: A Unified Approach for Sparse Retrieval and Contextual\n  Distillation in Conversational Search","summary":"  Conversational Search (CS) is the task of retrieving relevant documents from\na corpus within a conversational context, combining retrieval with\nconversational context modeling. With the explosion of Large Language Models\n(LLMs), the CS field has seen major improvements with LLMs rewriting user\nqueries, accounting for conversational context. However, engaging LLMs at\ninference time harms efficiency. Current methods address this by distilling\nembeddings from human-rewritten queries to learn the context modeling task.\nYet, these approaches predominantly focus on context modeling, and only treat\nthe contrastive component of the retrieval task within a\ndistillation-independent loss term. To address these limitations, we propose a\nnew distillation method, as a relaxation of the previous objective, unifying\nretrieval and context modeling. We relax the existing training objectives by\ndistilling similarity scores between conversations and documents, rather than\nrelying solely on representation learning. Our proposed distillation objective\nallows for more freedom in the representation space and leverages the\ncontrastive nature of document relevance. Through experiments on Learned Sparse\nRetrieval (LSR) across 5 CS datasets, our approach demonstrates substantial\nimprovements in both in-domain and out-of-domain retrieval performance,\noutperforming state-of-the-art with gains of up to 6 points in recall for\nout-of-domain datasets. Additionally, through the relaxation of the objective,\nwe propose a multi-teacher distillation, using multiple LLMs as teachers,\nyielding additional gains, and outperforming the teachers themselves in\nin-domain experiments. Finally, analysis of the sparsity of the models reveals\nthat our distillation allows for better control over the sparsity of the\ntrained models.\n","authors":["Simon Lupart","Mohammad Aliannejadi","Evangelos Kanoulas"],"pdf_url":"https://arxiv.org/pdf/2410.14609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14567v1","updated":"2024-10-18T16:11:29Z","published":"2024-10-18T16:11:29Z","title":"RAG-ConfusionQA: A Benchmark for Evaluating LLMs on Confusing Questions","summary":"  Conversational AI agents use Retrieval Augmented Generation (RAG) to provide\nverifiable document-grounded responses to user inquiries. However, many natural\nquestions do not have good answers: about 25\\% contain false\nassumptions~\\cite{Yu2023:CREPE}, and over 50\\% are\nambiguous~\\cite{Min2020:AmbigQA}. RAG agents need high-quality data to improve\ntheir responses to confusing questions. This paper presents a novel synthetic\ndata generation method to efficiently create a diverse set of context-grounded\nconfusing questions from a given document corpus. We conduct an empirical\ncomparative evaluation of several large language models as RAG agents to\nmeasure the accuracy of confusion detection and appropriate response\ngeneration. We contribute a benchmark dataset to the public domain.\n","authors":["Zhiyuan Peng","Jinming Nian","Alexandre Evfimievski","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2410.14567v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2407.14346v2","updated":"2024-10-18T13:59:54Z","published":"2024-07-19T14:28:53Z","title":"Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals","summary":"  Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.\n","authors":["Akash Kumar Mohankumar","Gururaj K","Gagan Madan","Amit Singh"],"pdf_url":"https://arxiv.org/pdf/2407.14346v2.pdf","comment":"Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure"},{"id":"http://arxiv.org/abs/2409.14217v2","updated":"2024-10-18T13:25:29Z","published":"2024-09-21T18:39:53Z","title":"Revisiting BPR: A Replicability Study of a Common Recommender System\n  Baseline","summary":"  Bayesian Personalized Ranking (BPR), a collaborative filtering approach based\non matrix factorization, frequently serves as a benchmark for recommender\nsystems research. However, numerous studies often overlook the nuances of BPR\nimplementation, claiming that it performs worse than newly proposed methods\nacross various tasks. In this paper, we thoroughly examine the features of the\nBPR model, indicating their impact on its performance, and investigate\nopen-source BPR implementations. Our analysis reveals inconsistencies between\nthese implementations and the original BPR paper, leading to a significant\ndecrease in performance of up to 50% for specific implementations. Furthermore,\nthrough extensive experiments on real-world datasets under modern evaluation\nsettings, we demonstrate that with proper tuning of its hyperparameters, the\nBPR model can achieve performance levels close to state-of-the-art methods on\nthe top-n recommendation tasks and even outperform them on specific datasets.\nSpecifically, on the Million Song Dataset, the BPR model with hyperparameters\ntuning statistically significantly outperforms Mult-VAE by 10% in NDCG@100 with\nbinary relevance function.\n","authors":["Aleksandr Milogradskii","Oleg Lashinin","Alexander P","Marina Ananyeva","Sergey Kolesnikov"],"pdf_url":"https://arxiv.org/pdf/2409.14217v2.pdf","comment":"This paper is accepted at the Reproducibility track of the ACM RecSys\n  '24 conference"},{"id":"http://arxiv.org/abs/2410.14452v1","updated":"2024-10-18T13:24:18Z","published":"2024-10-18T13:24:18Z","title":"SPFresh: Incremental In-Place Update for Billion-Scale Vector Search","summary":"  Approximate Nearest Neighbor Search (ANNS) is now widely used in various\napplications, ranging from information retrieval, question answering, and\nrecommendation, to search for similar high-dimensional vectors. As the amount\nof vector data grows continuously, it becomes important to support updates to\nvector index, the enabling technique that allows for efficient and accurate\nANNS on vectors. Because of the curse of high dimensionality, it is often\ncostly to identify the right neighbors of a single new vector, a necessary\nprocess for index update. To amortize update costs, existing systems maintain a\nsecondary index to accumulate updates, which are merged by the main index by\nglobal rebuilding the entire index periodically. However, this approach has\nhigh fluctuations of search latency and accuracy, not even to mention that it\nrequires substantial resources and is extremely time-consuming for rebuilds. We\nintroduce SPFresh, a system that supports in-place vector updates. At the heart\nof SPFresh is LIRE, a lightweight incremental rebalancing protocol to split\nvector partitions and reassign vectors in the nearby partitions to adapt to\ndata distribution shift. LIRE achieves low-overhead vector updates by only\nreassigning vectors at the boundary between partitions, where in a high-quality\nvector index the amount of such vectors are deemed small. With LIRE, SPFresh\nprovides superior query latency and accuracy to solutions based on global\nrebuild, with only 1% of DRAM and less than 10% cores needed at the peak\ncompared to the state-of-the-art, in a billion scale vector index with 1% of\ndaily vector update rate.\n","authors":["Yuming Xu","Hengyu Liang","Jin Li","Shuotao Xu","Qi Chen","Qianxi Zhang","Cheng Li","Ziyue Yang","Fan Yang","Yuqing Yang","Peng Cheng","Mao Yang"],"pdf_url":"https://arxiv.org/pdf/2410.14452v1.pdf","comment":"SOSP 23"},{"id":"http://arxiv.org/abs/2410.13428v2","updated":"2024-10-18T09:55:18Z","published":"2024-10-17T10:51:34Z","title":"Generate and Instantiate What You Prefer: Text-Guided Diffusion for\n  Sequential Recommendation","summary":"  Recent advancements in generative recommendation systems, particularly in the\nrealm of sequential recommendation tasks, have shown promise in enhancing\ngeneralization to new items. Among these approaches, diffusion-based generative\nrecommendation has emerged as an effective tool, leveraging its ability to\ncapture data distributions and generate high-quality samples. Despite\neffectiveness, two primary challenges have been identified: 1) the lack of\nconsistent modeling of data distribution for oracle items; and 2) the\ndifficulty in scaling to more informative control signals beyond historical\ninteractions. These issues stem from the uninformative nature of ID embeddings,\nwhich necessitate random initialization and limit the incorporation of\nadditional control signals. To address these limitations, we propose iDreamRec\nto involve more concrete prior knowledge to establish item embeddings,\nparticularly through detailed item text descriptions and advanced Text\nEmbedding Models (TEM). More importantly, by converting item descriptions into\nembeddings aligned with TEM, we enable the integration of intention\ninstructions as control signals to guide the generation of oracle items.\nExperimental results on four datasets demonstrate that iDreamRec not only\noutperforms existing diffusion-based generative recommenders but also\nfacilitates the incorporation of intention instructions for more precise and\neffective recommendation generation.\n","authors":["Guoqing Hu","Zhangyi Yang","Zhibo Cai","An Zhang","Xiang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14331v1","updated":"2024-10-18T09:43:30Z","published":"2024-10-18T09:43:30Z","title":"ChartifyText: Automated Chart Generation from Data-Involved Texts via\n  LLM","summary":"  Text documents with numerical values involved are widely used in various\napplications such as scientific research, economy, public health and\njournalism. However, it is difficult for readers to quickly interpret such\ndata-involved texts and gain deep insights. To fill this research gap, this\nwork aims to automatically generate charts to accurately convey the underlying\ndata and ideas to readers, which is essentially a challenging task. The\nchallenges originate from text ambiguities, intrinsic sparsity and uncertainty\nof data in text documents, and subjective sentiment differences. Specifically,\nwe propose ChartifyText, a novel fully-automated approach that leverages Large\nLanguage Models (LLMs) to convert complex data-involved texts to expressive\ncharts. It consists of two major modules: tabular data inference and expressive\nchart generation. The tabular data inference module employs systematic prompt\nengineering to guide the LLM (e.g., GPT-4) to infer table data, where data\nranges, uncertainties, missing data values and corresponding subjective\nsentiments are explicitly considered. The expressive chart generation module\naugments standard charts with intuitive visual encodings and concise texts to\naccurately convey the underlying data and insights. We extensively evaluate the\neffectiveness of ChartifyText on real-world data-involved text documents\nthrough case studies, in-depth interviews with three visualization experts, and\na carefully-designed user study with 15 participants. The results demonstrate\nthe usefulness and effectiveness of ChartifyText in helping readers efficiently\nand effectively make sense of data-involved texts.\n","authors":["Songheng Zhang","Lei Wang","Toby Jia-Jun Li","Qiaomu Shen","Yixin Cao","Yong Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14626v2","updated":"2024-10-18T08:56:18Z","published":"2023-10-23T07:00:51Z","title":"Conversational Recommender System and Large Language Model Are Made for\n  Each Other in E-commerce Pre-sales Dialogue","summary":"  E-commerce pre-sales dialogue aims to understand and elicit user needs and\npreferences for the items they are seeking so as to provide appropriate\nrecommendations. Conversational recommender systems (CRSs) learn user\nrepresentation and provide accurate recommendations based on dialogue context,\nbut rely on external knowledge. Large language models (LLMs) generate responses\nthat mimic pre-sales dialogues after fine-tuning, but lack domain-specific\nknowledge for accurate recommendations. Intuitively, the strengths of LLM and\nCRS in E-commerce pre-sales dialogues are complementary, yet no previous work\nhas explored this. This paper investigates the effectiveness of combining LLM\nand CRS in E-commerce pre-sales dialogues, proposing two collaboration methods:\nCRS assisting LLM and LLM assisting CRS. We conduct extensive experiments on a\nreal-world dataset of Ecommerce pre-sales dialogues. We analyze the impact of\ntwo collaborative approaches with two CRSs and two LLMs on four tasks of\nEcommerce pre-sales dialogue. We find that collaborations between CRS and LLM\ncan be very effective in some cases.\n","authors":["Yuanxing Liu","Wei-Nan Zhang","Yifan Chen","Yuchi Zhang","Haopeng Bai","Fan Feng","Hengbin Cui","Yongbin Li","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2310.14626v2.pdf","comment":"EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2410.13230v2","updated":"2024-10-18T08:36:37Z","published":"2024-10-17T05:33:50Z","title":"Starbucks: Improved Training for 2D Matryoshka Embeddings","summary":"  Effective approaches that can scale embedding model depth (i.e. layers) and\nembedding size allow for the creation of models that are highly scalable across\ndifferent computational resources and task requirements. While the recently\nproposed 2D Matryoshka training approach can efficiently produce a single\nembedding model such that its sub-layers and sub-dimensions can measure text\nsimilarity, its effectiveness is significantly worse than if smaller models\nwere trained separately. To address this issue, we propose Starbucks, a new\ntraining strategy for Matryoshka-like embedding models, which encompasses both\nthe fine-tuning and pre-training phases. For the fine-tuning phase, we discover\nthat, rather than sampling a random sub-layer and sub-dimensions for each\ntraining steps, providing a fixed list of layer-dimension pairs, from small\nsize to large sizes, and computing the loss across all pairs significantly\nimproves the effectiveness of 2D Matryoshka embedding models, bringing them on\npar with their separately trained counterparts. To further enhance performance,\nwe introduce a new pre-training strategy, which applies masked autoencoder\nlanguage modelling to sub-layers and sub-dimensions during pre-training,\nresulting in a stronger backbone for subsequent fine-tuning of the embedding\nmodel. Experimental results on both semantic text similarity and retrieval\nbenchmarks demonstrate that the proposed pre-training and fine-tuning\nstrategies significantly improved the effectiveness over 2D Matryoshka models,\nenabling Starbucks models to perform more efficiently and effectively than\nseparately trained models.\n","authors":["Shengyao Zhuang","Shuai Wang","Bevan Koopman","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2410.13230v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06572v2","updated":"2024-10-18T08:20:38Z","published":"2024-06-03T17:07:46Z","title":"Graph Neural Network Enhanced Retrieval for Question Answering of LLMs","summary":"  Retrieval augmented generation has revolutionized large language model (LLM)\noutputs by providing factual supports. Nevertheless, it struggles to capture\nall the necessary knowledge for complex reasoning questions. Existing retrieval\nmethods typically divide reference documents into passages, treating them in\nisolation. These passages, however, are often interrelated, such as passages\nthat are contiguous or share the same keywords. Therefore, it is crucial to\nrecognize such relatedness for enhancing the retrieval process. In this paper,\nwe propose a novel retrieval method, called GNN-Ret, which leverages graph\nneural networks (GNNs) to enhance retrieval by exploiting the relatedness\nbetween passages. Specifically, we first construct a graph of passages by\nconnecting passages that are structure-related or keyword-related. A graph\nneural network (GNN) is then leveraged to exploit the relationships between\npassages and improve the retrieval of supporting passages. Furthermore, we\nextend our method to handle multi-hop reasoning questions using a recurrent\ngraph neural network (RGNN), named RGNN-Ret. At each step, RGNN-Ret integrates\nthe graphs of passages from previous steps, thereby enhancing the retrieval of\nsupporting passages. Extensive experiments on benchmark datasets demonstrate\nthat GNN-Ret achieves higher accuracy for question answering with a single\nquery of LLMs than strong baselines that require multiple queries, and RGNN-Ret\nfurther improves accuracy and achieves state-of-the-art performance, with up to\n10.4% accuracy improvement on the 2WikiMQA dataset.\n","authors":["Zijian Li","Qingyan Guo","Jiawei Shao","Lei Song","Jiang Bian","Jun Zhang","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2406.06572v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.14241v1","updated":"2024-10-18T07:44:12Z","published":"2024-10-18T07:44:12Z","title":"Graph Neural Patching for Cold-Start Recommendations","summary":"  The cold start problem in recommender systems remains a critical challenge.\nCurrent solutions often train hybrid models on auxiliary data for both cold and\nwarm users/items, potentially degrading the experience for the latter. This\ndrawback limits their viability in practical scenarios where the satisfaction\nof existing warm users/items is paramount. Although graph neural networks\n(GNNs) excel at warm recommendations by effective collaborative signal\nmodeling, they haven't been effectively leveraged for the cold-start issue\nwithin a user-item graph, which is largely due to the lack of initial\nconnections for cold user/item entities. Addressing this requires a GNN adept\nat cold-start recommendations without sacrificing performance for existing\nones. To this end, we introduce Graph Neural Patching for Cold-Start\nRecommendations (GNP), a customized GNN framework with dual functionalities:\nGWarmer for modeling collaborative signal on existing warm users/items and\nPatching Networks for simulating and enhancing GWarmer's performance on\ncold-start recommendations. Extensive experiments on three benchmark datasets\nconfirm GNP's superiority in recommending both warm and cold users/items.\n","authors":["Hao Chen","Yu Yang","Yuanchen Bei","Zefan Wang","Yue Xu","Feiran Huang"],"pdf_url":"https://arxiv.org/pdf/2410.14241v1.pdf","comment":"13 pages, accepted by Australasian Database Conference 2024. arXiv\n  admin note: substantial text overlap with arXiv:2209.12215"},{"id":"http://arxiv.org/abs/2406.00012v2","updated":"2024-10-18T06:07:06Z","published":"2024-05-20T09:24:45Z","title":"FINED: Feed Instance-Wise Information Need with Essential and\n  Disentangled Parametric Knowledge from the Past","summary":"  Recommender models play a vital role in various industrial scenarios, while\noften faced with the catastrophic forgetting problem caused by the fast\nshifting data distribution. To alleviate this problem, a common approach is to\nreuse knowledge from the historical data. However, preserving the vast and\nfast-accumulating data is hard, which causes dramatic storage overhead.\nMemorizing old data through a parametric knowledge base is then proposed, which\ncompresses the vast amount of raw data into model parameters. Despite the\nflexibility, how to improve the memorization and generalization capabilities of\nthe parametric knowledge base and suit the flexible information need of each\ninstance are challenging. In this paper, we propose FINED to Feed INstance-wise\ninformation need with Essential and Disentangled parametric knowledge from past\ndata for recommendation enhancement. Concretely, we train a knowledge extractor\nthat extracts knowledge patterns of arbitrary order from past data and a\nknowledge encoder that memorizes the arbitrary order patterns, which serves as\nthe retrieval key generator and memory network respectively in the following\nknowledge reusing phase. The whole process is regularized by the proposed two\nconstraints, which improve the capabilities of the parametric knowledge base\nwithout increasing the size of it. The essential principle helps to compress\nthe input into representative vectors that capture the task-relevant\ninformation and filter out the noisy information. The disentanglement principle\nreduces the redundancy of stored information and pushes the knowledge base to\nfocus on capturing the disentangled invariant patterns. These two rules\ntogether promote rational compression of information for robust and generalized\nknowledge representations. Extensive experiments on two datasets justify the\neffectiveness of the proposed method.\n","authors":["Kounianhua Du","Jizheng Chen","Jianghao Lin","Menghui Zhu","Bo Chen","Shuai Li","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.00012v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14170v1","updated":"2024-10-18T04:20:46Z","published":"2024-10-18T04:20:46Z","title":"Personalized Image Generation with Large Multimodal Models","summary":"  Personalized content filtering, such as recommender systems, has become a\ncritical infrastructure to alleviate information overload. However, these\nsystems merely filter existing content and are constrained by its limited\ndiversity, making it difficult to meet users' varied content needs. To address\nthis limitation, personalized content generation has emerged as a promising\ndirection with broad applications. Nevertheless, most existing research focuses\non personalized text generation, with relatively little attention given to\npersonalized image generation. The limited work in personalized image\ngeneration faces challenges in accurately capturing users' visual preferences\nand needs from noisy user-interacted images and complex multimodal\ninstructions. Worse still, there is a lack of supervised data for training\npersonalized image generation models.\n  To overcome the challenges, we propose a Personalized Image Generation\nFramework named Pigeon, which adopts exceptional large multimodal models with\nthree dedicated modules to capture users' visual preferences and needs from\nnoisy user history and multimodal instructions. To alleviate the data scarcity,\nwe introduce a two-stage preference alignment scheme, comprising masked\npreference reconstruction and pairwise preference alignment, to align Pigeon\nwith the personalized image generation task. We apply Pigeon to personalized\nsticker and movie poster generation, where extensive quantitative results and\nhuman evaluation highlight its superiority over various generative baselines.\n","authors":["Yiyan Xu","Wenjie Wang","Yang Zhang","Tang Biao","Peng Yan","Fuli Feng","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2410.14170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14167v1","updated":"2024-10-18T04:17:49Z","published":"2024-10-18T04:17:49Z","title":"Optimizing Retrieval-Augmented Generation with Elasticsearch for\n  Enhanced Question-Answering Systems","summary":"  This study aims to improve the accuracy and quality of large-scale language\nmodels (LLMs) in answering questions by integrating Elasticsearch into the\nRetrieval Augmented Generation (RAG) framework. The experiment uses the\nStanford Question Answering Dataset (SQuAD) version 2.0 as the test dataset and\ncompares the performance of different retrieval methods, including traditional\nmethods based on keyword matching or semantic similarity calculation, BM25-RAG\nand TF-IDF- RAG, and the newly proposed ES-RAG scheme. The results show that\nES-RAG not only has obvious advantages in retrieval efficiency but also\nperforms well in key indicators such as accuracy, which is 0.51 percentage\npoints higher than TF-IDF-RAG. In addition, Elasticsearch's powerful search\ncapabilities and rich configuration options enable the entire\nquestion-answering system to better handle complex queries and provide more\nflexible and efficient responses based on the diverse needs of users. Future\nresearch directions can further explore how to optimize the interaction\nmechanism between Elasticsearch and LLM, such as introducing higher-level\nsemantic understanding and context-awareness capabilities, to achieve a more\nintelligent and humanized question-answering experience.\n","authors":["Jiajing Chen","Runyuan Bao","Hongye Zheng","Zhen Qi","Jianjun Wei","Jiacheng Hu"],"pdf_url":"https://arxiv.org/pdf/2410.14167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14122v1","updated":"2024-10-18T02:31:36Z","published":"2024-10-18T02:31:36Z","title":"Towards Robust Transcription: Exploring Noise Injection Strategies for\n  Training Data Augmentation","summary":"  Recent advancements in Automatic Piano Transcription (APT) have significantly\nimproved system performance, but the impact of noisy environments on the system\nperformance remains largely unexplored. This study investigates the impact of\nwhite noise at various Signal-to-Noise Ratio (SNR) levels on state-of-the-art\nAPT models and evaluates the performance of the Onsets and Frames model when\ntrained on noise-augmented data. We hope this research provides valuable\ninsights as preliminary work toward developing transcription models that\nmaintain consistent performance across a range of acoustic conditions.\n","authors":["Yonghyun Kim","Alexander Lerch"],"pdf_url":"https://arxiv.org/pdf/2410.14122v1.pdf","comment":"Accepted to the Late-Breaking Demo Session of the 25th International\n  Society for Music Information Retrieval (ISMIR) Conference, 2024"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2410.14673v1","updated":"2024-10-18T17:59:25Z","published":"2024-10-18T17:59:25Z","title":"Self-supervised contrastive learning performs non-linear system\n  identification","summary":"  Self-supervised learning (SSL) approaches have brought tremendous success\nacross many tasks and domains. It has been argued that these successes can be\nattributed to a link between SSL and identifiable representation learning:\nTemporal structure and auxiliary variables ensure that latent representations\nare related to the true underlying generative factors of the data. Here, we\ndeepen this connection and show that SSL can perform system identification in\nlatent space. We propose DynCL, a framework to uncover linear, switching linear\nand non-linear dynamics under a non-linear observation model, give theoretical\nguarantees and validate them empirically.\n","authors":["Rodrigo González Laiz","Tobias Schmidt","Steffen Schneider"],"pdf_url":"https://arxiv.org/pdf/2410.14673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14670v1","updated":"2024-10-18T17:58:53Z","published":"2024-10-18T17:58:53Z","title":"Decomposing The Dark Matter of Sparse Autoencoders","summary":"  Sparse autoencoders (SAEs) are a promising technique for decomposing language\nmodel activations into interpretable linear features. However, current SAEs\nfall short of completely explaining model performance, resulting in \"dark\nmatter\": unexplained variance in activations. This work investigates dark\nmatter as an object of study in its own right. Surprisingly, we find that much\nof SAE dark matter--about half of the error vector itself and >90% of its\nnorm--can be linearly predicted from the initial activation vector.\nAdditionally, we find that the scaling behavior of SAE error norms at a per\ntoken level is remarkably predictable: larger SAEs mostly struggle to\nreconstruct the same contexts as smaller SAEs. We build on the linear\nrepresentation hypothesis to propose models of activations that might lead to\nthese observations, including postulating a new type of \"introduced error\";\nthese insights imply that the part of the SAE error vector that cannot be\nlinearly predicted (\"nonlinear\" error) might be fundamentally different from\nthe linearly predictable component. To validate this hypothesis, we empirically\nanalyze nonlinear SAE error and show that 1) it contains fewer not yet learned\nfeatures, 2) SAEs trained on it are quantitatively worse, 3) it helps predict\nSAE per-token scaling behavior, and 4) it is responsible for a proportional\namount of the downstream increase in cross entropy loss when SAE activations\nare inserted into the model. Finally, we examine two methods to reduce\nnonlinear SAE error at a fixed sparsity: inference time gradient pursuit, which\nleads to a very slight decrease in nonlinear error, and linear transformations\nfrom earlier layer SAE outputs, which leads to a larger reduction.\n","authors":["Joshua Engels","Logan Riggs","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2410.14670v1.pdf","comment":"Code at https://github.com/JoshEngels/SAE-Dark-Matter"},{"id":"http://arxiv.org/abs/2410.14667v1","updated":"2024-10-18T17:57:01Z","published":"2024-10-18T17:57:01Z","title":"Stochastic Gradient Descent Jittering for Inverse Problems: Alleviating\n  the Accuracy-Robustness Tradeoff","summary":"  Inverse problems aim to reconstruct unseen data from corrupted or perturbed\nmeasurements. While most work focuses on improving reconstruction quality,\ngeneralization accuracy and robustness are equally important, especially for\nsafety-critical applications. Model-based architectures (MBAs), such as loop\nunrolling methods, are considered more interpretable and achieve better\nreconstructions. Empirical evidence suggests that MBAs are more robust to\nperturbations than black-box solvers, but the accuracy-robustness tradeoff in\nMBAs remains underexplored. In this work, we propose a simple yet effective\ntraining scheme for MBAs, called SGD jittering, which injects noise\niteration-wise during reconstruction. We theoretically demonstrate that SGD\njittering not only generalizes better than the standard mean squared error\ntraining but is also more robust to average-case attacks. We validate SGD\njittering using denoising toy examples, seismic deconvolution, and single-coil\nMRI reconstruction. The proposed method achieves cleaner reconstructions for\nout-of-distribution data and demonstrates enhanced robustness to adversarial\nattacks.\n","authors":["Peimeng Guan","Mark A. Davenport"],"pdf_url":"https://arxiv.org/pdf/2410.14667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14666v1","updated":"2024-10-18T17:56:11Z","published":"2024-10-18T17:56:11Z","title":"DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie\n  Character-Aware Discourse Graph","summary":"  Summarizing movie screenplays presents a unique set of challenges compared to\nstandard document summarization. Screenplays are not only lengthy, but also\nfeature a complex interplay of characters, dialogues, and scenes, with numerous\ndirect and subtle relationships and contextual nuances that are difficult for\nmachine learning models to accurately capture and comprehend. Recent attempts\nat screenplay summarization focus on fine-tuning transformer-based pre-trained\nmodels, but these models often fall short in capturing long-term dependencies\nand latent relationships, and frequently encounter the \"lost in the middle\"\nissue. To address these challenges, we introduce DiscoGraMS, a novel resource\nthat represents movie scripts as a movie character-aware discourse graph (CaD\nGraph). This approach is well-suited for various downstream tasks, such as\nsummarization, question-answering, and salience detection. The model aims to\npreserve all salient information, offering a more comprehensive and faithful\nrepresentation of the screenplay's content. We further explore a baseline\nmethod that combines the CaD Graph with the corresponding movie script through\na late fusion of graph and text modalities, and we present very initial\npromising results.\n","authors":["Maitreya Prafulla Chitale","Uday Bindal","Rajakrishnan Rajkumar","Rahul Mishra"],"pdf_url":"https://arxiv.org/pdf/2410.14666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14665v1","updated":"2024-10-18T17:55:15Z","published":"2024-10-18T17:55:15Z","title":"Online Reinforcement Learning with Passive Memory","summary":"  This paper considers an online reinforcement learning algorithm that\nleverages pre-collected data (passive memory) from the environment for online\ninteraction. We show that using passive memory improves performance and further\nprovide theoretical guarantees for regret that turns out to be near-minimax\noptimal. Results show that the quality of passive memory determines\nsub-optimality of the incurred regret. The proposed approach and results hold\nin both continuous and discrete state-action spaces.\n","authors":["Anay Pattanaik","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2410.14665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06331v2","updated":"2024-10-18T17:53:46Z","published":"2024-10-08T20:12:11Z","title":"Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing","summary":"  The locate-then-edit paradigm has shown significant promise for knowledge\nediting (KE) in Large Language Models (LLMs). While previous methods perform\nwell on single-hop fact recall tasks, they consistently struggle with multi-hop\nfactual recall tasks involving newly edited knowledge. In this paper,\nleveraging tools in mechanistic interpretability, we first identify that in\nmulti-hop tasks, LLMs tend to retrieve implicit subject knowledge from deeper\nMLP layers, unlike single-hop tasks, which rely on earlier layers. This\ndistinction explains the poor performance of current methods in multi-hop\nqueries, as they primarily focus on editing shallow layers, leaving deeper\nlayers unchanged. To address this, we propose IFMET, a novel locate-then-edit\nKE approach designed to edit both shallow and deep MLP layers. IFMET employs\nmulti-hop editing prompts and supplementary sets to locate and modify knowledge\nacross different reasoning stages. Experimental results demonstrate that IFMET\nsignificantly improves performance on multi-hop factual recall tasks,\neffectively overcoming the limitations of previous locate-then-edit methods.\n","authors":["Zhuoran Zhang","Yongxiang Li","Zijian Kan","Keyuan Cheng","Lijie Hu","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06331v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2410.14660v1","updated":"2024-10-18T17:51:51Z","published":"2024-10-18T17:51:51Z","title":"A Large Language Model-Driven Reward Design Framework via Dynamic\n  Feedback for Reinforcement Learning","summary":"  Large Language Models (LLMs) have shown significant potential in designing\nreward functions for Reinforcement Learning (RL) tasks. However, obtaining\nhigh-quality reward code often involves human intervention, numerous LLM\nqueries, or repetitive RL training. To address these issues, we propose CARD, a\nLLM-driven Reward Design framework that iteratively generates and improves\nreward function code. Specifically, CARD includes a Coder that generates and\nverifies the code, while a Evaluator provides dynamic feedback to guide the\nCoder in improving the code, eliminating the need for human feedback. In\naddition to process feedback and trajectory feedback, we introduce Trajectory\nPreference Evaluation (TPE), which evaluates the current reward function based\non trajectory preferences. If the code fails the TPE, the Evaluator provides\npreference feedback, avoiding RL training at every iteration and making the\nreward function better aligned with the task objective. Empirical results on\nMeta-World and ManiSkill2 demonstrate that our method achieves an effective\nbalance between task performance and token efficiency, outperforming or\nmatching the baselines across all tasks. On 10 out of 12 tasks, CARD shows\nbetter or comparable performance to policies trained with expert-designed\nrewards, and our method even surpasses the oracle on 3 tasks.\n","authors":["Shengjie Sun","Runze Liu","Jiafei Lyu","Jing-Wen Yang","Liangpeng Zhang","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2410.14660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14659v1","updated":"2024-10-18T17:51:37Z","published":"2024-10-18T17:51:37Z","title":"Harnessing Causality in Reinforcement Learning With Bagged Decision\n  Times","summary":"  We consider reinforcement learning (RL) for a class of problems with bagged\ndecision times. A bag contains a finite sequence of consecutive decision times.\nThe transition dynamics are non-Markovian and non-stationary within a bag.\nFurther, all actions within a bag jointly impact a single reward, observed at\nthe end of the bag. Our goal is to construct an online RL algorithm to maximize\nthe discounted sum of the bag-specific rewards. To handle non-Markovian\ntransitions within a bag, we utilize an expert-provided causal directed acyclic\ngraph (DAG). Based on the DAG, we construct the states as a dynamical Bayesian\nsufficient statistic of the observed history, which results in Markovian state\ntransitions within and across bags. We then frame this problem as a periodic\nMarkov decision process (MDP) that allows non-stationarity within a period. An\nonline RL algorithm based on Bellman-equations for stationary MDPs is\ngeneralized to handle periodic MDPs. To justify the proposed RL algorithm, we\nshow that our constructed state achieves the maximal optimal value function\namong all state constructions for a periodic MDP. Further we prove the Bellman\noptimality equations for periodic MDPs. We evaluate the proposed method on\ntestbed variants, constructed with real data from a mobile health clinical\ntrial.\n","authors":["Daiqi Gao","Hsin-Yu Lai","Predrag Klasnja","Susan A. Murphy"],"pdf_url":"https://arxiv.org/pdf/2410.14659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14655v1","updated":"2024-10-18T17:48:27Z","published":"2024-10-18T17:48:27Z","title":"Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated\n  Tokens","summary":"  Language models are often trained to maximize the likelihood of the next\ntoken given past tokens in the training dataset. However, during inference\ntime, they are utilized differently, generating text sequentially and\nauto-regressively by using previously generated tokens as input to predict the\nnext one. Marginal differences in predictions at each step can cascade over\nsuccessive steps, resulting in different distributions from what the models\nwere trained for and potentially leading to unpredictable behavior. This paper\nproposes two simple approaches based on model own generation to address this\ndiscrepancy between the training and inference time. Our first approach is\nBatch-Scheduled Sampling, where, during training, we stochastically choose\nbetween the ground-truth token from the dataset and the model's own generated\ntoken as input to predict the next token. This is done in an offline manner,\nmodifying the context window by interleaving ground-truth tokens with those\ngenerated by the model. Our second approach is Reference-Answer-based\nCorrection, where we explicitly incorporate a self-correction capability into\nthe model during training. This enables the model to effectively self-correct\nthe gaps between the generated sequences and the ground truth data without\nrelying on an external oracle model. By incorporating our proposed strategies\nduring training, we have observed an overall improvement in performance\ncompared to baseline methods, as demonstrated by our extensive experiments\nusing summarization, general question-answering, and math question-answering\ntasks.\n","authors":["Zhepeng Cen","Yao Liu","Siliang Zeng","Pratik Chaudhar","Huzefa Rangwala","George Karypis","Rasool Fakoor"],"pdf_url":"https://arxiv.org/pdf/2410.14655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14649v1","updated":"2024-10-18T17:46:37Z","published":"2024-10-18T17:46:37Z","title":"EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary\n  Search","summary":"  The high computational costs of large language models (LLMs) have led to a\nflurry of research on LLM compression, via methods such as quantization,\nsparsification, or structured pruning. A new frontier in this area is given by\n\\emph{dynamic, non-uniform} compression methods, which adjust the compression\nlevels (e.g., sparsity) per-block or even per-layer in order to minimize\naccuracy loss, while guaranteeing a global compression threshold. Yet, current\nmethods rely on heuristics for identifying the \"importance\" of a given layer\ntowards the loss, based on assumptions such as \\emph{error monotonicity}, i.e.\nthat the end-to-end model compression error is proportional to the sum of\nlayer-wise errors. In this paper, we revisit this area, and propose a new and\ngeneral approach for dynamic compression that is provably optimal in a given\ninput range. We begin from the motivating observation that, in general,\n\\emph{error monotonicity does not hold for LLMs}: compressed models with lower\nsum of per-layer errors can perform \\emph{worse} than models with higher error\nsums. To address this, we propose a new general evolutionary framework for\ndynamic LLM compression called EvoPress, which has provable convergence, and\nlow sample and evaluation complexity. We show that these theoretical guarantees\nlead to highly competitive practical performance for dynamic compression of\nLlama, Mistral and Phi models. Via EvoPress, we set new state-of-the-art\nresults across all compression approaches: structural pruning (block/layer\ndropping), unstructured sparsity, as well as quantization with dynamic\nbitwidths. Our code is available at https://github.com/IST-DASLab/EvoPress.\n","authors":["Oliver Sieberling","Denis Kuznedelev","Eldar Kurtic","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2410.14649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14640v1","updated":"2024-10-18T17:41:19Z","published":"2024-10-18T17:41:19Z","title":"HR-Bandit: Human-AI Collaborated Linear Recourse Bandit","summary":"  Human doctors frequently recommend actionable recourses that allow patients\nto modify their conditions to access more effective treatments. Inspired by\nsuch healthcare scenarios, we propose the Recourse Linear UCB\n($\\textsf{RLinUCB}$) algorithm, which optimizes both action selection and\nfeature modifications by balancing exploration and exploitation. We further\nextend this to the Human-AI Linear Recourse Bandit ($\\textsf{HR-Bandit}$),\nwhich integrates human expertise to enhance performance. $\\textsf{HR-Bandit}$\noffers three key guarantees: (i) a warm-start guarantee for improved initial\nperformance, (ii) a human-effort guarantee to minimize required human\ninteractions, and (iii) a robustness guarantee that ensures sublinear regret\neven when human decisions are suboptimal. Empirical results, including a\nhealthcare case study, validate its superior performance against existing\nbenchmarks.\n","authors":["Junyu Cao","Ruijiang Gao","Esmaeil Keyvanshokooh"],"pdf_url":"https://arxiv.org/pdf/2410.14640v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2410.14639v1","updated":"2024-10-18T17:40:58Z","published":"2024-10-18T17:40:58Z","title":"Convergence of Manifold Filter-Combine Networks","summary":"  In order to better understand manifold neural networks (MNNs), we introduce\nManifold Filter-Combine Networks (MFCNs). The filter-combine framework\nparallels the popular aggregate-combine paradigm for graph neural networks\n(GNNs) and naturally suggests many interesting families of MNNs which can be\ninterpreted as the manifold analog of various popular GNNs. We then propose a\nmethod for implementing MFCNs on high-dimensional point clouds that relies on\napproximating the manifold by a sparse graph. We prove that our method is\nconsistent in the sense that it converges to a continuum limit as the number of\ndata points tends to infinity.\n","authors":["David R. Johnson","Joyce Chew","Siddharth Viswanath","Edward De Brouwer","Deanna Needell","Smita Krishnaswamy","Michael Perlmutter"],"pdf_url":"https://arxiv.org/pdf/2410.14639v1.pdf","comment":"Accepted to NeurIPS Workshop on Symmetry and Geometry in Neural\n  Representations (Extended Abstract Track)"},{"id":"http://arxiv.org/abs/2410.14634v1","updated":"2024-10-18T17:35:33Z","published":"2024-10-18T17:35:33Z","title":"Parallel Backpropagation for Inverse of a Convolution with Application\n  to Normalizing Flows","summary":"  Inverse of an invertible convolution is an important operation that comes up\nin Normalizing Flows, Image Deblurring, etc. The naive algorithm for\nbackpropagation of this operation using Gaussian elimination has running time\n$O(n^3)$ where $n$ is the number of pixels in the image. We give a fast\nparallel backpropagation algorithm with running time $O(\\sqrt{n})$ for a square\nimage and provide a GPU implementation of the same. Inverse Convolutions are\nusually used in Normalizing Flows in the sampling pass, making them slow. We\npropose to use Inverse Convolutions in the forward (image to latent vector)\npass of the Normalizing flow. Since the sampling pass is the inverse of the\nforward pass, it will use convolutions only, resulting in efficient sampling\ntimes. We use our parallel backpropagation algorithm for optimizing the inverse\nconvolution layer resulting in fast training times also. We implement this\napproach in various Normalizing Flow backbones, resulting in our Inverse-Flow\nmodels. We benchmark Inverse-Flow on standard datasets and show significantly\nimproved sampling times with similar bits per dimension compared to previous\nmodels.\n","authors":["Sandeep Nagar","Girish Varma"],"pdf_url":"https://arxiv.org/pdf/2410.14634v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2109.09889v3","updated":"2024-10-18T17:32:27Z","published":"2021-09-21T00:09:03Z","title":"A Distance-based Anomaly Detection Framework for Deep Reinforcement\n  Learning","summary":"  In deep reinforcement learning (RL) systems, abnormal states pose significant\nrisks by potentially triggering unpredictable behaviors and unsafe actions,\nthus impeding the deployment of RL systems in real-world scenarios. It is\ncrucial for reliable decision-making systems to have the capability to cast an\nalert whenever they encounter unfamiliar observations that they are not\nequipped to handle. In this paper, we propose a novel Mahalanobis\ndistance-based (MD) anomaly detection framework, called \\textit{MDX}, for deep\nRL algorithms. MDX simultaneously addresses random, adversarial, and\nout-of-distribution (OOD) state outliers in both offline and online settings.\nIt utilizes Mahalanobis distance within class-conditional distributions for\neach action and operates within a statistical hypothesis testing framework\nunder the Gaussian assumption. We further extend it to robust and\ndistribution-free versions by incorporating Robust MD and conformal inference\ntechniques. Through extensive experiments on classical control environments,\nAtari games, and autonomous driving scenarios, we demonstrate the effectiveness\nof our MD-based detection framework. MDX offers a simple, unified, and\npractical anomaly detection tool for enhancing the safety and reliability of RL\nsystems in real-world applications.\n","authors":["Hongming Zhang","Ke Sun","Bo Xu","Linglong Kong","Martin Müller"],"pdf_url":"https://arxiv.org/pdf/2109.09889v3.pdf","comment":"19 pages, 21 figures"},{"id":"http://arxiv.org/abs/2410.14630v1","updated":"2024-10-18T17:30:20Z","published":"2024-10-18T17:30:20Z","title":"On the Regularization of Learnable Embeddings for Time Series Processing","summary":"  In processing multiple time series, accounting for the individual features of\neach sequence can be challenging. To address this, modern deep learning methods\nfor time series analysis combine a shared (global) model with local layers,\nspecific to each time series, often implemented as learnable embeddings.\nIdeally, these local embeddings should encode meaningful representations of the\nunique dynamics of each sequence. However, when these are learned end-to-end as\nparameters of a forecasting model, they may end up acting as mere sequence\nidentifiers. Shared processing blocks may then become reliant on such\nidentifiers, limiting their transferability to new contexts. In this paper, we\naddress this issue by investigating methods to regularize the learning of local\nlearnable embeddings for time series processing. Specifically, we perform the\nfirst extensive empirical study on the subject and show how such\nregularizations consistently improve performance in widely adopted\narchitectures. Furthermore, we show that methods preventing the co-adaptation\nof local and global parameters are particularly effective in this context. This\nhypothesis is validated by comparing several methods preventing the downstream\nmodels from relying on sequence identifiers, going as far as completely\nresetting the embeddings during training. The obtained results provide an\nimportant contribution to understanding the interplay between learnable local\nparameters and shared processing layers: a key challenge in modern time series\nprocessing models and a step toward developing effective foundation models for\ntime series.\n","authors":["Luca Butera","Giovanni De Felice","Andrea Cini","Cesare Alippi"],"pdf_url":"https://arxiv.org/pdf/2410.14630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14629v1","updated":"2024-10-18T17:30:17Z","published":"2024-10-18T17:30:17Z","title":"SIMformer: Single-Layer Vanilla Transformer Can Learn Free-Space\n  Trajectory Similarity","summary":"  Free-space trajectory similarity calculation, e.g., DTW, Hausdorff, and\nFrechet, often incur quadratic time complexity, thus learning-based methods\nhave been proposed to accelerate the computation. The core idea is to train an\nencoder to transform trajectories into representation vectors and then compute\nvector similarity to approximate the ground truth. However, existing methods\nface dual challenges of effectiveness and efficiency: 1) they all utilize\nEuclidean distance to compute representation similarity, which leads to the\nsevere curse of dimensionality issue -- reducing the distinguishability among\nrepresentations and significantly affecting the accuracy of subsequent\nsimilarity search tasks; 2) most of them are trained in triplets manner and\noften necessitate additional information which downgrades the efficiency; 3)\nprevious studies, while emphasizing the scalability in terms of efficiency,\noverlooked the deterioration of effectiveness when the dataset size grows. To\ncope with these issues, we propose a simple, yet accurate, fast, scalable model\nthat only uses a single-layer vanilla transformer encoder as the feature\nextractor and employs tailored representation similarity functions to\napproximate various ground truth similarity measures. Extensive experiments\ndemonstrate our model significantly mitigates the curse of dimensionality issue\nand outperforms the state-of-the-arts in effectiveness, efficiency, and\nscalability.\n","authors":["Chuang Yang","Renhe Jiang","Xiaohang Xu","Chuan Xiao","Kaoru Sezaki"],"pdf_url":"https://arxiv.org/pdf/2410.14629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14625v1","updated":"2024-10-18T17:27:07Z","published":"2024-10-18T17:27:07Z","title":"Enhancing AI Accessibility in Veterinary Medicine: Linking Classifiers\n  and Electronic Health Records","summary":"  In the rapidly evolving landscape of veterinary healthcare, integrating\nmachine learning (ML) clinical decision-making tools with electronic health\nrecords (EHRs) promises to improve diagnostic accuracy and patient care.\nHowever, the seamless integration of ML classifiers into existing EHRs in\nveterinary medicine is frequently hindered by the rigidity of EHR systems or\nthe limited availability of IT resources. To address this shortcoming, we\npresent Anna, a freely-available software solution that provides ML classifier\nresults for EHR laboratory data in real-time.\n","authors":["Chun Yin Kong","Picasso Vasquez","Makan Farhoodimoghadam","Chris Brandt","Titus C. Brown","Krystle L. Reagan","Allison Zwingenberger","Stefan M. Keller"],"pdf_url":"https://arxiv.org/pdf/2410.14625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14623v1","updated":"2024-10-18T17:22:38Z","published":"2024-10-18T17:22:38Z","title":"syren-new: Precise formulae for the linear and nonlinear matter power\n  spectra with massive neutrinos and dynamical dark energy","summary":"  Current and future large scale structure surveys aim to constrain the\nneutrino mass and the equation of state of dark energy. We aim to construct\naccurate and interpretable symbolic approximations to the linear and nonlinear\nmatter power spectra as a function of cosmological parameters in extended\n$\\Lambda$CDM models which contain massive neutrinos and non-constant equations\nof state for dark energy. This constitutes an extension of the syren-halofit\nemulators to incorporate these two effects, which we call syren-new\n(SYmbolic-Regression-ENhanced power spectrum emulator with NEutrinos and\n$W_0-w_a$). We also obtain a simple approximation to the derived parameter\n$\\sigma_8$ as a function of the cosmological parameters for these models. Our\nresults for the linear power spectrum are designed to emulate CLASS, whereas\nfor the nonlinear case we aim to match the results of EuclidEmulator2. We\ncompare our results to existing emulators and $N$-body simulations. Our\nanalytic emulators for $\\sigma_8$, the linear and nonlinear power spectra\nachieve root mean squared errors of 0.1%, 0.3% and 1.3%, respectively, across a\nwide range of cosmological parameters, redshifts and wavenumbers. We verify\nthat emulator-related discrepancies are subdominant compared to observational\nerrors and other modelling uncertainties when computing shear power spectra for\nLSST-like surveys. Our expressions have similar accuracy to existing\n(numerical) emulators, but are at least an order of magnitude faster, both on a\nCPU and GPU. Our work greatly improves the accuracy, speed and range of\napplicability of current symbolic approximations to the linear and nonlinear\nmatter power spectra. We provide publicly available code for all symbolic\napproximations found.\n","authors":["Ce Sui","Deaglan J. Bartlett","Shivam Pandey","Harry Desmond","Pedro G. Ferreira","Benjamin D. Wandelt"],"pdf_url":"https://arxiv.org/pdf/2410.14623v1.pdf","comment":"18 pages, 15 figures"},{"id":"http://arxiv.org/abs/2410.14621v1","updated":"2024-10-18T17:21:25Z","published":"2024-10-18T17:21:25Z","title":"JAMUN: Transferable Molecular Conformational Ensemble Generation with\n  Walk-Jump Sampling","summary":"  Conformational ensembles of protein structures are immensely important both\nto understanding protein function, and for drug discovery in novel modalities\nsuch as cryptic pockets. Current techniques for sampling ensembles are\ncomputationally inefficient, or do not transfer to systems outside their\ntraining data. We present walk-Jump Accelerated Molecular ensembles with\nUniversal Noise (JAMUN), a step towards the goal of efficiently sampling the\nBoltzmann distribution of arbitrary proteins. By extending Walk-Jump Sampling\nto point clouds, JAMUN enables ensemble generation at orders of magnitude\nfaster rates than traditional molecular dynamics or state-of-the-art ML\nmethods. Further, JAMUN is able to predict the stable basins of small peptides\nthat were not seen during training.\n","authors":["Ameya Daigavane","Bodhi P. Vani","Saeed Saremi","Joseph Kleinhenz","Joshua Rackers"],"pdf_url":"https://arxiv.org/pdf/2410.14621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10989v2","updated":"2024-10-18T17:21:17Z","published":"2024-10-14T18:17:01Z","title":"Liger Kernel: Efficient Triton Kernels for LLM Training","summary":"  Training Large Language Models (LLMs) efficiently at scale presents a\nformidable challenge, driven by their ever-increasing computational demands and\nthe need for enhanced performance. In this work, we introduce Liger-Kernel, an\nopen-sourced set of Triton kernels developed specifically for LLM training.\nWith kernel optimization techniques like kernel operation fusing and input\nchunking, our kernels achieve on average a 20% increase in training throughput\nand a 60% reduction in GPU memory usage for popular LLMs compared to\nHuggingFace implementations. In addition, Liger-Kernel is designed with\nmodularity, accessibility, and adaptability in mind, catering to both casual\nand expert users. Comprehensive benchmarks and integration tests are built in\nto ensure compatibility, performance, correctness, and convergence across\ndiverse computing environments and model architectures.\n  The source code is available under a permissive license at:\ngithub.com/linkedin/Liger-Kernel.\n","authors":["Pin-Lun Hsu","Yun Dai","Vignesh Kothapalli","Qingquan Song","Shao Tang","Siyu Zhu","Steven Shimizu","Shivam Sahni","Haowen Ning","Yanning Chen"],"pdf_url":"https://arxiv.org/pdf/2410.10989v2.pdf","comment":"17 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.10101v2","updated":"2024-10-18T17:15:09Z","published":"2024-10-14T02:41:01Z","title":"Learning Linear Attention in Polynomial Time","summary":"  Previous research has explored the computational expressivity of Transformer\nmodels in simulating Boolean circuits or Turing machines. However, the\nlearnability of these simulators from observational data has remained an open\nquestion. Our study addresses this gap by providing the first polynomial-time\nlearnability results (specifically strong, agnostic PAC learning) for\nsingle-layer Transformers with linear attention. We show that linear attention\nmay be viewed as a linear predictor in a suitably defined RKHS. As a\nconsequence, the problem of learning any linear transformer may be converted\ninto the problem of learning an ordinary linear predictor in an expanded\nfeature space, and any such predictor may be converted back into a multiheaded\nlinear transformer. Moving to generalization, we show how to efficiently\nidentify training datasets for which every empirical risk minimizer is\nequivalent (up to trivial symmetries) to the linear Transformer that generated\nthe data, thereby guaranteeing the learned model will correctly generalize\nacross all inputs. Finally, we provide examples of computations expressible via\nlinear attention and therefore polynomial-time learnable, including associative\nmemories, finite automata, and a class of Universal Turing Machine (UTMs) with\npolynomially bounded computation histories. We empirically validate our\ntheoretical findings on three tasks: learning random linear attention networks,\nkey--value associations, and learning to execute finite automata. Our findings\nbridge a critical gap between theoretical expressivity and learnability of\nTransformers, and show that flexible and general models of computation are\nefficiently learnable.\n","authors":["Morris Yau","Ekin Akyürek","Jiayuan Mao","Joshua B. Tenenbaum","Stefanie Jegelka","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2410.10101v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14616v1","updated":"2024-10-18T17:14:28Z","published":"2024-10-18T17:14:28Z","title":"Benchmarking Deep Reinforcement Learning for Navigation in Denied Sensor\n  Environments","summary":"  Deep Reinforcement learning (DRL) is used to enable autonomous navigation in\nunknown environments. Most research assume perfect sensor data, but real-world\nenvironments may contain natural and artificial sensor noise and denial. Here,\nwe present a benchmark of both well-used and emerging DRL algorithms in a\nnavigation task with configurable sensor denial effects. In particular, we are\ninterested in comparing how different DRL methods (e.g. model-free PPO vs.\nmodel-based DreamerV3) are affected by sensor denial. We show that DreamerV3\noutperforms other methods in the visual end-to-end navigation task with a\ndynamic goal - and other methods are not able to learn this. Furthermore,\nDreamerV3 generally outperforms other methods in sensor-denied environments. In\norder to improve robustness, we use adversarial training and demonstrate an\nimproved performance in denied environments, although this generally comes with\na performance cost on the vanilla environments. We anticipate this benchmark of\ndifferent DRL methods and the usage of adversarial training to be a starting\npoint for the development of more elaborate navigation strategies that are\ncapable of dealing with uncertain and denied sensor readings.\n","authors":["Mariusz Wisniewski","Paraskevas Chatzithanos","Weisi Guo","Antonios Tsourdos"],"pdf_url":"https://arxiv.org/pdf/2410.14616v1.pdf","comment":"31 pages, 19 figures. For associated code, see\n  https://github.com/mazqtpopx/cranfield-navigation-gym"},{"id":"http://arxiv.org/abs/2410.14615v1","updated":"2024-10-18T17:13:29Z","published":"2024-10-18T17:13:29Z","title":"Asymptotically Optimal Change Detection for Unnormalized Pre- and\n  Post-Change Distributions","summary":"  This paper addresses the problem of detecting changes when only unnormalized\npre- and post-change distributions are accessible. This situation happens in\nmany scenarios in physics such as in ferromagnetism, crystallography,\nmagneto-hydrodynamics, and thermodynamics, where the energy models are\ndifficult to normalize.\n  Our approach is based on the estimation of the Cumulative Sum (CUSUM)\nstatistics, which is known to produce optimal performance. We first present an\nintuitively appealing approximation method. Unfortunately, this produces a\nbiased estimator of the CUSUM statistics and may cause performance degradation.\nWe then propose the Log-Partition Approximation Cumulative Sum (LPA-CUSUM)\nalgorithm based on thermodynamic integration (TI) in order to estimate the\nlog-ratio of normalizing constants of pre- and post-change distributions. It is\nproved that this approach gives an unbiased estimate of the log-partition\nfunction and the CUSUM statistics, and leads to an asymptotically optimal\nperformance. Moreover, we derive a relationship between the required sample\nsize for thermodynamic integration and the desired detection delay performance,\noffering guidelines for practical parameter selection. Numerical studies are\nprovided demonstrating the efficacy of our approach.\n","authors":["Arman Adibi","Sanjeev Kulkarni","H. Vincent Poor","Taposh Banerjee","Vahid Tarokh"],"pdf_url":"https://arxiv.org/pdf/2410.14615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06402v2","updated":"2024-10-18T17:10:05Z","published":"2024-03-11T03:28:13Z","title":"One size doesn't fit all: Predicting the Number of Examples for\n  In-Context Learning","summary":"  In-context learning (ICL) refers to the process of adding a small number of\nlocalized examples (ones that are semantically similar to the input) from a\ntraining set of labelled data to an LLM's prompt with an objective to\neffectively control the generative process seeking to improve the downstream\ntask performance. Existing ICL approaches use an identical number of examples\n(a pre-configured hyper-parameter) for each data instance. Our work alleviates\nthe limitations of this 'one fits all' approach by dynamically predicting the\nnumber of examples for each data instance to be used in few-shot inference with\nLLMs. In particular, we employ a multi-label classifier, the parameters of\nwhich are fitted using a training set, where the label for each instance in the\ntraining set indicates if using a specific value of k (number of most similar\nexamples from 0 up to a maximum value) leads to correct k-shot downstream\npredictions. Our experiments on a number of text classification benchmarks show\nthat AICL substantially outperforms standard ICL by up to 17%.\n","authors":["Manish Chandra","Debasis Ganguly","Iadh Ounis"],"pdf_url":"https://arxiv.org/pdf/2403.06402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.20601v2","updated":"2024-10-18T17:07:01Z","published":"2023-10-31T16:37:01Z","title":"Modular Boundaries in Recurrent Neural Networks","summary":"  Recent theoretical and experimental work in neuroscience has focused on the\nrepresentational and dynamical character of neural manifolds --subspaces in\nneural activity space wherein many neurons coactivate. Importantly, neural\npopulations studied under this \"neural manifold hypothesis\" are continuous and\nnot cleanly divided into separate neural populations. This perspective clashes\nwith the \"modular hypothesis\" of brain organization, wherein neural elements\nmaintain an \"all-or-nothing\" affiliation with modules. In line with this\nmodular hypothesis, recent research on recurrent neural networks suggests that\nmulti-task networks become modular across training, such that different modules\nspecialize for task-general dynamical motifs. If the modular hypothesis is\ntrue, then it would be important to use a dimensionality reduction technique\nthat captures modular structure. Here, we investigate the features of such a\nmethod. We leverage RNNs as a model system to study the character of modular\nneural populations, using a community detection method from network science\nknown as modularity maximization to partition neurons into distinct modules.\nThese partitions allow us to ask the following question: do these modular\nboundaries matter to the system? ...\n","authors":["Jacob Tanner","Sina Mansour L.","Ludovico Coletta","Alessandro Gozzi","Richard F. Betzel"],"pdf_url":"https://arxiv.org/pdf/2310.20601v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14606v1","updated":"2024-10-18T17:00:29Z","published":"2024-10-18T17:00:29Z","title":"Streaming Deep Reinforcement Learning Finally Works","summary":"  Natural intelligence processes experience as a continuous stream, sensing,\nacting, and learning moment-by-moment in real time. Streaming learning, the\nmodus operandi of classic reinforcement learning (RL) algorithms like\nQ-learning and TD, mimics natural learning by using the most recent sample\nwithout storing it. This approach is also ideal for resource-constrained,\ncommunication-limited, and privacy-sensitive applications. However, in deep RL,\nlearners almost always use batch updates and replay buffers, making them\ncomputationally expensive and incompatible with streaming learning. Although\nthe prevalence of batch deep RL is often attributed to its sample efficiency, a\nmore critical reason for the absence of streaming deep RL is its frequent\ninstability and failure to learn, which we refer to as stream barrier. This\npaper introduces the stream-x algorithms, the first class of deep RL algorithms\nto overcome stream barrier for both prediction and control and match sample\nefficiency of batch RL. Through experiments in Mujoco Gym, DM Control Suite,\nand Atari Games, we demonstrate stream barrier in existing algorithms and\nsuccessful stable learning with our stream-x algorithms: stream Q, stream AC,\nand stream TD, achieving the best model-free performance in DM Control Dog\nenvironments. A set of common techniques underlies the stream-x algorithms,\nenabling their success with a single set of hyperparameters and allowing for\neasy extension to other algorithms, thereby reviving streaming RL.\n","authors":["Mohamed Elsayed","Gautham Vasan","A. Rupam Mahmood"],"pdf_url":"https://arxiv.org/pdf/2410.14606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14604v1","updated":"2024-10-18T16:57:27Z","published":"2024-10-18T16:57:27Z","title":"Learning to Control the Smoothness of Graph Convolutional Network\n  Features","summary":"  The pioneering work of Oono and Suzuki [ICLR, 2020] and Cai and Wang\n[arXiv:2006.13318] initializes the analysis of the smoothness of graph\nconvolutional network (GCN) features. Their results reveal an intricate\nempirical correlation between node classification accuracy and the ratio of\nsmooth to non-smooth feature components. However, the optimal ratio that favors\nnode classification is unknown, and the non-smooth features of deep GCN with\nReLU or leaky ReLU activation function diminish. In this paper, we propose a\nnew strategy to let GCN learn node features with a desired smoothness --\nadapting to data and tasks -- to enhance node classification. Our approach has\nthree key steps: (1) We establish a geometric relationship between the input\nand output of ReLU or leaky ReLU. (2) Building on our geometric insights, we\naugment the message-passing process of graph convolutional layers (GCLs) with a\nlearnable term to modulate the smoothness of node features with computational\nefficiency. (3) We investigate the achievable ratio between smooth and\nnon-smooth feature components for GCNs with the augmented message-passing\nscheme. Our extensive numerical results show that the augmented message-passing\nschemes significantly improve node classification for GCN and some related\nmodels.\n","authors":["Shih-Hsin Wang","Justin Baker","Cory Hauck","Bao Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14604v1.pdf","comment":"48 pages"},{"id":"http://arxiv.org/abs/2410.14602v1","updated":"2024-10-18T16:57:05Z","published":"2024-10-18T16:57:05Z","title":"How Does Data Diversity Shape the Weight Landscape of Neural Networks?","summary":"  To enhance the generalization of machine learning models to unseen data,\ntechniques such as dropout, weight decay ($L_2$ regularization), and noise\naugmentation are commonly employed. While regularization methods (i.e., dropout\nand weight decay) are geared toward adjusting model parameters to prevent\noverfitting, data augmentation increases the diversity of the input training\nset, a method purported to improve accuracy and calibration error. In this\npaper, we investigate the impact of each of these techniques on the parameter\nspace of neural networks, with the goal of understanding how they alter the\nweight landscape in transfer learning scenarios. To accomplish this, we employ\nRandom Matrix Theory to analyze the eigenvalue distributions of pre-trained\nmodels, fine-tuned using these techniques but using different levels of data\ndiversity, for the same downstream tasks. We observe that diverse data\ninfluences the weight landscape in a similar fashion as dropout. Additionally,\nwe compare commonly used data augmentation methods with synthetic data created\nby generative models. We conclude that synthetic data can bring more diversity\ninto real input data, resulting in a better performance on out-of-distribution\ntest instances.\n","authors":["Yang Ba","Michelle V. Mancenido","Rong Pan"],"pdf_url":"https://arxiv.org/pdf/2410.14602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09639v2","updated":"2024-10-18T16:50:56Z","published":"2024-06-14T00:08:04Z","title":"TGB 2.0: A Benchmark for Learning on Temporal Knowledge Graphs and\n  Heterogeneous Graphs","summary":"  Multi-relational temporal graphs are powerful tools for modeling real-world\ndata, capturing the evolving and interconnected nature of entities over time.\nRecently, many novel models are proposed for ML on such graphs intensifying the\nneed for robust evaluation and standardized benchmark datasets. However, the\navailability of such resources remains scarce and evaluation faces added\ncomplexity due to reproducibility issues in experimental protocols. To address\nthese challenges, we introduce Temporal Graph Benchmark 2.0 (TGB 2.0), a novel\nbenchmarking framework tailored for evaluating methods for predicting future\nlinks on Temporal Knowledge Graphs and Temporal Heterogeneous Graphs with a\nfocus on large-scale datasets, extending the Temporal Graph Benchmark. TGB 2.0\nfacilitates comprehensive evaluations by presenting eight novel datasets\nspanning five domains with up to 53 million edges. TGB 2.0 datasets are\nsignificantly larger than existing datasets in terms of number of nodes, edges,\nor timestamps. In addition, TGB 2.0 provides a reproducible and realistic\nevaluation pipeline for multi-relational temporal graphs. Through extensive\nexperimentation, we observe that 1) leveraging edge-type information is crucial\nto obtain high performance, 2) simple heuristic baselines are often competitive\nwith more complex methods, 3) most methods fail to run on our largest datasets,\nhighlighting the need for research on more scalable methods.\n","authors":["Julia Gastinger","Shenyang Huang","Mikhail Galkin","Erfan Loghmani","Ali Parviz","Farimah Poursafaei","Jacob Danovitch","Emanuele Rossi","Ioannis Koutis","Heiner Stuckenschmidt","Reihaneh Rabbany","Guillaume Rabusseau"],"pdf_url":"https://arxiv.org/pdf/2406.09639v2.pdf","comment":"29 pages, 8 figures, 11 tables, accepted at NeurIPS 2024 Track on\n  Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2410.14592v1","updated":"2024-10-18T16:43:10Z","published":"2024-10-18T16:43:10Z","title":"Contractivity and linear convergence in bilinear saddle-point problems:\n  An operator-theoretic approach","summary":"  We study the convex-concave bilinear saddle-point problem $\\min_x \\max_y f(x)\n+ y^\\top Ax - g(y)$, where both, only one, or none of the functions $f$ and $g$\nare strongly convex, and suitable rank conditions on the matrix $A$ hold. The\nsolution of this problem is at the core of many machine learning tasks. By\nemploying tools from operator theory, we systematically prove the contractivity\n(in turn, the linear convergence) of several first-order primal-dual\nalgorithms, including the Chambolle-Pock method. Our approach results in\nconcise and elegant proofs, and it yields new convergence guarantees and\ntighter bounds compared to known results.\n","authors":["Colin Dirren","Mattia Bianchi","Panagiotis D. Grontas","John Lygeros","Florian Dörfler"],"pdf_url":"https://arxiv.org/pdf/2410.14592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14591v1","updated":"2024-10-18T16:41:37Z","published":"2024-10-18T16:41:37Z","title":"A Lipschitz spaces view of infinitely wide shallow neural networks","summary":"  We revisit the mean field parametrization of shallow neural networks, using\nsigned measures on unbounded parameter spaces and duality pairings that take\ninto account the regularity and growth of activation functions. This setting\ndirectly leads to the use of unbalanced Kantorovich-Rubinstein norms defined by\nduality with Lipschitz functions, and of spaces of measures dual to those of\ncontinuous functions with controlled growth. These allow to make transparent\nthe need for total variation and moment bounds or penalization to obtain\nexistence of minimizers of variational formulations, under which we prove a\ncompactness result in strong Kantorovich-Rubinstein norm, and in the absence of\nwhich we show several examples demonstrating undesirable behavior. Further, the\nKantorovich-Rubinstein setting enables us to combine the advantages of a\ncompletely linear parametrization and ensuing reproducing kernel Banach space\nframework with optimal transport insights. We showcase this synergy with\nrepresenter theorems and uniform large data limits for empirical risk\nminimization, and in proposed formulations for distillation and fusion\napplications.\n","authors":["Francesca Bartolucci","Marcello Carioni","José A. Iglesias","Yury Korolev","Emanuele Naldi","Stefano Vigogna"],"pdf_url":"https://arxiv.org/pdf/2410.14591v1.pdf","comment":"39 pages, 1 table"},{"id":"http://arxiv.org/abs/2410.14588v1","updated":"2024-10-18T16:38:55Z","published":"2024-10-18T16:38:55Z","title":"Learning With Multi-Group Guarantees For Clusterable Subpopulations","summary":"  A canonical desideratum for prediction problems is that performance\nguarantees should hold not just on average over the population, but also for\nmeaningful subpopulations within the overall population. But what constitutes a\nmeaningful subpopulation? In this work, we take the perspective that relevant\nsubpopulations should be defined with respect to the clusters that naturally\nemerge from the distribution of individuals for which predictions are being\nmade. In this view, a population refers to a mixture model whose components\nconstitute the relevant subpopulations. We suggest two formalisms for capturing\nper-subgroup guarantees: first, by attributing each individual to the component\nfrom which they were most likely drawn, given their features; and second, by\nattributing each individual to all components in proportion to their relative\nlikelihood of having been drawn from each component. Using online calibration\nas a case study, we study a \\variational algorithm that provides guarantees for\neach of these formalisms by handling all plausible underlying subpopulation\nstructures simultaneously, and achieve an $O(T^{1/2})$ rate even when the\nsubpopulations are not well-separated. In comparison, the more natural\ncluster-then-predict approach that first recovers the structure of the\nsubpopulations and then makes predictions suffers from a $O(T^{2/3})$ rate and\nrequires the subpopulations to be separable. Along the way, we prove that\nproviding per-subgroup calibration guarantees for underlying clusters can be\neasier than learning the clusters: separation between median subgroup features\nis required for the latter but not the former.\n","authors":["Jessica Dai","Nika Haghtalab","Eric Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.14588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14587v1","updated":"2024-10-18T16:37:52Z","published":"2024-10-18T16:37:52Z","title":"Neuro-Symbolic Traders: Assessing the Wisdom of AI Crowds in Markets","summary":"  Deep generative models are becoming increasingly used as tools for financial\nanalysis. However, it is unclear how these models will influence financial\nmarkets, especially when they infer financial value in a semi-autonomous way.\nIn this work, we explore the interplay between deep generative models and\nmarket dynamics. We develop a form of virtual traders that use deep generative\nmodels to make buy/sell decisions, which we term neuro-symbolic traders, and\nexpose them to a virtual market. Under our framework, neuro-symbolic traders\nare agents that use vision-language models to discover a model of the\nfundamental value of an asset. Agents develop this model as a stochastic\ndifferential equation, calibrated to market data using gradient descent. We\ntest our neuro-symbolic traders on both synthetic data and real financial time\nseries, including an equity stock, commodity, and a foreign exchange pair. We\nthen expose several groups of neuro-symbolic traders to a virtual market\nenvironment. This market environment allows for feedback between the traders\nbelief of the underlying value to the observed price dynamics. We find that\nthis leads to price suppression compared to the historical data, highlighting a\nfuture risk to market stability. Our work is a first step towards quantifying\nthe effect of deep generative agents on markets dynamics and sets out some of\nthe potential risks and benefits of this approach in the future.\n","authors":["Namid R. Stillman","Rory Baggott"],"pdf_url":"https://arxiv.org/pdf/2410.14587v1.pdf","comment":"8 pages, 4 figures, ACM format"},{"id":"http://arxiv.org/abs/2410.14586v1","updated":"2024-10-18T16:37:28Z","published":"2024-10-18T16:37:28Z","title":"Neural Combinatorial Clustered Bandits for Recommendation Systems","summary":"  We consider the contextual combinatorial bandit setting where in each round,\nthe learning agent, e.g., a recommender system, selects a subset of \"arms,\"\ne.g., products, and observes rewards for both the individual base arms, which\nare a function of known features (called \"context\"), and the super arm (the\nsubset of arms), which is a function of the base arm rewards. The agent's goal\nis to simultaneously learn the unknown reward functions and choose the\nhighest-reward arms. For example, the \"reward\" may represent a user's\nprobability of clicking on one of the recommended products. Conventional bandit\nmodels, however, employ restrictive reward function models in order to obtain\nperformance guarantees. We make use of deep neural networks to estimate and\nlearn the unknown reward functions and propose Neural UCB Clustering\n(NeUClust), which adopts a clustering approach to select the super arm in every\nround by exploiting underlying structure in the context space. Unlike prior\nneural bandit works, NeUClust uses a neural network to estimate the super arm\nreward and select the super arm, thus eliminating the need for a known\noptimization oracle. We non-trivially extend prior neural combinatorial bandit\nworks to prove that NeUClust achieves\n$\\widetilde{O}\\left(\\widetilde{d}\\sqrt{T}\\right)$ regret, where $\\widetilde{d}$\nis the effective dimension of a neural tangent kernel matrix, $T$ the number of\nrounds. Experiments on real world recommendation datasets show that NeUClust\nachieves better regret and reward than other contextual combinatorial and\nneural bandit algorithms.\n","authors":["Baran Atalar","Carlee Joe-Wong"],"pdf_url":"https://arxiv.org/pdf/2410.14586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14581v1","updated":"2024-10-18T16:32:06Z","published":"2024-10-18T16:32:06Z","title":"Optimizing Attention with Mirror Descent: Generalized Max-Margin Token\n  Selection","summary":"  Attention mechanisms have revolutionized several domains of artificial\nintelligence, such as natural language processing and computer vision, by\nenabling models to selectively focus on relevant parts of the input data. While\nrecent work has characterized the optimization dynamics of gradient descent\n(GD) in attention-based models and the structural properties of its preferred\nsolutions, less is known about more general optimization algorithms such as\nmirror descent (MD). In this paper, we investigate the convergence properties\nand implicit biases of a family of MD algorithms tailored for softmax attention\nmechanisms, with the potential function chosen as the $p$-th power of the\n$\\ell_p$-norm. Specifically, we show that these algorithms converge in\ndirection to a generalized hard-margin SVM with an $\\ell_p$-norm objective when\napplied to a classification problem using a softmax attention model. Notably,\nour theoretical results reveal that the convergence rate is comparable to that\nof traditional GD in simpler models, despite the highly nonlinear and nonconvex\nnature of the present problem. Additionally, we delve into the joint\noptimization dynamics of the key-query matrix and the decoder, establishing\nconditions under which this complex joint optimization converges to their\nrespective hard-margin SVM solutions. Lastly, our numerical experiments on real\ndata demonstrate that MD algorithms improve generalization over standard GD and\nexcel in optimal token selection.\n","authors":["Aaron Alvarado Kristanto Julistiono","Davoud Ataee Tarzanagh","Navid Azizan"],"pdf_url":"https://arxiv.org/pdf/2410.14581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14579v1","updated":"2024-10-18T16:27:04Z","published":"2024-10-18T16:27:04Z","title":"Towards Unsupervised Validation of Anomaly-Detection Models","summary":"  Unsupervised validation of anomaly-detection models is a highly challenging\ntask. While the common practices for model validation involve a labeled\nvalidation set, such validation sets cannot be constructed when the underlying\ndatasets are unlabeled. The lack of robust and efficient unsupervised\nmodel-validation techniques presents an acute challenge in the implementation\nof automated anomaly-detection pipelines, especially when there exists no prior\nknowledge of the model's performance on similar datasets. This work presents a\nnew paradigm to automated validation of anomaly-detection models, inspired by\nreal-world, collaborative decision-making mechanisms. We focus on two\ncommonly-used, unsupervised model-validation tasks -- model selection and model\nevaluation -- and provide extensive experimental results that demonstrate the\naccuracy and robustness of our approach on both tasks.\n","authors":["Lihi Idan"],"pdf_url":"https://arxiv.org/pdf/2410.14579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14578v1","updated":"2024-10-18T16:26:45Z","published":"2024-10-18T16:26:45Z","title":"Large Language Models Are Overparameterized Text Encoders","summary":"  Large language models (LLMs) demonstrate strong performance as text embedding\nmodels when finetuned with supervised contrastive training. However, their\nlarge size balloons inference time and memory requirements. In this paper, we\nshow that by pruning the last $p\\%$ layers of an LLM before supervised training\nfor only 1000 steps, we can achieve a proportional reduction in memory and\ninference time. We evaluate four different state-of-the-art LLMs on text\nembedding tasks and find that our method can prune up to 30\\% of layers with\nnegligible impact on performance and up to 80\\% with only a modest drop. With\nonly three lines of code, our method is easily implemented in any pipeline for\ntransforming LLMs to text encoders. We also propose $\\text{L}^3 \\text{Prune}$,\na novel layer-pruning strategy based on the model's initial loss that provides\ntwo optimal pruning configurations: a large variant with negligible performance\nloss and a small variant for resource-constrained settings. On average, the\nlarge variant prunes 21\\% of the parameters with a $-0.3$ performance drop, and\nthe small variant only suffers from a $-5.1$ decrease while pruning 74\\% of the\nmodel. We consider these results strong evidence that LLMs are\noverparameterized for text embedding tasks, and can be easily pruned.\n","authors":["Thennal D K","Tim Fischer","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2410.14578v1.pdf","comment":"8 pages of content + 1 for limitations and ethical considerations, 14\n  pages in total including references and appendix, 5+1 figures"},{"id":"http://arxiv.org/abs/2410.13174v2","updated":"2024-10-18T16:26:30Z","published":"2024-10-17T02:57:35Z","title":"Scalable Drift Monitoring in Medical Imaging AI","summary":"  The integration of artificial intelligence (AI) into medical imaging has\nadvanced clinical diagnostics but poses challenges in managing model drift and\nensuring long-term reliability. To address these challenges, we develop MMC+,\nan enhanced framework for scalable drift monitoring, building upon the\nCheXstray framework that introduced real-time drift detection for medical\nimaging AI models using multi-modal data concordance. This work extends the\noriginal framework's methodologies, providing a more scalable and adaptable\nsolution for real-world healthcare settings and offers a reliable and\ncost-effective alternative to continuous performance monitoring addressing\nlimitations of both continuous and periodic monitoring methods. MMC+ introduces\ncritical improvements to the original framework, including more robust handling\nof diverse data streams, improved scalability with the integration of\nfoundation models like MedImageInsight for high-dimensional image embeddings\nwithout site-specific training, and the introduction of uncertainty bounds to\nbetter capture drift in dynamic clinical environments. Validated with\nreal-world data from Massachusetts General Hospital during the COVID-19\npandemic, MMC+ effectively detects significant data shifts and correlates them\nwith model performance changes. While not directly predicting performance\ndegradation, MMC+ serves as an early warning system, indicating when AI systems\nmay deviate from acceptable performance bounds and enabling timely\ninterventions. By emphasizing the importance of monitoring diverse data streams\nand evaluating data shifts alongside model performance, this work contributes\nto the broader adoption and integration of AI solutions in clinical settings.\n","authors":["Jameson Merkow","Felix J. Dorfner","Xiyu Yang","Alexander Ersoy","Giridhar Dasegowda","Mannudeep Kalra","Matthew P. Lungren","Christopher P. Bridge","Ivan Tarapov"],"pdf_url":"https://arxiv.org/pdf/2410.13174v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14574v1","updated":"2024-10-18T16:20:22Z","published":"2024-10-18T16:20:22Z","title":"MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts","summary":"  Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled\nscalability in deep learning. SMoE has the potential to exponentially increase\nparameter count while maintaining the efficiency of the model by only\nactivating a small subset of these parameters for a given sample. However, it\nhas been observed that SMoE suffers from unstable training and has difficulty\nadapting to new distributions, leading to the model's lack of robustness to\ndata contamination. To overcome these limitations, we first establish a\nconnection between the dynamics of the expert representations in SMoEs and\ngradient descent on a multi-objective optimization problem. Leveraging our\nframework, we then integrate momentum into SMoE and propose a new family of\nSMoEs named MomentumSMoE. We theoretically prove and numerically demonstrate\nthat MomentumSMoE is more stable and robust than SMoE. In particular, we verify\nthe advantages of MomentumSMoE over SMoE on a variety of practical tasks\nincluding ImageNet-1K object recognition and WikiText-103 language modeling. We\ndemonstrate the applicability of MomentumSMoE to many types of SMoE models,\nincluding those in the Sparse MoE model for vision (V-MoE) and the Generalist\nLanguage Model (GLaM). We also show that other advanced momentum-based\noptimization methods, such as Adam, can be easily incorporated into the\nMomentumSMoE framework for designing new SMoE models with even better\nperformance, almost negligible additional computation cost, and simple\nimplementations.\n","authors":["Rachel S. Y. Teo","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.14574v1.pdf","comment":"10 pages in the main text. Published at NeurIPS 2024. The code is\n  available at https://github.com/rachtsy/MomentumSMoE"},{"id":"http://arxiv.org/abs/2410.14573v1","updated":"2024-10-18T16:20:17Z","published":"2024-10-18T16:20:17Z","title":"Building Trust in Black-box Optimization: A Comprehensive Framework for\n  Explainability","summary":"  Optimizing costly black-box functions within a constrained evaluation budget\npresents significant challenges in many real-world applications. Surrogate\nOptimization (SO) is a common resolution, yet its proprietary nature introduced\nby the complexity of surrogate models and the sampling core (e.g., acquisition\nfunctions) often leads to a lack of explainability and transparency. While\nexisting literature has primarily concentrated on enhancing convergence to\nglobal optima, the practical interpretation of newly proposed strategies\nremains underexplored, especially in batch evaluation settings. In this paper,\nwe propose \\emph{Inclusive} Explainability Metrics for Surrogate Optimization\n(IEMSO), a comprehensive set of model-agnostic metrics designed to enhance the\ntransparency, trustworthiness, and explainability of the SO approaches. Through\nthese metrics, we provide both intermediate and post-hoc explanations to\npractitioners before and after performing expensive evaluations to gain trust.\nWe consider four primary categories of metrics, each targeting a specific\naspect of the SO process: Sampling Core Metrics, Batch Properties Metrics,\nOptimization Process Metrics, and Feature Importance. Our experimental\nevaluations demonstrate the significant potential of the proposed metrics\nacross different benchmarks.\n","authors":["Nazanin Nezami","Hadis Anahideh"],"pdf_url":"https://arxiv.org/pdf/2410.14573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14570v1","updated":"2024-10-18T16:16:52Z","published":"2024-10-18T16:16:52Z","title":"Understanding the difficulty of low-precision post-training quantization\n  of large language models","summary":"  Large language models of high parameter counts are computationally expensive,\nyet can be made much more efficient by compressing their weights to very low\nnumerical precision. This can be achieved either through post-training\nquantization by minimizing local, layer-wise quantization errors, or through\nquantization-aware fine-tuning by minimizing the global loss function. In this\nstudy, we discovered that, under the same data constraint, the former approach\nnearly always fared worse than the latter, a phenomenon particularly prominent\nwhen the numerical precision is very low. We further showed that this\ndifficulty of post-training quantization arose from stark misalignment between\noptimization of the local and global objective functions. Our findings explains\nlimited utility in minimization of local quantization error and the importance\nof direct quantization-aware fine-tuning, in the regime of large models at very\nlow precision.\n","authors":["Zifei Xu","Sayeh Sharify","Wanzin Yazar","Tristan Webb","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12616v2","updated":"2024-10-18T16:09:52Z","published":"2024-06-18T13:44:07Z","title":"Learning diffusion at lightspeed","summary":"  Diffusion regulates numerous natural processes and the dynamics of many\nsuccessful generative models. Existing models to learn the diffusion terms from\nobservational data rely on complex bilevel optimization problems and model only\nthe drift of the system. We propose a new simple model, JKOnet*, which bypasses\nthe complexity of existing architectures while presenting significantly\nenhanced representational capabilities: JKOnet* recovers the potential,\ninteraction, and internal energy components of the underlying diffusion\nprocess. JKOnet* minimizes a simple quadratic loss and outperforms other\nbaselines in terms of sample efficiency, computational complexity, and\naccuracy. Additionally, JKOnet* provides a closed-form optimal solution for\nlinearly parametrized functionals, and, when applied to predict the evolution\nof cellular processes from real-world data, it achieves state-of-the-art\naccuracy at a fraction of the computational cost of all existing methods. Our\nmethodology is based on the interpretation of diffusion processes as\nenergy-minimizing trajectories in the probability space via the so-called JKO\nscheme, which we study via its first-order optimality conditions.\n","authors":["Antonio Terpin","Nicolas Lanzetti","Martin Gadea","Florian Dörfler"],"pdf_url":"https://arxiv.org/pdf/2406.12616v2.pdf","comment":"Accepted for presentation at, and publication in the proceedings of,\n  the 38th Conference on Neural Information Processing Systems (NeurIPS 2024,\n  oral)"},{"id":"http://arxiv.org/abs/2410.14556v1","updated":"2024-10-18T15:59:54Z","published":"2024-10-18T15:59:54Z","title":"Measuring Diversity: Axioms and Challenges","summary":"  The concept of diversity is widely used in various applications: from image\nor molecule generation to recommender systems. Thus, being able to properly\nmeasure diversity is important. This paper addresses the problem of quantifying\ndiversity for a set of objects. First, we make a systematic review of existing\ndiversity measures and explore their undesirable behavior in some cases. Based\non this review, we formulate three desirable properties (axioms) of a reliable\ndiversity measure: monotonicity, uniqueness, and continuity. We show that none\nof the existing measures has all three properties and thus these measures are\nnot suitable for quantifying diversity. Then, we construct two examples of\nmeasures that have all the desirable properties, thus proving that the list of\naxioms is not self-contradicting. Unfortunately, the constructed examples are\ntoo computationally complex for practical use, thus we pose an open problem of\nconstructing a diversity measure that has all the listed properties and can be\ncomputed in practice.\n","authors":["Mikhail Mironov","Liudmila Prokhorenkova"],"pdf_url":"https://arxiv.org/pdf/2410.14556v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2409.15652v3","updated":"2024-10-18T15:45:39Z","published":"2024-09-24T01:29:24Z","title":"English offensive text detection using CNN based Bi-GRU model","summary":"  Over the years, the number of users of social media has increased\ndrastically. People frequently share their thoughts through social platforms,\nand this leads to an increase in hate content. In this virtual community,\nindividuals share their views, express their feelings, and post photos, videos,\nblogs, and more. Social networking sites like Facebook and Twitter provide\nplatforms to share vast amounts of content with a single click. However, these\nplatforms do not impose restrictions on the uploaded content, which may include\nabusive language and explicit images unsuitable for social media. To resolve\nthis issue, a new idea must be implemented to divide the inappropriate content.\nNumerous studies have been done to automate the process. In this paper, we\npropose a new Bi-GRU-CNN model to classify whether the text is offensive or\nnot. The combination of the Bi-GRU and CNN models outperforms the existing\nmodel.\n","authors":["Tonmoy Roy","Md Robiul Islam","Asif Ahammad Miazee","Anika Antara","Al Amin","Sunjim Hossain"],"pdf_url":"https://arxiv.org/pdf/2409.15652v3.pdf","comment":"5 pages and 6 figures"},{"id":"http://arxiv.org/abs/2410.14548v1","updated":"2024-10-18T15:43:34Z","published":"2024-10-18T15:43:34Z","title":"Boosting K-means for Big Data by Fusing Data Streaming with Global\n  Optimization","summary":"  K-means clustering is a cornerstone of data mining, but its efficiency\ndeteriorates when confronted with massive datasets. To address this limitation,\nwe propose a novel heuristic algorithm that leverages the Variable Neighborhood\nSearch (VNS) metaheuristic to optimize K-means clustering for big data. Our\napproach is based on the sequential optimization of the partial objective\nfunction landscapes obtained by restricting the Minimum Sum-of-Squares\nClustering (MSSC) formulation to random samples from the original big dataset.\nWithin each landscape, systematically expanding neighborhoods of the currently\nbest (incumbent) solution are explored by reinitializing all degenerate and a\nvarying number of additional centroids. Extensive and rigorous experimentation\non a large number of real-world datasets reveals that by transforming the\ntraditional local search into a global one, our algorithm significantly\nenhances the accuracy and efficiency of K-means clustering in big data\nenvironments, becoming the new state of the art in the field.\n","authors":["Ravil Mussabayev","Rustam Mussabayev"],"pdf_url":"https://arxiv.org/pdf/2410.14548v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11206v2","updated":"2024-10-18T15:43:02Z","published":"2024-06-17T04:53:47Z","title":"Retraining with Predicted Hard Labels Provably Increases Model Accuracy","summary":"  The performance of a model trained with \\textit{noisy labels} is often\nimproved by simply \\textit{retraining} the model with its own predicted\n\\textit{hard} labels (i.e., $1$/$0$ labels). Yet, a detailed theoretical\ncharacterization of this phenomenon is lacking. In this paper, we theoretically\nanalyze retraining in a linearly separable setting with randomly corrupted\nlabels given to us and prove that retraining can improve the population\naccuracy obtained by initially training with the given (noisy) labels. To the\nbest of our knowledge, this is the first such theoretical result. Retraining\nfinds application in improving training with local label differential privacy\n(DP) which involves training with noisy labels. We empirically show that\nretraining selectively on the samples for which the predicted label matches the\ngiven label significantly improves label DP training at \\textit{no extra\nprivacy cost}; we call this \\textit{consensus-based retraining}. As an example,\nwhen training ResNet-18 on CIFAR-100 with $\\epsilon=3$ label DP, we obtain\n$6.4\\%$ improvement in accuracy with consensus-based retraining.\n","authors":["Rudrajit Das","Inderjit S. Dhillon","Alessandro Epasto","Adel Javanmard","Jieming Mao","Vahab Mirrokni","Sujay Sanghavi","Peilin Zhong"],"pdf_url":"https://arxiv.org/pdf/2406.11206v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15379v2","updated":"2024-10-18T15:38:16Z","published":"2024-04-23T07:16:13Z","title":"Clustering of timed sequences -- Application to the analysis of care\n  pathways","summary":"  Improving the future of healthcare starts by better understanding the current\nactual practices in hospital settings. This motivates the objective of\ndiscovering typical care pathways from patient data. Revealing typical care\npathways can be achieved through clustering. The difficulty in clustering care\npathways, represented by sequences of timestamped events, lies in defining a\nsemantically appropriate metric and clustering algorithms. In this article, we\nadapt two methods developed for time series to the clustering of timed\nsequences: the drop-DTW metric and the DBA approach for the construction of\naveraged time sequences. These methods are then applied in clustering\nalgorithms to propose original and sound clustering algorithms for timed\nsequences. This approach is experimented with and evaluated on synthetic and\nreal-world data.\n","authors":["Thomas Guyet","Pierre Pinson","Enoal Gesny"],"pdf_url":"https://arxiv.org/pdf/2404.15379v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14539v1","updated":"2024-10-18T15:29:04Z","published":"2024-10-18T15:29:04Z","title":"Diffusion-based Semi-supervised Spectral Algorithm for Regression on\n  Manifolds","summary":"  We introduce a novel diffusion-based spectral algorithm to tackle regression\nanalysis on high-dimensional data, particularly data embedded within\nlower-dimensional manifolds. Traditional spectral algorithms often fall short\nin such contexts, primarily due to the reliance on predetermined kernel\nfunctions, which inadequately address the complex structures inherent in\nmanifold-based data. By employing graph Laplacian approximation, our method\nuses the local estimation property of heat kernel, offering an adaptive,\ndata-driven approach to overcome this obstacle. Another distinct advantage of\nour algorithm lies in its semi-supervised learning framework, enabling it to\nfully use the additional unlabeled data. This ability enhances the performance\nby allowing the algorithm to dig the spectrum and curvature of the data\nmanifold, providing a more comprehensive understanding of the dataset.\nMoreover, our algorithm performs in an entirely data-driven manner, operating\ndirectly within the intrinsic manifold structure of the data, without requiring\nany predefined manifold information. We provide a convergence analysis of our\nalgorithm. Our findings reveal that the algorithm achieves a convergence rate\nthat depends solely on the intrinsic dimension of the underlying manifold,\nthereby avoiding the curse of dimensionality associated with the higher ambient\ndimension.\n","authors":["Weichun Xia","Jiaxin Jiang","Lei Shi"],"pdf_url":"https://arxiv.org/pdf/2410.14539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12874v2","updated":"2024-10-18T15:26:55Z","published":"2024-10-14T18:11:53Z","title":"On Debiasing Text Embeddings Through Context Injection","summary":"  Current advances in Natural Language Processing (NLP) have made it\nincreasingly feasible to build applications leveraging textual data. Generally,\nthe core of these applications rely on having a good semantic representation of\ntext into vectors, via embedding models. However, it has been shown that these\nembeddings capture and perpetuate biases already present in text. While a few\ntechniques have been proposed to debias embeddings, they do not take advantage\nof the recent advances in context understanding of modern embedding models. In\nthis paper, we fill this gap by conducting a review of 19 embedding models by\nquantifying their biases and how well they respond to context injection as a\nmean of debiasing. We show that higher performing models are more prone to\ncapturing biases, but are also better at incorporating context. Surprisingly,\nwe find that while models can easily embed affirmative semantics, they fail at\nembedding neutral semantics. Finally, in a retrieval task, we show that biases\nin embeddings can lead to non-desirable outcomes. We use our new-found insights\nto design a simple algorithm for top $k$ retrieval, where $k$ is dynamically\nselected. We show that our algorithm is able to retrieve all relevant gendered\nand neutral chunks.\n","authors":["Thomas Uriot"],"pdf_url":"https://arxiv.org/pdf/2410.12874v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14535v1","updated":"2024-10-18T15:23:29Z","published":"2024-10-18T15:23:29Z","title":"Comparing Differentiable and Dynamic Ray Tracing: Introducing the\n  Multipath Lifetime Map","summary":"  With the increasing presence of dynamic scenarios, such as Vehicle-to-Vehicle\ncommunications, radio propagation modeling tools must adapt to the rapidly\nchanging nature of the radio channel. Recently, both Differentiable and Dynamic\nRay Tracing frameworks have emerged to address these challenges. However, there\nis often confusion about how these approaches differ and which one should be\nused in specific contexts. In this paper, we provide an overview of these two\ntechniques and a comparative analysis against two state-of-the-art tools:\n3DSCAT from UniBo and Sionna from NVIDIA. To provide a more precise\ncharacterization of the scope of these methods, we introduce a novel\nsimulation-based metric, the Multipath Lifetime Map, which enables the\nevaluation of spatial and temporal coherence in radio channels only based on\nthe geometrical description of the environment. Finally, our metrics are\nevaluated on a classic urban street canyon scenario, yielding similar results\nto those obtained from measurement campaigns.\n","authors":["Jérome Eertmans","Enrico Maria Vittuci","Vittorio Degli Esposti","Laurent Jacques","Claude Oestges"],"pdf_url":"https://arxiv.org/pdf/2410.14535v1.pdf","comment":"5 pages, 5 figures, 1 table, submitted to EuCAP 2025"},{"id":"http://arxiv.org/abs/2404.07864v2","updated":"2024-10-18T15:23:26Z","published":"2024-04-11T15:57:12Z","title":"Inferring Change Points in High-Dimensional Regression via Approximate\n  Message Passing","summary":"  We consider the problem of localizing change points in a generalized linear\nmodel (GLM), a model that covers many widely studied problems in statistical\nlearning including linear, logistic, and rectified linear regression. We\npropose a novel and computationally efficient Approximate Message Passing (AMP)\nalgorithm for estimating both the signals and the change point locations, and\nrigorously characterize its performance in the high-dimensional limit where the\nnumber of parameters $p$ is proportional to the number of samples $n$. This\ncharacterization is in terms of a state evolution recursion, which allows us to\nprecisely compute performance measures such as the asymptotic Hausdorff error\nof our change point estimates, and allows us to tailor the algorithm to take\nadvantage of any prior structural information on the signals and change points.\nMoreover, we show how our AMP iterates can be used to efficiently compute a\nBayesian posterior distribution over the change point locations in the\nhigh-dimensional limit. We validate our theory via numerical experiments, and\ndemonstrate the favorable performance of our estimators on both synthetic and\nreal data in the settings of linear, logistic, and rectified linear regression.\n","authors":["Gabriel Arpino","Xiaoqi Liu","Julia Gontarek","Ramji Venkataramanan"],"pdf_url":"https://arxiv.org/pdf/2404.07864v2.pdf","comment":"43 pages, 9 figures. A preliminary version of this paper appeared in\n  ICML 2024"},{"id":"http://arxiv.org/abs/2408.05807v3","updated":"2024-10-18T15:19:04Z","published":"2024-08-11T15:56:44Z","title":"Kernel Density Estimators in Large Dimensions","summary":"  This paper studies Kernel Density Estimation for a high-dimensional\ndistribution $\\rho(x)$. Traditional approaches have focused on the limit of\nlarge number of data points $n$ and fixed dimension $d$. We analyze instead the\nregime where both the number $n$ of data points $y_i$ and their dimensionality\n$d$ grow with a fixed ratio $\\alpha=(\\log n)/d$. Our study reveals three\ndistinct statistical regimes for the kernel-based estimate of the density $\\hat\n\\rho_h^{\\mathcal {D}}(x)=\\frac{1}{n h^d}\\sum_{i=1}^n\nK\\left(\\frac{x-y_i}{h}\\right)$, depending on the bandwidth $h$: a classical\nregime for large bandwidth where the Central Limit Theorem (CLT) holds, which\nis akin to the one found in traditional approaches. Below a certain value of\nthe bandwidth, $h_{CLT}(\\alpha)$, we find that the CLT breaks down. The\nstatistics of $\\hat\\rho_h^{\\mathcal {D}}(x)$ for a fixed $x$ drawn from\n$\\rho(x)$ is given by a heavy-tailed distribution (an alpha-stable\ndistribution). In particular below a value $h_G(\\alpha)$, we find that\n$\\hat\\rho_h^{\\mathcal {D}}(x)$ is governed by extreme value statistics: only a\nfew points in the database matter and give the dominant contribution to the\ndensity estimator. We provide a detailed analysis for high-dimensional\nmultivariate Gaussian data. We show that the optimal bandwidth threshold based\non Kullback-Leibler divergence lies in the new statistical regime identified in\nthis paper. As known by practitioners, when decreasing the bandwidth a\nKernel-estimated estimated changes from a smooth curve to a collections of\npeaks centred on the data points. Our findings reveal that this general\nphenomenon is related to sharp transitions between phases characterized by\ndifferent statistical properties, and offer new insights for Kernel density\nestimation in high-dimensional settings.\n","authors":["Giulio Biroli","Marc Mézard"],"pdf_url":"https://arxiv.org/pdf/2408.05807v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14533v1","updated":"2024-10-18T15:14:25Z","published":"2024-10-18T15:14:25Z","title":"The Traveling Bandit: A Framework for Bayesian Optimization with\n  Movement Costs","summary":"  This paper introduces a framework for Bayesian Optimization (BO) with metric\nmovement costs, addressing a critical challenge in practical applications where\ninput alterations incur varying costs. Our approach is a convenient plug-in\nthat seamlessly integrates with the existing literature on batched algorithms,\nwhere designs within batches are observed following the solution of a Traveling\nSalesman Problem. The proposed method provides a theoretical guarantee of\nconvergence in terms of movement costs for BO. Empirically, our method\neffectively reduces average movement costs over time while maintaining\ncomparable regret performance to conventional BO methods. This framework also\nshows promise for broader applications in various bandit settings with movement\ncosts.\n","authors":["Qiyuan Chen","Raed Al Kontar"],"pdf_url":"https://arxiv.org/pdf/2410.14533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14532v1","updated":"2024-10-18T15:13:07Z","published":"2024-10-18T15:13:07Z","title":"Using Sentiment and Technical Analysis to Predict Bitcoin with Machine\n  Learning","summary":"  Cryptocurrencies have gained significant attention in recent years due to\ntheir decentralized nature and potential for financial innovation. Thus, the\nability to accurately predict its price has become a subject of great interest\nfor investors, traders, and researchers. Some works in the literature show how\nBitcoin's market sentiment correlates with its price fluctuations in the\nmarket. However, papers that consider the sentiment of the market associated\nwith financial Technical Analysis indicators in order to predict Bitcoin's\nprice are still scarce. In this paper, we present a novel approach for\npredicting Bitcoin price movements by combining the Fear & Greedy Index, a\nmeasure of market sentiment, Technical Analysis indicators, and the potential\nof Machine Learning algorithms. This work represents a preliminary study on the\nimportance of sentiment metrics in cryptocurrency forecasting. Our initial\nexperiments demonstrate promising results considering investment returns,\nsurpassing the Buy & Hold baseline, and offering valuable insights about the\ncombination of indicators of sentiment and market in a cryptocurrency\nprediction model.\n","authors":["Arthur Emanuel de Oliveira Carosia"],"pdf_url":"https://arxiv.org/pdf/2410.14532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14528v1","updated":"2024-10-18T15:10:55Z","published":"2024-10-18T15:10:55Z","title":"Domain Adaptive Safety Filters via Deep Operator Learning","summary":"  Learning-based approaches for constructing Control Barrier Functions (CBFs)\nare increasingly being explored for safety-critical control systems. However,\nthese methods typically require complete retraining when applied to unseen\nenvironments, limiting their adaptability. To address this, we propose a\nself-supervised deep operator learning framework that learns the mapping from\nenvironmental parameters to the corresponding CBF, rather than learning the CBF\ndirectly. Our approach leverages the residual of a parametric Partial\nDifferential Equation (PDE), where the solution defines a parametric CBF\napproximating the maximal control invariant set. This framework accommodates\ncomplex safety constraints, higher relative degrees, and actuation limits. We\ndemonstrate the effectiveness of the method through numerical experiments on\nnavigation tasks involving dynamic obstacles.\n","authors":["Lakshmideepakreddy Manda","Shaoru Chen","Mahyar Fazlyab"],"pdf_url":"https://arxiv.org/pdf/2410.14528v1.pdf","comment":"63rd IEEE Conference on Decision and Control (CDC)"},{"id":"http://arxiv.org/abs/2410.14522v1","updated":"2024-10-18T15:06:50Z","published":"2024-10-18T15:06:50Z","title":"Rethinking Distance Metrics for Counterfactual Explainability","summary":"  Counterfactual explanations have been a popular method of post-hoc\nexplainability for a variety of settings in Machine Learning. Such methods\nfocus on explaining classifiers by generating new data points that are similar\nto a given reference, while receiving a more desirable prediction. In this\nwork, we investigate a framing for counterfactual generation methods that\nconsiders counterfactuals not as independent draws from a region around the\nreference, but as jointly sampled with the reference from the underlying data\ndistribution. Through this framing, we derive a distance metric, tailored for\ncounterfactual similarity that can be applied to a broad range of settings.\nThrough both quantitative and qualitative analyses of counterfactual generation\nmethods, we show that this framing allows us to express more nuanced\ndependencies among the covariates.\n","authors":["Joshua Nathaniel Williams","Anurag Katakkar","Hoda Heidari","J. Zico Kolter"],"pdf_url":"https://arxiv.org/pdf/2410.14522v1.pdf","comment":"13 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2410.14515v1","updated":"2024-10-18T14:54:40Z","published":"2024-10-18T14:54:40Z","title":"Efficient Annotator Reliability Assessment and Sample Weighting for\n  Knowledge-Based Misinformation Detection on Social Media","summary":"  Misinformation spreads rapidly on social media, confusing the truth and\ntargetting potentially vulnerable people. To effectively mitigate the negative\nimpact of misinformation, it must first be accurately detected before applying\na mitigation strategy, such as X's community notes, which is currently a manual\nprocess. This study takes a knowledge-based approach to misinformation\ndetection, modelling the problem similarly to one of natural language\ninference. The EffiARA annotation framework is introduced, aiming to utilise\ninter- and intra-annotator agreement to understand the reliability of each\nannotator and influence the training of large language models for\nclassification based on annotator reliability. In assessing the EffiARA\nannotation framework, the Russo-Ukrainian Conflict Knowledge-Based\nMisinformation Classification Dataset (RUC-MCD) was developed and made publicly\navailable. This study finds that sample weighting using annotator reliability\nperforms the best, utilising both inter- and intra-annotator agreement and\nsoft-label training. The highest classification performance achieved using\nLlama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large.\n","authors":["Owen Cook","Charlie Grimshaw","Ben Wu","Sophie Dillon","Jack Hicks","Luke Jones","Thomas Smith","Matyas Szert","Xingyi Song"],"pdf_url":"https://arxiv.org/pdf/2410.14515v1.pdf","comment":"8 pages, 3 figures, 3 tables. Code available here:\n  https://github.com/MiniEggz/ruc-misinfo"},{"id":"http://arxiv.org/abs/2406.07361v2","updated":"2024-10-18T14:38:03Z","published":"2024-06-11T15:28:48Z","title":"Deep Implicit Optimization for Robust and Flexible Image Registration","summary":"  Deep Learning in Image Registration (DLIR) methods have been tremendously\nsuccessful in image registration due to their speed and ability to incorporate\nweak label supervision at training time. However, DLIR methods forego many of\nthe benefits of classical optimization-based methods. The functional nature of\ndeep networks do not guarantee that the predicted transformation is a local\nminima of the registration objective, the representation of the transformation\n(displacement/velocity field/affine) is fixed, and the networks are not robust\nto domain shift. Our method aims to bridge this gap between classical and\nlearning methods by incorporating optimization as a layer in a deep network. A\ndeep network is trained to predict multi-scale dense feature images that are\nregistered using a black box iterative optimization solver. This optimal warp\nis then used to minimize image and label alignment errors. By implicitly\ndifferentiating end-to-end through an iterative optimization solver, our\nlearned features are registration and label-aware, and the warp functions are\nguaranteed to be local minima of the registration objective in the feature\nspace. Our framework shows excellent performance on in-domain datasets, and is\nagnostic to domain shift such as anisotropy and varying intensity profiles. For\nthe first time, our method allows switching between arbitrary transformation\nrepresentations (free-form to diffeomorphic) at test time with zero retraining.\nEnd-to-end feature learning also facilitates interpretability of features, and\nout-of-the-box promptability using additional label-fidelity terms at\ninference.\n","authors":["Rohit Jena","Pratik Chaudhari","James C. Gee"],"pdf_url":"https://arxiv.org/pdf/2406.07361v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08979v2","updated":"2024-10-18T14:35:53Z","published":"2024-10-11T16:54:07Z","title":"Overcoming Slow Decision Frequencies in Continuous Control: Model-Based\n  Sequence Reinforcement Learning for Model-Free Control","summary":"  Reinforcement learning (RL) is rapidly reaching and surpassing human-level\ncontrol capabilities. However, state-of-the-art RL algorithms often require\ntimesteps and reaction times significantly faster than human capabilities,\nwhich is impractical in real-world settings and typically necessitates\nspecialized hardware. Such speeds are difficult to achieve in the real world\nand often requires specialized hardware. We introduce Sequence Reinforcement\nLearning (SRL), an RL algorithm designed to produce a sequence of actions for a\ngiven input state, enabling effective control at lower decision frequencies.\nSRL addresses the challenges of learning action sequences by employing both a\nmodel and an actor-critic architecture operating at different temporal scales.\nWe propose a \"temporal recall\" mechanism, where the critic uses the model to\nestimate intermediate states between primitive actions, providing a learning\nsignal for each individual action within the sequence. Once training is\ncomplete, the actor can generate action sequences independently of the model,\nachieving model-free control at a slower frequency. We evaluate SRL on a suite\nof continuous control tasks, demonstrating that it achieves performance\ncomparable to state-of-the-art algorithms while significantly reducing actor\nsample complexity. To better assess performance across varying decision\nfrequencies, we introduce the Frequency-Averaged Score (FAS) metric. Our\nresults show that SRL significantly outperforms traditional RL algorithms in\nterms of FAS, making it particularly suitable for applications requiring\nvariable decision frequencies. Additionally, we compare SRL with model-based\nonline planning, showing that SRL achieves superior FAS while leveraging the\nsame model during training that online planners use for planning.\n","authors":["Devdhar Patel","Hava Siegelmann"],"pdf_url":"https://arxiv.org/pdf/2410.08979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13012v2","updated":"2024-10-18T14:32:21Z","published":"2024-10-16T20:14:02Z","title":"Sample Compression Scheme Reductions","summary":"  We present novel reductions from sample compression schemes in multiclass\nclassification, regression, and adversarially robust learning settings to\nbinary sample compression schemes. Assuming we have a compression scheme for\nbinary classes of size $f(d_\\mathrm{VC})$, where $d_\\mathrm{VC}$ is the VC\ndimension, then we have the following results: (1) If the binary compression\nscheme is a majority-vote or a stable compression scheme, then there exists a\nmulticlass compression scheme of size $O(f(d_\\mathrm{G}))$, where\n$d_\\mathrm{G}$ is the graph dimension. Moreover, for general binary compression\nschemes, we obtain a compression of size $O(f(d_\\mathrm{G})\\log|Y|)$, where $Y$\nis the label space. (2) If the binary compression scheme is a majority-vote or\na stable compression scheme, then there exists an $\\epsilon$-approximate\ncompression scheme for regression over $[0,1]$-valued functions of size\n$O(f(d_\\mathrm{P}))$, where $d_\\mathrm{P}$ is the pseudo-dimension. For general\nbinary compression schemes, we obtain a compression of size\n$O(f(d_\\mathrm{P})\\log(1/\\epsilon))$. These results would have significant\nimplications if the sample compression conjecture, which posits that any binary\nconcept class with a finite VC dimension admits a binary compression scheme of\nsize $O(d_\\mathrm{VC})$, is resolved (Littlestone and Warmuth, 1986; Floyd and\nWarmuth, 1995; Warmuth, 2003). Our results would then extend the proof of the\nconjecture immediately to other settings. We establish similar results for\nadversarially robust learning and also provide an example of a concept class\nthat is robustly learnable but has no bounded-size compression scheme,\ndemonstrating that learnability is not equivalent to having a compression\nscheme independent of the sample size, unlike in binary classification, where\ncompression of size $2^{O(d_\\mathrm{VC})}$ is attainable (Moran and Yehudayoff,\n2016).\n","authors":["Idan Attias","Steve Hanneke","Arvind Ramaswami"],"pdf_url":"https://arxiv.org/pdf/2410.13012v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14489v1","updated":"2024-10-18T14:19:13Z","published":"2024-10-18T14:19:13Z","title":"An Integrated Deep Learning Model for Skin Cancer Detection Using Hybrid\n  Feature Fusion Technique","summary":"  Skin cancer is a serious and potentially fatal disease caused by DNA damage.\nEarly detection significantly increases survival rates, making accurate\ndiagnosis crucial. In this groundbreaking study, we present a hybrid framework\nbased on Deep Learning (DL) that achieves precise classification of benign and\nmalignant skin lesions. Our approach begins with dataset preprocessing to\nenhance classification accuracy, followed by training two separate pre-trained\nDL models, InceptionV3 and DenseNet121. By fusing the results of each model\nusing the weighted sum rule, our system achieves exceptional accuracy rates.\nSpecifically, we achieve a 92.27% detection accuracy rate, 92.33% sensitivity,\n92.22% specificity, 90.81% precision, and 91.57% F1-score, outperforming\nexisting models and demonstrating the robustness and trustworthiness of our\nhybrid approach. Our study represents a significant advance in skin cancer\ndiagnosis and provides a promising foundation for further research in the\nfield. With the potential to save countless lives through earlier detection,\nour hybrid deep-learning approach is a game-changer in the fight against skin\ncancer.\n","authors":["Maksuda Akter","Rabea Khatun","Md. Alamin Talukder","Md. Manowarul Islam","Md. Ashraf Uddin"],"pdf_url":"https://arxiv.org/pdf/2410.14489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14488v1","updated":"2024-10-18T14:16:54Z","published":"2024-10-18T14:16:54Z","title":"ANT: Adaptive Noise Schedule for Time Series Diffusion Models","summary":"  Advances in diffusion models for generative artificial intelligence have\nrecently propagated to the time series (TS) domain, demonstrating\nstate-of-the-art performance on various tasks. However, prior works on TS\ndiffusion models often borrow the framework of existing works proposed in other\ndomains without considering the characteristics of TS data, leading to\nsuboptimal performance. In this work, we propose Adaptive Noise schedule for\nTime series diffusion models (ANT), which automatically predetermines proper\nnoise schedules for given TS datasets based on their statistics representing\nnon-stationarity. Our intuition is that an optimal noise schedule should\nsatisfy the following desiderata: 1) It linearly reduces the non-stationarity\nof TS data so that all diffusion steps are equally meaningful, 2) the data is\ncorrupted to the random noise at the final step, and 3) the number of steps is\nsufficiently large. The proposed method is practical for use in that it\neliminates the necessity of finding the optimal noise schedule with a small\nadditional cost to compute the statistics for given datasets, which can be done\noffline before training. We validate the effectiveness of our method across\nvarious tasks, including TS forecasting, refinement, and generation, on\ndatasets from diverse domains. Code is available at this repository:\nhttps://github.com/seunghan96/ANT.\n","authors":["Seunghan Lee","Kibok Lee","Taeyoung Park"],"pdf_url":"https://arxiv.org/pdf/2410.14488v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.14485v1","updated":"2024-10-18T14:10:16Z","published":"2024-10-18T14:10:16Z","title":"CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers and\n  Fully-Connected Neural Networks for Causally Constrained Predictions","summary":"  Artificial Neural Networks (ANNs), including fully-connected networks and\ntransformers, are highly flexible and powerful function approximators, widely\napplied in fields like computer vision and natural language processing.\nHowever, their inability to inherently respect causal structures can limit\ntheir robustness, making them vulnerable to covariate shift and difficult to\ninterpret/explain. This poses significant challenges for their reliability in\nreal-world applications. In this paper, we introduce Causal Fully-Connected\nNeural Networks (CFCNs) and Causal Transformers (CaTs), two general model\nfamilies designed to operate under predefined causal constraints, as specified\nby a Directed Acyclic Graph (DAG). These models retain the powerful function\napproximation abilities of traditional neural networks while adhering to the\nunderlying structural constraints, improving robustness, reliability, and\ninterpretability at inference time. This approach opens new avenues for\ndeploying neural networks in more demanding, real-world scenarios where\nrobustness and explainability is critical.\n","authors":["Matthew J. Vowels","Mathieu Rochat","Sina Akbari"],"pdf_url":"https://arxiv.org/pdf/2410.14485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14484v1","updated":"2024-10-18T14:08:41Z","published":"2024-10-18T14:08:41Z","title":"Transfer Reinforcement Learning in Heterogeneous Action Spaces using\n  Subgoal Mapping","summary":"  In this paper, we consider a transfer reinforcement learning problem\ninvolving agents with different action spaces. Specifically, for any new unseen\ntask, the goal is to use a successful demonstration of this task by an expert\nagent in its action space to enable a learner agent learn an optimal policy in\nits own different action space with fewer samples than those required if the\nlearner was learning on its own. Existing transfer learning methods across\ndifferent action spaces either require handcrafted mappings between those\naction spaces provided by human experts, which can induce bias in the learning\nprocedure, or require the expert agent to share its policy parameters with the\nlearner agent, which does not generalize well to unseen tasks. In this work, we\npropose a method that learns a subgoal mapping between the expert agent policy\nand the learner agent policy. Since the expert agent and the learner agent have\ndifferent action spaces, their optimal policies can have different subgoal\ntrajectories. We learn this subgoal mapping by training a Long Short Term\nMemory (LSTM) network for a distribution of tasks and then use this mapping to\npredict the learner subgoal sequence for unseen tasks, thereby improving the\nspeed of learning by biasing the agent's policy towards the predicted learner\nsubgoal sequence. Through numerical experiments, we demonstrate that the\nproposed learning scheme can effectively find the subgoal mapping underlying\nthe given distribution of tasks. Moreover, letting the learner agent imitate\nthe expert agent's policy with the learnt subgoal mapping can significantly\nimprove the sample efficiency and training time of the learner agent in unseen\nnew tasks.\n","authors":["Kavinayan P. Sivakumar","Yan Zhang","Zachary Bell","Scott Nivison","Michael M. Zavlanos"],"pdf_url":"https://arxiv.org/pdf/2410.14484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14483v1","updated":"2024-10-18T14:06:49Z","published":"2024-10-18T14:06:49Z","title":"Spectral Representations for Accurate Causal Uncertainty Quantification\n  with Gaussian Processes","summary":"  Accurate uncertainty quantification for causal effects is essential for\nrobust decision making in complex systems, but remains challenging in\nnon-parametric settings. One promising framework represents conditional\ndistributions in a reproducing kernel Hilbert space and places Gaussian process\npriors on them to infer posteriors on causal effects, but requires restrictive\nnuclear dominant kernels and approximations that lead to unreliable uncertainty\nestimates. In this work, we introduce a method, IMPspec, that addresses these\nlimitations via a spectral representation of the Hilbert space. We show that\nposteriors in this model can be obtained explicitly, by extending a result in\nHilbert space regression theory. We also learn the spectral representation to\noptimise posterior calibration. Our method achieves state-of-the-art\nperformance in uncertainty quantification and causal Bayesian optimisation\nacross simulations and a healthcare application.\n","authors":["Hugh Dance","Peter Orbanz","Arthur Gretton"],"pdf_url":"https://arxiv.org/pdf/2410.14483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09804v2","updated":"2024-10-18T14:03:05Z","published":"2024-10-13T11:15:38Z","title":"BlackDAN: A Black-Box Multi-Objective Approach for Effective and\n  Contextual Jailbreaking of Large Language Models","summary":"  While large language models (LLMs) exhibit remarkable capabilities across\nvarious tasks, they encounter potential security risks such as jailbreak\nattacks, which exploit vulnerabilities to bypass security measures and generate\nharmful outputs. Existing jailbreak strategies mainly focus on maximizing\nattack success rate (ASR), frequently neglecting other critical factors,\nincluding the relevance of the jailbreak response to the query and the level of\nstealthiness. This narrow focus on single objectives can result in ineffective\nattacks that either lack contextual relevance or are easily recognizable. In\nthis work, we introduce BlackDAN, an innovative black-box attack framework with\nmulti-objective optimization, aiming to generate high-quality prompts that\neffectively facilitate jailbreaking while maintaining contextual relevance and\nminimizing detectability. BlackDAN leverages Multiobjective Evolutionary\nAlgorithms (MOEAs), specifically the NSGA-II algorithm, to optimize jailbreaks\nacross multiple objectives including ASR, stealthiness, and semantic relevance.\nBy integrating mechanisms like mutation, crossover, and Pareto-dominance,\nBlackDAN provides a transparent and interpretable process for generating\njailbreaks. Furthermore, the framework allows customization based on user\npreferences, enabling the selection of prompts that balance harmfulness,\nrelevance, and other factors. Experimental results demonstrate that BlackDAN\noutperforms traditional single-objective methods, yielding higher success rates\nand improved robustness across various LLMs and multimodal LLMs, while ensuring\njailbreak responses are both relevant and less detectable.\n","authors":["Xinyuan Wang","Victor Shea-Jay Huang","Renmiao Chen","Hao Wang","Chengwei Pan","Lei Sha","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2410.09804v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14479v1","updated":"2024-10-18T14:02:34Z","published":"2024-10-18T14:02:34Z","title":"Backdoored Retrievers for Prompt Injection Attacks on Retrieval\n  Augmented Generation of Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating coherent text but remain limited by the static nature of their\ntraining data. Retrieval Augmented Generation (RAG) addresses this issue by\ncombining LLMs with up-to-date information retrieval, but also expand the\nattack surface of the system. This paper investigates prompt injection attacks\non RAG, focusing on malicious objectives beyond misinformation, such as\ninserting harmful links, promoting unauthorized services, and initiating\ndenial-of-service behaviors. We build upon existing corpus poisoning techniques\nand propose a novel backdoor attack aimed at the fine-tuning process of the\ndense retriever component. Our experiments reveal that corpus poisoning can\nachieve significant attack success rates through the injection of a small\nnumber of compromised documents into the retriever corpus. In contrast,\nbackdoor attacks demonstrate even higher success rates but necessitate a more\ncomplex setup, as the victim must fine-tune the retriever using the attacker\npoisoned dataset.\n","authors":["Cody Clop","Yannick Teglia"],"pdf_url":"https://arxiv.org/pdf/2410.14479v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.14477v1","updated":"2024-10-18T14:02:06Z","published":"2024-10-18T14:02:06Z","title":"Laplace Transform Based Low-Complexity Learning of Continuous Markov\n  Semigroups","summary":"  Markov processes serve as a universal model for many real-world random\nprocesses. This paper presents a data-driven approach for learning these models\nthrough the spectral decomposition of the infinitesimal generator (IG) of the\nMarkov semigroup. The unbounded nature of IGs complicates traditional methods\nsuch as vector-valued regression and Hilbert-Schmidt operator analysis.\nExisting techniques, including physics-informed kernel regression, are\ncomputationally expensive and limited in scope, with no recovery guarantees for\ntransfer operator methods when the time-lag is small. We propose a novel method\nthat leverages the IG's resolvent, characterized by the Laplace transform of\ntransfer operators. This approach is robust to time-lag variations, ensuring\naccurate eigenvalue learning even for small time-lags. Our statistical analysis\napplies to a broader class of Markov processes than current methods while\nreducing computational complexity from quadratic to linear in the state\ndimension. Finally, we illustrate the behaviour of our method in two\nexperiments.\n","authors":["Vladimir R. Kostic","Karim Lounici","Hélène Halconruy","Timothée Devergne","Pietro Novelli","Massimiliano Pontil"],"pdf_url":"https://arxiv.org/pdf/2410.14477v1.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2410.14475v1","updated":"2024-10-18T14:00:44Z","published":"2024-10-18T14:00:44Z","title":"Enhancing Cryptocurrency Market Forecasting: Advanced Machine Learning\n  Techniques and Industrial Engineering Contributions","summary":"  Cryptocurrencies, as decentralized digital assets, have experienced rapid\ngrowth and adoption, with over 23,000 cryptocurrencies and a market\ncapitalization nearing \\$1.1 trillion (about \\$3,400 per person in the US) as\nof 2023. This dynamic market presents significant opportunities and risks,\nhighlighting the need for accurate price prediction models to manage\nvolatility. This chapter comprehensively reviews machine learning (ML)\ntechniques applied to cryptocurrency price prediction from 2014 to 2024. We\nexplore various ML algorithms, including linear models, tree-based approaches,\nand advanced deep learning architectures such as transformers and large\nlanguage models. Additionally, we examine the role of sentiment analysis in\ncapturing market sentiment from textual data like social media posts and news\narticles to anticipate price fluctuations. With expertise in optimizing complex\nsystems and processes, industrial engineers are pivotal in enhancing these\nmodels. They contribute by applying principles of process optimization,\nefficiency, and risk mitigation to improve computational performance and data\nmanagement. This chapter highlights the evolving landscape of cryptocurrency\nprice prediction, the integration of emerging technologies, and the significant\nrole of industrial engineers in refining predictive models. By addressing\ncurrent limitations and exploring future research directions, this chapter aims\nto advance the development of more accurate and robust prediction systems,\nsupporting better-informed investment decisions and more stable market\nbehavior.\n","authors":["Jannatun Nayeem Pinky","Ramya Akula"],"pdf_url":"https://arxiv.org/pdf/2410.14475v1.pdf","comment":"63 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.14470v1","updated":"2024-10-18T13:54:46Z","published":"2024-10-18T13:54:46Z","title":"How Do Training Methods Influence the Utilization of Vision Models?","summary":"  Not all learnable parameters (e.g., weights) contribute equally to a neural\nnetwork's decision function. In fact, entire layers' parameters can sometimes\nbe reset to random values with little to no impact on the model's decisions. We\nrevisit earlier studies that examined how architecture and task complexity\ninfluence this phenomenon and ask: is this phenomenon also affected by how we\ntrain the model? We conducted experimental evaluations on a diverse set of\nImageNet-1k classification models to explore this, keeping the architecture and\ntraining data constant but varying the training pipeline. Our findings reveal\nthat the training method strongly influences which layers become critical to\nthe decision function for a given task. For example, improved training regimes\nand self-supervised training increase the importance of early layers while\nsignificantly under-utilizing deeper layers. In contrast, methods such as\nadversarial training display an opposite trend. Our preliminary results extend\nprevious findings, offering a more nuanced understanding of the inner mechanics\nof neural networks.\n  Code: https://github.com/paulgavrikov/layer_criticality\n","authors":["Paul Gavrikov","Shashank Agnihotri","Margret Keuper","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2410.14470v1.pdf","comment":"Accepted at the Interpretable AI: Past, Present and Future Workshop\n  at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.14466v1","updated":"2024-10-18T13:51:25Z","published":"2024-10-18T13:51:25Z","title":"Flow-based Sampling for Entanglement Entropy and the Machine Learning of\n  Defects","summary":"  We introduce a novel technique to numerically calculate R\\'enyi entanglement\nentropies in lattice quantum field theory using generative models. We describe\nhow flow-based approaches can be combined with the replica trick using a custom\nneural-network architecture around a lattice defect connecting two replicas.\nNumerical tests for the $\\phi^4$ scalar field theory in two and three\ndimensions demonstrate that our technique outperforms state-of-the-art Monte\nCarlo calculations, and exhibit a promising scaling with the defect size.\n","authors":["Andrea Bulgarelli","Elia Cellini","Karl Jansen","Stefan Kühn","Alessandro Nada","Shinichi Nakajima","Kim A. Nicoli","Marco Panero"],"pdf_url":"https://arxiv.org/pdf/2410.14466v1.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2404.18550v4","updated":"2024-10-18T13:50:10Z","published":"2024-04-29T09:45:46Z","title":"IncidentResponseGPT: Generating Traffic Incident Response Plans with\n  Generative Artificial Intelligence","summary":"  The proposed IncidentResponseGPT framework - a novel system that applies\ngenerative artificial intelligence (AI) to potentially enhance the efficiency\nand effectiveness of traffic incident response. This model allows for synthesis\nof region-specific incident response guidelines and generates incident response\nplans adapted to specific area, aiming to expedite decision-making for traffic\nmanagement authorities. This approach aims to accelerate incident resolution\ntimes by suggesting various recommendations (e.g. optimal rerouting strategies,\nestimating resource needs) to minimize the overall impact on the urban traffic\nnetwork. The system suggests specific actions, including dynamic lane closures,\noptimized rerouting and dispatching appropriate emergency resources. We utilize\nthe Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) to\nrank generated response plans based on criteria like impact minimization and\nresource efficiency based on their proximity to an human-proposed solution.\n","authors":["Artur Grigorev","Adriana-Simona Mihaita Khaled Saleh","Yuming Ou"],"pdf_url":"https://arxiv.org/pdf/2404.18550v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14464v1","updated":"2024-10-18T13:48:01Z","published":"2024-10-18T13:48:01Z","title":"Electrocardiogram-Language Model for Few-Shot Question Answering with\n  Meta Learning","summary":"  Electrocardiogram (ECG) interpretation requires specialized expertise, often\ninvolving synthesizing insights from ECG signals with complex clinical queries\nposed in natural language. The scarcity of labeled ECG data coupled with the\ndiverse nature of clinical inquiries presents a significant challenge for\ndeveloping robust and adaptable ECG diagnostic systems. This work introduces a\nnovel multimodal meta-learning method for few-shot ECG question answering,\naddressing the challenge of limited labeled data while leveraging the rich\nknowledge encoded within large language models (LLMs). Our LLM-agnostic\napproach integrates a pre-trained ECG encoder with a frozen LLM (e.g., LLaMA\nand Gemma) via a trainable fusion module, enabling the language model to reason\nabout ECG data and generate clinically meaningful answers. Extensive\nexperiments demonstrate superior generalization to unseen diagnostic tasks\ncompared to supervised baselines, achieving notable performance even with\nlimited ECG leads. For instance, in a 5-way 5-shot setting, our method using\nLLaMA-3.1-8B achieves accuracy of 84.6%, 77.3%, and 69.6% on single verify,\nchoose and query question types, respectively. These results highlight the\npotential of our method to enhance clinical ECG interpretation by combining\nsignal processing with the nuanced language understanding capabilities of LLMs,\nparticularly in data-constrained scenarios.\n","authors":["Jialu Tang","Tong Xia","Yuan Lu","Cecilia Mascolo","Aaqib Saeed"],"pdf_url":"https://arxiv.org/pdf/2410.14464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14461v1","updated":"2024-10-18T13:40:44Z","published":"2024-10-18T13:40:44Z","title":"The Propensity for Density in Feed-forward Models","summary":"  Does the process of training a neural network to solve a task tend to use all\nof the available weights even when the task could be solved with fewer weights?\nTo address this question we study the effects of pruning fully connected,\nconvolutional and residual models while varying their widths. We find that the\nproportion of weights that can be pruned without degrading performance is\nlargely invariant to model size. Increasing the width of a model has little\neffect on the density of the pruned model relative to the increase in absolute\nsize of the pruned network. In particular, we find substantial prunability\nacross a large range of model sizes, where our biggest model is 50 times as\nwide as our smallest model. We explore three hypotheses that could explain\nthese findings.\n","authors":["Nandi Schoots","Alex Jackson","Ali Kholmovaia","Peter McBurney","Murray Shanahan"],"pdf_url":"https://arxiv.org/pdf/2410.14461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13663v4","updated":"2024-10-18T13:16:57Z","published":"2024-06-19T16:10:26Z","title":"Model Internals-based Answer Attribution for Trustworthy\n  Retrieval-Augmented Generation","summary":"  Ensuring the verifiability of model answers is a fundamental challenge for\nretrieval-augmented generation (RAG) in the question answering (QA) domain.\nRecently, self-citation prompting was proposed to make large language models\n(LLMs) generate citations to supporting documents along with their answers.\nHowever, self-citing LLMs often struggle to match the required format, refer to\nnon-existent sources, and fail to faithfully reflect LLMs' context usage\nthroughout the generation. In this work, we present MIRAGE --Model\nInternals-based RAG Explanations -- a plug-and-play approach using model\ninternals for faithful answer attribution in RAG applications. MIRAGE detects\ncontext-sensitive answer tokens and pairs them with retrieved documents\ncontributing to their prediction via saliency methods. We evaluate our proposed\napproach on a multilingual extractive QA dataset, finding high agreement with\nhuman answer attribution. On open-ended QA, MIRAGE achieves citation quality\nand efficiency comparable to self-citation while also allowing for a\nfiner-grained control of attribution parameters. Our qualitative evaluation\nhighlights the faithfulness of MIRAGE's attributions and underscores the\npromising application of model internals for RAG answer attribution.\n","authors":["Jirui Qi","Gabriele Sarti","Raquel Fernández","Arianna Bisazza"],"pdf_url":"https://arxiv.org/pdf/2406.13663v4.pdf","comment":"Accepted by EMNLP 2024 Main Conference. Code and data released at\n  https://github.com/Betswish/MIRAGE"},{"id":"http://arxiv.org/abs/2410.12804v2","updated":"2024-10-18T13:15:50Z","published":"2024-09-30T12:05:07Z","title":"Hip Fracture Patient Pathways and Agent-based Modelling","summary":"  Increased healthcare demand is significantly straining European services.\nDigital solutions including advanced modelling techniques offer a promising\nsolution to optimising patient flow without impacting day-to-day healthcare\nprovision. In this work we outline an ongoing project that aims to optimise\nhealthcare resources using agent-based simulations.\n","authors":["Alison N. O'Connor","Stephen E. Ryan","Gauri Vaidya","Paul Harford","Meghana Kshirsagar"],"pdf_url":"https://arxiv.org/pdf/2410.12804v2.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.11443v2","updated":"2024-10-18T13:09:00Z","published":"2024-10-15T09:47:49Z","title":"Are High-Degree Representations Really Unnecessary in Equivariant Graph\n  Neural Networks?","summary":"  Equivariant Graph Neural Networks (GNNs) that incorporate E(3) symmetry have\nachieved significant success in various scientific applications. As one of the\nmost successful models, EGNN leverages a simple scalarization technique to\nperform equivariant message passing over only Cartesian vectors (i.e.,\n1st-degree steerable vectors), enjoying greater efficiency and efficacy\ncompared to equivariant GNNs using higher-degree steerable vectors. This\nsuccess suggests that higher-degree representations might be unnecessary. In\nthis paper, we disprove this hypothesis by exploring the expressivity of\nequivariant GNNs on symmetric structures, including $k$-fold rotations and\nregular polyhedra. We theoretically demonstrate that equivariant GNNs will\nalways degenerate to a zero function if the degree of the output\nrepresentations is fixed to 1 or other specific values. Based on this\ntheoretical insight, we propose HEGNN, a high-degree version of EGNN to\nincrease the expressivity by incorporating high-degree steerable vectors while\nmaintaining EGNN's efficiency through the scalarization trick. Our extensive\nexperiments demonstrate that HEGNN not only aligns with our theoretical\nanalyses on toy datasets consisting of symmetric structures, but also shows\nsubstantial improvements on more complicated datasets such as $N$-body and\nMD17. Our theoretical findings and empirical results potentially open up new\npossibilities for the research of equivariant GNNs.\n","authors":["Jiacheng Cen","Anyi Li","Ning Lin","Yuxiang Ren","Zihe Wang","Wenbing Huang"],"pdf_url":"https://arxiv.org/pdf/2410.11443v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11877v5","updated":"2024-10-18T13:03:05Z","published":"2024-05-20T08:41:15Z","title":"A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI:\n  The First Romanian Natural Language Inference Corpus","summary":"  Natural language inference (NLI), the task of recognizing the entailment\nrelationship in sentence pairs, is an actively studied topic serving as a proxy\nfor natural language understanding. Despite the relevance of the task in\nbuilding conversational agents and improving text classification, machine\ntranslation and other NLP tasks, to the best of our knowledge, there is no\npublicly available NLI corpus for the Romanian language. To this end, we\nintroduce the first Romanian NLI corpus (RoNLI) comprising 58K training\nsentence pairs, which are obtained via distant supervision, and 6K validation\nand test sentence pairs, which are manually annotated with the correct labels.\nWe conduct experiments with multiple machine learning methods based on distant\nlearning, ranging from shallow models based on word embeddings to\ntransformer-based neural networks, to establish a set of competitive baselines.\nFurthermore, we improve on the best model by employing a new curriculum\nlearning strategy based on data cartography. Our dataset and code to reproduce\nthe baselines are available at https://github.com/Eduard6421/RONLI.\n","authors":["Eduard Poesina","Cornelia Caragea","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2405.11877v5.pdf","comment":"Accepted at ACL 2024 (Main)"},{"id":"http://arxiv.org/abs/2410.14436v1","updated":"2024-10-18T12:53:23Z","published":"2024-10-18T12:53:23Z","title":"Learning to refine domain knowledge for biological network inference","summary":"  Perturbation experiments allow biologists to discover causal relationships\nbetween variables of interest, but the sparsity and high dimensionality of\nthese data pose significant challenges for causal structure learning\nalgorithms. Biological knowledge graphs can bootstrap the inference of causal\nstructures in these situations, but since they compile vastly diverse\ninformation, they can bias predictions towards well-studied systems.\nAlternatively, amortized causal structure learning algorithms encode inductive\nbiases through data simulation and train supervised models to recapitulate\nthese synthetic graphs. However, realistically simulating biology is arguably\neven harder than understanding a specific system. In this work, we take\ninspiration from both strategies and propose an amortized algorithm for\nrefining domain knowledge, based on data observations. On real and synthetic\ndatasets, we show that our approach outperforms baselines in recovering ground\ntruth causal graphs and identifying errors in the prior knowledge with limited\ninterventional data.\n","authors":["Peiwen Li","Menghua Wu"],"pdf_url":"https://arxiv.org/pdf/2410.14436v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14433v1","updated":"2024-10-18T12:51:19Z","published":"2024-10-18T12:51:19Z","title":"A Bioinformatic Approach Validated Utilizing Machine Learning Algorithms\n  to Identify Relevant Biomarkers and Crucial Pathways in Gallbladder Cancer","summary":"  Gallbladder cancer (GBC) is the most frequent cause of disease among biliary\ntract neoplasms. Identifying the molecular mechanisms and biomarkers linked to\nGBC progression has been a significant challenge in scientific research. Few\nrecent studies have explored the roles of biomarkers in GBC. Our study aimed to\nidentify biomarkers in GBC using machine learning (ML) and bioinformatics\ntechniques. We compared GBC tumor samples with normal samples to identify\ndifferentially expressed genes (DEGs) from two microarray datasets (GSE100363,\nGSE139682) obtained from the NCBI GEO database. A total of 146 DEGs were found,\nwith 39 up-regulated and 107 down-regulated genes. Functional enrichment\nanalysis of these DEGs was performed using Gene Ontology (GO) terms and\nREACTOME pathways through DAVID. The protein-protein interaction network was\nconstructed using the STRING database. To identify hub genes, we applied three\nranking algorithms: Degree, MNC, and Closeness Centrality. The intersection of\nhub genes from these algorithms yielded 11 hub genes. Simultaneously, two\nfeature selection methods (Pearson correlation and recursive feature\nelimination) were used to identify significant gene subsets. We then developed\nML models using SVM and RF on the GSE100363 dataset, with validation on\nGSE139682, to determine the gene subset that best distinguishes GBC samples.\nThe hub genes outperformed the other gene subsets. Finally, NTRK2, COL14A1,\nSCN4B, ATP1A2, SLC17A7, SLIT3, COL7A1, CLDN4, CLEC3B, ADCYAP1R1, and MFAP4 were\nidentified as crucial genes, with SLIT3, COL7A1, and CLDN4 being strongly\nlinked to GBC development and prediction.\n","authors":["Rabea Khatun","Wahia Tasnim","Maksuda Akter","Md Manowarul Islam","Md. Ashraf Uddin","Md. Zulfiker Mahmud","Saurav Chandra Das"],"pdf_url":"https://arxiv.org/pdf/2410.14433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14429v1","updated":"2024-10-18T12:48:22Z","published":"2024-10-18T12:48:22Z","title":"FashionR2R: Texture-preserving Rendered-to-Real Image Translation with\n  Diffusion Models","summary":"  Modeling and producing lifelike clothed human images has attracted\nresearchers' attention from different areas for decades, with the complexity\nfrom highly articulated and structured content. Rendering algorithms decompose\nand simulate the imaging process of a camera, while are limited by the accuracy\nof modeled variables and the efficiency of computation. Generative models can\nproduce impressively vivid human images, however still lacking in\ncontrollability and editability. This paper studies photorealism enhancement of\nrendered images, leveraging generative power from diffusion models on the\ncontrolled basis of rendering. We introduce a novel framework to translate\nrendered images into their realistic counterparts, which consists of two\nstages: Domain Knowledge Injection (DKI) and Realistic Image Generation (RIG).\nIn DKI, we adopt positive (real) domain finetuning and negative (rendered)\ndomain embedding to inject knowledge into a pretrained Text-to-image (T2I)\ndiffusion model. In RIG, we generate the realistic image corresponding to the\ninput rendered image, with a Texture-preserving Attention Control (TAC) to\npreserve fine-grained clothing textures, exploiting the decoupled features\nencoded in the UNet structure. Additionally, we introduce SynFashion dataset,\nfeaturing high-quality digital clothing images with diverse textures. Extensive\nexperimental results demonstrate the superiority and effectiveness of our\nmethod in rendered-to-real image translation.\n","authors":["Rui Hu","Qian He","Gaofeng He","Jiedong Zhuang","Huang Chen","Huafeng Liu","Huamin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14429v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.14426v1","updated":"2024-10-18T12:41:41Z","published":"2024-10-18T12:41:41Z","title":"Predicting time-varying flux and balance in metabolic systems using\n  structured neural-ODE processes","summary":"  We develop a novel data-driven framework as an alternative to dynamic flux\nbalance analysis, bypassing the demand for deep domain knowledge and manual\nefforts to formulate the optimization problem. The proposed framework is\nend-to-end, which trains a structured neural ODE process (SNODEP) model to\nestimate flux and balance samples using gene-expression time-series data.\nSNODEP is designed to circumvent the limitations of the standard neural ODE\nprocess model, including restricting the latent and decoder sampling\ndistributions to be normal and lacking structure between context points for\ncalculating the latent, thus more suitable for modeling the underlying dynamics\nof a metabolic system. Through comprehensive experiments ($156$ in total), we\ndemonstrate that SNODEP not only predicts the unseen time points of real-world\ngene-expression data and the flux and balance estimates well but can even\ngeneralize to more challenging unseen knockout configurations and irregular\ndata sampling scenarios, all essential for metabolic pathway analysis. We hope\nour work can serve as a catalyst for building more scalable and powerful models\nfor genome-scale metabolic analysis. Our code is available at:\n\\url{https://github.com/TrustMLRG/SNODEP}.\n","authors":["Santanu Rathod","Pietro Lio","Xiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.14426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03546v2","updated":"2024-10-18T12:41:23Z","published":"2023-10-05T13:57:53Z","title":"Plug-and-Play Posterior Sampling under Mismatched Measurement and Prior\n  Models","summary":"  Posterior sampling has been shown to be a powerful Bayesian approach for\nsolving imaging inverse problems. The recent plug-and-play unadjusted Langevin\nalgorithm (PnP-ULA) has emerged as a promising method for Monte Carlo sampling\nand minimum mean squared error (MMSE) estimation by combining physical\nmeasurement models with deep-learning priors specified using image denoisers.\nHowever, the intricate relationship between the sampling distribution of\nPnP-ULA and the mismatched data-fidelity and denoiser has not been\ntheoretically analyzed. We address this gap by proposing a posterior-L2\npseudometric and using it to quantify an explicit error bound for PnP-ULA under\nmismatched posterior distribution. We numerically validate our theory on\nseveral inverse problems such as sampling from Gaussian mixture models and\nimage deblurring. Our results suggest that the sensitivity of the sampling\ndistribution of PnP-ULA to a mismatch in the measurement model and the denoiser\ncan be precisely characterized.\n","authors":["Marien Renaud","Jiaming Liu","Valentin de Bortoli","Andrés Almansa","Ulugbek S. Kamilov"],"pdf_url":"https://arxiv.org/pdf/2310.03546v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14423v1","updated":"2024-10-18T12:37:51Z","published":"2024-10-18T12:37:51Z","title":"Integrating Deep Learning with Fundus and Optical Coherence Tomography\n  for Cardiovascular Disease Prediction","summary":"  Early identification of patients at risk of cardiovascular diseases (CVD) is\ncrucial for effective preventive care, reducing healthcare burden, and\nimproving patients' quality of life. This study demonstrates the potential of\nretinal optical coherence tomography (OCT) imaging combined with fundus\nphotographs for identifying future adverse cardiac events. We used data from\n977 patients who experienced CVD within a 5-year interval post-image\nacquisition, alongside 1,877 control participants without CVD, totaling 2,854\nsubjects. We propose a novel binary classification network based on a\nMulti-channel Variational Autoencoder (MCVAE), which learns a latent embedding\nof patients' fundus and OCT images to classify individuals into two groups:\nthose likely to develop CVD in the future and those who are not. Our model,\ntrained on both imaging modalities, achieved promising results (AUROC 0.78 +/-\n0.02, accuracy 0.68 +/- 0.002, precision 0.74 +/- 0.02, sensitivity 0.73 +/-\n0.02, and specificity 0.68 +/- 0.01), demonstrating its efficacy in identifying\npatients at risk of future CVD events based on their retinal images. This study\nhighlights the potential of retinal OCT imaging and fundus photographs as\ncost-effective, non-invasive alternatives for predicting cardiovascular disease\nrisk. The widespread availability of these imaging techniques in optometry\npractices and hospitals further enhances their potential for large-scale CVD\nrisk screening. Our findings contribute to the development of standardized,\naccessible methods for early CVD risk identification, potentially improving\npreventive care strategies and patient outcomes.\n","authors":["Cynthia Maldonado-Garcia","Arezoo Zakeri","Alejandro F Frangi","Nishant Ravikumar"],"pdf_url":"https://arxiv.org/pdf/2410.14423v1.pdf","comment":"Part of the book series: Lecture Notes in Computer Science\n  ((LNCS,volume 15155))"},{"id":"http://arxiv.org/abs/2410.14420v1","updated":"2024-10-18T12:33:10Z","published":"2024-10-18T12:33:10Z","title":"Asymptotic non-linear shrinkage formulas for weighted sample covariance","summary":"  We compute asymptotic non-linear shrinkage formulas for covariance and\nprecision matrix estimators for weighted sample covariances, in the spirit of\nLedoit and P\\'ech\\'e. We detail explicitly the formulas for\nexponentially-weighted sample covariances. Those new tools pave a way for\napplying non-linear shrinkage methods on weighted sample covariance. We show\nexperimentally the performance of the asymptotic shrinkage formulas. Finally,\nwe test the robustness of the theory to a heavy-tailed distributions.\n","authors":["Benoit Oriol"],"pdf_url":"https://arxiv.org/pdf/2410.14420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14416v1","updated":"2024-10-18T12:29:10Z","published":"2024-10-18T12:29:10Z","title":"An explainable machine learning approach for energy forecasting at the\n  household level","summary":"  Electricity forecasting has been a recurring research topic, as it is key to\nfinding the right balance between production and consumption. While most papers\nare focused on the national or regional scale, few are interested in the\nhousehold level. Desegregated forecast is a common topic in Machine Learning\n(ML) literature but lacks explainability that household energy forecasts\nrequire. This paper specifically targets the challenges of forecasting\nelectricity use at the household level. This paper confronts common Machine\nLearning algorithms to electricity household forecasts, weighing the pros and\ncons, including accuracy and explainability with well-known key metrics.\nFurthermore, we also confront them in this paper with the business challenges\nspecific to this sector such as explainability or outliers resistance. We\nintroduce a custom decision tree, aiming at providing a fair estimate of the\nenergy consumption, while being explainable and consistent with human\nintuition. We show that this novel method allows greater explainability without\nsacrificing much accuracy. The custom tree methodology can be used in various\nbusiness use cases but is subject to limitations, such as a lack of resilience\nwith outliers.\n","authors":["Pauline Béraud","Margaux Rioux","Michel Babany","Philippe de La Chevasnerie","Damien Theis","Giacomo Teodori","Chloé Pinguet","Romane Rigaud","François Leclerc"],"pdf_url":"https://arxiv.org/pdf/2410.14416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10918v5","updated":"2024-10-18T12:27:07Z","published":"2024-06-16T12:46:40Z","title":"Multi-LLM QA with Embodied Exploration","summary":"  Large language models (LLMs) have grown in popularity due to their natural\nlanguage interface and pre trained knowledge, leading to rapidly increasing\nsuccess in question-answering (QA) tasks. More recently, multi-agent systems\nwith LLM-based agents (Multi-LLM) have been utilized increasingly more for QA.\nIn these scenarios, the models may each answer the question and reach a\nconsensus or each model is specialized to answer different domain questions.\nHowever, most prior work dealing with Multi-LLM QA has focused on scenarios\nwhere the models are asked in a zero-shot manner or are given information\nsources to extract the answer. For question answering of an unknown\nenvironment, embodied exploration of the environment is first needed to answer\nthe question. This skill is necessary for personalizing embodied AI to\nenvironments such as households. There is a lack of insight into whether a\nMulti-LLM system can handle question-answering based on observations from\nembodied exploration. In this work, we address this gap by investigating the\nuse of Multi-Embodied LLM Explorers (MELE) for QA in an unknown environment.\nMultiple LLM-based agents independently explore and then answer queries about a\nhousehold environment. We analyze different aggregation methods to generate a\nsingle, final answer for each query: debating, majority voting, and training a\ncentral answer module (CAM). Using CAM, we observe a $46\\%$ higher accuracy\ncompared against the other non-learning-based aggregation methods. We provide\ncode and the query dataset for further research.\n","authors":["Bhrij Patel","Vishnu Sashank Dorbala","Amrit Singh Bedi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2406.10918v5.pdf","comment":"16 pages, 9 Figures, 5 Tables"},{"id":"http://arxiv.org/abs/2410.14413v1","updated":"2024-10-18T12:26:51Z","published":"2024-10-18T12:26:51Z","title":"WeSpeR: Population spectrum retrieval and spectral density estimation of\n  weighted sample covariance","summary":"  The spectrum of the weighted sample covariance shows a asymptotic non random\nbehavior when the dimension grows with the number of samples. In this setting,\nwe prove that the asymptotic spectral distribution $F$ of the weighted sample\ncovariance has a continuous density on $\\mathbb{R}^*$. We address then the\npractical problem of numerically finding this density. We propose a procedure\nto compute it, to determine the support of $F$ and define an efficient grid on\nit. We use this procedure to design the $\\textit{WeSpeR}$ algorithm, which\nestimates the spectral density and retrieves the true spectral covariance\nspectrum. Empirical tests confirm the good properties of the $\\textit{WeSpeR}$\nalgorithm.\n","authors":["Benoit Oriol"],"pdf_url":"https://arxiv.org/pdf/2410.14413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10547v2","updated":"2024-10-18T12:25:46Z","published":"2024-07-15T08:57:02Z","title":"Learning Social Cost Functions for Human-Aware Path Planning","summary":"  Achieving social acceptance is one of the main goals of Social Robotic\nNavigation. Despite this topic has received increasing interest in recent\nyears, most of the research has focused on driving the robotic agent along\nobstacle-free trajectories, planning around estimates of future human motion to\nrespect personal distances and optimize navigation. However, social\ninteractions in everyday life are also dictated by norms that do not strictly\ndepend on movement, such as when standing at the end of a queue rather than\ncutting it. In this paper, we propose a novel method to recognize common social\nscenarios and modify a traditional planner's cost function to adapt to them.\nThis solution enables the robot to carry out different social navigation\nbehaviors that would not arise otherwise, maintaining the robustness of\ntraditional navigation. Our approach allows the robot to learn different social\nnorms with a single learned model, rather than having different modules for\neach task. As a proof of concept, we consider the tasks of queuing and respect\ninteraction spaces of groups of people talking to one another, but the method\ncan be extended to other human activities that do not involve motion.\n","authors":["Andrea Eirale","Matteo Leonetti","Marcello Chiaberge"],"pdf_url":"https://arxiv.org/pdf/2407.10547v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14411v1","updated":"2024-10-18T12:24:05Z","published":"2024-10-18T12:24:05Z","title":"SNAC: Multi-Scale Neural Audio Codec","summary":"  Neural audio codecs have recently gained popularity because they can\nrepresent audio signals with high fidelity at very low bitrates, making it\nfeasible to use language modeling approaches for audio generation and\nunderstanding. Residual Vector Quantization (RVQ) has become the standard\ntechnique for neural audio compression using a cascade of VQ codebooks. This\npaper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ\nwhere the quantizers can operate at different temporal resolutions. By applying\na hierarchy of quantizers at variable frame rates, the codec adapts to the\naudio structure across multiple timescales. This leads to more efficient\ncompression, as demonstrated by extensive objective and subjective evaluations.\nThe code and model weights are open-sourced at\nhttps://github.com/hubertsiuzdak/snac.\n","authors":["Hubert Siuzdak","Florian Grötschla","Luca A. Lanzendörfer"],"pdf_url":"https://arxiv.org/pdf/2410.14411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.14437v3","updated":"2024-10-18T12:20:54Z","published":"2022-12-29T19:21:33Z","title":"An algorithm for clustering with confidence-based must-link and\n  cannot-link constraints","summary":"  We study here the semi-supervised $k$-clustering problem where information is\navailable on whether pairs of objects are in the same or in different clusters.\nThis information is either available with certainty or with a limited level of\nconfidence. We introduce the PCCC (Pairwise-Confidence-Constraints-Clustering)\nalgorithm, which iteratively assigns objects to clusters while accounting for\nthe information provided on the pairs of objects. Our algorithm uses integer\nprogramming for the assignment of objects which allows to include relationships\nas hard constraints that are guaranteed to be satisfied or as soft constraints\nthat can be violated subject to a penalty. This flexibility distinguishes our\nalgorithm from the state-of-the-art in which all pairwise constraints are\neither considered hard, or all are considered soft. We developed an enhanced\nmulti-start approach and a model-size reduction technique for the integer\nprogram that contributes to the effectiveness and the efficiency of the\nalgorithm. Unlike existing algorithms, our algorithm scales to large-scale\ninstances with up to 60,000 objects, 100 clusters, and millions of cannot-link\nconstraints (which are the most challenging constraints to incorporate). We\ncompare the PCCC algorithm with state-of-the-art approaches in an extensive\ncomputational study. Even though the PCCC algorithm is more general than the\nstate-of-the-art approaches in its applicability, it outperforms the\nstate-of-the-art approaches on instances with all hard or all soft constraints\nboth in terms of runtime and various metrics of solution quality. The code of\nthe PCCC algorithm is publicly available on GitHub.\n","authors":["Philipp Baumann","Dorit S. Hochbaum"],"pdf_url":"https://arxiv.org/pdf/2212.14437v3.pdf","comment":"To appear in INFORMS Journal on Computing"},{"id":"http://arxiv.org/abs/2405.16504v2","updated":"2024-10-18T12:20:11Z","published":"2024-05-26T09:57:45Z","title":"Explaining Modern Gated-Linear RNNs via a Unified Implicit Attention\n  Formulation","summary":"  Recent advances in efficient sequence modeling have led to attention-free\nlayers, such as Mamba, RWKV, and various gated RNNs, all featuring\nsub-quadratic complexity in sequence length and excellent scaling properties,\nenabling the construction of a new type of foundation models. In this paper, we\npresent a unified view of these models, formulating such layers as implicit\ncausal self-attention layers. The formulation includes most of their\nsub-components and is not limited to a specific part of the architecture. The\nframework compares the underlying mechanisms on similar grounds for different\nlayers and provides a direct means for applying explainability methods. Our\nexperiments show that our attention matrices and attribution method outperform\nan alternative and a more limited formulation that was recently proposed for\nMamba. For the other architectures for which our method is the first to provide\nsuch a view, our method is effective and competitive in the relevant metrics\ncompared to the results obtained by state-of-the-art Transformer explainability\nmethods. Our code is publicly available.\n","authors":["Itamar Zimerman","Ameen Ali","Lior Wolf"],"pdf_url":"https://arxiv.org/pdf/2405.16504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12950v2","updated":"2024-10-18T12:19:41Z","published":"2024-06-18T12:54:47Z","title":"MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular\n  Property Prediction","summary":"  Molecular property prediction (MPP) is a fundamental and crucial task in drug\ndiscovery. However, prior methods are limited by the requirement for a large\nnumber of labeled molecules and their restricted ability to generalize for\nunseen and new tasks, both of which are essential for real-world applications.\nTo address these challenges, we present MolecularGPT for few-shot MPP. From a\nperspective on instruction tuning, we fine-tune large language models (LLMs)\nbased on curated molecular instructions spanning over 1000 property prediction\ntasks. This enables building a versatile and specialized LLM that can be\nadapted to novel MPP tasks without any fine-tuning through zero- and few-shot\nin-context learning (ICL). MolecularGPT exhibits competitive in-context\nreasoning capabilities across 10 downstream evaluation datasets, setting new\nbenchmarks for few-shot molecular prediction tasks. More importantly, with just\ntwo-shot examples, MolecularGPT can outperform standard supervised graph neural\nnetwork methods on 4 out of 7 datasets. It also excels state-of-the-art LLM\nbaselines by up to 15.7% increase on classification accuracy and decrease of\n17.9 on regression metrics (e.g., RMSE) under zero-shot. This study\ndemonstrates the potential of LLMs as effective few-shot molecular property\npredictors. The code is available at https://github.com/NYUSHCS/MolecularGPT.\n","authors":["Yuyan Liu","Sirui Ding","Sheng Zhou","Wenqi Fan","Qiaoyu Tan"],"pdf_url":"https://arxiv.org/pdf/2406.12950v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09567v2","updated":"2024-10-18T12:18:01Z","published":"2024-10-12T15:29:18Z","title":"Timeseria: an object-oriented time series processing library","summary":"  Timeseria is an object-oriented time series processing library implemented in\nPython, which aims at making it easier to manipulate time series data and to\nbuild statistical and machine learning models on top of it. Unlike common data\nanalysis frameworks, it builds up from well defined and reusable logical units\n(objects), which can be easily combined together in order to ensure a high\nlevel of consistency. Thanks to this approach, Timeseria can address by design\nseveral non-trivial issues often underestimated, such as handling data losses,\nnon-uniform sampling rates, differences between aggregated data and punctual\nobservations, time zones, daylight saving times, and more. Timeseria comes with\na comprehensive set of base data structures, common data manipulation\noperations, and extensible models for data reconstruction, forecasting and\nanomaly detection. It also integrates a powerful plotting engine capable of\nhandling even millions of data points.\n","authors":["Stefano Alberto Russo","Giuliano Taffoni","Luca Bortolussi"],"pdf_url":"https://arxiv.org/pdf/2410.09567v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14393v1","updated":"2024-10-18T11:55:34Z","published":"2024-10-18T11:55:34Z","title":"Debug Smarter, Not Harder: AI Agents for Error Resolution in\n  Computational Notebooks","summary":"  Computational notebooks became indispensable tools for research-related\ndevelopment, offering unprecedented interactivity and flexibility in the\ndevelopment process. However, these benefits come at the cost of\nreproducibility and an increased potential for bugs. With the rise of\ncode-fluent Large Language Models empowered with agentic techniques, smart\nbug-fixing tools with a high level of autonomy have emerged. However, those\ntools are tuned for classical script programming and still struggle with\nnon-linear computational notebooks. In this paper, we present an AI agent\ndesigned specifically for error resolution in a computational notebook. We have\ndeveloped an agentic system capable of exploring a notebook environment by\ninteracting with it -- similar to how a user would -- and integrated the system\ninto the JetBrains service for collaborative data science called Datalore. We\nevaluate our approach against the pre-existing single-action solution by\ncomparing costs and conducting a user study. Users rate the error resolution\ncapabilities of the agentic system higher but experience difficulties with UI.\nWe share the results of the study and consider them valuable for further\nimproving user-agent collaboration.\n","authors":["Konstantin Grotov","Artem Borzilov","Maksim Krivobok","Timofey Bryksin","Yaroslav Zharov"],"pdf_url":"https://arxiv.org/pdf/2410.14393v1.pdf","comment":"Accepted to EMNLP 2024 System Demonstrations"},{"id":"http://arxiv.org/abs/2410.14390v1","updated":"2024-10-18T11:50:54Z","published":"2024-10-18T11:50:54Z","title":"Personalizing Low-Rank Bayesian Neural Networks Via Federated Learning","summary":"  To support real-world decision-making, it is crucial for models to be\nwell-calibrated, i.e., to assign reliable confidence estimates to their\npredictions. Uncertainty quantification is particularly important in\npersonalized federated learning (PFL), as participating clients typically have\nsmall local datasets, making it difficult to unambiguously determine optimal\nmodel parameters. Bayesian PFL (BPFL) methods can potentially enhance\ncalibration, but they often come with considerable computational and memory\nrequirements due to the need to track the variances of all the individual model\nparameters. Furthermore, different clients may exhibit heterogeneous\nuncertainty levels owing to varying local dataset sizes and distributions. To\naddress these challenges, we propose LR-BPFL, a novel BPFL method that learns a\nglobal deterministic model along with personalized low-rank Bayesian\ncorrections. To tailor the local model to each client's inherent uncertainty\nlevel, LR-BPFL incorporates an adaptive rank selection mechanism. We evaluate\nLR-BPFL across a variety of datasets, demonstrating its advantages in terms of\ncalibration, accuracy, as well as computational and memory requirements.\n","authors":["Boning Zhang","Dongzhu Liu","Osvaldo Simeone","Guanchu Wang","Dimitrios Pezaros","Guangxu Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.14390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14389v1","updated":"2024-10-18T11:49:40Z","published":"2024-10-18T11:49:40Z","title":"SurgeryV2: Bridging the Gap Between Model Merging and Multi-Task\n  Learning with Deep Representation Surgery","summary":"  Model merging-based multitask learning (MTL) offers a promising approach for\nperforming MTL by merging multiple expert models without requiring access to\nraw training data. However, in this paper, we examine the merged model's\nrepresentation distribution and uncover a critical issue of \"representation\nbias\". This bias arises from a significant distribution gap between the\nrepresentations of the merged and expert models, leading to the suboptimal\nperformance of the merged MTL model. To address this challenge, we first\npropose a representation surgery solution called Surgery. Surgery is a\nlightweight, task-specific module that aligns the final layer representations\nof the merged model with those of the expert models, effectively alleviating\nbias and improving the merged model's performance. Despite these improvements,\na performance gap remains compared to the traditional MTL method. Further\nanalysis reveals that representation bias phenomena exist at each layer of the\nmerged model, and aligning representations only in the last layer is\ninsufficient for fully reducing systemic bias because biases introduced at each\nlayer can accumulate and interact in complex ways. To tackle this, we then\npropose a more comprehensive solution, deep representation surgery (also called\nSurgeryV2), which mitigates representation bias across all layers, and thus\nbridges the performance gap between model merging-based MTL and traditional\nMTL. Finally, we design an unsupervised optimization objective to optimize both\nthe Surgery and SurgeryV2 modules. Our experimental results show that\nincorporating these modules into state-of-the-art (SOTA) model merging schemes\nleads to significant performance gains. Notably, our SurgeryV2 scheme reaches\nalmost the same level as individual expert models or the traditional MTL model.\nThe code is available at \\url{https://github.com/EnnengYang/SurgeryV2}.\n","authors":["Enneng Yang","Li Shen","Zhenyi Wang","Guibing Guo","Xingwei Wang","Xiaocun Cao","Jie Zhang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2410.14389v1.pdf","comment":"This paper is an extended version of our previous work\n  [arXiv:2402.02705] presented at ICML 2024"},{"id":"http://arxiv.org/abs/2410.06927v2","updated":"2024-10-18T11:47:40Z","published":"2024-10-09T14:21:59Z","title":"Spectral and Rhythm Features for Audio Classification with Deep\n  Convolutional Neural Networks","summary":"  Convolutional neural networks (CNNs) are widely used in computer vision. They\ncan be used not only for conventional digital image material to recognize\npatterns, but also for feature extraction from digital imagery representing\nspectral and rhythm features extracted from time-domain digital audio signals\nfor the acoustic classification of sounds. Different spectral and rhythm\nfeature representations like mel-scaled spectrograms, mel-frequency cepstral\ncoefficients (MFCCs), cyclic tempograms, short-time Fourier transform (STFT)\nchromagrams, constant-Q transform (CQT) chromagrams and chroma energy\nnormalized statistics (CENS) chromagrams are investigated in terms of the audio\nclassification performance using a deep convolutional neural network. It can be\nclearly shown that the mel-scaled spectrograms and the mel-frequency cepstral\ncoefficients (MFCCs) perform significantly better than the other spectral and\nrhythm features investigated in this research for audio classification tasks\nusing deep CNNs. The experiments were carried out with the aid of the ESC-50\ndataset with 2,000 labeled environmental audio recordings.\n","authors":["Friedrich Wolf-Monheim"],"pdf_url":"https://arxiv.org/pdf/2410.06927v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14388v1","updated":"2024-10-18T11:44:29Z","published":"2024-10-18T11:44:29Z","title":"Unscrambling disease progression at scale: fast inference of event\n  permutations with optimal transport","summary":"  Disease progression models infer group-level temporal trajectories of change\nin patients' features as a chronic degenerative condition plays out. They\nprovide unique insight into disease biology and staging systems with\nindividual-level clinical utility. Discrete models consider disease progression\nas a latent permutation of events, where each event corresponds to a feature\nbecoming measurably abnormal. However, permutation inference using traditional\nmaximum likelihood approaches becomes prohibitive due to combinatoric\nexplosion, severely limiting model dimensionality and utility. Here we leverage\nideas from optimal transport to model disease progression as a latent\npermutation matrix of events belonging to the Birkhoff polytope, facilitating\nfast inference via optimisation of the variational lower bound. This enables a\nfactor of 1000 times faster inference than the current state of the art and,\ncorrespondingly, supports models with several orders of magnitude more features\nthan the current state of the art can consider. Experiments demonstrate the\nincrease in speed, accuracy and robustness to noise in simulation. Further\nexperiments with real-world imaging data from two separate datasets, one from\nAlzheimer's disease patients, the other age-related macular degeneration,\nshowcase, for the first time, pixel-level disease progression events in the\nbrain and eye, respectively. Our method is low compute, interpretable and\napplicable to any progressive condition and data modality, giving it broad\npotential clinical utility.\n","authors":["Peter A. Wijeratne","Daniel C. Alexander"],"pdf_url":"https://arxiv.org/pdf/2410.14388v1.pdf","comment":"Pre-print of version accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2407.13625v2","updated":"2024-10-18T11:43:28Z","published":"2024-07-18T15:59:37Z","title":"Distributionally and Adversarially Robust Logistic Regression via\n  Intersecting Wasserstein Balls","summary":"  Adversarially robust optimization (ARO) has become the de facto standard for\ntraining models to defend against adversarial attacks during testing. However,\ndespite their robustness, these models often suffer from severe overfitting. To\nmitigate this issue, several successful approaches have been proposed,\nincluding replacing the empirical distribution in training with: (i) a\nworst-case distribution within an ambiguity set, leading to a distributionally\nrobust (DR) counterpart of ARO; or (ii) a mixture of the empirical distribution\nwith one derived from an auxiliary dataset (e.g., synthetic, external, or\nout-of-domain). Building on the first approach, we explore the Wasserstein DR\ncounterpart of ARO for logistic regression and show it admits a tractable\nconvex optimization reformulation. Adopting the second approach, we enhance the\nDR framework by intersecting its ambiguity set with one constructed from an\nauxiliary dataset, which yields significant improvements when the Wasserstein\ndistance between the data-generating and auxiliary distributions can be\nestimated. We analyze the resulting optimization problem, develop efficient\nsolutions, and show that our method outperforms benchmark approaches on\nstandard datasets.\n","authors":["Aras Selvi","Eleonora Kreacic","Mohsen Ghassemi","Vamsi Potluru","Tucker Balch","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2407.13625v2.pdf","comment":"33 pages, 3 color figures, under review at a conference"},{"id":"http://arxiv.org/abs/2410.14386v1","updated":"2024-10-18T11:38:29Z","published":"2024-10-18T11:38:29Z","title":"Investigating the Capabilities of Deep Learning for Processing and\n  Interpreting One-Shot Multi-offset GPR Data: A Numerical Case Study for Lunar\n  and Martian Environments","summary":"  Ground-penetrating radar (GPR) is a mature geophysical method that has gained\nincreasing popularity in planetary science over the past decade. GPR has been\nutilised both for Lunar and Martian missions providing pivotal information\nregarding the near surface geology of Terrestrial planets. Within that context,\nnumerous processing pipelines have been suggested to address the unique\nchallenges present in planetary setups. These processing pipelines often\nrequire manual tuning resulting to ambiguous outputs open to non-unique\ninterpretations. These pitfalls combined with the large number of planetary GPR\ndata (kilometers in magnitude), highlight the necessity for automatic,\nobjective and advanced processing and interpretation schemes. The current paper\ninvestigates the potential of deep learning for interpreting and processing GPR\ndata. The one-shot multi-offset configuration is investigated via a coherent\nnumerical case study, showcasing the potential of deep learning for A)\nreconstructing the dielectric distribution of the the near surface of\nTerrestrial planets, and B) filling missing or bad-quality traces. Special care\nwas taken for the numerical data to be both realistic and challenging.\nMoreover, the generated synthetic data are properly labelled and made publicly\navailable for training future data-driven pipelines and contributing towards\ndeveloping pre-trained foundation models for GPR.\n","authors":["Iraklis Giannakis","Craig Warren","Antonios Giannopoulos","Georgios Leontidis","Yan Su","Feng Zhou","Javier Martin-Torres","Nectaria Diamanti"],"pdf_url":"https://arxiv.org/pdf/2410.14386v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14659v2","updated":"2024-10-18T11:32:20Z","published":"2023-10-23T07:53:47Z","title":"Predicting Accurate Lagrangian Multipliers for Mixed Integer Linear\n  Programs","summary":"  Lagrangian relaxation stands among the most efficient approaches for solving\na Mixed Integer Linear Programs (MILP) with difficult constraints. Given any\nduals for these constraints, called Lagrangian Multipliers (LMs), it returns a\nbound on the optimal value of the MILP, and Lagrangian methods seek the LMs\ngiving the best such bound. But these methods generally rely on iterative\nalgorithms resembling gradient descent to maximize the concave piecewise linear\ndual function: the computational burden grows quickly with the number of\nrelaxed constraints. We introduce a deep learning approach that bypasses the\ndescent, effectively amortizing the local, per instance, optimization. A\nprobabilistic encoder based on a graph convolutional network computes\nhigh-dimensional representations of relaxed constraints in MILP instances. A\ndecoder then turns these representations into LMs. We train the encoder and\ndecoder jointly by directly optimizing the bound obtained from the predicted\nmultipliers. Numerical experiments show that our approach closes up to 85~\\% of\nthe gap between the continuous relaxation and the best Lagrangian bound, and\nprovides a high quality warm-start for descent based Lagrangian methods.\n","authors":["Francesco Demelas","Joseph Le Roux","Mathieu Lacroix","Axel Parmentier"],"pdf_url":"https://arxiv.org/pdf/2310.14659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09388v2","updated":"2024-10-18T11:23:18Z","published":"2024-10-12T06:39:31Z","title":"3-D Magnetotelluric Deep Learning Inversion Guided by Pseudo-Physical\n  Information","summary":"  Magnetotelluric deep learning (DL) inversion methods based on joint\ndata-driven and physics-driven have become a hot topic in recent years. When\nmapping observation data (or forward modeling data) to the resistivity model\nusing neural networks (NNs), incorporating the error (loss) term of the\ninversion resistivity's forward modeling response--which introduces physical\ninformation about electromagnetic field propagation--can significantly enhance\nthe inversion accuracy. To efficiently achieve data-physical dual-driven MT\ndeep learning inversion for large-scale 3-D MT data, we propose using DL\nforward modeling networks to compute this portion of the loss. This approach\nintroduces pseudo-physical information through the forward modeling of NN\nsimulation, further guiding the inversion network fitting. Specifically, we\nfirst pre-train the forward modeling networks as fixed forward modeling\noperators, then transfer and integrate them into the inversion network\ntraining, and finally optimize the inversion network by minimizing the\nmultinomial loss. Theoretical experimental results indicate that despite some\nsimulation errors in DL forward modeling, the introduced pseudo-physical\ninformation still enhances inversion accuracy and significantly mitigates the\noverfitting problem during training. Additionally, we propose a new input mode\nthat involves masking and adding noise to the data, simulating the field data\nenvironment of 3-D MT inversion, thereby making the method more flexible and\neffective for practical applications.\n","authors":["Peifan Jiang","Xuben Wang","Shuang Wang","Fei Deng","Kunpeng Wang","Bin Wang","Yuhan Yang","Islam Fadel"],"pdf_url":"https://arxiv.org/pdf/2410.09388v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14380v1","updated":"2024-10-18T11:07:26Z","published":"2024-10-18T11:07:26Z","title":"Dual-Label LearningWith Irregularly Present Labels","summary":"  In multi-task learning, we often encounter the case when the presence of\nlabels across samples exhibits irregular patterns: samples can be fully\nlabeled, partially labeled or unlabeled. Taking drug analysis as an example,\nmultiple toxicity properties of a drug molecule may not be concurrently\navailable due to experimental limitations. It triggers a demand for a new\ntraining and inference mechanism that could accommodate irregularly present\nlabels and maximize the utility of any available label information. In this\nwork, we focus on the two-label learning task, and propose a novel training and\ninference framework, Dual-Label Learning (DLL). The DLL framework formulates\nthe problem into a dual-function system, in which the two functions should\nsimultaneously satisfy standard supervision, structural duality and\nprobabilistic duality. DLL features a dual-tower model architecture that\nexplicitly captures the information exchange between labels, aimed at\nmaximizing the utility of partially available labels in understanding label\ncorrelation. During training, label imputation for missing labels is conducted\nas part of the forward propagation process, while during inference, labels are\nregarded as unknowns of a bivariate system of equations and are solved jointly.\nTheoretical analysis guarantees the feasibility of DLL, and extensive\nexperiments are conducted to verify that by explicitly modeling label\ncorrelation and maximizing the utility of available labels, our method makes\nconsistently better predictions than baseline approaches by up to a 10% gain in\nF1-score or MAPE. Remarkably, our method provided with data at a label missing\nrate as high as 60% can achieve similar or even better results than baseline\napproaches at a label missing rate of only 10%.\n","authors":["Mingqian Li","Qiao Han","Yiteng Zhai","Ruifeng Li","Yao Yang","Hongyang Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14375v1","updated":"2024-10-18T11:06:23Z","published":"2024-10-18T11:06:23Z","title":"Fine-Tuning Pre-trained Language Models for Robust Causal Representation\n  Learning","summary":"  The fine-tuning of pre-trained language models (PLMs) has been shown to be\neffective across various domains. By using domain-specific supervised data, the\ngeneral-purpose representation derived from PLMs can be transformed into a\ndomain-specific representation. However, these methods often fail to generalize\nto out-of-domain (OOD) data due to their reliance on non-causal\nrepresentations, often described as spurious features. Existing methods either\nmake use of adjustments with strong assumptions about lack of hidden common\ncauses, or mitigate the effect of spurious features using multi-domain data. In\nthis work, we investigate how fine-tuned pre-trained language models aid\ngeneralizability from single-domain scenarios under mild assumptions, targeting\nmore general and practical real-world scenarios. We show that a robust\nrepresentation can be derived through a so-called causal front-door adjustment,\nbased on a decomposition assumption, using fine-tuned representations as a\nsource of data augmentation. Comprehensive experiments in both synthetic and\nreal-world settings demonstrate the superior generalizability of the proposed\nmethod compared to existing approaches. Our work thus sheds light on the domain\ngeneralization problem by introducing links between fine-tuning and causal\nmechanisms into representation learning.\n","authors":["Jialin Yu","Yuxiang Zhou","Yulan He","Nevin L. Zhang","Ricardo Silva"],"pdf_url":"https://arxiv.org/pdf/2410.14375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01129v4","updated":"2024-10-18T10:46:43Z","published":"2024-08-02T09:18:41Z","title":"A Survey of Mamba","summary":"  As one of the most representative DL techniques, Transformer architecture has\nempowered numerous advanced models, especially the large language models (LLMs)\nthat comprise billions of parameters, becoming a cornerstone in deep learning.\nDespite the impressive achievements, Transformers still face inherent\nlimitations, particularly the time-consuming inference resulting from the\nquadratic computation complexity of attention calculation. Recently, a novel\narchitecture named Mamba, drawing inspiration from classical state space models\n(SSMs), has emerged as a promising alternative for building foundation models,\ndelivering comparable modeling abilities to Transformers while preserving\nnear-linear scalability concerning sequence length. This has sparked an\nincreasing number of studies actively exploring Mamba's potential to achieve\nimpressive performance across diverse domains. Given such rapid evolution,\nthere is a critical need for a systematic review that consolidates existing\nMamba-empowered models, offering a comprehensive understanding of this emerging\nmodel architecture. In this survey, we therefore conduct an in-depth\ninvestigation of recent Mamba-associated studies, covering three main aspects:\nthe advancements of Mamba-based models, the techniques of adapting Mamba to\ndiverse data, and the applications where Mamba can excel. Specifically, we\nfirst review the foundational knowledge of various representative deep learning\nmodels and the details of Mamba-1&2 as preliminaries. Then, to showcase the\nsignificance of Mamba for AI, we comprehensively review the related studies\nfocusing on Mamba models' architecture design, data adaptability, and\napplications. Finally, we present a discussion of current limitations and\nexplore various promising research directions to provide deeper insights for\nfuture investigations.\n","authors":["Haohao Qu","Liangbo Ning","Rui An","Wenqi Fan","Tyler Derr","Hui Liu","Xin Xu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2408.01129v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13458v5","updated":"2024-10-18T10:31:27Z","published":"2023-03-23T17:26:12Z","title":"Optimization Dynamics of Equivariant and Augmented Neural Networks","summary":"  We investigate the optimization of neural networks on symmetric data, and\ncompare the strategy of constraining the architecture to be equivariant to that\nof using data augmentation. Our analysis reveals that that the relative\ngeometry of the admissible and the equivariant layers, respectively, plays a\nkey role. Under natural assumptions on the data, network, loss, and group of\nsymmetries, we show that compatibility of the spaces of admissible layers and\nequivariant layers, in the sense that the corresponding orthogonal projections\ncommute, implies that the sets of equivariant stationary points are identical\nfor the two strategies. If the linear layers of the network also are given a\nunitary parametrization, the set of equivariant layers is even invariant under\nthe gradient flow for augmented models. Our analysis however also reveals that\neven in the latter situation, stationary points may be unstable for augmented\ntraining although they are stable for the manifestly equivariant models.\n","authors":["Oskar Nordenfors","Fredrik Ohlsson","Axel Flinth"],"pdf_url":"https://arxiv.org/pdf/2303.13458v5.pdf","comment":"v4: Some discussions added, along with an updated experiment section.\n  v3: Completely revised manuscript: New framework for neural nets, new main\n  result (involving compability condition), new experiments, new author. v2:\n  Revised manuscript. Mostly small edits, apart from new experiments (see\n  Appendix E)"},{"id":"http://arxiv.org/abs/2310.11244v4","updated":"2024-10-18T10:21:31Z","published":"2023-10-17T13:12:32Z","title":"Entity Matching using Large Language Models","summary":"  Entity matching is the task of deciding whether two entity descriptions refer\nto the same real-world entity. Entity matching is a central step in most data\nintegration pipelines. Many state-of-the-art entity matching methods rely on\npre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks\nof these models for entity matching are that (i) the models require significant\namounts of task-specific training data and (ii) the fine-tuned models are not\nrobust concerning out-of-distribution entities. This paper investigates using\ngenerative large language models (LLMs) as a less task-specific training\ndata-dependent and more robust alternative to PLM-based matchers. The study\ncovers hosted and open-source LLMs which can be run locally. We evaluate these\nmodels in a zero-shot scenario and a scenario where task-specific training data\nis available. We compare different prompt designs and the prompt sensitivity of\nthe models. We show that there is no single best prompt but that the prompt\nneeds to be tuned for each model/dataset combination. We further investigate\n(i) the selection of in-context demonstrations, (ii) the generation of matching\nrules, as well as (iii) fine-tuning LLMs using the same pool of training data.\nOur experiments show that the best LLMs require no or only a few training\nexamples to perform comparably to PLMs that were fine-tuned using thousands of\nexamples. LLM-based matchers further exhibit higher robustness to unseen\nentities. We show that GPT4 can generate structured explanations for matching\ndecisions and can automatically identify potential causes of matching errors by\nanalyzing explanations of wrong decisions. We demonstrate that the model can\ngenerate meaningful textual descriptions of the identified error classes, which\ncan help data engineers to improve entity matching pipelines.\n","authors":["Ralph Peeters","Aaron Steiner","Christian Bizer"],"pdf_url":"https://arxiv.org/pdf/2310.11244v4.pdf","comment":"Published in Proceedings of the 28th International Conference on\n  Extending Database Technology (EDBT), 25th March-28th March, 2025, ISBN\n  978-3-89318-098-1 on OpenProceedings.org"},{"id":"http://arxiv.org/abs/2203.08975v2","updated":"2024-10-18T10:14:58Z","published":"2022-03-16T22:39:46Z","title":"A Survey of Multi-Agent Deep Reinforcement Learning with Communication","summary":"  Communication is an effective mechanism for coordinating the behaviors of\nmultiple agents, broadening their views of the environment, and to support\ntheir collaborations. In the field of multi-agent deep reinforcement learning\n(MADRL), agents can improve the overall learning performance and achieve their\nobjectives by communication. Agents can communicate various types of messages,\neither to all agents or to specific agent groups, or conditioned on specific\nconstraints. With the growing body of research work in MADRL with communication\n(Comm-MADRL), there is a lack of a systematic and structural approach to\ndistinguish and classify existing Comm-MADRL approaches. In this paper, we\nsurvey recent works in the Comm-MADRL field and consider various aspects of\ncommunication that can play a role in designing and developing multi-agent\nreinforcement learning systems. With these aspects in mind, we propose 9\ndimensions along which Comm-MADRL approaches can be analyzed, developed, and\ncompared. By projecting existing works into the multi-dimensional space, we\ndiscover interesting trends. We also propose some novel directions for\ndesigning future Comm-MADRL systems through exploring possible combinations of\nthe dimensions.\n","authors":["Changxi Zhu","Mehdi Dastani","Shihan Wang"],"pdf_url":"https://arxiv.org/pdf/2203.08975v2.pdf","comment":"34 pages, 5 figures, 13 tables; published on Autonomous Agents and\n  Multi-Agent Systems"},{"id":"http://arxiv.org/abs/2410.14347v1","updated":"2024-10-18T09:57:59Z","published":"2024-10-18T09:57:59Z","title":"A Scientific Machine Learning Approach for Predicting and Forecasting\n  Battery Degradation in Electric Vehicles","summary":"  Carbon emissions are rising at an alarming rate, posing a significant threat\nto global efforts to mitigate climate change. Electric vehicles have emerged as\na promising solution, but their reliance on lithium-ion batteries introduces\nthe critical challenge of battery degradation. Accurate prediction and\nforecasting of battery degradation over both short and long time spans are\nessential for optimizing performance, extending battery life, and ensuring\neffective long-term energy management. This directly influences the\nreliability, safety, and sustainability of EVs, supporting their widespread\nadoption and aligning with key UN SDGs. In this paper, we present a novel\napproach to the prediction and long-term forecasting of battery degradation\nusing Scientific Machine Learning framework which integrates domain knowledge\nwith neural networks, offering more interpretable and scientifically grounded\nsolutions for both predicting short-term battery health and forecasting\ndegradation over extended periods. This hybrid approach captures both known and\nunknown degradation dynamics, improving predictive accuracy while reducing data\nrequirements. We incorporate ground-truth data to inform our models, ensuring\nthat both the predictions and forecasts reflect practical conditions. The model\nachieved MSE of 9.90 with the UDE and 11.55 with the NeuralODE, in experimental\ndata, a loss of 1.6986 with the UDE, and a MSE of 2.49 in the NeuralODE,\ndemonstrating the enhanced precision of our approach. This integration of\ndata-driven insights with SciML's strengths in interpretability and scalability\nallows for robust battery management. By enhancing battery longevity and\nminimizing waste, our approach contributes to the sustainability of energy\nsystems and accelerates the global transition toward cleaner, more responsible\nenergy solutions, aligning with the UN's SDG agenda.\n","authors":["Sharv Murgai","Hrishikesh Bhagwat","Raj Abhijit Dandekar","Rajat Dandekar","Sreedath Panat"],"pdf_url":"https://arxiv.org/pdf/2410.14347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14334v1","updated":"2024-10-18T09:44:35Z","published":"2024-10-18T09:44:35Z","title":"Evaluating the evaluators: Towards human-aligned metrics for missing\n  markers reconstruction","summary":"  Animation data is often obtained through optical motion capture systems,\nwhich utilize a multitude of cameras to establish the position of optical\nmarkers. However, system errors or occlusions can result in missing markers,\nthe manual cleaning of which can be time-consuming. This has sparked interest\nin machine learning-based solutions for missing marker reconstruction in the\nacademic community. Most academic papers utilize a simplistic mean square error\nas the main metric. In this paper, we show that this metric does not correlate\nwith subjective perception of the fill quality. We introduce and evaluate a set\nof better-correlated metrics that can drive progress in the field.\n","authors":["Taras Kucherenko","Derek Peristy","Judith Bütepage"],"pdf_url":"https://arxiv.org/pdf/2410.14334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11677v2","updated":"2024-10-18T09:41:53Z","published":"2024-10-15T15:14:22Z","title":"Understanding Likelihood Over-optimisation in Direct Alignment\n  Algorithms","summary":"  Direct Alignment Algorithms (DAAs), such as Direct Preference Optimisation\n(DPO) and Identity Preference Optimisation (IPO), have emerged as alternatives\nto online Reinforcement Learning from Human Feedback (RLHF) algorithms such as\nProximal Policy Optimisation (PPO) for aligning language models to human\npreferences, without the need for explicit reward modelling. These methods\ngenerally aim to increase the likelihood of generating better (preferred)\ncompletions while discouraging worse (non-preferred) ones, while staying close\nto the original model's behaviour. In this work, we explore the relationship\nbetween completion likelihood and model performance in state-of-the-art DAAs,\nand identify a critical issue of likelihood over-optimisation. Contrary to\nexpectations, we find that higher likelihood of better completions and larger\nmargins between better and worse completion likelihoods do not necessarily lead\nto better performance, and may even degrade it. Our analysis reveals that while\nhigher likelihood correlates with better memorisation of factual knowledge\npatterns, a slightly lower completion likelihood tends to improve output\ndiversity, thus leading to better generalisation to unseen scenarios. Moreover,\nwe identify two key indicators that signal when over-optimised output diversity\nbegins to harm performance: Decreasing Entropy over Top-k Tokens and\nDiminishing Top-k Probability Mass. Our experimental results validate that\nthese indicators are reliable signs of declining performance under different\nregularisations, helping prevent over-optimisation and improve alignment with\nhuman preferences.\n","authors":["Zhengyan Shi","Sander Land","Acyr Locatelli","Matthieu Geist","Max Bartolo"],"pdf_url":"https://arxiv.org/pdf/2410.11677v2.pdf","comment":"Preprint Version"},{"id":"http://arxiv.org/abs/2410.14326v1","updated":"2024-10-18T09:37:38Z","published":"2024-10-18T09:37:38Z","title":"Fast proxy centers for Jeffreys centroids: The Jeffreys-Fisher-Rao and\n  the inductive Gauss-Bregman centers","summary":"  The symmetric Kullback-Leibler centroid also called the Jeffreys centroid of\na set of mutually absolutely continuous probability distributions on a measure\nspace provides a notion of centrality which has proven useful in many tasks\nincluding information retrieval, information fusion, and clustering in image,\nvideo and sound processing. However, the Jeffreys centroid is not available in\nclosed-form for sets of categorical or normal distributions, two widely used\nstatistical models, and thus need to be approximated numerically in practice.\nIn this paper, we first propose the new Jeffreys-Fisher-Rao center defined as\nthe Fisher-Rao midpoint of the sided Kullback-Leibler centroids as a plug-in\nreplacement of the Jeffreys centroid. This Jeffreys-Fisher-Rao center admits a\ngeneric formula for uni-parameter exponential family distributions, and\nclosed-form formula for categorical and normal distributions, matches exactly\nthe Jeffreys centroid for same-mean normal distributions, and is experimentally\nobserved in practice to be close to the Jeffreys centroid. Second, we define a\nnew type of inductive centers generalizing the principle of Gauss\narithmetic-geometric double sequence mean for pairs of densities of any given\nexponential family. This center is shown experimentally to approximate very\nwell the Jeffreys centroid and is suggested to use when the Jeffreys-Fisher-Rao\ncenter is not available in closed form. Moreover, this Gauss-Bregman inductive\ncenter always converges and matches the Jeffreys centroid for sets of same-mean\nnormal distributions. We report on our experiments demonstrating the use of the\nJeffreys-Fisher-Rao and Gauss-Bregman centers instead of the Jeffreys centroid.\nFinally, we conclude this work by reinterpreting these fast proxy centers of\nJeffreys centroids under the lens of dually flat spaces in information\ngeometry.\n","authors":["Frank Nielsen"],"pdf_url":"https://arxiv.org/pdf/2410.14326v1.pdf","comment":"35 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.14325v1","updated":"2024-10-18T09:37:05Z","published":"2024-10-18T09:37:05Z","title":"Debiasing Mini-Batch Quadratics for Applications in Deep Learning","summary":"  Quadratic approximations form a fundamental building block of machine\nlearning methods. E.g., second-order optimizers try to find the Newton step\ninto the minimum of a local quadratic proxy to the objective function; and the\nsecond-order approximation of a network's loss function can be used to quantify\nthe uncertainty of its outputs via the Laplace approximation. When computations\non the entire training set are intractable - typical for deep learning - the\nrelevant quantities are computed on mini-batches. This, however, distorts and\nbiases the shape of the associated stochastic quadratic approximations in an\nintricate way with detrimental effects on applications. In this paper, we (i)\nshow that this bias introduces a systematic error, (ii) provide a theoretical\nexplanation for it, (iii) explain its relevance for second-order optimization\nand uncertainty quantification via the Laplace approximation in deep learning,\nand (iv) develop and evaluate debiasing strategies.\n","authors":["Lukas Tatzel","Bálint Mucsányi","Osane Hackel","Philipp Hennig"],"pdf_url":"https://arxiv.org/pdf/2410.14325v1.pdf","comment":"Main text (including references): 13 pages, 6 figures; Supplements:\n  25 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.14315v1","updated":"2024-10-18T09:21:10Z","published":"2024-10-18T09:21:10Z","title":"Optimizing importance weighting in the presence of sub-population shifts","summary":"  A distribution shift between the training and test data can severely harm\nperformance of machine learning models. Importance weighting addresses this\nissue by assigning different weights to data points during training. We argue\nthat existing heuristics for determining the weights are suboptimal, as they\nneglect the increase of the variance of the estimated model due to the finite\nsample size of the training data. We interpret the optimal weights in terms of\na bias-variance trade-off, and propose a bi-level optimization procedure in\nwhich the weights and model parameters are optimized simultaneously. We apply\nthis optimization to existing importance weighting techniques for last-layer\nretraining of deep neural networks in the presence of sub-population shifts and\nshow empirically that optimizing weights significantly improves generalization\nperformance.\n","authors":["Floris Holstege","Bram Wouters","Noud van Giersbergen","Cees Diks"],"pdf_url":"https://arxiv.org/pdf/2410.14315v1.pdf","comment":"Preprint. Currently under review"},{"id":"http://arxiv.org/abs/2405.18921v2","updated":"2024-10-18T09:05:18Z","published":"2024-05-29T09:24:25Z","title":"GLANCE: Global Actions in a Nutshell for Counterfactual Explainability","summary":"  The widespread deployment of machine learning systems in critical real-world\ndecision-making applications has highlighted the urgent need for counterfactual\nexplainability methods that operate effectively. Global counterfactual\nexplanations, expressed as actions to offer recourse, aim to provide succinct\nexplanations and insights applicable to large population subgroups.\nEffectiveness is measured by the fraction of the population that is provided\nrecourse, ensuring that the actions benefit as many individuals as possible.\nKeeping the cost of actions low ensures the proposed recourse actions remain\npractical and actionable. Limiting the number of actions that provide global\ncounterfactuals is essential to maximize interpretability. The primary\nchallenge, therefore, is balancing these trade-offs, i.e., maximizing\neffectiveness, minimizing cost, while maintaining a small number of actions. We\nintroduce GLANCE, a versatile and adaptive framework, comprising two\nalgorithms, that allows the careful balancing of the trade-offs among the three\nkey objectives, with the size objective functioning as a tunable parameter to\nkeep the actions few and easy to interpret. C-GLANCE employs a clustering\napproach that considers both the feature space and the space of counterfactual\nactions, thereby accounting for the distribution of points in a way that aligns\nwith the structure of the model. T-GLANCE provides additional features to\nenhance flexibility. It employs a tree-based approach, that allows users to\nspecify split features, to build a decision tree with a single counterfactual\naction at each node that can be used as a subgroup policy. Our extensive\nexperimental evaluation demonstrates that our method consistently shows greater\nrobustness and performance compared to existing methods across various datasets\nand models.\n","authors":["Loukas Kavouras","Eleni Psaroudaki","Konstantinos Tsopelas","Dimitrios Rontogiannis","Nikolaos Theologitis","Dimitris Sacharidis","Giorgos Giannopoulos","Dimitrios Tomaras","Kleopatra Markou","Dimitrios Gunopulos","Dimitris Fotakis","Ioannis Emiris"],"pdf_url":"https://arxiv.org/pdf/2405.18921v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12013v2","updated":"2024-10-18T08:57:30Z","published":"2024-06-26T12:33:34Z","title":"Dating ancient manuscripts using radiocarbon and AI-based writing style\n  analysis","summary":"  Determining the chronology of ancient handwritten manuscripts is essential\nfor reconstructing the evolution of ideas. For the Dead Sea Scrolls, this is\nparticularly important. However, there is an almost complete lack of\ndate-bearing manuscripts evenly distributed across the timeline and written in\nsimilar scripts available for palaeographic comparison. Here, we present Enoch,\na state-of-the-art AI-based date-prediction model, trained on the basis of new\nradiocarbon-dated samples of the scrolls. Enoch uses established\nhandwriting-style descriptors and applies Bayesian ridge regression. The\nchallenge of this study is that the number of radiocarbon-dated manuscripts is\nsmall, while current machine learning requires an abundance of training data.\nWe show that by using combined angular and allographic writing style feature\nvectors and applying Bayesian ridge regression, Enoch could predict the\nradiocarbon-based dates from style, supported by leave-one-out validation, with\nvaried MAEs of 27.9 to 30.7 years relative to the radiocarbon dating. Enoch was\nthen used to estimate the dates of 135 unseen manuscripts, revealing that 79\nper cent of the samples were considered 'realistic' upon palaeographic post-hoc\nevaluation. We present a new chronology of the scrolls. The radiocarbon ranges\nand Enoch's style-based predictions are often older than the traditionally\nassumed palaeographic estimates. In the range of 300-50 BCE, Enoch's date\nprediction provides an improved granularity. The study is in line with current\ndevelopments in multimodal machine-learning techniques, and the methods can be\nused for date prediction in other partially-dated manuscript collections. This\nresearch shows how Enoch's quantitative, probability-based approach can be a\ntool for palaeographers and historians, re-dating ancient Jewish key texts and\ncontributing to current debates on Jewish and Christian origins.\n","authors":["Mladen Popović","Maruf A. Dhali","Lambert Schomaker","Johannes van der Plicht","Kaare Lund Rasmussen","Jacopo La Nasa","Ilaria Degano","Maria Perla Colombini","Eibert Tigchelaar"],"pdf_url":"https://arxiv.org/pdf/2407.12013v2.pdf","comment":"16 pages of main article, 103 pages of supplementary materials; the\n  first version of this article is originally prepared in July 2023 after the\n  completion of all the experiments"},{"id":"http://arxiv.org/abs/2410.13754v2","updated":"2024-10-18T08:56:52Z","published":"2024-10-17T16:52:28Z","title":"MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures","summary":"  Perceiving and generating diverse modalities are crucial for AI models to\neffectively learn from and engage with real-world signals, necessitating\nreliable evaluations for their development. We identify two major issues in\ncurrent evaluations: (1) inconsistent standards, shaped by different\ncommunities with varying protocols and maturity levels; and (2) significant\nquery, grading, and generalization biases. To address these, we introduce\nMixEval-X, the first any-to-any, real-world benchmark designed to optimize and\nstandardize evaluations across diverse input and output modalities. We propose\nmulti-modal benchmark mixture and adaptation-rectification pipelines to\nreconstruct real-world task distributions, ensuring evaluations generalize\neffectively to real-world use cases. Extensive meta-evaluations show our\napproach effectively aligns benchmark samples with real-world task\ndistributions. Meanwhile, MixEval-X's model rankings correlate strongly with\nthat of crowd-sourced real-world evaluations (up to 0.98) while being much more\nefficient. We provide comprehensive leaderboards to rerank existing models and\norganizations and offer insights to enhance understanding of multi-modal\nevaluations and inform future research.\n","authors":["Jinjie Ni","Yifan Song","Deepanway Ghosal","Bo Li","David Junhao Zhang","Xiang Yue","Fuzhao Xue","Zian Zheng","Kaichen Zhang","Mahir Shah","Kabir Jain","Yang You","Michael Shieh"],"pdf_url":"https://arxiv.org/pdf/2410.13754v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05172v3","updated":"2024-10-18T08:44:31Z","published":"2023-06-08T13:11:20Z","title":"FLEdge: Benchmarking Federated Machine Learning Applications in Edge\n  Computing Systems","summary":"  Federated Learning (FL) has become a viable technique for realizing\nprivacy-enhancing distributed deep learning on the network edge. Heterogeneous\nhardware, unreliable client devices, and energy constraints often characterize\nedge computing systems. In this paper, we propose FLEdge, which complements\nexisting FL benchmarks by enabling a systematic evaluation of client\ncapabilities. We focus on computational and communication bottlenecks, client\nbehavior, and data security implications. Our experiments with models varying\nfrom 14K to 80M trainable parameters are carried out on dedicated hardware with\nemulated network characteristics and client behavior. We find that\nstate-of-the-art embedded hardware has significant memory bottlenecks, leading\nto 4x longer processing times than on modern data center GPUs.\n","authors":["Herbert Woisetschläger","Alexander Isenko","Ruben Mayer","Shiqiang Wang","Hans-Arno Jacobsen"],"pdf_url":"https://arxiv.org/pdf/2306.05172v3.pdf","comment":"Paper accepted for publication at the ACM/IFIP Middleware Conference\n  2024. Please cite the published version via\n  https://doi.org/10.1145/3652892.3700751"},{"id":"http://arxiv.org/abs/2410.14281v1","updated":"2024-10-18T08:38:12Z","published":"2024-10-18T08:38:12Z","title":"PTR: A Pre-trained Language Model for Trajectory Recovery","summary":"  Spatiotemporal trajectory data is vital for web-of-things services and is\nextensively collected and analyzed by web-based hardware and platforms.\nHowever, issues such as service interruptions and network instability often\nlead to sparsely recorded trajectories, resulting in a loss of detailed\nmovement data. As a result, recovering these trajectories to restore missing\ninformation becomes essential. Despite progress, several challenges remain\nunresolved. First, the lack of large-scale dense trajectory data hampers the\nperformance of existing deep learning methods, which rely heavily on abundant\ndata for supervised training. Second, current methods struggle to generalize\nacross sparse trajectories with varying sampling intervals, necessitating\nseparate re-training for each interval and increasing computational costs.\nThird, external factors crucial for the recovery of missing points are not\nfully incorporated.\n  To address these challenges, we propose a framework called PTR. This\nframework mitigates the issue of limited dense trajectory data by leveraging\nthe capabilities of pre-trained language models (PLMs). PTR incorporates an\nexplicit trajectory prompt and is trained on datasets with multiple sampling\nintervals, enabling it to generalize effectively across different intervals in\nsparse trajectories. To capture external factors, we introduce an implicit\ntrajectory prompt that models road conditions, providing richer information for\nrecovering missing points. Additionally, we present a trajectory embedder that\nencodes trajectory points and transforms the embeddings of both observed and\nmissing points into a format comprehensible to PLMs. Experimental results on\ntwo public trajectory datasets with three sampling intervals demonstrate the\nefficacy and scalability of PTR.\n","authors":["Tonglong Wei","Yan Lin","Youfang Lin","Shengnan Guo","Jilin Hu","Gao Cong","Huaiyu Wan"],"pdf_url":"https://arxiv.org/pdf/2410.14281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13196v2","updated":"2024-10-18T08:33:19Z","published":"2024-10-17T03:56:12Z","title":"Context-Enhanced Multi-View Trajectory Representation Learning: Bridging\n  the Gap through Self-Supervised Models","summary":"  Modeling trajectory data with generic-purpose dense representations has\nbecome a prevalent paradigm for various downstream applications, such as\ntrajectory classification, travel time estimation and similarity computation.\nHowever, existing methods typically rely on trajectories from a single spatial\nview, limiting their ability to capture the rich contextual information that is\ncrucial for gaining deeper insights into movement patterns across different\ngeospatial contexts. To this end, we propose MVTraj, a novel multi-view\nmodeling method for trajectory representation learning. MVTraj integrates\ndiverse contextual knowledge, from GPS to road network and points-of-interest\nto provide a more comprehensive understanding of trajectory data. To align the\nlearning process across multiple views, we utilize GPS trajectories as a bridge\nand employ self-supervised pretext tasks to capture and distinguish movement\npatterns across different spatial views. Following this, we treat trajectories\nfrom different views as distinct modalities and apply a hierarchical\ncross-modal interaction module to fuse the representations, thereby enriching\nthe knowledge derived from multiple sources. Extensive experiments on\nreal-world datasets demonstrate that MVTraj significantly outperforms existing\nbaselines in tasks associated with various spatial views, validating its\neffectiveness and practical utility in spatio-temporal modeling.\n","authors":["Tangwen Qian","Junhe Li","Yile Chen","Gao Cong","Tao Sun","Fei Wang","Yongjun Xu"],"pdf_url":"https://arxiv.org/pdf/2410.13196v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14270v1","updated":"2024-10-18T08:25:28Z","published":"2024-10-18T08:25:28Z","title":"Stochastic Quasi-Newton Optimization in Large Dimensions Including Deep\n  Network Training","summary":"  Our proposal is on a new stochastic optimizer for non-convex and possibly\nnon-smooth objective functions typically defined over large dimensional design\nspaces. Towards this, we have tried to bridge noise-assisted global search and\nfaster local convergence, the latter being the characteristic feature of a\nNewton-like search. Our specific scheme -- acronymed FINDER (Filtering Informed\nNewton-like and Derivative-free Evolutionary Recursion), exploits the nonlinear\nstochastic filtering equations to arrive at a derivative-free update that has\nresemblance with the Newton search employing the inverse Hessian of the\nobjective function. Following certain simplifications of the update to enable a\nlinear scaling with dimension and a few other enhancements, we apply FINDER to\na range of problems, starting with some IEEE benchmark objective functions to a\ncouple of archetypal data-driven problems in deep networks to certain cases of\nphysics-informed deep networks. The performance of the new method vis-\\'a-vis\nthe well-known Adam and a few others bears evidence to its promise and\npotentialities for large dimensional optimization problems of practical\ninterest.\n","authors":["Uttam Suman","Mariya Mamajiwala","Mukul Saxena","Ankit Tyagi","Debasish Roy"],"pdf_url":"https://arxiv.org/pdf/2410.14270v1.pdf","comment":"19 pages, 12 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.14269v1","updated":"2024-10-18T08:24:07Z","published":"2024-10-18T08:24:07Z","title":"On time series clustering with k-means","summary":"  There is a long history of research into time series clustering using\ndistance-based partitional clustering. Many of the most popular algorithms\nadapt k-means (also known as Lloyd's algorithm) to exploit time dependencies in\nthe data by specifying a time series distance function. However, these\nalgorithms are often presented with k-means configured in various ways,\naltering key parameters such as the initialisation strategy. This variability\nmakes it difficult to compare studies because k-means is known to be highly\nsensitive to its configuration. To address this, we propose a standard\nLloyd's-based model for TSCL that adopts an end-to-end approach, incorporating\na specialised distance function not only in the assignment step but also in the\ninitialisation and stopping criteria. By doing so, we create a unified\nstructure for comparing seven popular Lloyd's-based TSCL algorithms. This\ncommon framework enables us to more easily attribute differences in clustering\nperformance to the distance function itself, rather than variations in the\nk-means configuration.\n","authors":["Christopher Holder","Anthony Bagnall","Jason Lines"],"pdf_url":"https://arxiv.org/pdf/2410.14269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14268v1","updated":"2024-10-18T08:22:07Z","published":"2024-10-18T08:22:07Z","title":"MoDification: Mixture of Depths Made Easy","summary":"  Long-context efficiency has recently become a trending topic in serving large\nlanguage models (LLMs). And mixture of depths (MoD) is proposed as a perfect\nfit to bring down both latency and memory. In this paper, however, we discover\nthat MoD can barely transform existing LLMs without costly training over an\nextensive number of tokens. To enable the transformations from any LLMs to MoD\nones, we showcase top-k operator in MoD should be promoted to threshold-p\noperator, and refinement to architecture and data should also be crafted along.\nAll these designs form our method termed MoDification. Through a comprehensive\nset of experiments covering model scales from 3B to 70B, we exhibit\nMoDification strikes an excellent balance between efficiency and effectiveness.\nMoDification can achieve up to ~1.2x speedup in latency and ~1.8x reduction in\nmemory compared to original LLMs especially in long-context applications.\n","authors":["Chen Zhang","Meizhi Zhong","Qimeng Wang","Xuantao Lu","Zheyu Ye","Chengqiang Lu","Yan Gao","Yao Hu","Kehai Chen","Min Zhang","Dawei Song"],"pdf_url":"https://arxiv.org/pdf/2410.14268v1.pdf","comment":"12 pages, 9 figures, 5 tables, work in progress"},{"id":"http://arxiv.org/abs/2403.13784v6","updated":"2024-10-18T08:20:22Z","published":"2024-03-20T17:47:08Z","title":"The Model Openness Framework: Promoting Completeness and Openness for\n  Reproducibility, Transparency, and Usability in Artificial Intelligence","summary":"  Generative artificial intelligence (AI) offers numerous opportunities for\nresearch and innovation, but its commercialization has raised concerns about\nthe transparency and safety of frontier AI models. Most models lack the\nnecessary components for full understanding, auditing, and reproducibility, and\nsome model producers use restrictive licenses whilst claiming that their models\nare \"open source\". To address these concerns, we introduce the Model Openness\nFramework (MOF), a three-tiered ranked classification system that rates machine\nlearning models based on their completeness and openness, following open\nscience principles. For each MOF class, we specify code, data, and\ndocumentation components of the model development lifecycle that must be\nreleased and under which open licenses. In addition, the Model Openness Tool\n(MOT) provides a user-friendly reference implementation to evaluate the\nopenness and completeness of models against the MOF classification system.\nTogether, the MOF and MOT provide timely practical guidance for (i) model\nproducers to enhance the openness and completeness of their publicly-released\nmodels, and (ii) model consumers to identify open models and their constituent\ncomponents that can be permissively used, studied, modified, and redistributed.\nThrough the MOF, we seek to establish completeness and openness as core tenets\nof responsible AI research and development, and to promote best practices in\nthe burgeoning open AI ecosystem.\n","authors":["Matt White","Ibrahim Haddad","Cailean Osborne","Xiao-Yang Yanglet Liu","Ahmed Abdelmonsef","Sachin Varghese","Arnaud Le Hors"],"pdf_url":"https://arxiv.org/pdf/2403.13784v6.pdf","comment":"28 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2406.00125v3","updated":"2024-10-18T08:18:36Z","published":"2024-05-31T18:32:46Z","title":"TotalVibeSegmentator: Full Body MRI Segmentation for the NAKO and UK\n  Biobank","summary":"  Objectives: To present a publicly available torso segmentation network for\nlarge epidemiology datasets on volumetric interpolated breath-hold examination\n(VIBE) images. Materials & Methods: We extracted preliminary segmentations from\nTotalSegmentator, spine, and body composition networks for VIBE images, then\nimproved them iteratively and retrained a nnUNet network. Using subsets of NAKO\n(85 subjects) and UK Biobank (16 subjects), we evaluated with Dice-score on a\nholdout set (12 subjects) and existing organ segmentation approach (1000\nsubjects), generating 71 semantic segmentation types for VIBE images. We\nprovide an additional network for the vertebra segments 22 individual vertebra\ntypes. Results: We achieved an average Dice score of 0.89 +- 0.07 overall 71\nsegmentation labels. We scored > 0.90 Dice-score on the abdominal organs except\nfor the pancreas with a Dice of 0.70. Conclusion: Our work offers a detailed\nand refined publicly available full torso segmentation on VIBE images.\n","authors":["Robert Graf","Paul-Sören Platzek","Evamaria Olga Riedel","Constanze Ramschütz","Sophie Starck","Hendrik Kristian Möller","Matan Atad","Henry Völzke","Robin Bülow","Carsten Oliver Schmidt","Julia Rüdebusch","Matthias Jung","Marco Reisert","Jakob Weiss","Maximilian Löffler","Fabian Bamberg","Bene Wiestler","Johannes C. Paetzold","Daniel Rueckert","Jan Stefan Kirschke"],"pdf_url":"https://arxiv.org/pdf/2406.00125v3.pdf","comment":"https://github.com/robert-graf/TotalVibeSegmentator"},{"id":"http://arxiv.org/abs/2402.16346v3","updated":"2024-10-18T08:09:14Z","published":"2024-02-26T07:00:24Z","title":"Boosting Graph Pooling with Persistent Homology","summary":"  Recently, there has been an emerging trend to integrate persistent homology\n(PH) into graph neural networks (GNNs) to enrich expressive power. However,\nnaively plugging PH features into GNN layers always results in marginal\nimprovement with low interpretability. In this paper, we investigate a novel\nmechanism for injecting global topological invariance into pooling layers using\nPH, motivated by the observation that filtration operation in PH naturally\naligns graph pooling in a cut-off manner. In this fashion, message passing in\nthe coarsened graph acts along persistent pooled topology, leading to improved\nperformance. Experimentally, we apply our mechanism to a collection of graph\npooling methods and observe consistent and substantial performance gain over\nseveral popular datasets, demonstrating its wide applicability and flexibility.\n","authors":["Chaolong Ying","Xinjian Zhao","Tianshu Yu"],"pdf_url":"https://arxiv.org/pdf/2402.16346v3.pdf","comment":"Published at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.14257v1","updated":"2024-10-18T08:05:37Z","published":"2024-10-18T08:05:37Z","title":"Revisiting SLO and Goodput Metrics in LLM Serving","summary":"  Large language models (LLMs) have achieved remarkable performance and are\nwidely deployed in various applications, while the serving of LLM inference has\nraised concerns about user experience and serving throughput. Accordingly,\nservice level objectives (SLOs) and goodput-the number of requests that meet\nSLOs per second-are introduced to evaluate the performance of LLM serving.\nHowever, existing metrics fail to capture the nature of user experience. We\nobserve two ridiculous phenomena in existing metrics: 1) delaying token\ndelivery can smooth the tail time between tokens (tail TBT) of a request and 2)\ndropping the request that fails to meet the SLOs midway can improve goodput.\n  In this paper, we revisit SLO and goodput metrics in LLM serving and propose\na unified metric framework smooth goodput including SLOs and goodput to reflect\nthe nature of user experience in LLM serving. The framework can adapt to\nspecific goals of different tasks by setting parameters. We re-evaluate the\nperformance of different LLM serving systems under multiple workloads based on\nthis unified framework and provide possible directions for future optimization\nof existing strategies. We hope that this framework can provide a unified\nstandard for evaluating LLM serving and foster researches in the field of LLM\nserving optimization to move in a cohesive direction.\n","authors":["Zhibin Wang","Shipeng Li","Yuhang Zhou","Xue Li","Rong Gu","Nguyen Cam-Tu","Chen Tian","Sheng Zhong"],"pdf_url":"https://arxiv.org/pdf/2410.14257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20988v3","updated":"2024-10-18T08:05:18Z","published":"2024-05-31T16:34:11Z","title":"Communication-Efficient Distributed Deep Learning via Federated Dynamic\n  Averaging","summary":"  Driven by the ever-growing volume and decentralized nature of data, coupled\nwith the need to harness this data and generate knowledge from it, has led to\nthe extensive use of distributed deep learning (DDL) techniques for training.\nThese techniques rely on local training that is performed at the distributed\nnodes based on locally collected data, followed by a periodic synchronization\nprocess that combines these models to create a global model. However, frequent\nsynchronization of DL models, encompassing millions to many billions of\nparameters, creates a communication bottleneck, severely hindering scalability.\nWorse yet, DDL algorithms typically waste valuable bandwidth, and make\nthemselves less practical in bandwidth-constrained federated settings, by\nrelying on overly simplistic, periodic, and rigid synchronization schedules.\nThese drawbacks also have a direct impact on the time required for the training\nprocess, necessitating excessive time for data communication. To address these\nshortcomings, we propose Federated Dynamic Averaging (FDA), a\ncommunication-efficient DDL strategy that dynamically triggers synchronization\nbased on the value of the model variance. In essence, the costly\nsynchronization step is triggered only if the local models, which are\ninitialized from a common global model after each synchronization, have\nsignificantly diverged. This decision is facilitated by the communication of a\nsmall local state from each distributed node/worker. Through extensive\nexperiments across a wide range of learning tasks we demonstrate that FDA\nreduces communication cost by orders of magnitude, compared to both traditional\nand cutting-edge communication-efficient algorithms. Additionally, we show that\nFDA maintains robust performance across diverse data heterogeneity settings.\n","authors":["Michail Theologitis","Georgios Frangias","Georgios Anestis","Vasilis Samoladas","Antonios Deligiannakis"],"pdf_url":"https://arxiv.org/pdf/2405.20988v3.pdf","comment":"Accepted as research paper at EDBT 2025"},{"id":"http://arxiv.org/abs/2311.16984v4","updated":"2024-10-18T08:04:36Z","published":"2023-11-28T17:35:38Z","title":"FedECA: A Federated External Control Arm Method for Causal Inference\n  with Time-To-Event Data in Distributed Settings","summary":"  External control arms (ECA) can inform the early clinical development of\nexperimental drugs and provide efficacy evidence for regulatory approval.\nHowever, the main challenge in implementing ECA lies in accessing real-world or\nhistorical clinical trials data. Indeed, regulations protecting patients'\nrights by strictly controlling data processing make pooling data from multiple\nsources in a central server often difficult. To address these limitations, we\ndevelop a new method, 'FedECA' that leverages federated learning (FL) to enable\ninverse probability of treatment weighting (IPTW) for time-to-event outcomes on\nseparate cohorts without needing to pool data. To showcase the potential of\nFedECA, we apply it in different settings of increasing complexity culminating\nwith a real-world use-case in which FedECA provides evidence for a differential\neffect between two drugs that would have otherwise gone unnoticed. By sharing\nour code, we hope FedECA will foster the creation of federated research\nnetworks and thus accelerate drug development.\n","authors":["Jean Ogier du Terrail","Quentin Klopfenstein","Honghao Li","Imke Mayer","Nicolas Loiseau","Mohammad Hallal","Michael Debouver","Thibault Camalon","Thibault Fouqueray","Jorge Arellano Castro","Zahia Yanes","Laetitia Dahan","Julien Taïeb","Pierre Laurent-Puig","Jean-Baptiste Bachet","Shulin Zhao","Remy Nicolle","Jérome Cros","Daniel Gonzalez","Robert Carreras-Torres","Adelaida Garcia Velasco","Kawther Abdilleh","Sudheer Doss","Félix Balazard","Mathieu Andreux"],"pdf_url":"https://arxiv.org/pdf/2311.16984v4.pdf","comment":"code available at: https://github.com/owkin/fedeca, bug in SMD\n  computation present in v1 and v2 has been fixed, many experiments on real\n  data have been added + fix in YODA experiments using imputed data instead of\n  raw data as well as typos and affiliations fix"},{"id":"http://arxiv.org/abs/2410.14254v1","updated":"2024-10-18T08:04:31Z","published":"2024-10-18T08:04:31Z","title":"RAZOR: Refining Accuracy by Zeroing Out Redundancies","summary":"  In many application domains, the proliferation of sensors and devices is\ngenerating vast volumes of data, imposing significant pressure on existing data\nanalysis and data mining techniques. Nevertheless, an increase in data volume\ndoes not inherently imply an increase in informational content, as a\nsubstantial portion may be redundant or represent noise. This challenge is\nparticularly evident in the deep learning domain, where the utility of\nadditional data is contingent on its informativeness. In the absence of such,\nlarger datasets merely exacerbate the computational cost and complexity of the\nlearning process. To address these challenges, we propose RAZOR, a novel\ninstance selection technique designed to extract a significantly smaller yet\nsufficiently informative subset from a larger set of instances without\ncompromising the learning process. RAZOR has been specifically engineered to be\nrobust, efficient, and scalable, making it suitable for large-scale datasets.\nUnlike many techniques in the literature, RAZOR is capable of operating in both\nsupervised and unsupervised settings. Experimental results demonstrate that\nRAZOR outperforms recent state-of-the-art techniques in terms of both\neffectiveness and efficiency.\n","authors":["Daniel Riccio","Genoveffa Tortora","Mara Sangiovanni"],"pdf_url":"https://arxiv.org/pdf/2410.14254v1.pdf","comment":"17 pages, 3 figures"},{"id":"http://arxiv.org/abs/2306.08670v5","updated":"2024-10-18T08:00:31Z","published":"2023-06-14T17:59:15Z","title":"Simple Opinion Dynamics for No-Regret Learning","summary":"  We study a cooperative multi-agent bandit setting in the distributed GOSSIP\nmodel: in every round, each of $n$ agents chooses an action from a common set,\nobserves the action's corresponding reward, and subsequently exchanges\ninformation with a single randomly chosen neighbor, which may inform its choice\nin the next round. We introduce and analyze families of memoryless and\ntime-independent protocols for this setting, inspired by opinion dynamics that\nare well-studied for other algorithmic tasks in the GOSSIP model. For\nstationary reward settings, we prove for the first time that these simple\nprotocols exhibit best-of-both-worlds behavior, simultaneously obtaining\nconstant cumulative regret scaling like $R(T)/T = \\widetilde O(1/T)$, and also\nreaching consensus on the highest-mean action within $\\widetilde O(\\sqrt{n})$\nrounds. We obtain these results by showing a new connection between the global\nevolution of these decentralized protocols and a class of zero-sum\nmultiplicative weights update} processes. Using this connection, we establish a\ngeneral framework for analyzing the population-level regret and other\nproperties of our protocols. Finally, we show our protocols are also\nsurprisingly robust to adversarial rewards, and in this regime we obtain\nsublinear regret scaling like $R(T)/T = \\widetilde O(1/\\sqrt{T})$ as long as\nthe number of rounds does not grow too fast as a function of $n$.\n","authors":["John Lazarsfeld","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2306.08670v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14242v1","updated":"2024-10-18T07:47:59Z","published":"2024-10-18T07:47:59Z","title":"Pseudo-label Refinement for Improving Self-Supervised Learning Systems","summary":"  Self-supervised learning systems have gained significant attention in recent\nyears by leveraging clustering-based pseudo-labels to provide supervision\nwithout the need for human annotations. However, the noise in these\npseudo-labels caused by the clustering methods poses a challenge to the\nlearning process leading to degraded performance. In this work, we propose a\npseudo-label refinement (SLR) algorithm to address this issue. The cluster\nlabels from the previous epoch are projected to the current epoch\ncluster-labels space and a linear combination of the new label and the\nprojected label is computed as a soft refined label containing the information\nfrom the previous epoch clusters as well as from the current epoch. In contrast\nto the common practice of using the maximum value as a cluster/class indicator,\nwe employ hierarchical clustering on these soft pseudo-labels to generate\nrefined hard-labels. This approach better utilizes the information embedded in\nthe soft labels, outperforming the simple maximum value approach for hard label\ngeneration. The effectiveness of the proposed SLR algorithm is evaluated in the\ncontext of person re-identification (Re-ID) using unsupervised domain\nadaptation (UDA). Experimental results demonstrate that the modified Re-ID\nbaseline, incorporating the SLR algorithm, achieves significantly improved mean\nAverage Precision (mAP) performance in various UDA tasks, including\nreal-to-synthetic, synthetic-to-real, and different real-to-real scenarios.\nThese findings highlight the efficacy of the SLR algorithm in enhancing the\nperformance of self-supervised learning systems.\n","authors":[" Zia-ur-Rehman","Arif Mahmood","Wenxiong Kang"],"pdf_url":"https://arxiv.org/pdf/2410.14242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14240v1","updated":"2024-10-18T07:44:12Z","published":"2024-10-18T07:44:12Z","title":"Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in\n  Dynamical Systems Reconstruction","summary":"  Dynamical systems (DS) theory is fundamental for many areas of science and\nengineering. It can provide deep insights into the behavior of systems evolving\nin time, as typically described by differential or recursive equations. A\ncommon approach to facilitate mathematical tractability and interpretability of\nDS models involves decomposing nonlinear DS into multiple linear DS separated\nby switching manifolds, i.e. piecewise linear (PWL) systems. PWL models are\npopular in engineering and a frequent choice in mathematics for analyzing the\ntopological properties of DS. However, hand-crafting such models is tedious and\nonly possible for very low-dimensional scenarios, while inferring them from\ndata usually gives rise to unnecessarily complex representations with very many\nlinear subregions. Here we introduce Almost-Linear Recurrent Neural Networks\n(AL-RNNs) which automatically and robustly produce most parsimonious PWL\nrepresentations of DS from time series data, using as few PWL nonlinearities as\npossible. AL-RNNs can be efficiently trained with any SOTA algorithm for\ndynamical systems reconstruction (DSR), and naturally give rise to a symbolic\nencoding of the underlying DS that provably preserves important topological\nproperties. We show that for the Lorenz and R\\\"ossler systems, AL-RNNs\ndiscover, in a purely data-driven way, the known topologically minimal PWL\nrepresentations of the corresponding chaotic attractors. We further illustrate\non two challenging empirical datasets that interpretable symbolic encodings of\nthe dynamics can be achieved, tremendously facilitating mathematical and\ncomputational analysis of the underlying systems.\n","authors":["Manuel Brenner","Christoph Jürgen Hemmer","Zahra Monfared","Daniel Durstewitz"],"pdf_url":"https://arxiv.org/pdf/2410.14240v1.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)"},{"id":"http://arxiv.org/abs/2410.14237v1","updated":"2024-10-18T07:37:36Z","published":"2024-10-18T07:37:36Z","title":"Unified Convergence Analysis for Score-Based Diffusion Models with\n  Deterministic Samplers","summary":"  Score-based diffusion models have emerged as powerful techniques for\ngenerating samples from high-dimensional data distributions. These models\ninvolve a two-phase process: first, injecting noise to transform the data\ndistribution into a known prior distribution, and second, sampling to recover\nthe original data distribution from noise. Among the various sampling methods,\ndeterministic samplers stand out for their enhanced efficiency. However,\nanalyzing these deterministic samplers presents unique challenges, as they\npreclude the use of established techniques such as Girsanov's theorem, which\nare only applicable to stochastic samplers. Furthermore, existing analysis for\ndeterministic samplers usually focuses on specific examples, lacking a\ngeneralized approach for general forward processes and various deterministic\nsamplers. Our paper addresses these limitations by introducing a unified\nconvergence analysis framework. To demonstrate the power of our framework, we\nanalyze the variance-preserving (VP) forward process with the exponential\nintegrator (EI) scheme, achieving iteration complexity of $\\tilde\nO(d^2/\\epsilon)$. Additionally, we provide a detailed analysis of Denoising\nDiffusion Implicit Models (DDIM)-type samplers, which have been underexplored\nin previous research, achieving polynomial iteration complexity.\n","authors":["Runjia Li","Qiwei Di","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2410.14237v1.pdf","comment":"68 pages"},{"id":"http://arxiv.org/abs/2305.01661v2","updated":"2024-10-18T07:15:51Z","published":"2023-05-02T08:28:55Z","title":"Integrating spoken instructions into flight trajectory prediction to\n  optimize automation in air traffic control","summary":"  The booming air transportation industry inevitably burdens air traffic\ncontrollers' workload, causing unexpected human factor-related incidents.\nCurrent air traffic control systems fail to consider spoken instructions for\ntraffic prediction, bringing significant challenges in detecting human errors\nduring real-time traffic operations. Here, we present an automation paradigm\nintegrating controlling intent into the information processing loop through the\nspoken instruction-aware flight trajectory prediction framework. A 3-stage\nprogressive multi-modal learning paradigm is proposed to address the modality\ngap between the trajectory and spoken instructions, as well as minimize the\ndata requirements. Experiments on a real-world dataset show the proposed\nframework achieves flight trajectory prediction with high predictability and\ntimeliness, obtaining over 20% relative reduction in mean deviation error.\nMoreover, the generalizability of the proposed framework is also confirmed by\nvarious model architectures. The proposed framework can formulate\nfull-automated information processing in real-world air traffic applications,\nsupporting human error detection and enhancing aviation safety.\n","authors":["Dongyue Guo","Zheng Zhang","Bo Yang","Jianwei Zhang","Hongyu Yang","Yi Lin"],"pdf_url":"https://arxiv.org/pdf/2305.01661v2.pdf","comment":"This paper has been accepted in principle by Nature Communications"},{"id":"http://arxiv.org/abs/2410.14223v1","updated":"2024-10-18T07:14:08Z","published":"2024-10-18T07:14:08Z","title":"G-NeuroDAVIS: A Neural Network model for generalized embedding, data\n  visualization and sample generation","summary":"  Visualizing high-dimensional datasets through a generalized embedding has\nbeen a challenge for a long time. Several methods have shown up for the same,\nbut still, they have not been able to generate a generalized embedding, which\nnot only can reveal the hidden patterns present in the data but also generate\nrealistic high-dimensional samples from it. Motivated by this aspect, in this\nstudy, a novel generative model, called G-NeuroDAVIS, has been developed, which\nis capable of visualizing high-dimensional data through a generalized\nembedding, and thereby generating new samples. The model leverages advanced\ngenerative techniques to produce high-quality embedding that captures the\nunderlying structure of the data more effectively than existing methods.\nG-NeuroDAVIS can be trained in both supervised and unsupervised settings. We\nrigorously evaluated our model through a series of experiments, demonstrating\nsuperior performance in classification tasks, which highlights the robustness\nof the learned representations. Furthermore, the conditional sample generation\ncapability of the model has been described through qualitative assessments,\nrevealing a marked improvement in generating realistic and diverse samples.\nG-NeuroDAVIS has outperformed the Variational Autoencoder (VAE) significantly\nin multiple key aspects, including embedding quality, classification\nperformance, and sample generation capability. These results underscore the\npotential of our generative model to serve as a powerful tool in various\napplications requiring high-quality data generation and representation\nlearning.\n","authors":["Chayan Maitra","Rajat K. De"],"pdf_url":"https://arxiv.org/pdf/2410.14223v1.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2409.09785v3","updated":"2024-10-18T07:11:35Z","published":"2024-09-15T16:32:49Z","title":"Large Language Model Based Generative Error Correction: A Challenge and\n  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition","summary":"  Given recent advances in generative AI technology, a key question is how\nlarge language models (LLMs) can enhance acoustic modeling tasks using text\ndecoding results from a frozen, pretrained automatic speech recognition (ASR)\nmodel. To explore new capabilities in language modeling for speech processing,\nwe introduce the generative speech transcription error correction (GenSEC)\nchallenge. This challenge comprises three post-ASR language modeling tasks: (i)\npost-ASR transcription correction, (ii) speaker tagging, and (iii) emotion\nrecognition. These tasks aim to emulate future LLM-based agents handling\nvoice-based interfaces while remaining accessible to a broad audience by\nutilizing open pretrained language models or agent-based APIs. We also discuss\ninsights from baseline evaluations, as well as lessons learned for designing\nfuture evaluations.\n","authors":["Chao-Han Huck Yang","Taejin Park","Yuan Gong","Yuanchao Li","Zhehuai Chen","Yen-Ting Lin","Chen Chen","Yuchen Hu","Kunal Dhawan","Piotr Żelasko","Chao Zhang","Yun-Nung Chen","Yu Tsao","Jagadeesh Balam","Boris Ginsburg","Sabato Marco Siniscalchi","Eng Siong Chng","Peter Bell","Catherine Lai","Shinji Watanabe","Andreas Stolcke"],"pdf_url":"https://arxiv.org/pdf/2409.09785v3.pdf","comment":"IEEE SLT 2024. The initial draft version has been done in December\n  2023. Post-ASR Text Processing and Understanding Community and LlaMA-7B\n  pre-training correction model:\n  https://huggingface.co/GenSEC-LLM/SLT-Task1-Llama2-7b-HyPo-baseline"},{"id":"http://arxiv.org/abs/2410.14219v1","updated":"2024-10-18T07:08:31Z","published":"2024-10-18T07:08:31Z","title":"Formal Explanations for Neuro-Symbolic AI","summary":"  Despite the practical success of Artificial Intelligence (AI), current neural\nAI algorithms face two significant issues. First, the decisions made by neural\narchitectures are often prone to bias and brittleness. Second, when a chain of\nreasoning is required, neural systems often perform poorly. Neuro-symbolic\nartificial intelligence is a promising approach that tackles these (and other)\nweaknesses by combining the power of neural perception and symbolic reasoning.\nMeanwhile, the success of AI has made it critical to understand its behaviour,\nleading to the development of explainable artificial intelligence (XAI). While\nneuro-symbolic AI systems have important advantages over purely neural AI, we\nstill need to explain their actions, which are obscured by the interactions of\nthe neural and symbolic components. To address the issue, this paper proposes a\nformal approach to explaining the decisions of neuro-symbolic systems. The\napproach hinges on the use of formal abductive explanations and on solving the\nneuro-symbolic explainability problem hierarchically. Namely, it first computes\na formal explanation for the symbolic component of the system, which serves to\nidentify a subset of the individual parts of neural information that needs to\nbe explained. This is followed by explaining only those individual neural\ninputs, independently of each other, which facilitates succinctness of\nhierarchical formal explanations and helps to increase the overall performance\nof the approach. Experimental results for a few complex reasoning tasks\ndemonstrate practical efficiency of the proposed approach, in comparison to\npurely neural systems, from the perspective of explanation size, explanation\ntime, training time, model sizes, and the quality of explanations reported.\n","authors":["Sushmita Paul","Jinqiang Yu","Jip J. Dekker","Alexey Ignatiev","Peter J. Stuckey"],"pdf_url":"https://arxiv.org/pdf/2410.14219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04808v3","updated":"2024-10-18T07:05:57Z","published":"2024-03-06T10:55:30Z","title":"WaterMax: breaking the LLM watermark detectability-robustness-quality\n  trade-off","summary":"  Watermarking is a technical means to dissuade malfeasant usage of Large\nLanguage Models. This paper proposes a novel watermarking scheme, so-called\nWaterMax, that enjoys high detectability while sustaining the quality of the\ngenerated text of the original LLM. Its new design leaves the LLM untouched (no\nmodification of the weights, logits, temperature, or sampling technique).\nWaterMax balances robustness and complexity contrary to the watermarking\ntechniques of the literature inherently provoking a trade-off between quality\nand robustness. Its performance is both theoretically proven and experimentally\nvalidated. It outperforms all the SotA techniques under the most complete\nbenchmark suite. Code available at https://github.com/eva-giboulot/WaterMax.\n","authors":["Eva Giboulot","Teddy Furon"],"pdf_url":"https://arxiv.org/pdf/2403.04808v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13602v2","updated":"2024-10-18T07:04:25Z","published":"2024-10-17T14:36:58Z","title":"Towards Satellite Non-IID Imagery: A Spectral Clustering-Assisted\n  Federated Learning Approach","summary":"  Low Earth orbit (LEO) satellites are capable of gathering abundant Earth\nobservation data (EOD) to enable different Internet of Things (IoT)\napplications. However, to accomplish an effective EOD processing mechanism, it\nis imperative to investigate: 1) the challenge of processing the observed data\nwithout transmitting those large-size data to the ground because the connection\nbetween the satellites and the ground stations is intermittent, and 2) the\nchallenge of processing the non-independent and identically distributed\n(non-IID) satellite data. In this paper, to cope with those challenges, we\npropose an orbit-based spectral clustering-assisted clustered federated\nself-knowledge distillation (OSC-FSKD) approach for each orbit of an LEO\nsatellite constellation, which retains the advantage of FL that the observed\ndata does not need to be sent to the ground. Specifically, we introduce\nnormalized Laplacian-based spectral clustering (NLSC) into federated learning\n(FL) to create clustered FL in each round to address the challenge resulting\nfrom non-IID data. Particularly, NLSC is adopted to dynamically group clients\ninto several clusters based on cosine similarities calculated by model updates.\nIn addition, self-knowledge distillation is utilized to construct each local\nclient, where the most recent updated local model is used to guide current\nlocal model training. Experiments demonstrate that the observation accuracy\nobtained by the proposed method is separately 1.01x, 2.15x, 1.10x, and 1.03x\nhigher than that of pFedSD, FedProx, FedAU, and FedALA approaches using the\nSAT4 dataset. The proposed method also shows superiority when using other\ndatasets.\n","authors":["Luyao Zou","Yu Min Park","Chu Myaet Thwal","Yan Kyaw Tun","Zhu Han","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2410.13602v2.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.14212v1","updated":"2024-10-18T07:01:56Z","published":"2024-10-18T07:01:56Z","title":"Comparative Evaluation of Clustered Federated Learning Method","summary":"  Over recent years, Federated Learning (FL) has proven to be one of the most\npromising methods of distributed learning which preserves data privacy. As the\nmethod evolved and was confronted to various real-world scenarios, new\nchallenges have emerged. One such challenge is the presence of highly\nheterogeneous (often referred as non-IID) data distributions among participants\nof the FL protocol. A popular solution to this hurdle is Clustered Federated\nLearning (CFL), which aims to partition clients into groups where the\ndistribution are homogeneous. In the literature, state-of-the-art CFL\nalgorithms are often tested using a few cases of data heterogeneities, without\nsystematically justifying the choices. Further, the taxonomy used for\ndifferentiating the different heterogeneity scenarios is not always\nstraightforward. In this paper, we explore the performance of two\nstate-of-theart CFL algorithms with respect to a proposed taxonomy of data\nheterogeneities in federated learning (FL). We work with three image\nclassification datasets and analyze the resulting clusters against the\nheterogeneity classes using extrinsic clustering metrics. Our objective is to\nprovide a clearer understanding of the relationship between CFL performances\nand data heterogeneity scenarios.\n","authors":["Michael Ben Ali","Omar El-Rifai","Imen Megdiche","André Peninou","Olivier Teste"],"pdf_url":"https://arxiv.org/pdf/2410.14212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16435v2","updated":"2024-10-18T06:56:10Z","published":"2024-05-26T05:22:38Z","title":"Node Identifiers: Compact, Discrete Representations for Efficient Graph\n  Learning","summary":"  We present a novel end-to-end framework that generates highly compact\n(typically 6-15 dimensions), discrete (int4 type), and interpretable node\nrepresentations, termed node identifiers (node IDs), to tackle inference\nchallenges on large-scale graphs. By employing vector quantization, we compress\ncontinuous node embeddings from multiple layers of a Graph Neural Network (GNN)\ninto discrete codes, applicable under both self-supervised and supervised\nlearning paradigms. These node IDs capture high-level abstractions of graph\ndata and offer interpretability that traditional GNN embeddings lack. Extensive\nexperiments on 34 datasets, encompassing node classification, graph\nclassification, link prediction, and attributed graph clustering tasks,\ndemonstrate that the generated node IDs significantly enhance speed and memory\nefficiency while achieving competitive performance compared to current\nstate-of-the-art methods.\n","authors":["Yuankai Luo","Hongkang Li","Qijiong Liu","Lei Shi","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2405.16435v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14208v1","updated":"2024-10-18T06:50:15Z","published":"2024-10-18T06:50:15Z","title":"Montessori-Instruct: Generate Influential Training Data Tailored for\n  Student Learning","summary":"  Synthetic data has been widely used to train large language models, but their\ngenerative nature inevitably introduces noisy, non-informative, and misleading\nlearning signals. In this paper, we propose Montessori-Instruct, a novel data\nsynthesis framework that tailors the data synthesis ability of the teacher\nlanguage model toward the student language model's learning process.\nSpecifically, we utilize local data influence of synthetic training data points\non students to characterize students' learning preferences. Then, we train the\nteacher model with Direct Preference Optimization (DPO) to generate synthetic\ndata tailored toward student learning preferences. Experiments with\nLlama3-8B-Instruct (teacher) and Llama3-8B (student) on Alpaca Eval and\nMT-Bench demonstrate that Montessori-Instruct significantly outperforms\nstandard synthesis methods by 18.35\\% and 46.24\\% relatively. Our method also\nbeats data synthesized by a stronger teacher model, GPT-4o. Further analysis\nconfirms the benefits of teacher's learning to generate more influential\ntraining data in the student's improved learning, the advantages of local data\ninfluence in accurately measuring student preferences, and the robustness of\nMontessori-Instruct across different student models. Our code and data are\nopen-sourced at https://github.com/cxcscmu/Montessori-Instruct.\n","authors":["Xiaochuan Li","Zichun Yu","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.14208v1.pdf","comment":"Codes and data are open-sourced at\n  https://github.com/cxcscmu/Montessori-Instruct"},{"id":"http://arxiv.org/abs/2410.14207v1","updated":"2024-10-18T06:47:39Z","published":"2024-10-18T06:47:39Z","title":"Flexi-Fuzz least squares SVM for Alzheimer's diagnosis: Tackling noise,\n  outliers, and class imbalance","summary":"  Alzheimer's disease (AD) is a leading neurodegenerative condition and the\nprimary cause of dementia, characterized by progressive cognitive decline and\nmemory loss. Its progression, marked by shrinkage in the cerebral cortex, is\nirreversible. Numerous machine learning algorithms have been proposed for the\nearly diagnosis of AD. However, they often struggle with the issues of noise,\noutliers, and class imbalance. To tackle the aforementioned limitations, in\nthis article, we introduce a novel, robust, and flexible membership scheme\ncalled Flexi-Fuzz. This scheme integrates a novel flexible weighting mechanism,\nclass probability, and imbalance ratio. The proposed flexible weighting\nmechanism assigns the maximum weight to samples within a specific proximity to\nthe center, with a gradual decrease in weight beyond a certain threshold. This\napproach ensures that samples near the class boundary still receive significant\nweight, maintaining their influence in the classification process. Class\nprobability is used to mitigate the impact of noisy samples, while the\nimbalance ratio addresses class imbalance. Leveraging this, we incorporate the\nproposed Flexi-Fuzz membership scheme into the least squares support vector\nmachines (LSSVM) framework, resulting in a robust and flexible model termed\nFlexi-Fuzz-LSSVM. We determine the class-center using two methods: the\nconventional mean approach and an innovative median approach, leading to two\nmodel variants, Flexi-Fuzz-LSSVM-I and Flexi-Fuzz-LSSVM-II. To validate the\neffectiveness of the proposed Flexi-Fuzz-LSSVM models, we evaluated them on\nbenchmark UCI and KEEL datasets, both with and without label noise.\nAdditionally, we tested the models on the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) dataset for AD diagnosis. Experimental results demonstrate\nthe superiority of the Flexi-Fuzz-LSSVM models over baseline models.\n","authors":["Mushir Akhtar","A. Quadir","M. Tanveer","Mohd. Arshad"],"pdf_url":"https://arxiv.org/pdf/2410.14207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19619v2","updated":"2024-10-18T06:40:37Z","published":"2024-06-28T03:02:25Z","title":"ScoreFusion: fusing score-based generative models via Kullback-Leibler\n  barycenters","summary":"  We introduce ScoreFusion, a theoretically grounded method for fusing multiple\npre-trained diffusion models that are assumed to generate from auxiliary\npopulations. ScoreFusion is particularly useful for enhancing the generative\nmodeling of a target population with limited observed data. Our starting point\nconsiders the family of KL barycenters of the auxiliary populations, which is\nproven to be an optimal parametric class in the KL sense, but difficult to\nlearn. Nevertheless, by recasting the learning problem as score matching in\ndenoising diffusion, we obtain a tractable way of computing the optimal KL\nbarycenter weights. We prove a dimension-free sample complexity bound in total\nvariation distance, provided that the auxiliary models are well fitted for\ntheir own task and the auxiliary tasks combined capture the target well. We\nalso explain a connection of the practice of checkpoint merging in AI art\ncreation to an approximation of our KL-barycenter-based fusion approach.\nHowever, our fusion method differs in key aspects, allowing generation of new\npopulations, as we illustrate in experiments.\n","authors":["Hao Liu","Junze Tony Ye","Jose Blanchet","Nian Si"],"pdf_url":"https://arxiv.org/pdf/2406.19619v2.pdf","comment":"53 pages, 15 figures"},{"id":"http://arxiv.org/abs/2311.01483v5","updated":"2024-10-18T06:38:11Z","published":"2023-11-02T14:47:06Z","title":"FedSN: A Federated Learning Framework over Heterogeneous LEO Satellite\n  Networks","summary":"  Recently, a large number of Low Earth Orbit (LEO) satellites have been\nlaunched and deployed successfully in space by commercial companies, such as\nSpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve\nnot only for communication but also for various machine learning applications,\nsuch as space modulation recognition, remote sensing image classification, etc.\nHowever, the ground station (GS) may be incapable of downloading such a large\nvolume of raw sensing data for centralized model training due to the limited\ncontact time with LEO satellites (e.g. 5 minutes). Therefore, federated\nlearning (FL) has emerged as the promising solution to address this problem via\non-device training. Unfortunately, to enable FL on LEO satellites, we still\nface three critical challenges that are i) heterogeneous computing and memory\ncapabilities, ii) limited uplink rate, and iii) model staleness. To this end,\nwe propose FedSN as a general FL framework to tackle the above challenges, and\nfully explore data diversity on LEO satellites. Specifically, we first present\na novel sub-structure scheme to enable heterogeneous local model training\nconsidering different computing, memory, and communication constraints on LEO\nsatellites. Additionally, we propose a pseudo-synchronous model aggregation\nstrategy to dynamically schedule model aggregation for compensating model\nstaleness. To further demonstrate the effectiveness of the FedSN, we evaluate\nit using space modulation recognition and remote sensing image classification\ntasks by leveraging the data from real-world satellite networks. Extensive\nexperimental results demonstrate that FedSN framework achieves higher accuracy,\nlower computing, and communication overhead than the state-of-the-art\nbenchmarks and the effectiveness of each components in FedSN.\n","authors":["Zheng Lin","Zhe Chen","Zihan Fang","Xianhao Chen","Xiong Wang","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2311.01483v5.pdf","comment":"15 pages, 17 figures"},{"id":"http://arxiv.org/abs/2404.02175v2","updated":"2024-10-18T06:33:19Z","published":"2024-04-01T11:23:31Z","title":"Social Dynamics of Consumer Response: A Unified Framework Integrating\n  Statistical Physics and Marketing Dynamics","summary":"  Understanding how consumers react to advertising inputs is essential for\nmarketers aiming to optimize advertising strategies and improve campaign\neffectiveness. This study examines the complex nature of consumer behaviour by\napplying theoretical frameworks derived from physics and social psychology. We\npresent an innovative equation that captures the relation between spending on\nadvertising and consumer response, using concepts such as symmetries, scaling\nlaws, and phase transitions. By validating our equation against well-known\nmodels such as the Michaelis-Menten and Hill equations, we prove its\neffectiveness in accurately representing the complexity of consumer response\ndynamics. The analysis emphasizes the importance of key model parameters, such\nas marketing effectiveness, response sensitivity, and behavioural sensitivity,\nin influencing consumer behaviour. The work explores the practical implications\nfor advertisers and marketers, as well as discussing the limitations and future\nresearch directions. In summary, this study provides a thorough framework for\ncomprehending and forecasting consumer reactions to advertising, which has\nimplications for optimizing advertising strategies and allocating resources.\n","authors":["Javier Marin"],"pdf_url":"https://arxiv.org/pdf/2404.02175v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16922v2","updated":"2024-10-18T06:15:42Z","published":"2024-05-27T08:13:39Z","title":"Theories of synaptic memory consolidation and intelligent plasticity for\n  continual learning","summary":"  Humans and animals learn throughout life. Such continual learning is crucial\nfor intelligence. In this chapter, we examine the pivotal role plasticity\nmechanisms with complex internal synaptic dynamics could play in enabling this\nability in neural networks. By surveying theoretical research, we highlight two\nfundamental enablers for continual learning. First, synaptic plasticity\nmechanisms must maintain and evolve an internal state over several behaviorally\nrelevant timescales. Second, plasticity algorithms must leverage the internal\nstate to intelligently regulate plasticity at individual synapses to facilitate\nthe seamless integration of new memories while avoiding detrimental\ninterference with existing ones. Our chapter covers successful applications of\nthese principles to deep neural networks and underscores the significance of\nsynaptic metaplasticity in sustaining continual learning capabilities. Finally,\nwe outline avenues for further research to understand the brain's superb\ncontinual learning abilities and harness similar mechanisms for artificial\nintelligence systems.\n","authors":["Friedemann Zenke","Axel Laborieux"],"pdf_url":"https://arxiv.org/pdf/2405.16922v2.pdf","comment":"An introductory-level book chapter. 35 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.14193v1","updated":"2024-10-18T06:07:22Z","published":"2024-10-18T06:07:22Z","title":"xPerT: Extended Persistence Transformer","summary":"  A persistence diagram provides a compact summary of persistent homology,\nwhich captures the topological features of a space at different scales.\nHowever, due to its nature as a set, incorporating it as a feature into a\nmachine learning framework is challenging. Several methods have been proposed\nto use persistence diagrams as input for machine learning models, but they\noften require complex preprocessing steps and extensive hyperparameter tuning.\nIn this paper, we propose a novel transformer architecture called the\n\\textit{Extended Persistence Transformer (xPerT)}, which is highly scalable\nthan the compared to Persformer, an existing transformer for persistence\ndiagrams. xPerT reduces GPU memory usage by over 90\\% and improves accuracy on\nmultiple datasets. Additionally, xPerT does not require complex preprocessing\nsteps or extensive hyperparameter tuning, making it easy to use in practice.\nOur code is available at https://github.com/sehunfromdaegu/ECG_JEPA.\n","authors":["Sehun Kim"],"pdf_url":"https://arxiv.org/pdf/2410.14193v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.14634v1","updated":"2024-10-18T17:35:33Z","published":"2024-10-18T17:35:33Z","title":"Parallel Backpropagation for Inverse of a Convolution with Application\n  to Normalizing Flows","summary":"  Inverse of an invertible convolution is an important operation that comes up\nin Normalizing Flows, Image Deblurring, etc. The naive algorithm for\nbackpropagation of this operation using Gaussian elimination has running time\n$O(n^3)$ where $n$ is the number of pixels in the image. We give a fast\nparallel backpropagation algorithm with running time $O(\\sqrt{n})$ for a square\nimage and provide a GPU implementation of the same. Inverse Convolutions are\nusually used in Normalizing Flows in the sampling pass, making them slow. We\npropose to use Inverse Convolutions in the forward (image to latent vector)\npass of the Normalizing flow. Since the sampling pass is the inverse of the\nforward pass, it will use convolutions only, resulting in efficient sampling\ntimes. We use our parallel backpropagation algorithm for optimizing the inverse\nconvolution layer resulting in fast training times also. We implement this\napproach in various Normalizing Flow backbones, resulting in our Inverse-Flow\nmodels. We benchmark Inverse-Flow on standard datasets and show significantly\nimproved sampling times with similar bits per dimension compared to previous\nmodels.\n","authors":["Sandeep Nagar","Girish Varma"],"pdf_url":"https://arxiv.org/pdf/2410.14634v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2404.13370v2","updated":"2024-10-18T16:44:05Z","published":"2024-04-20T13:15:27Z","title":"Movie101v2: Improved Movie Narration Benchmark","summary":"  Automatic movie narration aims to generate video-aligned plot descriptions to\nassist visually impaired audiences. Unlike standard video captioning, it\ninvolves not only describing key visual details but also inferring plots that\nunfold across multiple movie shots, presenting distinct and complex challenges.\nTo advance this field, we introduce Movie101v2, a large-scale, bilingual\ndataset with enhanced data quality specifically designed for movie narration.\nRevisiting the task, we propose breaking down the ultimate goal of automatic\nmovie narration into three progressive stages, offering a clear roadmap with\ncorresponding evaluation metrics. Based on our new benchmark, we baseline a\nrange of large vision-language models, including GPT-4V, and conduct an\nin-depth analysis of the challenges in narration generation. Our findings\nhighlight that achieving applicable movie narration generation is a fascinating\ngoal that requires significant research.\n","authors":["Zihao Yue","Yepeng Zhang","Ziheng Wang","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2404.13370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06729v2","updated":"2024-10-18T13:45:09Z","published":"2024-10-09T09:55:51Z","title":"Perceptual Quality Assessment of Octree-RAHT Encoded 3D Point Clouds","summary":"  No-reference bitstream-layer point cloud quality assessment (PCQA) can be\ndeployed without full decoding at any network node to achieve real-time quality\nmonitoring. In this work, we focus on the PCQA problem dedicated to Octree-RAHT\nencoding mode. First, to address the issue that existing PCQA databases have a\nsmall scale and limited distortion levels, we establish the WPC5.0 database\nwhich is the first one dedicated to Octree-RAHT encoding mode with a scale of\n400 distorted point clouds (PCs) including 4 geometric multiplied by 5 attitude\ndistortion levels. Then, we propose the first PCQA model dedicated to\nOctree-RAHT encoding mode by parsing PC bitstreams without full decoding. The\nmodel introduces texture bitrate (TBPP) to predict texture complexity (TC) and\nfurther derives the texture distortion factor. In addition, the Geometric\nQuantization Parameter (PQS) is used to estimate the geometric distortion\nfactor, which is then integrated into the model along with the texture\ndistortion factor to obtain the proposed PCQA model named streamPCQ-OR. The\nproposed model has been compared with other advanced PCQA methods on the\nWPC5.0, BASICS and M-PCCD databases, and experimental results show that our\nmodel has excellent performance while having very low computational complexity,\nproviding a reliable choice for time-critical applications. To facilitate\nsubsequent research, the database and source code will be publicly released at\nhttps://github.com/qdushl/Waterloo-Point-Cloud-Database-5.0.\n","authors":["Dongshuai Duan","Honglei Su","Qi Liu","Hui Yuan","Wei Gao","Jiarun Song","Zhou Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06729v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10291v2","updated":"2024-10-18T09:26:46Z","published":"2024-10-14T08:45:35Z","title":"Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal\n  Perspective","summary":"  Accurate interpretation and visualization of human instructions are crucial\nfor text-to-image (T2I) synthesis. However, current models struggle to capture\nsemantic variations from word order changes, and existing evaluations, relying\non indirect metrics like text-image similarity, fail to reliably assess these\nchallenges. This often obscures poor performance on complex or uncommon\nlinguistic patterns by the focus on frequent word combinations. To address\nthese deficiencies, we propose a novel metric called SemVarEffect and a\nbenchmark named SemVarBench, designed to evaluate the causality between\nsemantic variations in inputs and outputs in T2I synthesis. Semantic variations\nare achieved through two types of linguistic permutations, while avoiding\neasily predictable literal variations. Experiments reveal that the\nCogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.\nSemantic variations in object relations are less understood than attributes,\nscoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in\nUNet or Transformers plays a crucial role in handling semantic variations, a\nfactor previously overlooked by a focus on textual encoders. Our work\nestablishes an effective evaluation framework that advances the T2I synthesis\ncommunity's exploration of human instruction understanding. Our benchmark and\ncode are available at https://github.com/zhuxiangru/SemVarBench .\n","authors":["Xiangru Zhu","Penglei Sun","Yaoxian Song","Yanghua Xiao","Zhixu Li","Chengyu Wang","Jun Huang","Bei Yang","Xiaoxiao Xu"],"pdf_url":"https://arxiv.org/pdf/2410.10291v2.pdf","comment":"The only change in the current version update is the replacement of\n  the template with a more precise one"},{"id":"http://arxiv.org/abs/2410.13754v2","updated":"2024-10-18T08:56:52Z","published":"2024-10-17T16:52:28Z","title":"MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures","summary":"  Perceiving and generating diverse modalities are crucial for AI models to\neffectively learn from and engage with real-world signals, necessitating\nreliable evaluations for their development. We identify two major issues in\ncurrent evaluations: (1) inconsistent standards, shaped by different\ncommunities with varying protocols and maturity levels; and (2) significant\nquery, grading, and generalization biases. To address these, we introduce\nMixEval-X, the first any-to-any, real-world benchmark designed to optimize and\nstandardize evaluations across diverse input and output modalities. We propose\nmulti-modal benchmark mixture and adaptation-rectification pipelines to\nreconstruct real-world task distributions, ensuring evaluations generalize\neffectively to real-world use cases. Extensive meta-evaluations show our\napproach effectively aligns benchmark samples with real-world task\ndistributions. Meanwhile, MixEval-X's model rankings correlate strongly with\nthat of crowd-sourced real-world evaluations (up to 0.98) while being much more\nefficient. We provide comprehensive leaderboards to rerank existing models and\norganizations and offer insights to enhance understanding of multi-modal\nevaluations and inform future research.\n","authors":["Jinjie Ni","Yifan Song","Deepanway Ghosal","Bo Li","David Junhao Zhang","Xiang Yue","Fuzhao Xue","Zian Zheng","Kaichen Zhang","Mahir Shah","Kabir Jain","Yang You","Michael Shieh"],"pdf_url":"https://arxiv.org/pdf/2410.13754v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14154v1","updated":"2024-10-18T03:45:19Z","published":"2024-10-18T03:45:19Z","title":"RA-BLIP: Multimodal Adaptive Retrieval-Augmented Bootstrapping\n  Language-Image Pre-training","summary":"  Multimodal Large Language Models (MLLMs) have recently received substantial\ninterest, which shows their emerging potential as general-purpose models for\nvarious vision-language tasks. MLLMs involve significant external knowledge\nwithin their parameters; however, it is challenging to continually update these\nmodels with the latest knowledge, which involves huge computational costs and\npoor interpretability. Retrieval augmentation techniques have proven to be\neffective plugins for both LLMs and MLLMs. In this study, we propose multimodal\nadaptive Retrieval-Augmented Bootstrapping Language-Image Pre-training\n(RA-BLIP), a novel retrieval-augmented framework for various MLLMs. Considering\nthe redundant information within vision modality, we first leverage the\nquestion to instruct the extraction of visual information through interactions\nwith one set of learnable queries, minimizing irrelevant interference during\nretrieval and generation. Besides, we introduce a pre-trained multimodal\nadaptive fusion module to achieve question text-to-multimodal retrieval and\nintegration of multimodal knowledge by projecting visual and language\nmodalities into a unified semantic space. Furthermore, we present an Adaptive\nSelection Knowledge Generation (ASKG) strategy to train the generator to\nautonomously discern the relevance of retrieved knowledge, which realizes\nexcellent denoising performance. Extensive experiments on open multimodal\nquestion-answering datasets demonstrate that RA-BLIP achieves significant\nperformance and surpasses the state-of-the-art retrieval-augmented models.\n","authors":["Muhe Ding","Yang Ma","Pengda Qin","Jianlong Wu","Yuhong Li","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2410.14154v1.pdf","comment":"10 pages, 6 figures, Journal"},{"id":"http://arxiv.org/abs/2402.07640v3","updated":"2024-10-18T02:50:53Z","published":"2024-02-12T13:27:22Z","title":"Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image\n  Data","summary":"  The ability to generate sentiment-controlled feedback in response to\nmultimodal inputs comprising text and images addresses a critical gap in\nhuman-computer interaction. This capability allows systems to provide\nempathetic, accurate, and engaging responses, with useful applications in\neducation, healthcare, marketing, and customer service. To this end, we have\nconstructed a large-scale Controllable Multimodal Feedback Synthesis (CMFeed)\ndataset and propose a controllable feedback synthesis system. The system\nfeatures an encoder, decoder, and controllability block for textual and visual\ninputs. It extracts features using a transformer and Faster R-CNN networks,\ncombining them to generate feedback. The CMFeed dataset includes images, texts,\nreactions to the posts, human comments with relevance scores, and reactions to\nthese comments. These reactions train the model to produce feedback with\nspecified sentiments, achieving a sentiment classification accuracy of 77.23\\%,\nwhich is 18.82\\% higher than the accuracy without controllability. The system\nalso incorporates a similarity module for assessing feedback relevance through\nrank-based metrics and an interpretability technique to analyze the\ncontributions of textual and visual features during feedback generation. Access\nto the CMFeed dataset and the system's code is available at\nhttps://github.com/MIntelligence-Group/CMFeed.\n","authors":["Puneet Kumar","Sarthak Malik","Balasubramanian Raman","Xiaobai Li"],"pdf_url":"https://arxiv.org/pdf/2402.07640v3.pdf","comment":null}]}}